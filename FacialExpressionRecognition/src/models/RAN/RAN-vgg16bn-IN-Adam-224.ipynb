{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837d90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, json, math, time\n",
    "from typing import Union, List, Dict, cast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0929a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "\n",
    "training_size = 28317 + 3541\n",
    "\n",
    "__all__ = [\n",
    "    \"VGG\",\n",
    "    # \"vgg11\",\n",
    "    # \"vgg11_bn\",\n",
    "    # \"vgg13\",\n",
    "    # \"vgg13_bn\",\n",
    "    \"vgg16\",\n",
    "    \"vgg16_bn\",\n",
    "    # \"vgg19_bn\",\n",
    "    # \"vgg19\",\n",
    "]\n",
    "\n",
    "model_urls = {\n",
    "    \"vgg11\": \"https://download.pytorch.org/models/vgg11-8a719046.pth\",\n",
    "    \"vgg13\": \"https://download.pytorch.org/models/vgg13-19584684.pth\",\n",
    "    \"vgg16\": \"https://download.pytorch.org/models/vgg16-397923af.pth\",\n",
    "    \"vgg19\": \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\",\n",
    "    \"vgg11_bn\": \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\",\n",
    "    \"vgg13_bn\": \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\",\n",
    "    \"vgg16_bn\": \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\",\n",
    "    \"vgg19_bn\": \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\",\n",
    "}\n",
    "\n",
    "cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "    \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a248b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' \n",
    "if torch.cuda.device_count() > 0:  \n",
    "    print(\"Using %d GPU(s)\" % torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63d284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading & data processing\n",
    "\n",
    "def load_data(image_path, emotion_path, subset):\n",
    "    images = np.load(image_path)        # shape = (35393, 48, 48, 1)\n",
    "    images = images/255.0\n",
    "    images = np.float32(images)\n",
    "    emotions = np.load(emotion_path)    # shape = (35393, 8)\n",
    "    emotions = np.float32(emotions)\n",
    "    crops1 = images[:,:36,:36,:]        # shape = (35393, 36, 36, 1)\n",
    "    crops2 = images[:,:36,12:,:]        # shape = (35393, 36, 36, 1)\n",
    "    crops3 = images[:,12:,6:42,:]       # shape = (35393, 36, 36, 1)\n",
    "    crops4 = images[:,3:46,3:46,:]      # shape = (35393, 43, 43, 1)\n",
    "    crops5 = images[:,4:44,4:44,:]      # shape = (35393, 40, 40, 1)\n",
    "\n",
    "    if subset == 'train':\n",
    "        return images[:training_size], crops1[:training_size], crops2[:training_size], crops3[:training_size], crops4[:training_size], crops5[:training_size], emotions[:training_size]\n",
    "    if subset == 'test':\n",
    "        return images[training_size:], crops1[training_size:], crops2[training_size:], crops3[training_size:], crops4[training_size:], crops5[training_size:], emotions[training_size:]\n",
    "\n",
    "def fixed_crop(img, img_size):\n",
    "    img = torch.tensor(img)                             # (img_size, img_size, 1)\n",
    "    img = torch.reshape(img, (1, img_size, img_size))   # (1, img_size, img_size)\n",
    "    img = transforms.Resize([48, 48])(img)              # (1, 48, 48)\n",
    "    img = img.repeat(3, 1, 1)                           # (3, 48, 48)\n",
    "    \n",
    "    return img\n",
    "\n",
    "class FERPlusDataset(data.Dataset):\n",
    "    def __init__(self, image_path, emotion_path, subset):\n",
    "        assert(subset=='train' or subset=='test')\n",
    "        self.images, self.crops1, self.crops2, self.crops3, self.crops4, self.crops5, self.emotions = load_data(image_path, emotion_path, subset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        crop1 = self.crops1[index]\n",
    "        crop2 = self.crops2[index]\n",
    "        crop3 = self.crops3[index]\n",
    "        crop4 = self.crops4[index]\n",
    "        crop5 = self.crops5[index]\n",
    "        emotion = self.emotions[index]\n",
    "\n",
    "        image = fixed_crop(image, 48)\n",
    "        crop1 = fixed_crop(crop1, 36)\n",
    "        crop2 = fixed_crop(crop2, 36)\n",
    "        crop3 = fixed_crop(crop3, 36)\n",
    "        crop4 = fixed_crop(crop4, 43)\n",
    "        crop5 = fixed_crop(crop5, 40)\n",
    "\n",
    "        return image, crop1, crop2, crop3, crop4, crop5, emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f21caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.alpha = nn.Sequential(nn.Linear(512, 1),\n",
    "                                   nn.Sigmoid())\n",
    "        self.beta = nn.Sequential(nn.Linear(1024, 1),\n",
    "                                  nn.Sigmoid())\n",
    "        self.fc = nn.Sequential(nn.Linear(1024, 8),\n",
    "                                nn.LogSoftmax(dim=1))\n",
    "        \n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        vs = []\n",
    "        alphas = []\n",
    "        for i in range(6):\n",
    "            f = x[:,:,:,:,i] # x.shape = (32, 3, 48, 48, 6)\n",
    "            f = transforms.Resize([224, 224])(f)\n",
    "            f = self.features(f)\n",
    "            f = self.avgpool(f)\n",
    "            f = f.squeeze(3).squeeze(2)\n",
    "            vs.append(f)\n",
    "            alphas.append(self.alpha(f))\n",
    "        vs_stack = torch.stack(vs, dim=2)\n",
    "        alphas_stack = torch.stack(alphas, dim=2)\n",
    "        alphas_stack = F.softmax(alphas_stack, dim=2)\n",
    "        alphas_part_max = alphas_stack[:, :, 0:5].max(dim=2)[0]\n",
    "        alphas_org = alphas_stack[:, :, 0]\n",
    "        vm = vs_stack.mul(alphas_stack).sum(2).div(alphas_stack.sum(2))\n",
    "        for i in range(len(vs)):\n",
    "            vs[i] = torch.cat([vs[i], vm], dim=1)\n",
    "        vs_stack_4096 = torch.stack(vs, dim=2)\n",
    "\n",
    "        betas = []\n",
    "        for index, v in enumerate(vs):\n",
    "            betas.append(self.beta(v))\n",
    "        betas_stack = torch.stack(betas, dim=2)\n",
    "        betas_stack = F.softmax(betas_stack, dim=2)\n",
    "\n",
    "        output = vs_stack_4096.mul(betas_stack * alphas_stack).sum(2).div((betas_stack * alphas_stack).sum(2))\n",
    "        output = output.view(output.size(0), -1)\n",
    "        pred_score = self.fc(output)\n",
    "\n",
    "        return pred_score, alphas_part_max, alphas_org\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs[\"init_weights\"] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model_state_dict = model.state_dict()\n",
    "        for key in state_dict:\n",
    "            if not ((key == 'module.fc.weight') | (key == 'module.fc.bias')):\n",
    "                model_state_dict[key] = state_dict[key]\n",
    "        model.load_state_dict(model_state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    return _vgg(\"vgg16\", \"D\", False, pretrained, progress, **kwargs)\n",
    "\n",
    "    \n",
    "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
    "    return _vgg(\"vgg16_bn\", \"D\", True, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32b24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RBLoss, self).__init__()\n",
    "\n",
    "    def forward(self, alphas_part_max, alphas_org):\n",
    "        size = alphas_org.shape[0]\n",
    "        loss_wt = 0.0\n",
    "        for i in range(size):\n",
    "            loss_wt += max((torch.Tensor([0])).to(device), 0.02 - (alphas_part_max[i] - alphas_org[i]))\n",
    "        RBLoss = loss_wt / size\n",
    "        \n",
    "        return RBLoss\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    acc = 0\n",
    "    for i in range(batch_size):\n",
    "        true = target[i]\n",
    "        pred = output[i]\n",
    "        index_max = torch.argmax(pred)\n",
    "        if true[index_max] == torch.max(true):\n",
    "            acc += 1\n",
    "    acc = float(acc)/batch_size\n",
    "    return acc\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Adjust the learning rate of the optimizer\"\"\"\n",
    "    if epoch in [15, 30]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "            print('lr',param_group['lr'])\n",
    "\n",
    "class AverageMeter(object): \n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa59e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss1, loss2, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images, crops1, crops2, crops3, crops4, crops5, emotions) in enumerate(train_loader):\n",
    "        input = torch.zeros([images.shape[0], images.shape[1], images.shape[2], images.shape[3], 6])\n",
    "        input[:,:,:,:,0] = images.to(device)\n",
    "        input[:,:,:,:,1] = crops1.to(device)\n",
    "        input[:,:,:,:,2] = crops2.to(device)\n",
    "        input[:,:,:,:,3] = crops3.to(device)\n",
    "        input[:,:,:,:,4] = crops4.to(device)\n",
    "        input[:,:,:,:,5] = crops5.to(device)\n",
    "        target = emotions.to(device)\n",
    "\n",
    "        input_var = torch.autograd.Variable(input).to(device)\n",
    "        target_var = torch.autograd.Variable(target).to(device)\n",
    "        pred_score, alphas_part_max, alphas_org = model(input_var)\n",
    "\n",
    "        loss = loss1(pred_score, target_var) + loss2(alphas_part_max, alphas_org)\n",
    "        acc = accuracy(pred_score.data, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        accuracies.update(acc, input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print('\\r',\n",
    "              'Training [Epoch: {}/{} ({}/{})]: '\n",
    "              'Time {:.2f}s ({:.2f}s) '\n",
    "              'Loss {:.6f} ({:.6f}) '\n",
    "              'Accuracy {:.4f} ({:.4f})'\n",
    "              .format(epoch+1, epochs, i+1, len(train_loader), \n",
    "                      batch_time.val, batch_time.avg,\n",
    "                      losses.val, losses.avg,\n",
    "                      accuracies.val, accuracies.avg),\n",
    "              end='')\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def validate(val_loader, model, loss1, loss2):\n",
    "    with torch.no_grad():\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        accuracies = AverageMeter()\n",
    "\n",
    "        model.eval()\n",
    "        end = time.time()\n",
    "        print()\n",
    "        for i, (images, crops1, crops2, crops3, crops4, crops5, emotions) in enumerate(val_loader):\n",
    "            input = torch.zeros([images.shape[0], images.shape[1], images.shape[2], images.shape[3], 6])\n",
    "            input[:,:,:,:,0] = images.to(device).detach()\n",
    "            input[:,:,:,:,1] = crops1.to(device).detach()\n",
    "            input[:,:,:,:,2] = crops2.to(device).detach()\n",
    "            input[:,:,:,:,3] = crops3.to(device).detach()\n",
    "            input[:,:,:,:,4] = crops4.to(device).detach()\n",
    "            input[:,:,:,:,5] = crops5.to(device).detach()\n",
    "            target = emotions.to(device).detach()\n",
    "\n",
    "            input_var = torch.autograd.Variable(input).to(device).detach()\n",
    "            target_var = torch.autograd.Variable(target).to(device).detach()\n",
    "            pred_score, alphas_part_max, alphas_org = model(input_var)\n",
    "\n",
    "            loss = loss1(pred_score, target_var) + loss2(alphas_part_max, alphas_org)\n",
    "            acc = accuracy(pred_score.data, target)\n",
    "            losses.update(loss.data[0], input.size(0))\n",
    "            accuracies.update(acc, input.size(0))\n",
    "            \n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print('\\r',\n",
    "                  'Test [Batch: {}/{}]: '\n",
    "                  'Time {:.2f}s ({:.2f}s) '\n",
    "                  'Loss {:.6f} ({:.6f}) '\n",
    "                  'Accuracy {:.4f} ({:.4f})'\n",
    "                  .format(i+1, len(val_loader), \n",
    "                          batch_time.val, batch_time.avg,\n",
    "                          losses.val, losses.avg,\n",
    "                          accuracies.val, accuracies.avg),\n",
    "                  end='')\n",
    "            torch.cuda.empty_cache()\n",
    "        print(' *** Test Accuracy {:.4f} ***'.format(accuracies.avg))\n",
    "              \n",
    "        return accuracies.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57239ef8-298f-4181-83e9-0319694e895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e45c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size_train = 16\n",
    "batch_size_test = 16\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "image_path = '../dataset/aligned_images.npy'\n",
    "emotion_path = '../dataset/emotions_multi.npy'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        emotion_path,\n",
    "        'train'\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        emotion_path,\n",
    "        'test'\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6094a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16_bn(pretrained=True)\n",
    "model.apply(inplace_relu)\n",
    "if torch.cuda.device_count() > 1: \n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "#summary(model,(3,48,48,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4286b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = nn.KLDivLoss(reduction='batchmean')\n",
    "loss1 = loss1.to(device)\n",
    "loss2 = RBLoss() \n",
    "loss2 = loss2.to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training [Epoch: 1/40 (1992/1992)]: Time 1.00s (0.42s) Loss 1.446051 (0.501939) Accuracy 0.5000 (0.7495)\n",
      " Test [Batch: 221/221]: Time 0.52s (0.17s) Loss 0.619732 (0.422211) Accuracy 0.4667 (0.7904) *** Test Accuracy 0.7904 ***\n",
      " Training [Epoch: 2/40 (1992/1992)]: Time 0.24s (0.42s) Loss 0.334198 (0.337904) Accuracy 0.5000 (0.8282)\n",
      " Test [Batch: 221/221]: Time 0.17s (0.17s) Loss 0.457853 (0.370668) Accuracy 0.8000 (0.8153) *** Test Accuracy 0.8153 ***\n",
      " Training [Epoch: 3/40 (69/1992)]: Time 0.41s (0.41s) Loss 0.305416 (0.284468) Accuracy 0.8750 (0.8533)"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for epoch in range(epochs):\n",
    "    #if epoch==0: torch.cuda.empty_cache()\n",
    "    #adjust_learning_rate(optimizer, epoch)\n",
    "    train(train_loader, model, loss1, loss2, optimizer, epoch)\n",
    "    accs.append(validate(val_loader, model, loss1, loss2))\n",
    "    with open('./results/vgg16-IN-Adam-224_bmean_freemem.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(accs, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb33845",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./results/'):\n",
    "    os.mkdir('./results/')\n",
    "with open('./results/vgg16-IN-Adam-224_bmean_freemem.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(accs, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
