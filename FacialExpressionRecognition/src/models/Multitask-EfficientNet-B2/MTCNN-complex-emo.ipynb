{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "Torch: 1.10.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn import svm,metrics,preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#from scipy.misc import imread, imresize\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from robust_optimization import RobustOptimizer\n",
    "import copy\n",
    "import timm\n",
    "import torch.utils.data as data\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "lr = 3e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "training_size = 28317 + 3541\n",
    "IMG_SIZE=260\n",
    "image_path = '../code/dataset/images.npy'\n",
    "training_emotion_path = \"../code/dataset/emotions_1.npy\"\n",
    "test_emotion_path = '../code/dataset/emotions_multi.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 35393\n",
    "training_size = 28317 + 3541\n",
    "emotions = np.load(test_emotion_path)\n",
    "test_subset_indices = []\n",
    "for i in range(sample_size):\n",
    "    if np.count_nonzero(emotions[i] > 0) >= 4:\n",
    "        test_subset_indices.append(i)\n",
    "test_subset_indices = random.sample(test_subset_indices, k=sample_size-training_size)\n",
    "train_subset_indices = [item for item in range(sample_size) if not item in test_subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_path, training_emotion_path, subset):\n",
    "    images = np.load(image_path)   \n",
    "    images = np.float32(images)\n",
    "    if subset == 'train':\n",
    "        training_emotions = np.load(training_emotion_path) \n",
    "        training_emotions = np.float32(training_emotions)\n",
    "        return images[train_subset_indices], training_emotions[train_subset_indices]\n",
    "    if subset == 'test':\n",
    "        test_emotions = np.load(test_emotion_path)\n",
    "        test_emotions = np.float32(test_emotions)\n",
    "        return images[test_subset_indices], test_emotions[test_subset_indices]\n",
    "\n",
    "def img_proc(img):\n",
    "    img = torch.tensor(img)                             # (48, 48, 1)\n",
    "    img = torch.reshape(img, (1, 48, 48))               # (1, 48, 48)\n",
    "    img = transforms.Resize([IMG_SIZE, IMG_SIZE])(img)  # (1, 260, 260)\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img = img.repeat(3, 1, 1)                           # (3, 260, 260)\n",
    "    return img\n",
    "\n",
    "class FERPlusDataset(data.Dataset):\n",
    "    def __init__(self, image_path, training_emotion_path, subset):\n",
    "        assert(subset=='train' or subset=='test')\n",
    "        self.images, self.emotions = load_data(image_path, training_emotion_path, subset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        emotion = self.emotions[index]\n",
    "\n",
    "        return img_proc(image), emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        training_emotion_path,\n",
    "        'train'\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        training_emotion_path,\n",
    "        'test'\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    acc = 0\n",
    "    for i in range(batch_size):\n",
    "        true = target[i]\n",
    "        pred = output[i]\n",
    "        index_max = torch.argmax(pred)\n",
    "        if true[index_max] == torch.max(true):\n",
    "            acc += 1\n",
    "    acc = float(acc)/batch_size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_epochs=epochs, learningrate=lr, robust=False):\n",
    "    # optimizer\n",
    "    if robust:\n",
    "        optimizer = RobustOptimizer(filter(lambda p: p.requires_grad, model.parameters()), optim.Adam, lr=learningrate)\n",
    "    else:\n",
    "        optimizer=optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learningrate)\n",
    "\n",
    "    best_acc=0\n",
    "    best_model=None\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        model.train()\n",
    "        for data, label in tqdm(train_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            if robust:\n",
    "                #optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "  \n",
    "                # second forward-backward pass\n",
    "                output = model(data)\n",
    "                loss = criterion(output, label)\n",
    "                loss.backward()\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == label.argmax(dim=1)).float().mean()\n",
    "            epoch_accuracy += acc / len(train_loader)\n",
    "            epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in val_loader:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "\n",
    "                acc = accuracy(val_output, label)\n",
    "                epoch_val_accuracy += acc / len(val_loader)\n",
    "                epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "        )\n",
    "        if best_acc<epoch_val_accuracy:\n",
    "            best_acc=epoch_val_accuracy\n",
    "            best_model=copy.deepcopy(model.state_dict())\n",
    "        #scheduler.step()\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        print(f\"Best acc:{best_acc}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in val_loader:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "\n",
    "                acc = accuracy(val_output, label)\n",
    "                epoch_val_accuracy += acc / len(val_loader)\n",
    "                epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No best model Best acc:{best_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, requires_grad):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device) \n",
    "model=timm.create_model('tf_efficientnet_b0_ns', pretrained=False)\n",
    "model.classifier=torch.nn.Identity()\n",
    "model.load_state_dict(torch.load('state_vggface2_enet0_new.pt'))\n",
    "\n",
    "model.classifier=nn.Sequential(nn.Linear(in_features=1280, out_features=8))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9eb09e3e8c4087bdf2ab28943ce00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 - loss : 0.0620 - acc: 0.7173 - val_loss : 0.0299 - val_acc: 0.7647\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427963b866384377b37947f08c88eb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - loss : 0.0562 - acc: 0.7544 - val_loss : 0.0282 - val_acc: 0.7771\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab011fa4997e427da84f81af862120bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - loss : 0.0552 - acc: 0.7595 - val_loss : 0.0351 - val_acc: 0.7516\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4839fbf9a5484782dbab8a24b96c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - loss : 0.0547 - acc: 0.7652 - val_loss : 0.0311 - val_acc: 0.7732\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c332b255834edd9b22d43be6fbc141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - loss : 0.0538 - acc: 0.7685 - val_loss : 0.0304 - val_acc: 0.7842\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e0b3dbb03f41809d16ff55ba41c641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - loss : 0.0545 - acc: 0.7675 - val_loss : 0.0288 - val_acc: 0.7807\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eb0e2006f943fdb45dd63004fc8b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 - loss : 0.0544 - acc: 0.7666 - val_loss : 0.0307 - val_acc: 0.7807\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca70b901b8f54a078b09d39e9e3031a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 - loss : 0.0537 - acc: 0.7701 - val_loss : 0.0337 - val_acc: 0.7667\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16bdd55294b4860a024adc10b5a07e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 - loss : 0.0536 - acc: 0.7712 - val_loss : 0.0344 - val_acc: 0.7519\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f2774446484460b19a94ed383cf0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 - loss : 0.0541 - acc: 0.7691 - val_loss : 0.0286 - val_acc: 0.7809\n",
      "\n",
      "Best acc:0.7842342342342346\n",
      "val_loss : 0.0304 - val_acc: 0.7842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_parameter_requires_grad(model, requires_grad=False)\n",
    "set_parameter_requires_grad(model.classifier, requires_grad=True)\n",
    "train(model, 10, 1e-3, robust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(model, requires_grad=True)\n",
    "train(model, 30, 1e-4, robust=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH='enet_8859.pt'\n",
    "# torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load\n",
    "# print(PATH)\n",
    "# model = torch.load(PATH)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20284889676a59d6a6b8edb213666d2e02cd0c9d2ca44919af197643ac84589d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
