{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json, time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "training_size = 28317 + 3541    # training samples\n",
    "image_size = 48                 # image size\n",
    "H, W = image_size, image_size   # image height, image weight\n",
    "h, w = 3, 3     # number of grids\n",
    "C = 3           # number of channels\n",
    "D = 128         # token dimension\n",
    "K = 8           # number of output classes\n",
    "L = 12          # VTA depth\n",
    "N = 3           # number of tokens generated except Tcls\n",
    "heads = 8       # heads of MHSA\n",
    "dim_head = 64   # dimension of the project key vector\n",
    "dropout = 0.2   # dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' \n",
    "if torch.cuda.device_count() > 0:  \n",
    "    print(\"Using %d GPU(s)\" % torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading & data processing\n",
    "\n",
    "def load_data(image_path, emotion_path, subset):\n",
    "    images = np.load(image_path)        \n",
    "    images = images/255.0\n",
    "    images = np.float32(images)         # shape = (35393, 48, 48, 1)\n",
    "\n",
    "    emotions = np.load(emotion_path)    \n",
    "    emotions = np.float32(emotions)     # shape = (35393, 8)\n",
    "\n",
    "    if subset == 'train':\n",
    "        return images[:training_size], emotions[:training_size]\n",
    "    if subset == 'test':\n",
    "        return images[training_size:], emotions[training_size:]\n",
    "\n",
    "class FERPlusDataset(data.Dataset):\n",
    "    def __init__(self, image_path, emotion_path, subset):\n",
    "        assert(subset=='train' or subset=='test')\n",
    "        self.images, self.emotions = load_data(image_path, emotion_path, subset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        image = self.resize(image)\n",
    "        emotion = self.emotions[index]\n",
    "        return image, emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def resize(self, img):\n",
    "        img = torch.tensor(img)                     # (48, 48, 1)\n",
    "        img = torch.reshape(img, (1, 48, 48))       # (1, 48, 48)\n",
    "        if image_size != 48:\n",
    "            img = transforms.Resize([H, W])(img)    # (1, H, W)\n",
    "        img = img.repeat(C, 1, 1)                   # (3, H, W)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Transformer architecture\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads==1 and dim_head==dim)\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim*3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FER-VT architecture\n",
    "\n",
    "class FER_VT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FER_VT, self).__init__()\n",
    "        self.expander = torch.ones((int(H/h), int(W/w)), device=device)\n",
    "        self.LFN = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 64,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv2d(\n",
    "                in_channels = 64,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.FT1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.FT2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.RFN_lower = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                stride = 1,\n",
    "                bias = True\n",
    "            ),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                stride = 1,\n",
    "                bias = True\n",
    "            ),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.RFN_upper = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 3,\n",
    "                out_channels = 3,\n",
    "                kernel_size = 3,\n",
    "                padding = 1,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.activation = {}\n",
    "        self.backbone = models.resnet18()\n",
    "        self.backbone.layer2.register_forward_hook(self.get_activation('out3'))\n",
    "        self.backbone.layer3.register_forward_hook(self.get_activation('out2'))\n",
    "        self.backbone.layer4.register_forward_hook(self.get_activation('out1'))\n",
    "\n",
    "        self.PTF1 = nn.Sequential(  # (32, 512, 2, 2) -> (32, 512, 2, 2)\n",
    "            nn.Conv2d(  \n",
    "                in_channels = 512,\n",
    "                out_channels = 512,\n",
    "                kernel_size = 1,\n",
    "                padding = 0,\n",
    "                stride = 1,\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.PTF2 = nn.Sequential(  # (32, 256, 3, 3) -> (32, 512, 2, 2)\n",
    "            nn.Conv2d(  \n",
    "                in_channels = 256,\n",
    "                out_channels = 512,\n",
    "                kernel_size = 2,\n",
    "                padding = 0,\n",
    "                stride = 1,         # 48: 1; 222: 2\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.PTF3 = nn.Sequential(  # (32, 128, 6, 6) -> (32, 512, 2, 2)\n",
    "            nn.Conv2d(  \n",
    "                in_channels = 128,  \n",
    "                out_channels = 512, \n",
    "                kernel_size = 3,    # 48: 3; 222: 4\n",
    "                padding = 0,        # 48: 0; 222: 1\n",
    "                stride = 3,         # 48: 3; 222: 4\n",
    "                bias = False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "        self.token_embed = nn.Sequential(\n",
    "            nn.LayerNorm(512*2*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512*2*2, D)\n",
    "        )\n",
    "        self.Tcls = nn.Parameter(torch.randn((1, 1, D), requires_grad=True, device=device))\n",
    "        self.Epos = nn.Parameter(torch.randn((1, 1, (N+1)*D), requires_grad=True, device=device))\n",
    "        self.transformer = Transformer(dim=D, depth=L, heads=heads, dim_head=dim_head, mlp_dim=2*D)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(D),\n",
    "            nn.Linear(D, K),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # model weight initialization\n",
    "        self.apply(self.init_weights)\n",
    "        self.RFN_lower.apply(self.init_weights_RFN)\n",
    "        self.RFN_upper.apply(self.init_weights_RFN)\n",
    "        for ViT_module in [self.transformer, self.mlp_head]:\n",
    "            for name, m in ViT_module.named_modules():\n",
    "                self.init_weights_ViT(m, name)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight.data, 1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "    \n",
    "    def init_weights_RFN(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight.data, gain=0.1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight.data, 1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def init_weights_ViT(self, m, name):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.constant_(m.weight.data, 0)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "            # if 'qkv' in name:\n",
    "            #     val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n",
    "            #     nn.init.uniform_(m.weight, -val, val)\n",
    "            # else:\n",
    "            #     nn.init.xavier_uniform_(m.weight)\n",
    "            # if m.bias is not None:\n",
    "            #     nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activation[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):   # x.shape = (B, C, H, W)  \n",
    "        # 1. Grid-Wise Attention\n",
    "        # 1.1. Local Feature Extraction\n",
    "        I = x               # I.shape = (B, C, H, W)\n",
    "        B = x.shape[0]\n",
    "        grids = []          # grids: hw x (B, C, H/h, W/w)\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                crop = I[:, :, int(i*(H/h)):int((i+1)*(H/h)), int(j*(W/w)):int((j+1)*(W/w))]   \n",
    "                grids.append(crop)              # crop.shape = (B, C, H/h, W/w)   \n",
    "        for i in range(len(grids)):\n",
    "            grids[i] = self.LFN(grids[i])       # grids: hw x (B, C, H/h, W/w)\n",
    "        \n",
    "        # 1.2. Grid-Wise Attention Calculation\n",
    "        d_k = W/w\n",
    "        query = torch.zeros([h*w, B, C, int(H/h), int(W/w)], dtype=torch.float32, device=device)    \n",
    "        for i in range(h*w):                    # query.shape = (hw, B, C, H/h, W/w)\n",
    "            query[i] = grids[i]                 \n",
    "        key = torch.zeros([h*w, B, C, int(W/w), int(H/h)], dtype=torch.float32, device=device)      \n",
    "        for i in range(h*w):                    # key.shape = (hw, B, C, W/w, H/h)\n",
    "            key[i] = torch.transpose(grids[i], -1, -2)\n",
    "        scores = torch.matmul(query, key)/d_k   # scores.shape = (hw, B, C, H/h, H/h)\n",
    "        attn = nn.Softmax(dim=1)(scores)        # attn.shape = (hw, B, C, H/h, H/h)\n",
    "        I_hat = nn.AdaptiveAvgPool2d(1)(attn)   # I_hat.shape = (hw, B, C, 1, 1)\n",
    "        pattn = self.expander * I_hat           # pattn.shape = (hw, B, C, H/h, W/w)\n",
    "        I_tilde = torch.zeros([B, C, H, W], dtype=torch.float32, device=device) \n",
    "        for i in range(h):                      # I_tilde.shape = (B, C, H, W)\n",
    "            for j in range(w):\n",
    "                I_tilde[:,:,int(i*(H/h)):int((i+1)*(H/h)),int(j*(W/w)):int((j+1)*(W/w))] = pattn[i*w+j]\n",
    "        I_prime_tilde = I_tilde * I             # I_prime_tilde.shape = (B, C, H, W)\n",
    "\n",
    "        # 1.3. Residual Feature Fusion\n",
    "        RFN_lower_in = self.FT1(I) + self.FT2(I_prime_tilde)\n",
    "        RFN_lower_out = self.RFN_lower(RFN_lower_in)\n",
    "        I_bar = self.RFN_upper(RFN_lower_in + RFN_lower_out)    # I_bar.shape = (B, C, H, W)\n",
    "\n",
    "        # 2. Backbone Network (ResNet)\n",
    "        _ = self.backbone(I_bar)                    # I_bar.shape = (32, 3, 48, 48)\n",
    "        L3_double_prime = self.activation['out3']   # L3_double_prime.shape = (32, 128, 6, 6) 32 128 28 28\n",
    "        L2_double_prime = self.activation['out2']   # L2_double_prime.shape = (32, 256, 3, 3) 32 256 14 14\n",
    "        L1_double_prime = self.activation['out1']   # L1_double_prime.shape = (32, 512, 2, 2) 32 512 7 7\n",
    "        \n",
    "        # 3. Visual Transformer Attention\n",
    "        # 3.1. Visual Token Generation \n",
    "        # 3.1.1. Pyramid Feature Extraction\n",
    "        L1_prime = self.PTF1(L1_double_prime)   # L1_prime.shape = (32, 512, 2, 2)\n",
    "        L2_prime = self.PTF2(L2_double_prime)   # L2_prime.shape = (32, 512, 2, 2)\n",
    "        L3_prime = self.PTF3(L3_double_prime)   # L3_prime.shape = (32, 512, 2, 2)\n",
    "\n",
    "        # 3.1.2. Visual Token Embedding\n",
    "        C1H1W1 = L1_prime.shape[1] * L1_prime.shape[2] * L1_prime.shape[3]  # C1H1W1 = 2048\n",
    "        L1 = torch.reshape(L1_prime, (B, 1, C1H1W1))    # L1.shape = (32, 1, 2048)\n",
    "        L2 = torch.reshape(L2_prime, (B, 1, C1H1W1))    # L2.shape = (32, 1, 2048)\n",
    "        L3 = torch.reshape(L3_prime, (B, 1, C1H1W1))    # L3.shape = (32, 1, 2048)\n",
    "        T1 = self.token_embed(L1)   # T1.shape = (32, 1, 128)\n",
    "        T2 = self.token_embed(L2)   # T2.shape = (32, 1, 128)\n",
    "        T3 = self.token_embed(L3)   # T3.shape = (32, 1, 128)\n",
    "\n",
    "        # 3.2. Token-based Visual Transformer\n",
    "        Z0 = torch.concat([self.Tcls.repeat(B, 1, 1), T1, T2, T3], dim=1) + self.Epos.reshape(1, N+1, D).repeat(B, 1, 1)\n",
    "        ZL = self.transformer(Z0)               # Zi.shape = (32, 4, 128), i = 0, 1, ..., L\n",
    "        pred_scores = self.mlp_head(ZL[:,0,:])  # pred_scores.shape = (32, 8)\n",
    "        \n",
    "        return pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    acc = 0.0\n",
    "    for i in range(batch_size):\n",
    "        true = target[i]\n",
    "        pred = output[i]\n",
    "        index_max = torch.argmax(pred)\n",
    "        if true[index_max] == torch.max(true):\n",
    "            acc += 1.0\n",
    "    acc /= batch_size\n",
    "    return acc\n",
    "\n",
    "class AverageMeter(object): \n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_fn, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (images, emotions) in enumerate(train_loader):\n",
    "        input = images.to(device)\n",
    "        target = emotions.to(device)\n",
    "\n",
    "        input_var = (torch.autograd.Variable(input)).to(device)\n",
    "        target_var = (torch.autograd.Variable(target)).to(device)\n",
    "        pred_scores = model(input_var)\n",
    "\n",
    "        loss = loss_fn(pred_scores, target_var)\n",
    "        acc = accuracy(pred_scores.data, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        accuracies.update(acc, input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print('\\r',\n",
    "              'Training [Epoch: {}/{} ({}/{})]: '\n",
    "              'Time {:.2f}s ({:.2f}s) '\n",
    "              'Loss {:.6f} ({:.6f}) '\n",
    "              'Accuracy {:.4f} ({:.4f})'\n",
    "              .format(epoch+1, epochs, i+1, len(train_loader), \n",
    "                      batch_time.val, batch_time.avg,\n",
    "                      losses.val, losses.avg,\n",
    "                      accuracies.val, accuracies.avg),\n",
    "              end='')\n",
    "\n",
    "\n",
    "def validate(val_loader, model, loss_fn):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    print()\n",
    "\n",
    "    for i, (images, emotions) in enumerate(val_loader):\n",
    "        input = images.to(device).detach()\n",
    "        target = emotions.to(device).detach()\n",
    "\n",
    "        input_var = (torch.autograd.Variable(input)).to(device).detach()\n",
    "        target_var = (torch.autograd.Variable(target)).to(device).detach()\n",
    "        pred_scores = model(input_var)\n",
    "\n",
    "        loss = loss_fn(pred_scores, target_var)\n",
    "        acc = accuracy(pred_scores.data, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        accuracies.update(acc, input.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print('\\r',\n",
    "              'Test [Batch: {}/{}]: '\n",
    "              'Time {:.2f}s ({:.2f}s) '\n",
    "              'Loss {:.6f} ({:.6f}) '\n",
    "              'Accuracy {:.4f} ({:.4f})'\n",
    "              .format(i+1, len(val_loader), \n",
    "                      batch_time.val, batch_time.avg,\n",
    "                      losses.val, losses.avg,\n",
    "                      accuracies.val, accuracies.avg),\n",
    "              end='')\n",
    "\n",
    "    print(' *** Test Accuracy {:.4f} ***'.format(accuracies.avg))\n",
    "    return accuracies.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "batch_size_test = 32\n",
    "image_path = '../dataset/FERPlus/images.npy'\n",
    "emotion_path = '../dataset/FERPlus/emotions_multi.npy'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        emotion_path,\n",
    "        'train'\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    FERPlusDataset(\n",
    "        image_path,\n",
    "        emotion_path,\n",
    "        'test'\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FER_VT()\n",
    "if torch.cuda.device_count() > 1: \n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 0.1\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "loss_fn = loss_fn.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "accs = []\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, loss_fn, optimizer, epoch)\n",
    "    acc = validate(val_loader, model, loss_fn)\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./results/'):\n",
    "    os.mkdir('./results/')\n",
    "with open('./results/FER-VT.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(accs, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20284889676a59d6a6b8edb213666d2e02cd0c9d2ca44919af197643ac84589d"
  },
  "kernelspec": {
   "display_name": "kernel_for_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
