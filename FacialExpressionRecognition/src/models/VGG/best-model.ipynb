{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130967, 48, 48, 1)\n",
      "(130967, 8)\n",
      "(130967, 8)\n"
     ]
    }
   ],
   "source": [
    "# data augmentation test: rotate different degree (pay attention to adjustable filename etc.)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras import layers, Sequential,losses, metrics\n",
    "\n",
    "image_height = 48\n",
    "image_width = 48\n",
    "emotions_count = 8\n",
    "emotion_labels = ['neutral', 'happiness', 'surprise', 'sadness',\n",
    "                  'anger', 'disgust', 'fear', 'contempt']\n",
    "\n",
    "# !!! change sample size\n",
    "samples = 130967 # 2~130968\n",
    "training_samples = 28317 *4  # 2~113269 (Training)\n",
    "validation_samples = 3541 *4 # 113270~127433 (PublicTest)\n",
    "test_samples = 3535         # 127434~130968 (PrivateTest)\n",
    "\n",
    "# !!! change npy folder name\n",
    "image_path = \"./dataset3/images.npy\"\n",
    "emotion_multi_path = \"./dataset3/emotions_multi.npy\"\n",
    "emotion_single_path = \"./dataset3/emotions_single.npy\"\n",
    "images = np.load(image_path)\n",
    "emotions_multi = np.load(emotion_multi_path)\n",
    "emotions_single = np.load(emotion_single_path)\n",
    "\n",
    "# !!! change s/m dataset\n",
    "#emotions = emotions_single\n",
    "emotions = emotions_multi\n",
    "\n",
    "print(images.shape)\n",
    "print(emotions_multi.shape)\n",
    "print(emotions_single.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b74ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = losses.CategoricalCrossentropy()\n",
    "mse = losses.MeanSquaredError()\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "def model_acc(y_true, y_pred):\n",
    "    size = y_true.shape[0]\n",
    "    acc = 0\n",
    "    for i in range(size):\n",
    "        true = y_true[i]\n",
    "        pred = y_pred[i]           \n",
    "        index_max = tf.argmax(pred).numpy()\n",
    "        if true[index_max].numpy()==tf.reduce_max(true).numpy():\n",
    "            acc += 1\n",
    "    return acc/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0551b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_images shape: (127432, 48, 48, 1)\n",
      "training_emotions shape: (127432, 8)\n",
      "test_images shape: (3535, 48, 48, 1)\n",
      "test_emotions shape: (3535, 8)\n"
     ]
    }
   ],
   "source": [
    "images = tf.convert_to_tensor(images)\n",
    "\n",
    "emotions = tf.convert_to_tensor(emotions)\n",
    "\n",
    "images = layers.Rescaling(1./127.5, offset= -1)(images)\n",
    "\n",
    "training_size = training_samples + validation_samples\n",
    "test_size = test_samples\n",
    "\n",
    "training_images = images[:training_size]\n",
    "test_images = images[training_size:]\n",
    "training_emotions = emotions[:training_size]\n",
    "test_emotions = emotions[training_size:]\n",
    "\n",
    "print(\"training_images shape:\", training_images.shape)\n",
    "print(\"training_emotions shape:\", training_emotions.shape)\n",
    "print(\"test_images shape:\", test_images.shape)\n",
    "print(\"test_emotions shape:\", test_emotions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c4fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications import vgg16, resnet_v2\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.optimizer_v2 import adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1290bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darkl\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3703: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0243 - model_acc: 0.7277 - val_loss: 0.0165 - val_model_acc: 0.7985\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 2/40\n",
      "3983/3983 [==============================] - 263s 66ms/step - loss: 0.0148 - model_acc: 0.8192 - val_loss: 0.0155 - val_model_acc: 0.8143\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 3/40\n",
      "3983/3983 [==============================] - 260s 65ms/step - loss: 0.0116 - model_acc: 0.8531 - val_loss: 0.0133 - val_model_acc: 0.8366\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 4/40\n",
      "3983/3983 [==============================] - 259s 65ms/step - loss: 0.0093 - model_acc: 0.8797 - val_loss: 0.0123 - val_model_acc: 0.8397\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 5/40\n",
      "3983/3983 [==============================] - 258s 65ms/step - loss: 0.0077 - model_acc: 0.8982 - val_loss: 0.0123 - val_model_acc: 0.8467\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 6/40\n",
      "3983/3983 [==============================] - 258s 65ms/step - loss: 0.0065 - model_acc: 0.9155 - val_loss: 0.0118 - val_model_acc: 0.8467\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 7/40\n",
      "3983/3983 [==============================] - 258s 65ms/step - loss: 0.0055 - model_acc: 0.9277 - val_loss: 0.0119 - val_model_acc: 0.8479\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 8/40\n",
      "3983/3983 [==============================] - 259s 65ms/step - loss: 0.0048 - model_acc: 0.9373 - val_loss: 0.0116 - val_model_acc: 0.8535\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 9/40\n",
      "3983/3983 [==============================] - 261s 66ms/step - loss: 0.0042 - model_acc: 0.9446 - val_loss: 0.0116 - val_model_acc: 0.8473\n",
      "Epoch 10/40\n",
      "3983/3983 [==============================] - 261s 66ms/step - loss: 0.0038 - model_acc: 0.9498 - val_loss: 0.0118 - val_model_acc: 0.8510\n",
      "Epoch 11/40\n",
      "3983/3983 [==============================] - 265s 67ms/step - loss: 0.0034 - model_acc: 0.9556 - val_loss: 0.0116 - val_model_acc: 0.8529\n",
      "Epoch 12/40\n",
      "3983/3983 [==============================] - 267s 67ms/step - loss: 0.0030 - model_acc: 0.9583 - val_loss: 0.0115 - val_model_acc: 0.8572\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 13/40\n",
      "3983/3983 [==============================] - 265s 66ms/step - loss: 0.0028 - model_acc: 0.9636 - val_loss: 0.0115 - val_model_acc: 0.8527\n",
      "Epoch 14/40\n",
      "3983/3983 [==============================] - 271s 68ms/step - loss: 0.0026 - model_acc: 0.9647 - val_loss: 0.0113 - val_model_acc: 0.8561\n",
      "Epoch 15/40\n",
      "3983/3983 [==============================] - 267s 67ms/step - loss: 0.0023 - model_acc: 0.9685 - val_loss: 0.0118 - val_model_acc: 0.8518\n",
      "Epoch 16/40\n",
      "3983/3983 [==============================] - 269s 68ms/step - loss: 0.0022 - model_acc: 0.9692 - val_loss: 0.0112 - val_model_acc: 0.8555\n",
      "Epoch 17/40\n",
      "3983/3983 [==============================] - 268s 67ms/step - loss: 0.0021 - model_acc: 0.9713 - val_loss: 0.0112 - val_model_acc: 0.8608\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 18/40\n",
      "3983/3983 [==============================] - 271s 68ms/step - loss: 0.0019 - model_acc: 0.9732 - val_loss: 0.0113 - val_model_acc: 0.8568\n",
      "Epoch 19/40\n",
      "3983/3983 [==============================] - 268s 67ms/step - loss: 0.0017 - model_acc: 0.9776 - val_loss: 0.0124 - val_model_acc: 0.8489\n",
      "Epoch 20/40\n",
      "3983/3983 [==============================] - 265s 67ms/step - loss: 0.0017 - model_acc: 0.9756 - val_loss: 0.0122 - val_model_acc: 0.8515\n",
      "Epoch 21/40\n",
      "3983/3983 [==============================] - 261s 66ms/step - loss: 0.0017 - model_acc: 0.9765 - val_loss: 0.0114 - val_model_acc: 0.8614\n",
      "INFO:tensorflow:Assets written to: best_models\\FERPlus_flip_+-25_multi_mse\\assets\n",
      "Epoch 22/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0014 - model_acc: 0.9815 - val_loss: 0.0112 - val_model_acc: 0.8600\n",
      "Epoch 23/40\n",
      "3983/3983 [==============================] - 262s 66ms/step - loss: 0.0013 - model_acc: 0.9827 - val_loss: 0.0115 - val_model_acc: 0.8524\n",
      "Epoch 24/40\n",
      "3983/3983 [==============================] - 263s 66ms/step - loss: 0.0014 - model_acc: 0.9797 - val_loss: 0.0117 - val_model_acc: 0.8489\n",
      "Epoch 25/40\n",
      "3983/3983 [==============================] - 262s 66ms/step - loss: 0.0013 - model_acc: 0.9821 - val_loss: 0.0116 - val_model_acc: 0.8532\n",
      "Epoch 26/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0011 - model_acc: 0.9864 - val_loss: 0.0114 - val_model_acc: 0.8591\n",
      "Epoch 27/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0010 - model_acc: 0.9861 - val_loss: 0.0112 - val_model_acc: 0.8577\n",
      "Epoch 28/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0012 - model_acc: 0.9823 - val_loss: 0.0115 - val_model_acc: 0.8541\n",
      "Epoch 29/40\n",
      "3983/3983 [==============================] - 261s 66ms/step - loss: 8.9154e-04 - model_acc: 0.9883 - val_loss: 0.0113 - val_model_acc: 0.8578\n",
      "Epoch 30/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 0.0011 - model_acc: 0.9843 - val_loss: 0.0115 - val_model_acc: 0.8561\n",
      "Epoch 31/40\n",
      "3983/3983 [==============================] - 262s 66ms/step - loss: 8.3565e-04 - model_acc: 0.9889 - val_loss: 0.0130 - val_model_acc: 0.8411\n",
      "Epoch 32/40\n",
      "3983/3983 [==============================] - 264s 66ms/step - loss: 9.8853e-04 - model_acc: 0.9860 - val_loss: 0.0132 - val_model_acc: 0.8434\n",
      "Epoch 33/40\n",
      "3983/3983 [==============================] - 263s 66ms/step - loss: 7.8449e-04 - model_acc: 0.9896 - val_loss: 0.0115 - val_model_acc: 0.8597\n",
      "Epoch 34/40\n",
      "3983/3983 [==============================] - 259s 65ms/step - loss: 0.0011 - model_acc: 0.9848 - val_loss: 0.0130 - val_model_acc: 0.8417_acc: 0.9\n",
      "Epoch 35/40\n",
      "3983/3983 [==============================] - 258s 65ms/step - loss: 8.6350e-04 - model_acc: 0.9884 - val_loss: 0.0118 - val_model_acc: 0.8526\n",
      "Epoch 36/40\n",
      "3983/3983 [==============================] - 259s 65ms/step - loss: 6.4743e-04 - model_acc: 0.9918 - val_loss: 0.0118 - val_model_acc: 0.8574\n",
      "Epoch 37/40\n",
      "3983/3983 [==============================] - 265s 67ms/step - loss: 7.0790e-04 - model_acc: 0.9904 - val_loss: 0.0114 - val_model_acc: 0.8600\n",
      "Epoch 38/40\n",
      "3983/3983 [==============================] - 294s 74ms/step - loss: 8.5347e-04 - model_acc: 0.9884 - val_loss: 0.0116 - val_model_acc: 0.8532\n",
      "Epoch 39/40\n",
      "3983/3983 [==============================] - 270s 68ms/step - loss: 7.6045e-04 - model_acc: 0.9900 - val_loss: 0.0127 - val_model_acc: 0.8454\n",
      "Epoch 40/40\n",
      "3983/3983 [==============================] - 267s 67ms/step - loss: 8.3590e-04 - model_acc: 0.9883 - val_loss: 0.0147 - val_model_acc: 0.8298\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    base_model = vgg16.VGG16(include_top=False, \n",
    "                             weights=\"imagenet\", \n",
    "                             input_shape=(48,48,3))\n",
    "    base_model.trainable=True\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dense(emotions_count, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=adam.Adam(learning_rate=1e-4), \n",
    "                  loss=mse, \n",
    "                  metrics = [model_acc])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "best_model_save_path = \"best_models/FERPlus_flip_+-25_multi_mse\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_model_save_path,\n",
    "    save_weights_only=False, \n",
    "    monitor='val_model_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(x=tf.image.grayscale_to_rgb(training_images),\n",
    "          y=training_emotions,\n",
    "          batch_size=32,\n",
    "          epochs=40,\n",
    "          validation_data=(tf.image.grayscale_to_rgb(test_images), test_emotions),\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5960dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 32776     \n",
      "=================================================================\n",
      "Total params: 33,630,024\n",
      "Trainable params: 33,630,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"best_models/FERPlus_flip_+-25_multi_mse\"\n",
    "best_model = tf.keras.models.load_model(best_model_path, custom_objects={'model_acc': model_acc})\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
