{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nrrd\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import monai"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1676775145648
        },
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "id": "cb163df8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "gather": {
          "logged": 1675606236724
        }
      },
      "id": "2a01678e"
    },
    {
      "cell_type": "code",
      "source": [
        "class Ellipsoids(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, radii):\n",
        "        self.images = torch.FloatTensor(np.array(images))\n",
        "        self.radii = torch.FloatTensor(np.array(radii))\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        radius = self.radii[index]\n",
        "        return image, radius"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1676775145853
        }
      },
      "id": "caed0757"
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = sorted(glob('../dataset/Ellipsoids/segmentations/*.nrrd'))\n",
        "images = np.array([nrrd.read(path)[0][None, :] for path in image_paths])\n",
        "radii = np.array([float(path.split('_')[-1][:5]) for path in image_paths])[:, None]\n",
        "images.shape, radii.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "((81, 1, 64, 64, 64), (81, 1))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676775147648
        }
      },
      "id": "b587ea94-5293-42dc-9af3-94ed877fe8a3"
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 61\n",
        "perm = np.random.RandomState(seed=0).permutation(81)\n",
        "perm = {\n",
        "    'train': perm[:train_size],\n",
        "    'val': perm[train_size:],\n",
        "}\n",
        "modes = list(perm.keys())\n",
        "dataloaders = dict()\n",
        "for mode in modes:\n",
        "    dataloaders[mode] = torch.utils.data.DataLoader(\n",
        "        Ellipsoids(images[perm[mode]], radii[perm[mode]]),\n",
        "        batch_size=1,\n",
        "        shuffle=(mode == 'train'),\n",
        "        num_workers=6,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1676775147904
        }
      },
      "id": "dcdd91a2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {},
      "id": "f7dee1c3"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_autoencoder():\n",
        "    autoencoder = monai.networks.nets.AutoEncoder(\n",
        "        spatial_dims=3, in_channels=1, out_channels=1,\n",
        "        kernel_size=(3, 3, 3),\n",
        "        channels=[channel*1 for channel in (1, 2, 4, 8, 16)],\n",
        "        strides=(1, 2, 2, 2, 2),\n",
        "    )\n",
        "    return autoencoder\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = init_autoencoder().encode\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1, 1024),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.deconv = init_autoencoder().decode\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = torch.reshape(x, (1, 16, 4, 4, 4))\n",
        "        x = self.deconv(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676775148050
        }
      },
      "id": "66b49082-12e2-45a9-9ea2-3ad0225c54f1"
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_dir = '../dataset/Ellipsoids/models/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676775148262
        }
      },
      "id": "7990e17a-8ba5-4c9b-8191-304631db51b8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Training"
      ],
      "metadata": {},
      "id": "75d9f92e"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_encoder(model, dataloaders, num_epochs, learning_rate):\n",
        "    opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    opt.zero_grad()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.99)\n",
        "    loss_mae = torch.nn.L1Loss().to(device)\n",
        "    \n",
        "    t0 = time.time()\n",
        "    best_val_loss = np.Inf\n",
        "    \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for mode in ['train', 'val']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            losses = []\n",
        "            for image, radius in dataloaders[mode]:\n",
        "                image = image.to(device)\n",
        "                radius = radius.to(device)\n",
        "                \n",
        "                pred_radius = model(image)\n",
        "                loss = loss_mae(pred_radius, radius)\n",
        "                \n",
        "                if mode == 'train':\n",
        "                    opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            print(f'{mode} loss: {np.mean(losses)}')\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        val_loss = np.mean(losses)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{model_dir}/best_encoder.torch')\n",
        "        print(f'Best val loss: {best_val_loss}')\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "        t0 = time.time()\n",
        "        \n",
        "    print(f\"Training complete, model saved. Best model after epoch {best_epoch}\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1676775304158
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "3b7c100a-5118-4009-807e-4098b504934a"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().to(device)\n",
        "train_encoder(model=encoder, dataloaders=dataloaders, num_epochs=200, learning_rate=1e-4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/200\ntrain loss: 17.52146376156416\nval loss: 13.900359511375427\nBest val loss: 13.900359511375427\nTime: 1.7360072135925293\n\nEpoch 2/200\ntrain loss: 5.3993022402779\nval loss: 1.872096633911133\nBest val loss: 1.872096633911133\nTime: 1.6278278827667236\n\nEpoch 3/200\ntrain loss: 1.1942282817402825\nval loss: 1.0452473640441895\nBest val loss: 1.0452473640441895\nTime: 1.6498723030090332\n\nEpoch 4/200\ntrain loss: 0.8686908190367651\nval loss: 0.7083781719207763\nBest val loss: 0.7083781719207763\nTime: 1.690495252609253\n\nEpoch 5/200\ntrain loss: 0.6139494317476867\nval loss: 0.4434977054595947\nBest val loss: 0.4434977054595947\nTime: 1.6232569217681885\n\nEpoch 6/200\ntrain loss: 0.41545977357958186\nval loss: 0.2651317596435547\nBest val loss: 0.2651317596435547\nTime: 1.5865592956542969\n\nEpoch 7/200\ntrain loss: 0.5464769113259237\nval loss: 0.3924841403961182\nBest val loss: 0.2651317596435547\nTime: 1.5332262516021729\n\nEpoch 8/200\ntrain loss: 0.34663602172351277\nval loss: 0.2639397144317627\nBest val loss: 0.2639397144317627\nTime: 1.5791471004486084\n\nEpoch 9/200\ntrain loss: 0.32528231573886557\nval loss: 0.45143918991088866\nBest val loss: 0.2639397144317627\nTime: 1.500607967376709\n\nEpoch 10/200\ntrain loss: 0.4603605426725794\nval loss: 0.2725515365600586\nBest val loss: 0.2639397144317627\nTime: 1.5121431350708008\n\nEpoch 11/200\ntrain loss: 0.30233106456819125\nval loss: 0.28766641616821287\nBest val loss: 0.2639397144317627\nTime: 1.5158092975616455\n\nEpoch 12/200\ntrain loss: 0.23624274769767387\nval loss: 0.26804370880126954\nBest val loss: 0.2639397144317627\nTime: 1.523573398590088\n\nEpoch 13/200\ntrain loss: 0.3321719091446673\nval loss: 0.3360593795776367\nBest val loss: 0.2639397144317627\nTime: 1.5128958225250244\n\nEpoch 14/200\ntrain loss: 0.2929194872496558\nval loss: 0.22469730377197267\nBest val loss: 0.22469730377197267\nTime: 1.6389474868774414\n\nEpoch 15/200\ntrain loss: 0.287945137649286\nval loss: 0.2793248653411865\nBest val loss: 0.22469730377197267\nTime: 1.5475900173187256\n\nEpoch 16/200\ntrain loss: 0.281509274342021\nval loss: 0.3008368492126465\nBest val loss: 0.22469730377197267\nTime: 1.5334880352020264\n\nEpoch 17/200\ntrain loss: 0.29084121203813396\nval loss: 0.25449333190917967\nBest val loss: 0.22469730377197267\nTime: 1.5272064208984375\n\nEpoch 18/200\ntrain loss: 0.273242856635422\nval loss: 0.231312894821167\nBest val loss: 0.22469730377197267\nTime: 1.519059658050537\n\nEpoch 19/200\ntrain loss: 0.2269954994076588\nval loss: 0.21841650009155272\nBest val loss: 0.21841650009155272\nTime: 1.6455190181732178\n\nEpoch 20/200\ntrain loss: 0.2649159822307649\nval loss: 0.25661134719848633\nBest val loss: 0.21841650009155272\nTime: 1.516165018081665\n\nEpoch 21/200\ntrain loss: 0.21547889709472656\nval loss: 0.20000066757202148\nBest val loss: 0.20000066757202148\nTime: 1.5949406623840332\n\nEpoch 22/200\ntrain loss: 0.23596674497010278\nval loss: 0.2252945899963379\nBest val loss: 0.20000066757202148\nTime: 1.521094560623169\n\nEpoch 23/200\ntrain loss: 0.1900936501925109\nval loss: 0.45604877471923827\nBest val loss: 0.20000066757202148\nTime: 1.5423054695129395\n\nEpoch 24/200\ntrain loss: 0.185792688463555\nval loss: 0.2303342342376709\nBest val loss: 0.20000066757202148\nTime: 1.5193469524383545\n\nEpoch 25/200\ntrain loss: 0.2140810450569528\nval loss: 0.44003915786743164\nBest val loss: 0.20000066757202148\nTime: 1.5050926208496094\n\nEpoch 26/200\ntrain loss: 0.2755552823426294\nval loss: 0.20406508445739746\nBest val loss: 0.20000066757202148\nTime: 1.5137617588043213\n\nEpoch 27/200\ntrain loss: 0.3086554886864834\nval loss: 0.42464370727539064\nBest val loss: 0.20000066757202148\nTime: 1.5208089351654053\n\nEpoch 28/200\ntrain loss: 0.20170874673812117\nval loss: 0.17114858627319335\nBest val loss: 0.17114858627319335\nTime: 1.6218748092651367\n\nEpoch 29/200\ntrain loss: 0.16703368014976627\nval loss: 0.40609130859375\nBest val loss: 0.17114858627319335\nTime: 1.5327792167663574\n\nEpoch 30/200\ntrain loss: 0.2118193829645876\nval loss: 0.18852925300598145\nBest val loss: 0.17114858627319335\nTime: 1.5233714580535889\n\nEpoch 31/200\ntrain loss: 0.24532247762211035\nval loss: 0.22278037071228027\nBest val loss: 0.17114858627319335\nTime: 1.5071907043457031\n\nEpoch 32/200\ntrain loss: 0.166345299267378\nval loss: 0.28255858421325686\nBest val loss: 0.17114858627319335\nTime: 1.5141735076904297\n\nEpoch 33/200\ntrain loss: 0.2447827917630555\nval loss: 0.21557292938232422\nBest val loss: 0.17114858627319335\nTime: 1.5255906581878662\n\nEpoch 34/200\ntrain loss: 0.19859126356781506\nval loss: 0.16913142204284667\nBest val loss: 0.16913142204284667\nTime: 1.6326956748962402\n\nEpoch 35/200\ntrain loss: 0.2332298872900791\nval loss: 0.47242431640625\nBest val loss: 0.16913142204284667\nTime: 1.5141470432281494\n\nEpoch 36/200\ntrain loss: 0.1816731593647941\nval loss: 0.2355445384979248\nBest val loss: 0.16913142204284667\nTime: 1.5264029502868652\n\nEpoch 37/200\ntrain loss: 0.15588044338538998\nval loss: 0.22745189666748047\nBest val loss: 0.16913142204284667\nTime: 1.5271732807159424\n\nEpoch 38/200\ntrain loss: 0.28713356080602426\nval loss: 0.24961323738098146\nBest val loss: 0.16913142204284667\nTime: 1.5091307163238525\n\nEpoch 39/200\ntrain loss: 0.15575097818843653\nval loss: 0.1873307704925537\nBest val loss: 0.16913142204284667\nTime: 1.5139718055725098\n\nEpoch 40/200\ntrain loss: 0.15571722437123783\nval loss: 0.19628868103027344\nBest val loss: 0.16913142204284667\nTime: 1.5172936916351318\n\nEpoch 41/200\ntrain loss: 0.21223507552850443\nval loss: 0.2752089023590088\nBest val loss: 0.16913142204284667\nTime: 1.526792049407959\n\nEpoch 42/200\ntrain loss: 0.2070417247834753\nval loss: 0.2709228515625\nBest val loss: 0.16913142204284667\nTime: 1.51735258102417\n\nEpoch 43/200\ntrain loss: 0.14772891998291016\nval loss: 0.19757518768310547\nBest val loss: 0.16913142204284667\nTime: 1.5155110359191895\n\nEpoch 44/200\ntrain loss: 0.1489574870125192\nval loss: 0.21490249633789063\nBest val loss: 0.16913142204284667\nTime: 1.532031536102295\n\nEpoch 45/200\ntrain loss: 0.11498598192558913\nval loss: 0.1581730842590332\nBest val loss: 0.1581730842590332\nTime: 1.6098387241363525\n\nEpoch 46/200\ntrain loss: 0.16987747442526896\nval loss: 0.17147450447082518\nBest val loss: 0.1581730842590332\nTime: 1.5084059238433838\n\nEpoch 47/200\ntrain loss: 0.1639120539680856\nval loss: 0.16281566619873047\nBest val loss: 0.1581730842590332\nTime: 1.5033044815063477\n\nEpoch 48/200\ntrain loss: 0.13971253692126664\nval loss: 0.21922087669372559\nBest val loss: 0.1581730842590332\nTime: 1.510213851928711\n\nEpoch 49/200\ntrain loss: 0.18542675893814836\nval loss: 0.316973352432251\nBest val loss: 0.1581730842590332\nTime: 1.5366787910461426\n\nEpoch 50/200\ntrain loss: 0.15457659862080558\nval loss: 0.17944135665893554\nBest val loss: 0.1581730842590332\nTime: 1.5235519409179688\n\nEpoch 51/200\ntrain loss: 0.21296304171202612\nval loss: 0.17490267753601074\nBest val loss: 0.1581730842590332\nTime: 1.5278880596160889\n\nEpoch 52/200\ntrain loss: 0.13617595297391297\nval loss: 0.15669713020324708\nBest val loss: 0.15669713020324708\nTime: 1.645768165588379\n\nEpoch 53/200\ntrain loss: 0.12658384979748336\nval loss: 0.16183042526245117\nBest val loss: 0.15669713020324708\nTime: 1.525583267211914\n\nEpoch 54/200\ntrain loss: 0.21036638979051933\nval loss: 0.18063392639160156\nBest val loss: 0.15669713020324708\nTime: 1.5087170600891113\n\nEpoch 55/200\ntrain loss: 0.1429932547397301\nval loss: 0.3703010082244873\nBest val loss: 0.15669713020324708\nTime: 1.534721851348877\n\nEpoch 56/200\ntrain loss: 0.11873720513015497\nval loss: 0.13395218849182128\nBest val loss: 0.13395218849182128\nTime: 1.623805046081543\n\nEpoch 57/200\ntrain loss: 0.11970273002249296\nval loss: 0.14760146141052247\nBest val loss: 0.13395218849182128\nTime: 1.5302047729492188\n\nEpoch 58/200\ntrain loss: 0.10309070837302287\nval loss: 0.30135202407836914\nBest val loss: 0.13395218849182128\nTime: 1.5258255004882812\n\nEpoch 59/200\ntrain loss: 0.11441798288314069\nval loss: 0.1792224407196045\nBest val loss: 0.13395218849182128\nTime: 1.502171516418457\n\nEpoch 60/200\ntrain loss: 0.13129088917716605\nval loss: 0.1997525215148926\nBest val loss: 0.13395218849182128\nTime: 1.5065505504608154\n\nEpoch 61/200\ntrain loss: 0.1102887294331535\nval loss: 0.24931702613830567\nBest val loss: 0.13395218849182128\nTime: 1.5332145690917969\n\nEpoch 62/200\ntrain loss: 0.12702235237496798\nval loss: 0.18992390632629394\nBest val loss: 0.13395218849182128\nTime: 1.5132696628570557\n\nEpoch 63/200\ntrain loss: 0.1182017873545162\nval loss: 0.25525474548339844\nBest val loss: 0.13395218849182128\nTime: 1.527735710144043\n\nEpoch 64/200\ntrain loss: 0.1276303744706951\nval loss: 0.15659313201904296\nBest val loss: 0.13395218849182128\nTime: 1.5092964172363281\n\nEpoch 65/200\ntrain loss: 0.09993681360463627\nval loss: 0.2680623531341553\nBest val loss: 0.13395218849182128\nTime: 1.5134220123291016\n\nEpoch 66/200\ntrain loss: 0.15792441758953157\nval loss: 0.147277307510376\nBest val loss: 0.13395218849182128\nTime: 1.5388190746307373\n\nEpoch 67/200\ntrain loss: 0.09289494498831327\nval loss: 0.22561516761779785\nBest val loss: 0.13395218849182128\nTime: 1.516749382019043\n\nEpoch 68/200\ntrain loss: 0.1090138388461754\nval loss: 0.17904653549194335\nBest val loss: 0.13395218849182128\nTime: 1.5112884044647217\n\nEpoch 69/200\ntrain loss: 0.11730652168148854\nval loss: 0.14435601234436035\nBest val loss: 0.13395218849182128\nTime: 1.5100810527801514\n\nEpoch 70/200\ntrain loss: 0.11546983875212122\nval loss: 0.13901987075805664\nBest val loss: 0.13395218849182128\nTime: 1.5395398139953613\n\nEpoch 71/200\ntrain loss: 0.1325891213338883\nval loss: 0.15229663848876954\nBest val loss: 0.13395218849182128\nTime: 1.514500617980957\n\nEpoch 72/200\ntrain loss: 0.0929937988031106\nval loss: 0.14074397087097168\nBest val loss: 0.13395218849182128\nTime: 1.5150716304779053\n\nEpoch 73/200\ntrain loss: 0.12362522375388224\nval loss: 0.15732336044311523\nBest val loss: 0.13395218849182128\nTime: 1.5360534191131592\n\nEpoch 74/200\ntrain loss: 0.08104332157822906\nval loss: 0.16642332077026367\nBest val loss: 0.13395218849182128\nTime: 1.5165836811065674\n\nEpoch 75/200\ntrain loss: 0.10948237434762423\nval loss: 0.21008505821228027\nBest val loss: 0.13395218849182128\nTime: 1.5076866149902344\n\nEpoch 76/200\ntrain loss: 0.07786588199803086\nval loss: 0.18705577850341798\nBest val loss: 0.13395218849182128\nTime: 1.5099899768829346\n\nEpoch 77/200\ntrain loss: 0.0964940649564149\nval loss: 0.23544201850891114\nBest val loss: 0.13395218849182128\nTime: 1.5259380340576172\n\nEpoch 78/200\ntrain loss: 0.09010846497582607\nval loss: 0.1874070644378662\nBest val loss: 0.13395218849182128\nTime: 1.517880916595459\n\nEpoch 79/200\ntrain loss: 0.12777964795222047\nval loss: 0.15522518157958984\nBest val loss: 0.13395218849182128\nTime: 1.5145366191864014\n\nEpoch 80/200\ntrain loss: 0.05234436910660541\nval loss: 0.22102837562561034\nBest val loss: 0.13395218849182128\nTime: 1.530282735824585\n\nEpoch 81/200\ntrain loss: 0.1131007866781266\nval loss: 0.16408023834228516\nBest val loss: 0.13395218849182128\nTime: 1.556483268737793\n\nEpoch 82/200\ntrain loss: 0.11437684981549373\nval loss: 0.15000500679016113\nBest val loss: 0.13395218849182128\nTime: 1.5227110385894775\n\nEpoch 83/200\ntrain loss: 0.10835442777539862\nval loss: 0.140447998046875\nBest val loss: 0.13395218849182128\nTime: 1.521636724472046\n\nEpoch 84/200\ntrain loss: 0.13329457454994076\nval loss: 0.13903169631958007\nBest val loss: 0.13395218849182128\nTime: 1.50826096534729\n\nEpoch 85/200\ntrain loss: 0.09660506639324251\nval loss: 0.21776132583618163\nBest val loss: 0.13395218849182128\nTime: 1.5193064212799072\n\nEpoch 86/200\ntrain loss: 0.10935827161444993\nval loss: 0.13989362716674805\nBest val loss: 0.13395218849182128\nTime: 1.519566535949707\n\nEpoch 87/200\ntrain loss: 0.07879790321725313\nval loss: 0.19376296997070314\nBest val loss: 0.13395218849182128\nTime: 1.514869213104248\n\nEpoch 88/200\ntrain loss: 0.16902490522040695\nval loss: 0.2052095890045166\nBest val loss: 0.13395218849182128\nTime: 1.5028212070465088\n\nEpoch 89/200\ntrain loss: 0.08709834051913902\nval loss: 0.17794456481933593\nBest val loss: 0.13395218849182128\nTime: 1.5203137397766113\n\nEpoch 90/200\ntrain loss: 0.08252201705682473\nval loss: 0.1690354824066162\nBest val loss: 0.13395218849182128\nTime: 1.524914026260376\n\nEpoch 91/200\ntrain loss: 0.11942719631507749\nval loss: 0.17527475357055664\nBest val loss: 0.13395218849182128\nTime: 1.5217528343200684\n\nEpoch 92/200\ntrain loss: 0.10474613064625224\nval loss: 0.21733450889587402\nBest val loss: 0.13395218849182128\nTime: 1.5103654861450195\n\nEpoch 93/200\ntrain loss: 0.12145772527475826\nval loss: 0.22150578498840331\nBest val loss: 0.13395218849182128\nTime: 1.5168356895446777\n\nEpoch 94/200\ntrain loss: 0.07911555493464235\nval loss: 0.1410127639770508\nBest val loss: 0.13395218849182128\nTime: 1.520653486251831\n\nEpoch 95/200\ntrain loss: 0.06361823785500448\nval loss: 0.15931296348571777\nBest val loss: 0.13395218849182128\nTime: 1.5281527042388916\n\nEpoch 96/200\ntrain loss: 0.06443470814188973\nval loss: 0.13179473876953124\nBest val loss: 0.13179473876953124\nTime: 1.6400160789489746\n\nEpoch 97/200\ntrain loss: 0.06443831959708793\nval loss: 0.16144175529479982\nBest val loss: 0.13179473876953124\nTime: 1.51999831199646\n\nEpoch 98/200\ntrain loss: 0.09142906939397093\nval loss: 0.1633223056793213\nBest val loss: 0.13179473876953124\nTime: 1.5181739330291748\n\nEpoch 99/200\ntrain loss: 0.0892672773267402\nval loss: 0.1567234516143799\nBest val loss: 0.13179473876953124\nTime: 1.5107064247131348\n\nEpoch 100/200\ntrain loss: 0.0663481071347096\nval loss: 0.14408397674560547\nBest val loss: 0.13179473876953124\nTime: 1.5286362171173096\n\nEpoch 101/200\ntrain loss: 0.07787337068651544\nval loss: 0.13379182815551757\nBest val loss: 0.13179473876953124\nTime: 1.520155906677246\n\nEpoch 102/200\ntrain loss: 0.0909796230128554\nval loss: 0.17520203590393066\nBest val loss: 0.13179473876953124\nTime: 1.5121004581451416\n\nEpoch 103/200\ntrain loss: 0.07365228309005988\nval loss: 0.13531904220581054\nBest val loss: 0.13179473876953124\nTime: 1.5249032974243164\n\nEpoch 104/200\ntrain loss: 0.06523074478399558\nval loss: 0.18782129287719726\nBest val loss: 0.13179473876953124\nTime: 1.5184860229492188\n\nEpoch 105/200\ntrain loss: 0.06976837408347208\nval loss: 0.13988490104675294\nBest val loss: 0.13179473876953124\nTime: 1.5287721157073975\n\nEpoch 106/200\ntrain loss: 0.11240468259717597\nval loss: 0.22697749137878417\nBest val loss: 0.13179473876953124\nTime: 1.5215940475463867\n\nEpoch 107/200\ntrain loss: 0.0834640127713563\nval loss: 0.14591603279113768\nBest val loss: 0.13179473876953124\nTime: 1.5326025485992432\n\nEpoch 108/200\ntrain loss: 0.05881676908399238\nval loss: 0.15226011276245116\nBest val loss: 0.13179473876953124\nTime: 1.5290744304656982\n\nEpoch 109/200\ntrain loss: 0.0687797733994781\nval loss: 0.3192850112915039\nBest val loss: 0.13179473876953124\nTime: 1.5243828296661377\n\nEpoch 110/200\ntrain loss: 0.09781183961962091\nval loss: 0.12454910278320312\nBest val loss: 0.12454910278320312\nTime: 1.6782233715057373\n\nEpoch 111/200\ntrain loss: 0.07819629106365267\nval loss: 0.1386190891265869\nBest val loss: 0.12454910278320312\nTime: 1.501183271408081\n\nEpoch 112/200\ntrain loss: 0.07744482697033492\nval loss: 0.1343289852142334\nBest val loss: 0.12454910278320312\nTime: 1.5371298789978027\n\nEpoch 113/200\ntrain loss: 0.0847741111380155\nval loss: 0.17999229431152344\nBest val loss: 0.12454910278320312\nTime: 1.5654387474060059\n\nEpoch 114/200\ntrain loss: 0.06671428680419922\nval loss: 0.15278196334838867\nBest val loss: 0.12454910278320312\nTime: 1.5293164253234863\n\nEpoch 115/200\ntrain loss: 0.08060195797779521\nval loss: 0.144561767578125\nBest val loss: 0.12454910278320312\nTime: 1.531738519668579\n\nEpoch 116/200\ntrain loss: 0.08799774920354124\nval loss: 0.1563570022583008\nBest val loss: 0.12454910278320312\nTime: 1.517911672592163\n\nEpoch 117/200\ntrain loss: 0.09174343797027087\nval loss: 0.13620505332946778\nBest val loss: 0.12454910278320312\nTime: 1.5125508308410645\n\nEpoch 118/200\ntrain loss: 0.06962501025590741\nval loss: 0.14538612365722656\nBest val loss: 0.12454910278320312\nTime: 1.5162384510040283\n\nEpoch 119/200\ntrain loss: 0.07960117840376056\nval loss: 0.13774418830871582\nBest val loss: 0.12454910278320312\nTime: 1.5240819454193115\n\nEpoch 120/200\ntrain loss: 0.056571897913198\nval loss: 0.16157522201538085\nBest val loss: 0.12454910278320312\nTime: 1.531144142150879\n\nEpoch 121/200\ntrain loss: 0.07859570862816982\nval loss: 0.13376431465148925\nBest val loss: 0.12454910278320312\nTime: 1.534632682800293\n\nEpoch 122/200\ntrain loss: 0.08753531096411533\nval loss: 0.16050939559936522\nBest val loss: 0.12454910278320312\nTime: 1.5113444328308105\n\nEpoch 123/200\ntrain loss: 0.054084496419937886\nval loss: 0.1423872470855713\nBest val loss: 0.12454910278320312\nTime: 1.5289952754974365\n\nEpoch 124/200\ntrain loss: 0.06439784315765881\nval loss: 0.13762340545654297\nBest val loss: 0.12454910278320312\nTime: 1.5221490859985352\n\nEpoch 125/200\ntrain loss: 0.0720440755124952\nval loss: 0.15804471969604492\nBest val loss: 0.12454910278320312\nTime: 1.5155174732208252\n\nEpoch 126/200\ntrain loss: 0.05236380217505283\nval loss: 0.1510465621948242\nBest val loss: 0.12454910278320312\nTime: 1.5158967971801758\n\nEpoch 127/200\ntrain loss: 0.07621547824046651\nval loss: 0.16463117599487304\nBest val loss: 0.12454910278320312\nTime: 1.523714542388916\n\nEpoch 128/200\ntrain loss: 0.07609881729376121\nval loss: 0.20180892944335938\nBest val loss: 0.12454910278320312\nTime: 1.5318467617034912\n\nEpoch 129/200\ntrain loss: 0.07921406480132556\nval loss: 0.16054491996765136\nBest val loss: 0.12454910278320312\nTime: 1.5169093608856201\n\nEpoch 130/200\ntrain loss: 0.057714884398413484\nval loss: 0.13778290748596192\nBest val loss: 0.12454910278320312\nTime: 1.514596700668335\n\nEpoch 131/200\ntrain loss: 0.06057098263599833\nval loss: 0.138566255569458\nBest val loss: 0.12454910278320312\nTime: 1.5188884735107422\n\nEpoch 132/200\ntrain loss: 0.048775454036525036\nval loss: 0.13619842529296874\nBest val loss: 0.12454910278320312\nTime: 1.5255515575408936\n\nEpoch 133/200\ntrain loss: 0.08294457294901864\nval loss: 0.14835720062255858\nBest val loss: 0.12454910278320312\nTime: 1.526871919631958\n\nEpoch 134/200\ntrain loss: 0.047462525914927\nval loss: 0.1530430793762207\nBest val loss: 0.12454910278320312\nTime: 1.519914150238037\n\nEpoch 135/200\ntrain loss: 0.05284442276251121\nval loss: 0.1569298267364502\nBest val loss: 0.12454910278320312\nTime: 1.523904800415039\n\nEpoch 136/200\ntrain loss: 0.07297978635694159\nval loss: 0.14449620246887207\nBest val loss: 0.12454910278320312\nTime: 1.5240795612335205\n\nEpoch 137/200\ntrain loss: 0.04620824094678535\nval loss: 0.15337209701538085\nBest val loss: 0.12454910278320312\nTime: 1.533158540725708\n\nEpoch 138/200\ntrain loss: 0.06716110667244332\nval loss: 0.20498337745666503\nBest val loss: 0.12454910278320312\nTime: 1.5126593112945557\n\nEpoch 139/200\ntrain loss: 0.05471523472520172\nval loss: 0.1379979133605957\nBest val loss: 0.12454910278320312\nTime: 1.50921630859375\n\nEpoch 140/200\ntrain loss: 0.06062002651026992\nval loss: 0.1531928539276123\nBest val loss: 0.12454910278320312\nTime: 1.5332942008972168\n\nEpoch 141/200\ntrain loss: 0.06986410109723201\nval loss: 0.15941171646118163\nBest val loss: 0.12454910278320312\nTime: 1.5257899761199951\n\nEpoch 142/200\ntrain loss: 0.06007086644407179\nval loss: 0.13235926628112793\nBest val loss: 0.12454910278320312\nTime: 1.5259687900543213\n\nEpoch 143/200\ntrain loss: 0.04737106698458312\nval loss: 0.17006840705871581\nBest val loss: 0.12454910278320312\nTime: 1.5234043598175049\n\nEpoch 144/200\ntrain loss: 0.05083553126600922\nval loss: 0.13584046363830565\nBest val loss: 0.12454910278320312\nTime: 1.52980375289917\n\nEpoch 145/200\ntrain loss: 0.03654989649037846\nval loss: 0.12619848251342775\nBest val loss: 0.12454910278320312\nTime: 1.5761253833770752\n\nEpoch 146/200\ntrain loss: 0.05787188107850122\nval loss: 0.15761990547180177\nBest val loss: 0.12454910278320312\nTime: 1.5339231491088867\n\nEpoch 147/200\ntrain loss: 0.06515546704902024\nval loss: 0.14269280433654785\nBest val loss: 0.12454910278320312\nTime: 1.5191051959991455\n\nEpoch 148/200\ntrain loss: 0.0473888741164911\nval loss: 0.14280123710632325\nBest val loss: 0.12454910278320312\nTime: 1.5193228721618652\n\nEpoch 149/200\ntrain loss: 0.05805891068255315\nval loss: 0.135528039932251\nBest val loss: 0.12454910278320312\nTime: 1.5463273525238037\n\nEpoch 150/200\ntrain loss: 0.03618376372290439\nval loss: 0.1342792510986328\nBest val loss: 0.12454910278320312\nTime: 1.5212175846099854\n\nEpoch 151/200\ntrain loss: 0.050112536696136974\nval loss: 0.17111606597900392\nBest val loss: 0.12454910278320312\nTime: 1.523521900177002\n\nEpoch 152/200\ntrain loss: 0.072675814394091\nval loss: 0.12642369270324708\nBest val loss: 0.12454910278320312\nTime: 1.5392544269561768\n\nEpoch 153/200\ntrain loss: 0.04116252211273694\nval loss: 0.13161301612854004\nBest val loss: 0.12454910278320312\nTime: 1.5192365646362305\n\nEpoch 154/200\ntrain loss: 0.044235729780353485\nval loss: 0.1387026786804199\nBest val loss: 0.12454910278320312\nTime: 1.5199506282806396\n\nEpoch 155/200\ntrain loss: 0.06425214986332127\nval loss: 0.1295802116394043\nBest val loss: 0.12454910278320312\nTime: 1.5165164470672607\n\nEpoch 156/200\ntrain loss: 0.04255804468373783\nval loss: 0.13536486625671387\nBest val loss: 0.12454910278320312\nTime: 1.5120933055877686\n\nEpoch 157/200\ntrain loss: 0.053179912879818776\nval loss: 0.12892389297485352\nBest val loss: 0.12454910278320312\nTime: 1.5138657093048096\n\nEpoch 158/200\ntrain loss: 0.040578404410940704\nval loss: 0.1396017551422119\nBest val loss: 0.12454910278320312\nTime: 1.5213780403137207\n\nEpoch 159/200\ntrain loss: 0.05780698432297003\nval loss: 0.18103227615356446\nBest val loss: 0.12454910278320312\nTime: 1.5243613719940186\n\nEpoch 160/200\ntrain loss: 0.07333844606993628\nval loss: 0.15827436447143556\nBest val loss: 0.12454910278320312\nTime: 1.5283544063568115\n\nEpoch 161/200\ntrain loss: 0.045981938721703704\nval loss: 0.13558225631713866\nBest val loss: 0.12454910278320312\nTime: 1.5140049457550049\n\nEpoch 162/200\ntrain loss: 0.030775429772548987\nval loss: 0.1304083824157715\nBest val loss: 0.12454910278320312\nTime: 1.5097942352294922\n\nEpoch 163/200\ntrain loss: 0.04401310154649078\nval loss: 0.13383841514587402\nBest val loss: 0.12454910278320312\nTime: 1.5214993953704834\n\nEpoch 164/200\ntrain loss: 0.06032749863921619\nval loss: 0.13091187477111815\nBest val loss: 0.12454910278320312\nTime: 1.5154688358306885\n\nEpoch 165/200\ntrain loss: 0.03877761715748271\nval loss: 0.12529592514038085\nBest val loss: 0.12454910278320312\nTime: 1.5424134731292725\n\nEpoch 166/200\ntrain loss: 0.04699098868448226\nval loss: 0.12188138961791992\nBest val loss: 0.12188138961791992\nTime: 1.6307709217071533\n\nEpoch 167/200\ntrain loss: 0.035734598753882234\nval loss: 0.12775840759277343\nBest val loss: 0.12188138961791992\nTime: 1.5205647945404053\n\nEpoch 168/200\ntrain loss: 0.05712568564493148\nval loss: 0.13078432083129882\nBest val loss: 0.12188138961791992\nTime: 1.5223183631896973\n\nEpoch 169/200\ntrain loss: 0.07485982238269243\nval loss: 0.12807612419128417\nBest val loss: 0.12188138961791992\nTime: 1.5222804546356201\n\nEpoch 170/200\ntrain loss: 0.03620041393842854\nval loss: 0.14913158416748046\nBest val loss: 0.12188138961791992\nTime: 1.5050113201141357\n\nEpoch 171/200\ntrain loss: 0.04773952921883005\nval loss: 0.14812016487121582\nBest val loss: 0.12188138961791992\nTime: 1.5230605602264404\n\nEpoch 172/200\ntrain loss: 0.06468713478963883\nval loss: 0.12517843246459961\nBest val loss: 0.12188138961791992\nTime: 1.5227584838867188\n\nEpoch 173/200\ntrain loss: 0.058019434819456005\nval loss: 0.183453369140625\nBest val loss: 0.12188138961791992\nTime: 1.517118215560913\n\nEpoch 174/200\ntrain loss: 0.042602726670562245\nval loss: 0.1380840301513672\nBest val loss: 0.12188138961791992\nTime: 1.5100257396697998\n\nEpoch 175/200\ntrain loss: 0.04018916458380027\nval loss: 0.13304357528686522\nBest val loss: 0.12188138961791992\nTime: 1.5288777351379395\n\nEpoch 176/200\ntrain loss: 0.042028599098080495\nval loss: 0.14662842750549315\nBest val loss: 0.12188138961791992\nTime: 1.518127202987671\n\nEpoch 177/200\ntrain loss: 0.037185950357405864\nval loss: 0.13345632553100586\nBest val loss: 0.12188138961791992\nTime: 1.5264711380004883\n\nEpoch 178/200\ntrain loss: 0.05747246351398406\nval loss: 0.15458965301513672\nBest val loss: 0.12188138961791992\nTime: 1.5400111675262451\n\nEpoch 179/200\ntrain loss: 0.05188055507472304\nval loss: 0.13596663475036622\nBest val loss: 0.12188138961791992\nTime: 1.5447211265563965\n\nEpoch 180/200\ntrain loss: 0.04122444840728259\nval loss: 0.1272054672241211\nBest val loss: 0.12188138961791992\nTime: 1.5145642757415771\n\nEpoch 181/200\ntrain loss: 0.029837905383500896\nval loss: 0.12545280456542968\nBest val loss: 0.12188138961791992\nTime: 1.5308728218078613\n\nEpoch 182/200\ntrain loss: 0.03976803138607838\nval loss: 0.12315616607666016\nBest val loss: 0.12188138961791992\nTime: 1.5278377532958984\n\nEpoch 183/200\ntrain loss: 0.0581801992947938\nval loss: 0.15487518310546874\nBest val loss: 0.12188138961791992\nTime: 1.5220556259155273\n\nEpoch 184/200\ntrain loss: 0.04077065577272509\nval loss: 0.13346710205078124\nBest val loss: 0.12188138961791992\nTime: 1.5462555885314941\n\nEpoch 185/200\ntrain loss: 0.03059090160932697\nval loss: 0.1295934200286865\nBest val loss: 0.12188138961791992\nTime: 1.5104007720947266\n\nEpoch 186/200\ntrain loss: 0.038847219748575176\nval loss: 0.1378091335296631\nBest val loss: 0.12188138961791992\nTime: 1.5185604095458984\n\nEpoch 187/200\ntrain loss: 0.045116018076412016\nval loss: 0.13400492668151856\nBest val loss: 0.12188138961791992\nTime: 1.5206432342529297\n\nEpoch 188/200\ntrain loss: 0.054752349853515625\nval loss: 0.13123679161071777\nBest val loss: 0.12188138961791992\nTime: 1.535374402999878\n\nEpoch 189/200\ntrain loss: 0.0269023238635454\nval loss: 0.13864583969116212\nBest val loss: 0.12188138961791992\nTime: 1.5215494632720947\n\nEpoch 190/200\ntrain loss: 0.030300343622926804\nval loss: 0.1251767635345459\nBest val loss: 0.12188138961791992\nTime: 1.5167603492736816\n\nEpoch 191/200\ntrain loss: 0.028999047201187886\nval loss: 0.13043360710144042\nBest val loss: 0.12188138961791992\nTime: 1.5310838222503662\n\nEpoch 192/200\ntrain loss: 0.023664787167408427\nval loss: 0.12982988357543945\nBest val loss: 0.12188138961791992\nTime: 1.5250978469848633\n\nEpoch 193/200\ntrain loss: 0.029066507933569737\nval loss: 0.12659425735473634\nBest val loss: 0.12188138961791992\nTime: 1.5249764919281006\n\nEpoch 194/200\ntrain loss: 0.026569147579005508\nval loss: 0.13294382095336915\nBest val loss: 0.12188138961791992\nTime: 1.5243523120880127\n\nEpoch 195/200\ntrain loss: 0.029080375296170594\nval loss: 0.12959532737731932\nBest val loss: 0.12188138961791992\nTime: 1.5278699398040771\n\nEpoch 196/200\ntrain loss: 0.023716004168401\nval loss: 0.13600006103515624\nBest val loss: 0.12188138961791992\nTime: 1.5461416244506836\n\nEpoch 197/200\ntrain loss: 0.025151534158675395\nval loss: 0.125823974609375\nBest val loss: 0.12188138961791992\nTime: 1.5186100006103516\n\nEpoch 198/200\ntrain loss: 0.023568684937524013\nval loss: 0.1275874614715576\nBest val loss: 0.12188138961791992\nTime: 1.5268244743347168\n\nEpoch 199/200\ntrain loss: 0.03178172815041464\nval loss: 0.15037355422973633\nBest val loss: 0.12188138961791992\nTime: 1.5156559944152832\n\nEpoch 200/200\ntrain loss: 0.03137565049968782\nval loss: 0.13096246719360352\nBest val loss: 0.12188138961791992\nTime: 1.5209543704986572\n\nTraining complete, model saved. Best model after epoch 166\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1676775614693
        }
      },
      "id": "54c8cd4b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "38e3035c-fa8e-4f2f-a6dd-f1dcacace76f"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_decoder(model, dataloaders, num_epochs, learning_rate):\n",
        "    opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    opt.zero_grad()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.99)\n",
        "    loss_dice = monai.losses.DiceLoss(sigmoid=True, squared_pred=True).to(device)\n",
        "\n",
        "    metric = monai.metrics.DiceMetric(include_background=False, reduction='mean_batch')\n",
        "\n",
        "    t0 = time.time()\n",
        "    best_val_dsc = 0\n",
        "    \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for mode in ['train', 'val']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            losses = []\n",
        "            for image, radius in dataloaders[mode]:\n",
        "                image = image.to(device)\n",
        "                radius = radius.to(device)\n",
        "                \n",
        "                pred_segm = model(radius)\n",
        "                loss = loss_dice(pred_segm, image)\n",
        "                \n",
        "                if mode == 'train':\n",
        "                    opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                metric((pred_segm > 0.5).float(), image)\n",
        "\n",
        "            print(f'{mode} loss: {np.mean(losses)}')\n",
        "            mean_dsc = metric.aggregate().tolist()[0]\n",
        "            metric.reset()\n",
        "            print(f'{mode} DSC: {mean_dsc}')\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        if mean_dsc > best_val_dsc:\n",
        "            best_val_loss = mean_dsc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{model_dir}/best_decoder.torch')\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "        t0 = time.time()\n",
        "        \n",
        "    print(f\"Training complete, model saved. Best model after epoch {best_epoch}\")"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676775829012
        }
      },
      "id": "172e904d-9601-4d97-ba4f-aa8031cbea28"
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder().to(device)\n",
        "train_decoder(model=decoder, dataloaders=dataloaders, num_epochs=400, learning_rate=3e-4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/400\ntrain loss: 0.8785346687817183\ntrain DSC: 0.37331223487854004\nval loss: 0.8469746828079223\nval DSC: 0.5152209401130676\nTime: 2.258800506591797\n\nEpoch 2/400\ntrain loss: 0.8348241272519846\ntrain DSC: 0.5664443373680115\nval loss: 0.8111542105674744\nval DSC: 0.6486846208572388\nTime: 2.0835392475128174\n\nEpoch 3/400\ntrain loss: 0.8177562336452672\ntrain DSC: 0.6284284591674805\nval loss: 0.8024275362491607\nval DSC: 0.6639474630355835\nTime: 2.0355422496795654\n\nEpoch 4/400\ntrain loss: 0.8106378727271909\ntrain DSC: 0.6417031288146973\nval loss: 0.7956203520298004\nval DSC: 0.6958715319633484\nTime: 2.068946599960327\n\nEpoch 5/400\ntrain loss: 0.8042841964080686\ntrain DSC: 0.6601043343544006\nval loss: 0.7885804742574691\nval DSC: 0.7123348116874695\nTime: 2.034174919128418\n\nEpoch 6/400\ntrain loss: 0.7972859202838335\ntrain DSC: 0.6764822602272034\nval loss: 0.7806931883096695\nval DSC: 0.7121955752372742\nTime: 2.007211446762085\n\nEpoch 7/400\ntrain loss: 0.7895635464152352\ntrain DSC: 0.6833533048629761\nval loss: 0.7717754572629929\nval DSC: 0.7200721502304077\nTime: 2.0765738487243652\n\nEpoch 8/400\ntrain loss: 0.7804215306141338\ntrain DSC: 0.6917455792427063\nval loss: 0.7611671686172485\nval DSC: 0.7317509651184082\nTime: 2.008890390396118\n\nEpoch 9/400\ntrain loss: 0.7695134047602044\ntrain DSC: 0.6973577737808228\nval loss: 0.7486776798963547\nval DSC: 0.7295199036598206\nTime: 2.0277047157287598\n\nEpoch 10/400\ntrain loss: 0.7566035497383993\ntrain DSC: 0.6935905814170837\nval loss: 0.733573392033577\nval DSC: 0.7305793166160583\nTime: 2.0588738918304443\n\nEpoch 11/400\ntrain loss: 0.7410208211570489\ntrain DSC: 0.6912553906440735\nval loss: 0.7157922565937043\nval DSC: 0.7201875448226929\nTime: 2.0143609046936035\n\nEpoch 12/400\ntrain loss: 0.7224042073625033\ntrain DSC: 0.6835377216339111\nval loss: 0.6937379032373429\nval DSC: 0.7152219414710999\nTime: 2.0392203330993652\n\nEpoch 13/400\ntrain loss: 0.699933078445372\ntrain DSC: 0.6757556796073914\nval loss: 0.6682900428771973\nval DSC: 0.707179069519043\nTime: 2.055940866470337\n\nEpoch 14/400\ntrain loss: 0.6736048909484363\ntrain DSC: 0.6700273156166077\nval loss: 0.6386998683214188\nval DSC: 0.7020601034164429\nTime: 2.054837942123413\n\nEpoch 15/400\ntrain loss: 0.6437930083665692\ntrain DSC: 0.6677073240280151\nval loss: 0.6058099806308747\nval DSC: 0.7023084759712219\nTime: 2.1200742721557617\n\nEpoch 16/400\ntrain loss: 0.611403902046016\ntrain DSC: 0.6696456670761108\nval loss: 0.5708195954561234\nval DSC: 0.7068738341331482\nTime: 2.057798147201538\n\nEpoch 17/400\ntrain loss: 0.5775735124212796\ntrain DSC: 0.6780994534492493\nval loss: 0.5351268231868744\nval DSC: 0.7156985402107239\nTime: 2.0803048610687256\n\nEpoch 18/400\ntrain loss: 0.5431542767853034\ntrain DSC: 0.6905427575111389\nval loss: 0.49943022429943085\nval DSC: 0.7314793467521667\nTime: 2.05460786819458\n\nEpoch 19/400\ntrain loss: 0.5093184602065165\ntrain DSC: 0.704211413860321\nval loss: 0.4643023252487183\nval DSC: 0.747972846031189\nTime: 2.0788958072662354\n\nEpoch 20/400\ntrain loss: 0.4764075777569755\ntrain DSC: 0.7186564207077026\nval loss: 0.43256146609783175\nval DSC: 0.7506349682807922\nTime: 2.032454252243042\n\nEpoch 21/400\ntrain loss: 0.4471054008749665\ntrain DSC: 0.7258950471878052\nval loss: 0.40282349586486815\nval DSC: 0.7658628225326538\nTime: 2.0345470905303955\n\nEpoch 22/400\ntrain loss: 0.4198432046858991\ntrain DSC: 0.738710343837738\nval loss: 0.37542266547679903\nval DSC: 0.7778897285461426\nTime: 2.0717685222625732\n\nEpoch 23/400\ntrain loss: 0.3946002047570025\ntrain DSC: 0.7507818341255188\nval loss: 0.34991041123867034\nval DSC: 0.7901366949081421\nTime: 2.0821709632873535\n\nEpoch 24/400\ntrain loss: 0.371401418427952\ntrain DSC: 0.7609195113182068\nval loss: 0.32791122794151306\nval DSC: 0.8014047741889954\nTime: 2.038729429244995\n\nEpoch 25/400\ntrain loss: 0.3497073757843893\ntrain DSC: 0.7742666602134705\nval loss: 0.3055300235748291\nval DSC: 0.8150888681411743\nTime: 2.0737335681915283\n\nEpoch 26/400\ntrain loss: 0.32847389920813136\ntrain DSC: 0.7890140414237976\nval loss: 0.2847509980201721\nval DSC: 0.8322749137878418\nTime: 2.0194153785705566\n\nEpoch 27/400\ntrain loss: 0.3091679694222622\ntrain DSC: 0.8028393983840942\nval loss: 0.26609688997268677\nval DSC: 0.8419540524482727\nTime: 2.070129156112671\n\nEpoch 28/400\ntrain loss: 0.2917599648725791\ntrain DSC: 0.8131054639816284\nval loss: 0.24916642010211945\nval DSC: 0.8537529706954956\nTime: 2.0591094493865967\n\nEpoch 29/400\ntrain loss: 0.27521977561419125\ntrain DSC: 0.8210326433181763\nval loss: 0.2346421390771866\nval DSC: 0.8572460412979126\nTime: 2.049541711807251\n\nEpoch 30/400\ntrain loss: 0.2623136766621324\ntrain DSC: 0.8235192894935608\nval loss: 0.2236314058303833\nval DSC: 0.8617671728134155\nTime: 2.018094539642334\n\nEpoch 31/400\ntrain loss: 0.2515846959880141\ntrain DSC: 0.8269844651222229\nval loss: 0.21336946189403533\nval DSC: 0.8655151128768921\nTime: 2.077638626098633\n\nEpoch 32/400\ntrain loss: 0.2405522602503417\ntrain DSC: 0.8319302797317505\nval loss: 0.20325574576854705\nval DSC: 0.8657625317573547\nTime: 2.0612173080444336\n\nEpoch 33/400\ntrain loss: 0.23233046297167168\ntrain DSC: 0.8340761065483093\nval loss: 0.19515478909015654\nval DSC: 0.8669925928115845\nTime: 2.012270212173462\n\nEpoch 34/400\ntrain loss: 0.2236708402633667\ntrain DSC: 0.8361461162567139\nval loss: 0.1868010252714157\nval DSC: 0.8704282641410828\nTime: 2.127554416656494\n\nEpoch 35/400\ntrain loss: 0.2156445755333197\ntrain DSC: 0.8409570455551147\nval loss: 0.18044109642505646\nval DSC: 0.8706737756729126\nTime: 2.023911237716675\n\nEpoch 36/400\ntrain loss: 0.20950340540682683\ntrain DSC: 0.8416781425476074\nval loss: 0.17394950985908508\nval DSC: 0.8769121170043945\nTime: 2.007766008377075\n\nEpoch 37/400\ntrain loss: 0.20319790722893888\ntrain DSC: 0.8454792499542236\nval loss: 0.16766548454761504\nval DSC: 0.8786163330078125\nTime: 2.064748764038086\n\nEpoch 38/400\ntrain loss: 0.19704617828619284\ntrain DSC: 0.8477159738540649\nval loss: 0.16304029226303102\nval DSC: 0.8774991035461426\nTime: 2.025897264480591\n\nEpoch 39/400\ntrain loss: 0.19285416798513444\ntrain DSC: 0.8474316000938416\nval loss: 0.15903578102588653\nval DSC: 0.8814594149589539\nTime: 2.011066198348999\n\nEpoch 40/400\ntrain loss: 0.18681098398615104\ntrain DSC: 0.8516676425933838\nval loss: 0.15421913862228392\nval DSC: 0.8819164037704468\nTime: 2.05118727684021\n\nEpoch 41/400\ntrain loss: 0.1823741060788514\ntrain DSC: 0.8535668849945068\nval loss: 0.1502852261066437\nval DSC: 0.8802698850631714\nTime: 2.0294885635375977\n\nEpoch 42/400\ntrain loss: 0.17507144662200427\ntrain DSC: 0.8598909974098206\nval loss: 0.1517687976360321\nval DSC: 0.8856666684150696\nTime: 2.043794870376587\n\nEpoch 43/400\ntrain loss: 0.17564849482208003\ntrain DSC: 0.8574652075767517\nval loss: 0.1384733259677887\nval DSC: 0.8916909098625183\nTime: 2.0883538722991943\n\nEpoch 44/400\ntrain loss: 0.1701429965066128\ntrain DSC: 0.863259494304657\nval loss: 0.13748327195644378\nval DSC: 0.887370228767395\nTime: 2.1114349365234375\n\nEpoch 45/400\ntrain loss: 0.16227473978136406\ntrain DSC: 0.8690226674079895\nval loss: 0.1442870169878006\nval DSC: 0.8946202993392944\nTime: 2.1808414459228516\n\nEpoch 46/400\ntrain loss: 0.1466884671664629\ntrain DSC: 0.8896881937980652\nval loss: 0.13255650997161866\nval DSC: 0.9040285348892212\nTime: 2.0771400928497314\n\nEpoch 47/400\ntrain loss: 0.13176276742434892\ntrain DSC: 0.906397819519043\nval loss: 0.09894418120384216\nval DSC: 0.9336519241333008\nTime: 2.0338199138641357\n\nEpoch 48/400\ntrain loss: 0.1307380404628691\ntrain DSC: 0.9060279726982117\nval loss: 0.08954459130764007\nval DSC: 0.9413152933120728\nTime: 2.0305259227752686\n\nEpoch 49/400\ntrain loss: 0.10929815593312998\ntrain DSC: 0.9293242692947388\nval loss: 0.09389236271381378\nval DSC: 0.9367944598197937\nTime: 2.0852017402648926\n\nEpoch 50/400\ntrain loss: 0.10059798740949787\ntrain DSC: 0.9371169209480286\nval loss: 0.07140616774559021\nval DSC: 0.9604973793029785\nTime: 2.0494141578674316\n\nEpoch 51/400\ntrain loss: 0.09275200816451526\ntrain DSC: 0.9439743757247925\nval loss: 0.0684120386838913\nval DSC: 0.9615345001220703\nTime: 2.0567853450775146\n\nEpoch 52/400\ntrain loss: 0.09173843313436039\ntrain DSC: 0.9421575665473938\nval loss: 0.06400960683822632\nval DSC: 0.9645088315010071\nTime: 2.0368895530700684\n\nEpoch 53/400\ntrain loss: 0.08433243974310453\ntrain DSC: 0.9486923217773438\nval loss: 0.06437398493289948\nval DSC: 0.9612764120101929\nTime: 2.030115842819214\n\nEpoch 54/400\ntrain loss: 0.08190222157806647\ntrain DSC: 0.9499833583831787\nval loss: 0.06747033596038818\nval DSC: 0.954891562461853\nTime: 2.0364110469818115\n\nEpoch 55/400\ntrain loss: 0.0772588487531318\ntrain DSC: 0.9530198574066162\nval loss: 0.05656598508358002\nval DSC: 0.965319812297821\nTime: 2.0463850498199463\n\nEpoch 56/400\ntrain loss: 0.07259868891512761\ntrain DSC: 0.956416666507721\nval loss: 0.05958796143531799\nval DSC: 0.962977409362793\nTime: 2.071124315261841\n\nEpoch 57/400\ntrain loss: 0.07359708918899786\ntrain DSC: 0.9530012011528015\nval loss: 0.05938439071178436\nval DSC: 0.9584638476371765\nTime: 2.037569284439087\n\nEpoch 58/400\ntrain loss: 0.06690135940176542\ntrain DSC: 0.9594354033470154\nval loss: 0.04641817510128021\nval DSC: 0.9734140634536743\nTime: 2.096212387084961\n\nEpoch 59/400\ntrain loss: 0.06698253897369885\ntrain DSC: 0.9579916000366211\nval loss: 0.04693061113357544\nval DSC: 0.9702817797660828\nTime: 2.0466601848602295\n\nEpoch 60/400\ntrain loss: 0.06206470532495467\ntrain DSC: 0.9625683426856995\nval loss: 0.05726621448993683\nval DSC: 0.9586183428764343\nTime: 2.0337073802948\n\nEpoch 61/400\ntrain loss: 0.06288363796765686\ntrain DSC: 0.9598377346992493\nval loss: 0.0429539829492569\nval DSC: 0.9742790460586548\nTime: 2.0685133934020996\n\nEpoch 62/400\ntrain loss: 0.055864259844920675\ntrain DSC: 0.9673161506652832\nval loss: 0.03731460273265839\nval DSC: 0.9781872034072876\nTime: 2.0328261852264404\n\nEpoch 63/400\ntrain loss: 0.05228102793459032\ntrain DSC: 0.9694452285766602\nval loss: 0.03863914906978607\nval DSC: 0.9766340255737305\nTime: 2.0220720767974854\n\nEpoch 64/400\ntrain loss: 0.051598520552525756\ntrain DSC: 0.9686037302017212\nval loss: 0.035393691062927245\nval DSC: 0.9783145785331726\nTime: 2.1477088928222656\n\nEpoch 65/400\ntrain loss: 0.04898236227817223\ntrain DSC: 0.9705974459648132\nval loss: 0.03370743989944458\nval DSC: 0.9799726605415344\nTime: 2.044008255004883\n\nEpoch 66/400\ntrain loss: 0.04902011351507218\ntrain DSC: 0.9691563248634338\nval loss: 0.03311656713485718\nval DSC: 0.9777806401252747\nTime: 2.0830843448638916\n\nEpoch 67/400\ntrain loss: 0.04463743284100392\ntrain DSC: 0.9734654426574707\nval loss: 0.030765128135681153\nval DSC: 0.9815473556518555\nTime: 2.072420120239258\n\nEpoch 68/400\ntrain loss: 0.04357476703456191\ntrain DSC: 0.9734557867050171\nval loss: 0.032105690240859984\nval DSC: 0.9777930378913879\nTime: 2.0737483501434326\n\nEpoch 69/400\ntrain loss: 0.04312492198631412\ntrain DSC: 0.9731148481369019\nval loss: 0.03331847190856933\nval DSC: 0.9755153656005859\nTime: 2.0573716163635254\n\nEpoch 70/400\ntrain loss: 0.041124109361992504\ntrain DSC: 0.9746968746185303\nval loss: 0.029917287826538085\nval DSC: 0.9790928959846497\nTime: 2.0688271522521973\n\nEpoch 71/400\ntrain loss: 0.04011510727835483\ntrain DSC: 0.9746456742286682\nval loss: 0.029646795988082886\nval DSC: 0.9797605276107788\nTime: 2.059990882873535\n\nEpoch 72/400\ntrain loss: 0.04012622989592005\ntrain DSC: 0.9738344550132751\nval loss: 0.02884332835674286\nval DSC: 0.9800424575805664\nTime: 2.0088260173797607\n\nEpoch 73/400\ntrain loss: 0.03905111062722128\ntrain DSC: 0.9742856621742249\nval loss: 0.02786179482936859\nval DSC: 0.9807788729667664\nTime: 2.0604662895202637\n\nEpoch 74/400\ntrain loss: 0.03679650240257138\ntrain DSC: 0.9762417078018188\nval loss: 0.025986570119857787\nval DSC: 0.9818496704101562\nTime: 2.0491647720336914\n\nEpoch 75/400\ntrain loss: 0.0366736855663237\ntrain DSC: 0.9755510091781616\nval loss: 0.025658810138702394\nval DSC: 0.9811565279960632\nTime: 2.0370423793792725\n\nEpoch 76/400\ntrain loss: 0.03532895787817533\ntrain DSC: 0.976462721824646\nval loss: 0.02377713918685913\nval DSC: 0.9831314086914062\nTime: 2.0837926864624023\n\nEpoch 77/400\ntrain loss: 0.0346703079880261\ntrain DSC: 0.9767879247665405\nval loss: 0.03618883192539215\nval DSC: 0.9658921957015991\nTime: 2.0333969593048096\n\nEpoch 78/400\ntrain loss: 0.03494771679893869\ntrain DSC: 0.9755828380584717\nval loss: 0.023153743147850035\nval DSC: 0.9827926754951477\nTime: 2.040701389312744\n\nEpoch 79/400\ntrain loss: 0.03262249758986176\ntrain DSC: 0.9778369665145874\nval loss: 0.023476237058639528\nval DSC: 0.9826908111572266\nTime: 2.083174228668213\n\nEpoch 80/400\ntrain loss: 0.032554104679920634\ntrain DSC: 0.9776289463043213\nval loss: 0.02222340404987335\nval DSC: 0.983350932598114\nTime: 2.031973123550415\n\nEpoch 81/400\ntrain loss: 0.031533936008078155\ntrain DSC: 0.977877676486969\nval loss: 0.022007566690444947\nval DSC: 0.9833534359931946\nTime: 2.0384933948516846\n\nEpoch 82/400\ntrain loss: 0.030034476616343515\ntrain DSC: 0.979315996170044\nval loss: 0.021021950244903564\nval DSC: 0.9836450815200806\nTime: 2.071655035018921\n\nEpoch 83/400\ntrain loss: 0.029312100566801478\ntrain DSC: 0.9797412157058716\nval loss: 0.021226051449775695\nval DSC: 0.9836677312850952\nTime: 2.0738494396209717\n\nEpoch 84/400\ntrain loss: 0.02889509572357428\ntrain DSC: 0.9796079397201538\nval loss: 0.023049721121788026\nval DSC: 0.9813613891601562\nTime: 2.067765474319458\n\nEpoch 85/400\ntrain loss: 0.029642852603412064\ntrain DSC: 0.9782304763793945\nval loss: 0.020909377932548524\nval DSC: 0.9832625389099121\nTime: 2.145469903945923\n\nEpoch 86/400\ntrain loss: 0.028848267969537954\ntrain DSC: 0.9789487719535828\nval loss: 0.019932821393013\nval DSC: 0.983443558216095\nTime: 2.035492181777954\n\nEpoch 87/400\ntrain loss: 0.02730479787607662\ntrain DSC: 0.9801973104476929\nval loss: 0.01994435489177704\nval DSC: 0.9821707010269165\nTime: 2.089144706726074\n\nEpoch 88/400\ntrain loss: 0.027358463553131603\ntrain DSC: 0.9795210361480713\nval loss: 0.019824784994125367\nval DSC: 0.9840280413627625\nTime: 2.084249258041382\n\nEpoch 89/400\ntrain loss: 0.026120959735307536\ntrain DSC: 0.9809491634368896\nval loss: 0.018678367137908936\nval DSC: 0.9841554760932922\nTime: 2.0486414432525635\n\nEpoch 90/400\ntrain loss: 0.02545652135473783\ntrain DSC: 0.9812117218971252\nval loss: 0.019633516669273376\nval DSC: 0.9836162328720093\nTime: 2.0486342906951904\n\nEpoch 91/400\ntrain loss: 0.024762320713918717\ntrain DSC: 0.9817728996276855\nval loss: 0.019648295640945435\nval DSC: 0.9824390411376953\nTime: 2.0864624977111816\n\nEpoch 92/400\ntrain loss: 0.025687832324231257\ntrain DSC: 0.9802456498146057\nval loss: 0.019637349247932433\nval DSC: 0.9828190803527832\nTime: 2.039890766143799\n\nEpoch 93/400\ntrain loss: 0.02499414174283137\ntrain DSC: 0.9808560013771057\nval loss: 0.02010866403579712\nval DSC: 0.9799026250839233\nTime: 2.0470550060272217\n\nEpoch 94/400\ntrain loss: 0.02460814890314321\ntrain DSC: 0.9809316396713257\nval loss: 0.018310606479644775\nval DSC: 0.9820750951766968\nTime: 2.0710666179656982\n\nEpoch 95/400\ntrain loss: 0.023661813775046926\ntrain DSC: 0.9815778732299805\nval loss: 0.01803310215473175\nval DSC: 0.9840428233146667\nTime: 2.0342092514038086\n\nEpoch 96/400\ntrain loss: 0.02344231429647227\ntrain DSC: 0.9815194010734558\nval loss: 0.01948793828487396\nval DSC: 0.9829452633857727\nTime: 2.0322418212890625\n\nEpoch 97/400\ntrain loss: 0.023183980926138457\ntrain DSC: 0.9815678596496582\nval loss: 0.017487898468971252\nval DSC: 0.984603762626648\nTime: 2.06132173538208\n\nEpoch 98/400\ntrain loss: 0.022747181478093882\ntrain DSC: 0.9818841814994812\nval loss: 0.017152175307273865\nval DSC: 0.9833323359489441\nTime: 2.0830488204956055\n\nEpoch 99/400\ntrain loss: 0.02138580943717331\ntrain DSC: 0.983167290687561\nval loss: 0.016117414832115172\nval DSC: 0.9848718643188477\nTime: 2.1254355907440186\n\nEpoch 100/400\ntrain loss: 0.021833107119700948\ntrain DSC: 0.9824993014335632\nval loss: 0.017565953731536865\nval DSC: 0.9817022085189819\nTime: 2.049471139907837\n\nEpoch 101/400\ntrain loss: 0.021884901601760112\ntrain DSC: 0.982024610042572\nval loss: 0.017388612031936646\nval DSC: 0.9821964502334595\nTime: 2.065023183822632\n\nEpoch 102/400\ntrain loss: 0.02106069932218458\ntrain DSC: 0.9828547835350037\nval loss: 0.01594572365283966\nval DSC: 0.9845630526542664\nTime: 2.0909581184387207\n\nEpoch 103/400\ntrain loss: 0.0212956217468762\ntrain DSC: 0.9821063280105591\nval loss: 0.015857234597206116\nval DSC: 0.9850090146064758\nTime: 2.1394712924957275\n\nEpoch 104/400\ntrain loss: 0.020418383058954458\ntrain DSC: 0.9832149744033813\nval loss: 0.017046579718589784\nval DSC: 0.9816476702690125\nTime: 2.1322805881500244\n\nEpoch 105/400\ntrain loss: 0.020973293507685425\ntrain DSC: 0.9820961952209473\nval loss: 0.01714547872543335\nval DSC: 0.9839673042297363\nTime: 2.100130796432495\n\nEpoch 106/400\ntrain loss: 0.019592066280177383\ntrain DSC: 0.9837567210197449\nval loss: 0.0166395902633667\nval DSC: 0.9843063354492188\nTime: 2.097825765609741\n\nEpoch 107/400\ntrain loss: 0.020431708117000392\ntrain DSC: 0.9824487566947937\nval loss: 0.018491727113723756\nval DSC: 0.9820035696029663\nTime: 2.0554826259613037\n\nEpoch 108/400\ntrain loss: 0.019732649209069423\ntrain DSC: 0.9830462336540222\nval loss: 0.016234710812568665\nval DSC: 0.9832164645195007\nTime: 2.0632131099700928\n\nEpoch 109/400\ntrain loss: 0.022076844191942058\ntrain DSC: 0.9801405072212219\nval loss: 0.01721598207950592\nval DSC: 0.9812339544296265\nTime: 2.05780291557312\n\nEpoch 110/400\ntrain loss: 0.019249745079728424\ntrain DSC: 0.983272135257721\nval loss: 0.01624869704246521\nval DSC: 0.9829353094100952\nTime: 2.0959742069244385\n\nEpoch 111/400\ntrain loss: 0.018901711604634268\ntrain DSC: 0.9834868907928467\nval loss: 0.014767834544181823\nval DSC: 0.9843562841415405\nTime: 2.101346731185913\n\nEpoch 112/400\ntrain loss: 0.02035374719588483\ntrain DSC: 0.9815971255302429\nval loss: 0.01932440996170044\nval DSC: 0.9812841415405273\nTime: 2.1079611778259277\n\nEpoch 113/400\ntrain loss: 0.019012882084142965\ntrain DSC: 0.9830026030540466\nval loss: 0.014273592829704284\nval DSC: 0.984998345375061\nTime: 2.070279359817505\n\nEpoch 114/400\ntrain loss: 0.01817152148387471\ntrain DSC: 0.9838119745254517\nval loss: 0.01844431459903717\nval DSC: 0.9819369316101074\nTime: 2.0635581016540527\n\nEpoch 115/400\ntrain loss: 0.017680430998567673\ntrain DSC: 0.9842748045921326\nval loss: 0.014916133880615235\nval DSC: 0.9842411875724792\nTime: 2.1615042686462402\n\nEpoch 116/400\ntrain loss: 0.01776055625227631\ntrain DSC: 0.984041690826416\nval loss: 0.014233970642089843\nval DSC: 0.9840154647827148\nTime: 2.0514729022979736\n\nEpoch 117/400\ntrain loss: 0.017028919008911632\ntrain DSC: 0.9845544695854187\nval loss: 0.014240097999572755\nval DSC: 0.9840633273124695\nTime: 2.083761215209961\n\nEpoch 118/400\ntrain loss: 0.017115599796420237\ntrain DSC: 0.9846295714378357\nval loss: 0.013544058799743653\nval DSC: 0.9853568077087402\nTime: 2.1251516342163086\n\nEpoch 119/400\ntrain loss: 0.016677565261965892\ntrain DSC: 0.9848679304122925\nval loss: 0.014321976900100708\nval DSC: 0.985444188117981\nTime: 2.028036117553711\n\nEpoch 120/400\ntrain loss: 0.016495906915821014\ntrain DSC: 0.9850825071334839\nval loss: 0.015703827142715454\nval DSC: 0.9832057952880859\nTime: 2.059737205505371\n\nEpoch 121/400\ntrain loss: 0.016830880133832087\ntrain DSC: 0.9844731688499451\nval loss: 0.013317769765853882\nval DSC: 0.9853189587593079\nTime: 2.0395054817199707\n\nEpoch 122/400\ntrain loss: 0.016450618134170283\ntrain DSC: 0.9847376942634583\nval loss: 0.013834607601165772\nval DSC: 0.9851255416870117\nTime: 2.109480619430542\n\nEpoch 123/400\ntrain loss: 0.01752029383768801\ntrain DSC: 0.9834043979644775\nval loss: 0.015019327402114868\nval DSC: 0.9845539927482605\nTime: 2.0651285648345947\n\nEpoch 124/400\ntrain loss: 0.01643520789068253\ntrain DSC: 0.9845929741859436\nval loss: 0.013260203599929809\nval DSC: 0.9850282669067383\nTime: 2.0556745529174805\n\nEpoch 125/400\ntrain loss: 0.015896110261072877\ntrain DSC: 0.9851440787315369\nval loss: 0.0143390953540802\nval DSC: 0.9848178625106812\nTime: 2.0519285202026367\n\nEpoch 126/400\ntrain loss: 0.016424247475921132\ntrain DSC: 0.9844196438789368\nval loss: 0.01596450209617615\nval DSC: 0.9821043014526367\nTime: 2.1090810298919678\n\nEpoch 127/400\ntrain loss: 0.017030265487608363\ntrain DSC: 0.9834242463111877\nval loss: 0.016138964891433717\nval DSC: 0.9801532626152039\nTime: 2.162968158721924\n\nEpoch 128/400\ntrain loss: 0.016417433003910253\ntrain DSC: 0.9841890335083008\nval loss: 0.013068380951881408\nval DSC: 0.9850301742553711\nTime: 2.043593645095825\n\nEpoch 129/400\ntrain loss: 0.01515594955350532\ntrain DSC: 0.9855169653892517\nval loss: 0.012992742657661437\nval DSC: 0.9858285784721375\nTime: 2.0481557846069336\n\nEpoch 130/400\ntrain loss: 0.015280045446802358\ntrain DSC: 0.9854304194450378\nval loss: 0.01347063183784485\nval DSC: 0.9846624135971069\nTime: 2.1576340198516846\n\nEpoch 131/400\ntrain loss: 0.01635432243347168\ntrain DSC: 0.9838903546333313\nval loss: 0.013829830288887023\nval DSC: 0.9853927493095398\nTime: 2.067004919052124\n\nEpoch 132/400\ntrain loss: 0.015383492727748683\ntrain DSC: 0.9850289821624756\nval loss: 0.013035726547241212\nval DSC: 0.9859005212783813\nTime: 2.098752737045288\n\nEpoch 133/400\ntrain loss: 0.014968671759621043\ntrain DSC: 0.9853800535202026\nval loss: 0.013056647777557374\nval DSC: 0.9857456088066101\nTime: 2.0845065116882324\n\nEpoch 134/400\ntrain loss: 0.01537955491269221\ntrain DSC: 0.9847640991210938\nval loss: 0.012735792994499206\nval DSC: 0.9848299026489258\nTime: 2.0258677005767822\n\nEpoch 135/400\ntrain loss: 0.014533722986940478\ntrain DSC: 0.9858152270317078\nval loss: 0.01353917121887207\nval DSC: 0.9848067164421082\nTime: 2.0612618923187256\n\nEpoch 136/400\ntrain loss: 0.014718413352966309\ntrain DSC: 0.9854128360748291\nval loss: 0.01239345371723175\nval DSC: 0.9857147336006165\nTime: 2.0823326110839844\n\nEpoch 137/400\ntrain loss: 0.014571492789221591\ntrain DSC: 0.9855425357818604\nval loss: 0.01244385540485382\nval DSC: 0.9858730435371399\nTime: 2.0283491611480713\n\nEpoch 138/400\ntrain loss: 0.015402490975426847\ntrain DSC: 0.9844151735305786\nval loss: 0.01371762454509735\nval DSC: 0.9833331108093262\nTime: 2.055567502975464\n\nEpoch 139/400\ntrain loss: 0.01423468355272637\ntrain DSC: 0.9858002066612244\nval loss: 0.013098806142807007\nval DSC: 0.9845172762870789\nTime: 2.0905191898345947\n\nEpoch 140/400\ntrain loss: 0.014930963516235352\ntrain DSC: 0.9849397540092468\nval loss: 0.013274800777435303\nval DSC: 0.9836997985839844\nTime: 2.0400712490081787\n\nEpoch 141/400\ntrain loss: 0.015042643078037949\ntrain DSC: 0.984527051448822\nval loss: 0.012360399961471558\nval DSC: 0.9859646558761597\nTime: 2.0349643230438232\n\nEpoch 142/400\ntrain loss: 0.01414648431246398\ntrain DSC: 0.9857025146484375\nval loss: 0.01365385353565216\nval DSC: 0.9841972589492798\nTime: 2.0837182998657227\n\nEpoch 143/400\ntrain loss: 0.014238819724223653\ntrain DSC: 0.9854247570037842\nval loss: 0.012059015035629273\nval DSC: 0.9854611158370972\nTime: 2.067974805831909\n\nEpoch 144/400\ntrain loss: 0.013689333298167244\ntrain DSC: 0.9860071539878845\nval loss: 0.013404864072799682\nval DSC: 0.9850757718086243\nTime: 2.0176515579223633\n\nEpoch 145/400\ntrain loss: 0.014137034533453769\ntrain DSC: 0.9854698777198792\nval loss: 0.013054978847503663\nval DSC: 0.9845573306083679\nTime: 2.1187655925750732\n\nEpoch 146/400\ntrain loss: 0.013656487230394707\ntrain DSC: 0.9859459400177002\nval loss: 0.011911365389823913\nval DSC: 0.9857295751571655\nTime: 2.097322702407837\n\nEpoch 147/400\ntrain loss: 0.0136407555126753\ntrain DSC: 0.9859787225723267\nval loss: 0.012312796711921693\nval DSC: 0.9861063957214355\nTime: 2.048525094985962\n\nEpoch 148/400\ntrain loss: 0.013401520056802719\ntrain DSC: 0.9862167835235596\nval loss: 0.01261439621448517\nval DSC: 0.9855931997299194\nTime: 2.0801851749420166\n\nEpoch 149/400\ntrain loss: 0.01304357853092131\ntrain DSC: 0.9865427017211914\nval loss: 0.012795618176460266\nval DSC: 0.9846469759941101\nTime: 2.064192533493042\n\nEpoch 150/400\ntrain loss: 0.013888838838358394\ntrain DSC: 0.9853503108024597\nval loss: 0.012645959854125977\nval DSC: 0.9857974052429199\nTime: 2.0739328861236572\n\nEpoch 151/400\ntrain loss: 0.013231016573358755\ntrain DSC: 0.9861273765563965\nval loss: 0.012975913286209107\nval DSC: 0.9852991104125977\nTime: 2.0896620750427246\n\nEpoch 152/400\ntrain loss: 0.013330460571851886\ntrain DSC: 0.9860612750053406\nval loss: 0.012238961458206177\nval DSC: 0.9850786328315735\nTime: 2.0294151306152344\n\nEpoch 153/400\ntrain loss: 0.012766470674608574\ntrain DSC: 0.986613929271698\nval loss: 0.011874786019325257\nval DSC: 0.9855060577392578\nTime: 2.1195898056030273\n\nEpoch 154/400\ntrain loss: 0.013169806511675725\ntrain DSC: 0.985983669757843\nval loss: 0.012051448225975037\nval DSC: 0.9857890009880066\nTime: 2.114093542098999\n\nEpoch 155/400\ntrain loss: 0.013178594776841461\ntrain DSC: 0.9859741926193237\nval loss: 0.011717340350151062\nval DSC: 0.9861021041870117\nTime: 2.051757335662842\n\nEpoch 156/400\ntrain loss: 0.012521204401235112\ntrain DSC: 0.9869086742401123\nval loss: 0.011779990792274476\nval DSC: 0.9860948324203491\nTime: 2.0481436252593994\n\nEpoch 157/400\ntrain loss: 0.012719068370881628\ntrain DSC: 0.9864795804023743\nval loss: 0.01422262191772461\nval DSC: 0.9842607378959656\nTime: 2.0869345664978027\n\nEpoch 158/400\ntrain loss: 0.01262628836709945\ntrain DSC: 0.986565887928009\nval loss: 0.01188424825668335\nval DSC: 0.9855000376701355\nTime: 2.1244924068450928\n\nEpoch 159/400\ntrain loss: 0.012495987727993825\ntrain DSC: 0.9867059588432312\nval loss: 0.011786127090454101\nval DSC: 0.9848282933235168\nTime: 2.0985183715820312\n\nEpoch 160/400\ntrain loss: 0.012583689611466204\ntrain DSC: 0.9864394664764404\nval loss: 0.011805248260498048\nval DSC: 0.9848036766052246\nTime: 2.0868289470672607\n\nEpoch 161/400\ntrain loss: 0.012651852896956146\ntrain DSC: 0.9862719774246216\nval loss: 0.011418774724006653\nval DSC: 0.9858325719833374\nTime: 2.0771708488464355\n\nEpoch 162/400\ntrain loss: 0.012195496285547976\ntrain DSC: 0.9868974685668945\nval loss: 0.011385303735733033\nval DSC: 0.9859334826469421\nTime: 2.0581107139587402\n\nEpoch 163/400\ntrain loss: 0.011999367690477216\ntrain DSC: 0.9870865345001221\nval loss: 0.0129913330078125\nval DSC: 0.9850953221321106\nTime: 2.1240484714508057\n\nEpoch 164/400\ntrain loss: 0.012084987319883753\ntrain DSC: 0.9868664741516113\nval loss: 0.011609289050102233\nval DSC: 0.9857661128044128\nTime: 2.108865976333618\n\nEpoch 165/400\ntrain loss: 0.012305196191443771\ntrain DSC: 0.9865776300430298\nval loss: 0.01168881356716156\nval DSC: 0.9847151041030884\nTime: 2.0622782707214355\n\nEpoch 166/400\ntrain loss: 0.01198813856625166\ntrain DSC: 0.986958384513855\nval loss: 0.011915260553359985\nval DSC: 0.9853059649467468\nTime: 2.139862060546875\n\nEpoch 167/400\ntrain loss: 0.011903886912298984\ntrain DSC: 0.9870103597640991\nval loss: 0.011403355002403259\nval DSC: 0.9855192303657532\nTime: 2.1406924724578857\n\nEpoch 168/400\ntrain loss: 0.012083753210599305\ntrain DSC: 0.9867766499519348\nval loss: 0.011311861872673034\nval DSC: 0.9857168197631836\nTime: 2.225330114364624\n\nEpoch 169/400\ntrain loss: 0.01174177107263784\ntrain DSC: 0.9870287775993347\nval loss: 0.011267450451850892\nval DSC: 0.9857051968574524\nTime: 2.310370445251465\n\nEpoch 170/400\ntrain loss: 0.011904327595820193\ntrain DSC: 0.9870070219039917\nval loss: 0.011365082859992982\nval DSC: 0.9853981137275696\nTime: 2.065464735031128\n\nEpoch 171/400\ntrain loss: 0.011931524902093605\ntrain DSC: 0.9867803454399109\nval loss: 0.01151842176914215\nval DSC: 0.9858261346817017\nTime: 2.0714879035949707\n\nEpoch 172/400\ntrain loss: 0.01165972869904315\ntrain DSC: 0.9871681332588196\nval loss: 0.01137838065624237\nval DSC: 0.9850261807441711\nTime: 2.22493577003479\n\nEpoch 173/400\ntrain loss: 0.011481516673916677\ntrain DSC: 0.9873651266098022\nval loss: 0.011114329099655151\nval DSC: 0.9862771034240723\nTime: 2.101121664047241\n\nEpoch 174/400\ntrain loss: 0.01151959720205088\ntrain DSC: 0.987183153629303\nval loss: 0.011143121123313903\nval DSC: 0.9858726263046265\nTime: 2.075648546218872\n\nEpoch 175/400\ntrain loss: 0.011605589116205935\ntrain DSC: 0.9871554374694824\nval loss: 0.01152108609676361\nval DSC: 0.9858617782592773\nTime: 2.0917325019836426\n\nEpoch 176/400\ntrain loss: 0.013044393453441683\ntrain DSC: 0.9852708578109741\nval loss: 0.014243561029434203\nval DSC: 0.9808855056762695\nTime: 2.0693912506103516\n\nEpoch 177/400\ntrain loss: 0.012656186447768916\ntrain DSC: 0.985871434211731\nval loss: 0.013115549087524414\nval DSC: 0.9847362637519836\nTime: 2.0624587535858154\n\nEpoch 178/400\ntrain loss: 0.011652651380320064\ntrain DSC: 0.9868208169937134\nval loss: 0.011443802714347839\nval DSC: 0.98625648021698\nTime: 2.122725009918213\n\nEpoch 179/400\ntrain loss: 0.011718585842945536\ntrain DSC: 0.986736536026001\nval loss: 0.013592153787612915\nval DSC: 0.9842566251754761\nTime: 2.0660226345062256\n\nEpoch 180/400\ntrain loss: 0.011812646858027725\ntrain DSC: 0.9867836236953735\nval loss: 0.01200600266456604\nval DSC: 0.9848146438598633\nTime: 2.076627254486084\n\nEpoch 181/400\ntrain loss: 0.011652790132116099\ntrain DSC: 0.9869283437728882\nval loss: 0.014470231533050538\nval DSC: 0.980324923992157\nTime: 2.0723633766174316\n\nEpoch 182/400\ntrain loss: 0.011794512389136142\ntrain DSC: 0.9865460395812988\nval loss: 0.011234012246131898\nval DSC: 0.986009418964386\nTime: 2.1069512367248535\n\nEpoch 183/400\ntrain loss: 0.011676704297300245\ntrain DSC: 0.9866610169410706\nval loss: 0.013161680102348328\nval DSC: 0.9849294424057007\nTime: 2.0455434322357178\n\nEpoch 184/400\ntrain loss: 0.011324762320909345\ntrain DSC: 0.9872358441352844\nval loss: 0.011260914802551269\nval DSC: 0.9855979681015015\nTime: 2.1300132274627686\n\nEpoch 185/400\ntrain loss: 0.011409487880644251\ntrain DSC: 0.9870086908340454\nval loss: 0.01132698655128479\nval DSC: 0.9858527183532715\nTime: 2.0390639305114746\n\nEpoch 186/400\ntrain loss: 0.011128113895166115\ntrain DSC: 0.9872770309448242\nval loss: 0.011018452048301697\nval DSC: 0.9863669276237488\nTime: 2.0493245124816895\n\nEpoch 187/400\ntrain loss: 0.010904941402497838\ntrain DSC: 0.9875504970550537\nval loss: 0.011044326424598693\nval DSC: 0.9854650497436523\nTime: 2.2161200046539307\n\nEpoch 188/400\ntrain loss: 0.010836283691593857\ntrain DSC: 0.9875484108924866\nval loss: 0.011286240816116334\nval DSC: 0.9853321313858032\nTime: 2.1383461952209473\n\nEpoch 189/400\ntrain loss: 0.011081289072505763\ntrain DSC: 0.9874022603034973\nval loss: 0.011032813787460327\nval DSC: 0.9862889051437378\nTime: 2.0879321098327637\n\nEpoch 190/400\ntrain loss: 0.01097582109638902\ntrain DSC: 0.9874212741851807\nval loss: 0.011570820212364196\nval DSC: 0.9848035573959351\nTime: 2.21407151222229\n\nEpoch 191/400\ntrain loss: 0.010865919902676442\ntrain DSC: 0.9875527024269104\nval loss: 0.010988080501556396\nval DSC: 0.9857366681098938\nTime: 2.113708019256592\n\nEpoch 192/400\ntrain loss: 0.010858290508145192\ntrain DSC: 0.9874680042266846\nval loss: 0.010821598768234252\nval DSC: 0.9858666658401489\nTime: 2.0702404975891113\n\nEpoch 193/400\ntrain loss: 0.01087391278782829\ntrain DSC: 0.9874527454376221\nval loss: 0.011214545369148255\nval DSC: 0.9855947494506836\nTime: 2.3299224376678467\n\nEpoch 194/400\ntrain loss: 0.010656236625108563\ntrain DSC: 0.9877418279647827\nval loss: 0.01124768853187561\nval DSC: 0.9852529764175415\nTime: 2.104081869125366\n\nEpoch 195/400\ntrain loss: 0.01087700538947934\ntrain DSC: 0.9873676896095276\nval loss: 0.010893100500106811\nval DSC: 0.985923171043396\nTime: 2.113760232925415\n\nEpoch 196/400\ntrain loss: 0.010742342862926546\ntrain DSC: 0.9875273108482361\nval loss: 0.01105961799621582\nval DSC: 0.9861000776290894\nTime: 2.095294237136841\n\nEpoch 197/400\ntrain loss: 0.010563064794071386\ntrain DSC: 0.9877864718437195\nval loss: 0.011174705624580384\nval DSC: 0.9862104654312134\nTime: 2.101682424545288\n\nEpoch 198/400\ntrain loss: 0.010612610910759598\ntrain DSC: 0.987615168094635\nval loss: 0.0111756831407547\nval DSC: 0.9861308932304382\nTime: 2.0545477867126465\n\nEpoch 199/400\ntrain loss: 0.010654102583400538\ntrain DSC: 0.9877733588218689\nval loss: 0.01108231246471405\nval DSC: 0.985278308391571\nTime: 2.0736310482025146\n\nEpoch 200/400\ntrain loss: 0.010401756059928019\ntrain DSC: 0.9879613518714905\nval loss: 0.010902050137519836\nval DSC: 0.9861947298049927\nTime: 2.0293071269989014\n\nEpoch 201/400\ntrain loss: 0.01057456751338771\ntrain DSC: 0.9876565933227539\nval loss: 0.010788235068321227\nval DSC: 0.985785961151123\nTime: 2.231816053390503\n\nEpoch 202/400\ntrain loss: 0.010552840154679095\ntrain DSC: 0.987725555896759\nval loss: 0.01082051694393158\nval DSC: 0.9864408373832703\nTime: 2.0859177112579346\n\nEpoch 203/400\ntrain loss: 0.010377034789226094\ntrain DSC: 0.9878942966461182\nval loss: 0.01087794303894043\nval DSC: 0.9859954118728638\nTime: 2.1048312187194824\n\nEpoch 204/400\ntrain loss: 0.010582108966639785\ntrain DSC: 0.987616777420044\nval loss: 0.011233776807785034\nval DSC: 0.9859100580215454\nTime: 2.041837453842163\n\nEpoch 205/400\ntrain loss: 0.010298281419472616\ntrain DSC: 0.9879269599914551\nval loss: 0.010765713453292847\nval DSC: 0.9861453771591187\nTime: 2.1590986251831055\n\nEpoch 206/400\ntrain loss: 0.010674634917837674\ntrain DSC: 0.9874796867370605\nval loss: 0.011376595497131348\nval DSC: 0.9862213134765625\nTime: 2.116649627685547\n\nEpoch 207/400\ntrain loss: 0.010635239179017113\ntrain DSC: 0.9874465465545654\nval loss: 0.010780754685401916\nval DSC: 0.9856588244438171\nTime: 2.072922706604004\n\nEpoch 208/400\ntrain loss: 0.010480437122407506\ntrain DSC: 0.9876044988632202\nval loss: 0.01097027063369751\nval DSC: 0.9860925674438477\nTime: 2.1022629737854004\n\nEpoch 209/400\ntrain loss: 0.01028624909823058\ntrain DSC: 0.987958550453186\nval loss: 0.010823982954025268\nval DSC: 0.9857090711593628\nTime: 2.145920991897583\n\nEpoch 210/400\ntrain loss: 0.010267950472284536\ntrain DSC: 0.9878743290901184\nval loss: 0.010644209384918214\nval DSC: 0.986209511756897\nTime: 2.0592355728149414\n\nEpoch 211/400\ntrain loss: 0.010130948707705638\ntrain DSC: 0.9880750775337219\nval loss: 0.010648787021636963\nval DSC: 0.9861959218978882\nTime: 2.122871160507202\n\nEpoch 212/400\ntrain loss: 0.009933227398356453\ntrain DSC: 0.9882336258888245\nval loss: 0.01070958971977234\nval DSC: 0.9860307574272156\nTime: 2.0350229740142822\n\nEpoch 213/400\ntrain loss: 0.010661456428590368\ntrain DSC: 0.9874854683876038\nval loss: 0.010881850123405456\nval DSC: 0.9857372045516968\nTime: 2.0909242630004883\n\nEpoch 214/400\ntrain loss: 0.010609408871072238\ntrain DSC: 0.9873353838920593\nval loss: 0.010688316822052003\nval DSC: 0.9861106872558594\nTime: 2.1899256706237793\n\nEpoch 215/400\ntrain loss: 0.010390534752705058\ntrain DSC: 0.987631618976593\nval loss: 0.01063140332698822\nval DSC: 0.9861406087875366\nTime: 2.0839550495147705\n\nEpoch 216/400\ntrain loss: 0.010391369217731913\ntrain DSC: 0.9875831604003906\nval loss: 0.010853204131126403\nval DSC: 0.9860456585884094\nTime: 2.06595516204834\n\nEpoch 217/400\ntrain loss: 0.01007844190128514\ntrain DSC: 0.9879863262176514\nval loss: 0.011247566342353821\nval DSC: 0.9858217239379883\nTime: 2.1602606773376465\n\nEpoch 218/400\ntrain loss: 0.010089178554347305\ntrain DSC: 0.9880450367927551\nval loss: 0.010809913277626038\nval DSC: 0.985369861125946\nTime: 2.0772705078125\n\nEpoch 219/400\ntrain loss: 0.010168875827163946\ntrain DSC: 0.987825870513916\nval loss: 0.010719227790832519\nval DSC: 0.9857580065727234\nTime: 2.0820703506469727\n\nEpoch 220/400\ntrain loss: 0.01002601619626655\ntrain DSC: 0.9879972338676453\nval loss: 0.010494813323020935\nval DSC: 0.9865506291389465\nTime: 2.0936386585235596\n\nEpoch 221/400\ntrain loss: 0.010025498319844731\ntrain DSC: 0.9880386590957642\nval loss: 0.01071062684059143\nval DSC: 0.9858217239379883\nTime: 2.091123580932617\n\nEpoch 222/400\ntrain loss: 0.010023735585759898\ntrain DSC: 0.9879692196846008\nval loss: 0.010754397511482239\nval DSC: 0.9858626127243042\nTime: 2.109523057937622\n\nEpoch 223/400\ntrain loss: 0.009964681062542025\ntrain DSC: 0.9880602359771729\nval loss: 0.010568270087242126\nval DSC: 0.9861693382263184\nTime: 2.080533027648926\n\nEpoch 224/400\ntrain loss: 0.009993188693875173\ntrain DSC: 0.9880022406578064\nval loss: 0.010607022047042846\nval DSC: 0.9861897230148315\nTime: 2.0390255451202393\n\nEpoch 225/400\ntrain loss: 0.009936544738832067\ntrain DSC: 0.988043487071991\nval loss: 0.010564297437667847\nval DSC: 0.9862947463989258\nTime: 2.088046073913574\n\nEpoch 226/400\ntrain loss: 0.009932591289770408\ntrain DSC: 0.9880549907684326\nval loss: 0.010551819205284118\nval DSC: 0.9863084554672241\nTime: 2.0608487129211426\n\nEpoch 227/400\ntrain loss: 0.009882522411033755\ntrain DSC: 0.9881041646003723\nval loss: 0.011012551188468934\nval DSC: 0.9853447079658508\nTime: 2.1094422340393066\n\nEpoch 228/400\ntrain loss: 0.009868078544491628\ntrain DSC: 0.9881481528282166\nval loss: 0.010506761074066163\nval DSC: 0.9861427545547485\nTime: 2.108370304107666\n\nEpoch 229/400\ntrain loss: 0.010036266240917269\ntrain DSC: 0.987924337387085\nval loss: 0.01057039201259613\nval DSC: 0.9861281514167786\nTime: 2.089649200439453\n\nEpoch 230/400\ntrain loss: 0.009730054706823631\ntrain DSC: 0.9882248044013977\nval loss: 0.010424551367759705\nval DSC: 0.9865380525588989\nTime: 2.030292272567749\n\nEpoch 231/400\ntrain loss: 0.009887906371570025\ntrain DSC: 0.9880064725875854\nval loss: 0.010426726937294007\nval DSC: 0.9862679243087769\nTime: 2.0720367431640625\n\nEpoch 232/400\ntrain loss: 0.00988003074145708\ntrain DSC: 0.9880805015563965\nval loss: 0.010936513543128967\nval DSC: 0.985127329826355\nTime: 2.2350220680236816\n\nEpoch 233/400\ntrain loss: 0.00981107207595325\ntrain DSC: 0.9881297945976257\nval loss: 0.010411122441291809\nval DSC: 0.9863254427909851\nTime: 2.085193634033203\n\nEpoch 234/400\ntrain loss: 0.009677623139053095\ntrain DSC: 0.9883512258529663\nval loss: 0.010519883036613465\nval DSC: 0.9864844083786011\nTime: 2.0925166606903076\n\nEpoch 235/400\ntrain loss: 0.00992155270498307\ntrain DSC: 0.9879749417304993\nval loss: 0.01041606068611145\nval DSC: 0.9859841465950012\nTime: 2.171494245529175\n\nEpoch 236/400\ntrain loss: 0.009778242619311223\ntrain DSC: 0.9881978034973145\nval loss: 0.010804784297943116\nval DSC: 0.9858909845352173\nTime: 2.2042250633239746\n\nEpoch 237/400\ntrain loss: 0.00957747854170252\ntrain DSC: 0.9884299635887146\nval loss: 0.010938647389411926\nval DSC: 0.9848709106445312\nTime: 2.118398666381836\n\nEpoch 238/400\ntrain loss: 0.009669930231375773\ntrain DSC: 0.988261342048645\nval loss: 0.010491818189620972\nval DSC: 0.9861600995063782\nTime: 2.136674165725708\n\nEpoch 239/400\ntrain loss: 0.00955297516994789\ntrain DSC: 0.9883713126182556\nval loss: 0.01054232120513916\nval DSC: 0.9857633709907532\nTime: 2.079345464706421\n\nEpoch 240/400\ntrain loss: 0.009598313784990155\ntrain DSC: 0.9882262945175171\nval loss: 0.010476619005203247\nval DSC: 0.9857891201972961\nTime: 2.103746175765991\n\nEpoch 241/400\ntrain loss: 0.009709552663271545\ntrain DSC: 0.9881535172462463\nval loss: 0.010682758688926697\nval DSC: 0.9855670928955078\nTime: 2.097851037979126\n\nEpoch 242/400\ntrain loss: 0.009602400123095904\ntrain DSC: 0.9882795214653015\nval loss: 0.01106119155883789\nval DSC: 0.985664963722229\nTime: 2.1872994899749756\n\nEpoch 243/400\ntrain loss: 0.009898092903074671\ntrain DSC: 0.9879782199859619\nval loss: 0.010810905694961548\nval DSC: 0.9851466417312622\nTime: 2.1426475048065186\n\nEpoch 244/400\ntrain loss: 0.009589737556019768\ntrain DSC: 0.9883204102516174\nval loss: 0.010708525776863098\nval DSC: 0.9866379499435425\nTime: 2.2014219760894775\n\nEpoch 245/400\ntrain loss: 0.009561644225824074\ntrain DSC: 0.9883613586425781\nval loss: 0.010452067852020264\nval DSC: 0.9862993955612183\nTime: 2.0805463790893555\n\nEpoch 246/400\ntrain loss: 0.009570674818070208\ntrain DSC: 0.9882441759109497\nval loss: 0.01055167019367218\nval DSC: 0.9864969253540039\nTime: 2.066917657852173\n\nEpoch 247/400\ntrain loss: 0.009646942380998955\ntrain DSC: 0.9882059693336487\nval loss: 0.010339045524597168\nval DSC: 0.9864215850830078\nTime: 2.1221985816955566\n\nEpoch 248/400\ntrain loss: 0.009521382753966285\ntrain DSC: 0.988376796245575\nval loss: 0.010454511642456055\nval DSC: 0.9859689474105835\nTime: 2.0442819595336914\n\nEpoch 249/400\ntrain loss: 0.00952982316251661\ntrain DSC: 0.9884175658226013\nval loss: 0.010550561547279357\nval DSC: 0.9859622716903687\nTime: 2.1051442623138428\n\nEpoch 250/400\ntrain loss: 0.009453565370841105\ntrain DSC: 0.9883801937103271\nval loss: 0.010350093245506287\nval DSC: 0.9863625764846802\nTime: 2.118751287460327\n\nEpoch 251/400\ntrain loss: 0.009600471277705958\ntrain DSC: 0.9882587194442749\nval loss: 0.01065574586391449\nval DSC: 0.9856017231941223\nTime: 2.131683349609375\n\nEpoch 252/400\ntrain loss: 0.009550514768381587\ntrain DSC: 0.9882631301879883\nval loss: 0.010801681876182556\nval DSC: 0.9858158826828003\nTime: 2.0747218132019043\n\nEpoch 253/400\ntrain loss: 0.009447037196550214\ntrain DSC: 0.988466739654541\nval loss: 0.010442566871643067\nval DSC: 0.9859966039657593\nTime: 2.1550915241241455\n\nEpoch 254/400\ntrain loss: 0.009299852808967966\ntrain DSC: 0.9886327385902405\nval loss: 0.010294973850250244\nval DSC: 0.9865633845329285\nTime: 2.103996992111206\n\nEpoch 255/400\ntrain loss: 0.00955080008897625\ntrain DSC: 0.9881553649902344\nval loss: 0.01039678156375885\nval DSC: 0.986648440361023\nTime: 2.107341766357422\n\nEpoch 256/400\ntrain loss: 0.009594115077472124\ntrain DSC: 0.9882258176803589\nval loss: 0.010369402170181275\nval DSC: 0.986441433429718\nTime: 2.118961811065674\n\nEpoch 257/400\ntrain loss: 0.009409614273759186\ntrain DSC: 0.9884386658668518\nval loss: 0.010602343082427978\nval DSC: 0.9861539602279663\nTime: 2.1065614223480225\n\nEpoch 258/400\ntrain loss: 0.009319330825180303\ntrain DSC: 0.9884703159332275\nval loss: 0.010584968328475951\nval DSC: 0.9863296747207642\nTime: 2.0923662185668945\n\nEpoch 259/400\ntrain loss: 0.009412454777076596\ntrain DSC: 0.9883826971054077\nval loss: 0.010454884171485901\nval DSC: 0.986210823059082\nTime: 2.320857048034668\n\nEpoch 260/400\ntrain loss: 0.00939537853491111\ntrain DSC: 0.9884648323059082\nval loss: 0.010246241092681884\nval DSC: 0.9863046407699585\nTime: 2.0590431690216064\n\nEpoch 261/400\ntrain loss: 0.009240161200038722\ntrain DSC: 0.9886462092399597\nval loss: 0.010729959607124329\nval DSC: 0.9861236810684204\nTime: 2.125584363937378\n\nEpoch 262/400\ntrain loss: 0.009302584851374392\ntrain DSC: 0.9884923696517944\nval loss: 0.010728955268859863\nval DSC: 0.985661506652832\nTime: 2.103794574737549\n\nEpoch 263/400\ntrain loss: 0.00938278534373299\ntrain DSC: 0.9885066151618958\nval loss: 0.010449221730232239\nval DSC: 0.9858511090278625\nTime: 2.0975184440612793\n\nEpoch 264/400\ntrain loss: 0.00931883053701432\ntrain DSC: 0.9884932637214661\nval loss: 0.010417476296424866\nval DSC: 0.9859918355941772\nTime: 2.1312146186828613\n\nEpoch 265/400\ntrain loss: 0.009293519082616587\ntrain DSC: 0.9885392785072327\nval loss: 0.010247176885604859\nval DSC: 0.9862751960754395\nTime: 2.1029856204986572\n\nEpoch 266/400\ntrain loss: 0.009193617789471736\ntrain DSC: 0.9886534810066223\nval loss: 0.010338106751441955\nval DSC: 0.9861346483230591\nTime: 2.0789783000946045\n\nEpoch 267/400\ntrain loss: 0.009280963022200788\ntrain DSC: 0.9885053634643555\nval loss: 0.010285252332687378\nval DSC: 0.9864261746406555\nTime: 2.155366897583008\n\nEpoch 268/400\ntrain loss: 0.009352059637913938\ntrain DSC: 0.9883980751037598\nval loss: 0.010322609543800354\nval DSC: 0.9864004850387573\nTime: 2.104485273361206\n\nEpoch 269/400\ntrain loss: 0.009262559843845055\ntrain DSC: 0.9885769486427307\nval loss: 0.010379886627197266\nval DSC: 0.986149787902832\nTime: 2.1061720848083496\n\nEpoch 270/400\ntrain loss: 0.0093356761776033\ntrain DSC: 0.9883900284767151\nval loss: 0.010319745540618897\nval DSC: 0.9861103296279907\nTime: 2.043191432952881\n\nEpoch 271/400\ntrain loss: 0.009323603794222972\ntrain DSC: 0.9884328246116638\nval loss: 0.01039591133594513\nval DSC: 0.9866271018981934\nTime: 2.093993902206421\n\nEpoch 272/400\ntrain loss: 0.009206757193706075\ntrain DSC: 0.9885866641998291\nval loss: 0.010264462232589722\nval DSC: 0.986229419708252\nTime: 2.0540237426757812\n\nEpoch 273/400\ntrain loss: 0.009144944245698021\ntrain DSC: 0.9886925220489502\nval loss: 0.010369685292243958\nval DSC: 0.9859503507614136\nTime: 2.044266700744629\n\nEpoch 274/400\ntrain loss: 0.00918531515559212\ntrain DSC: 0.9886268377304077\nval loss: 0.010391131043434143\nval DSC: 0.9862492680549622\nTime: 2.1349618434906006\n\nEpoch 275/400\ntrain loss: 0.009100527059836466\ntrain DSC: 0.988736629486084\nval loss: 0.010418918728828431\nval DSC: 0.9860251545906067\nTime: 2.069242238998413\n\nEpoch 276/400\ntrain loss: 0.009328052645823995\ntrain DSC: 0.988369345664978\nval loss: 0.010305020213127136\nval DSC: 0.9864259958267212\nTime: 2.068368434906006\n\nEpoch 277/400\ntrain loss: 0.009185931721671682\ntrain DSC: 0.988580048084259\nval loss: 0.0103622168302536\nval DSC: 0.9867334365844727\nTime: 2.0949764251708984\n\nEpoch 278/400\ntrain loss: 0.009215014879820778\ntrain DSC: 0.9885377287864685\nval loss: 0.010229977965354919\nval DSC: 0.9864969253540039\nTime: 2.2255358695983887\n\nEpoch 279/400\ntrain loss: 0.009184707383640477\ntrain DSC: 0.9885791540145874\nval loss: 0.010286885499954223\nval DSC: 0.986126720905304\nTime: 2.0715532302856445\n\nEpoch 280/400\ntrain loss: 0.009081763322236107\ntrain DSC: 0.9886829853057861\nval loss: 0.010812380909919738\nval DSC: 0.9855257868766785\nTime: 2.0483860969543457\n\nEpoch 281/400\ntrain loss: 0.009273195852998828\ntrain DSC: 0.9885023832321167\nval loss: 0.010595673322677612\nval DSC: 0.9855588674545288\nTime: 2.023850679397583\n\nEpoch 282/400\ntrain loss: 0.009222352113880094\ntrain DSC: 0.988497257232666\nval loss: 0.010840219259262086\nval DSC: 0.9864069223403931\nTime: 2.0721731185913086\n\nEpoch 283/400\ntrain loss: 0.009181485801446633\ntrain DSC: 0.988579273223877\nval loss: 0.010416287183761596\nval DSC: 0.9863908886909485\nTime: 2.1065566539764404\n\nEpoch 284/400\ntrain loss: 0.009065890898470019\ntrain DSC: 0.9887686371803284\nval loss: 0.010302749276161195\nval DSC: 0.9859997034072876\nTime: 2.0748043060302734\n\nEpoch 285/400\ntrain loss: 0.00922035780109343\ntrain DSC: 0.9884783029556274\nval loss: 0.010514885187149048\nval DSC: 0.9864690899848938\nTime: 2.068894863128662\n\nEpoch 286/400\ntrain loss: 0.00906468708007062\ntrain DSC: 0.9886884093284607\nval loss: 0.01038576066493988\nval DSC: 0.9859286546707153\nTime: 2.100870132446289\n\nEpoch 287/400\ntrain loss: 0.009287317268183975\ntrain DSC: 0.988400936126709\nval loss: 0.010381752252578735\nval DSC: 0.9864794015884399\nTime: 2.028874158859253\n\nEpoch 288/400\ntrain loss: 0.009020265008582443\ntrain DSC: 0.9886982440948486\nval loss: 0.010393470525741577\nval DSC: 0.9859712719917297\nTime: 2.092722177505493\n\nEpoch 289/400\ntrain loss: 0.009001680084916412\ntrain DSC: 0.9887634515762329\nval loss: 0.010522216558456421\nval DSC: 0.9855412244796753\nTime: 2.052488327026367\n\nEpoch 290/400\ntrain loss: 0.009042366606290222\ntrain DSC: 0.9886934757232666\nval loss: 0.010326942801475525\nval DSC: 0.9865680932998657\nTime: 2.0433194637298584\n\nEpoch 291/400\ntrain loss: 0.009011164063312968\ntrain DSC: 0.9886686205863953\nval loss: 0.010410386323928832\nval DSC: 0.9863784909248352\nTime: 2.087874412536621\n\nEpoch 292/400\ntrain loss: 0.009053185337879618\ntrain DSC: 0.9887874126434326\nval loss: 0.010436499118804931\nval DSC: 0.986057460308075\nTime: 2.0487301349639893\n\nEpoch 293/400\ntrain loss: 0.0090964925093729\ntrain DSC: 0.9885400533676147\nval loss: 0.010448306798934937\nval DSC: 0.9856113195419312\nTime: 2.0737829208374023\n\nEpoch 294/400\ntrain loss: 0.009287211738648962\ntrain DSC: 0.9883834719657898\nval loss: 0.010337245464324952\nval DSC: 0.9858466982841492\nTime: 2.0337979793548584\n\nEpoch 295/400\ntrain loss: 0.008994151334293553\ntrain DSC: 0.9887125492095947\nval loss: 0.010415178537368775\nval DSC: 0.9865342378616333\nTime: 2.057680606842041\n\nEpoch 296/400\ntrain loss: 0.008987041770434771\ntrain DSC: 0.988765299320221\nval loss: 0.010224857926368713\nval DSC: 0.9864012598991394\nTime: 2.100804567337036\n\nEpoch 297/400\ntrain loss: 0.008886312852140333\ntrain DSC: 0.988947868347168\nval loss: 0.010308271646499634\nval DSC: 0.9862989187240601\nTime: 2.0447003841400146\n\nEpoch 298/400\ntrain loss: 0.008944870018568194\ntrain DSC: 0.9888195395469666\nval loss: 0.0103272944688797\nval DSC: 0.9863826632499695\nTime: 2.0778348445892334\n\nEpoch 299/400\ntrain loss: 0.008996077248307525\ntrain DSC: 0.9887616634368896\nval loss: 0.010317975282669067\nval DSC: 0.9863645434379578\nTime: 2.04449462890625\n\nEpoch 300/400\ntrain loss: 0.009027673572790428\ntrain DSC: 0.9886967539787292\nval loss: 0.010286664962768555\nval DSC: 0.9865491986274719\nTime: 2.029285430908203\n\nEpoch 301/400\ntrain loss: 0.00887985209949681\ntrain DSC: 0.9888894557952881\nval loss: 0.010285022854804992\nval DSC: 0.9866303205490112\nTime: 2.096299648284912\n\nEpoch 302/400\ntrain loss: 0.008963843838113253\ntrain DSC: 0.9887285232543945\nval loss: 0.01035292148590088\nval DSC: 0.9863770604133606\nTime: 2.091200828552246\n\nEpoch 303/400\ntrain loss: 0.008999064320423564\ntrain DSC: 0.9887611269950867\nval loss: 0.010226771235466003\nval DSC: 0.9863784909248352\nTime: 2.0532641410827637\n\nEpoch 304/400\ntrain loss: 0.009013879494588883\ntrain DSC: 0.9886902570724487\nval loss: 0.010227370262145995\nval DSC: 0.9863755106925964\nTime: 2.0959699153900146\n\nEpoch 305/400\ntrain loss: 0.009114775501313756\ntrain DSC: 0.9885440468788147\nval loss: 0.010464662313461303\nval DSC: 0.9866450428962708\nTime: 2.0846099853515625\n\nEpoch 306/400\ntrain loss: 0.00891774697381942\ntrain DSC: 0.9888195395469666\nval loss: 0.010196566581726074\nval DSC: 0.9862359762191772\nTime: 2.0503602027893066\n\nEpoch 307/400\ntrain loss: 0.008904655448725967\ntrain DSC: 0.988861620426178\nval loss: 0.010243573784828186\nval DSC: 0.9864064455032349\nTime: 2.111104965209961\n\nEpoch 308/400\ntrain loss: 0.008896204291797076\ntrain DSC: 0.9888147115707397\nval loss: 0.010266140103340149\nval DSC: 0.986493706703186\nTime: 2.057655096054077\n\nEpoch 309/400\ntrain loss: 0.008955140582850723\ntrain DSC: 0.9887690544128418\nval loss: 0.010273158550262451\nval DSC: 0.9866020083427429\nTime: 2.0690979957580566\n\nEpoch 310/400\ntrain loss: 0.00889806082991303\ntrain DSC: 0.9888179302215576\nval loss: 0.010261821746826171\nval DSC: 0.9862223863601685\nTime: 2.1230454444885254\n\nEpoch 311/400\ntrain loss: 0.008897296717909516\ntrain DSC: 0.9888473153114319\nval loss: 0.010198527574539184\nval DSC: 0.9862347841262817\nTime: 2.045825481414795\n\nEpoch 312/400\ntrain loss: 0.008887791242755827\ntrain DSC: 0.9887994527816772\nval loss: 0.010213533043861389\nval DSC: 0.9862201809883118\nTime: 2.0443944931030273\n\nEpoch 313/400\ntrain loss: 0.008960465915867539\ntrain DSC: 0.9886689186096191\nval loss: 0.010285574197769164\nval DSC: 0.9863178133964539\nTime: 2.056722402572632\n\nEpoch 314/400\ntrain loss: 0.00889163329953053\ntrain DSC: 0.9888352751731873\nval loss: 0.010227930545806885\nval DSC: 0.986446738243103\nTime: 2.060650587081909\n\nEpoch 315/400\ntrain loss: 0.008829570207439486\ntrain DSC: 0.9888821244239807\nval loss: 0.01018528938293457\nval DSC: 0.9864779710769653\nTime: 2.0763275623321533\n\nEpoch 316/400\ntrain loss: 0.008849323772993243\ntrain DSC: 0.9888777136802673\nval loss: 0.010305815935134887\nval DSC: 0.9864528775215149\nTime: 2.0783231258392334\n\nEpoch 317/400\ntrain loss: 0.00906101113460103\ntrain DSC: 0.9886569380760193\nval loss: 0.010260611772537231\nval DSC: 0.9863314628601074\nTime: 2.074348211288452\n\nEpoch 318/400\ntrain loss: 0.00889712474385246\ntrain DSC: 0.9887565970420837\nval loss: 0.010292783379554749\nval DSC: 0.9866136312484741\nTime: 2.0545003414154053\n\nEpoch 319/400\ntrain loss: 0.00878372739572994\ntrain DSC: 0.9889242649078369\nval loss: 0.01022455096244812\nval DSC: 0.986203670501709\nTime: 2.12249755859375\n\nEpoch 320/400\ntrain loss: 0.00880466914567791\ntrain DSC: 0.9889101982116699\nval loss: 0.010249969363212586\nval DSC: 0.9861682653427124\nTime: 2.0323662757873535\n\nEpoch 321/400\ntrain loss: 0.00887991952114418\ntrain DSC: 0.9888404011726379\nval loss: 0.010233220458030701\nval DSC: 0.9866596460342407\nTime: 2.0920495986938477\n\nEpoch 322/400\ntrain loss: 0.008968960066310695\ntrain DSC: 0.988660991191864\nval loss: 0.010296642780303955\nval DSC: 0.9861634969711304\nTime: 2.1020126342773438\n\nEpoch 323/400\ntrain loss: 0.008839430379085854\ntrain DSC: 0.9888991713523865\nval loss: 0.010224723815917968\nval DSC: 0.9862344861030579\nTime: 2.067368507385254\n\nEpoch 324/400\ntrain loss: 0.008859688141306893\ntrain DSC: 0.9888356924057007\nval loss: 0.010210189223289489\nval DSC: 0.9866626858711243\nTime: 2.146942377090454\n\nEpoch 325/400\ntrain loss: 0.008817123585059995\ntrain DSC: 0.9888870120048523\nval loss: 0.01033952236175537\nval DSC: 0.9858515858650208\nTime: 2.1041784286499023\n\nEpoch 326/400\ntrain loss: 0.00880711000473773\ntrain DSC: 0.9888674020767212\nval loss: 0.010235080122947693\nval DSC: 0.9864533543586731\nTime: 2.088482618331909\n\nEpoch 327/400\ntrain loss: 0.00875358307947878\ntrain DSC: 0.9889665246009827\nval loss: 0.01035381555557251\nval DSC: 0.9859228134155273\nTime: 2.11007022857666\n\nEpoch 328/400\ntrain loss: 0.008854617837999687\ntrain DSC: 0.9888204336166382\nval loss: 0.010312765836715698\nval DSC: 0.9861894845962524\nTime: 2.0835044384002686\n\nEpoch 329/400\ntrain loss: 0.008795385477972812\ntrain DSC: 0.9889594316482544\nval loss: 0.010169664025306701\nval DSC: 0.986305832862854\nTime: 2.0598361492156982\n\nEpoch 330/400\ntrain loss: 0.008788043358286873\ntrain DSC: 0.9889199137687683\nval loss: 0.010147720575332642\nval DSC: 0.9867076873779297\nTime: 2.0527193546295166\n\nEpoch 331/400\ntrain loss: 0.008819138417478467\ntrain DSC: 0.9888554811477661\nval loss: 0.010226264595985413\nval DSC: 0.9862270355224609\nTime: 2.0439951419830322\n\nEpoch 332/400\ntrain loss: 0.008789385928482305\ntrain DSC: 0.9889218807220459\nval loss: 0.010202273726463318\nval DSC: 0.9865037202835083\nTime: 2.060635566711426\n\nEpoch 333/400\ntrain loss: 0.008781793664713374\ntrain DSC: 0.988838791847229\nval loss: 0.010213926434516907\nval DSC: 0.9866312742233276\nTime: 2.050732135772705\n\nEpoch 334/400\ntrain loss: 0.00877042872006776\ntrain DSC: 0.9889183044433594\nval loss: 0.01017802655696869\nval DSC: 0.9863726496696472\nTime: 2.118131637573242\n\nEpoch 335/400\ntrain loss: 0.008813755434067523\ntrain DSC: 0.988842248916626\nval loss: 0.010387864708900452\nval DSC: 0.9862507581710815\nTime: 2.0637331008911133\n\nEpoch 336/400\ntrain loss: 0.008772082993241608\ntrain DSC: 0.9888852834701538\nval loss: 0.010178279876708985\nval DSC: 0.9863241314888\nTime: 2.05458927154541\n\nEpoch 337/400\ntrain loss: 0.008762091886801798\ntrain DSC: 0.9888849854469299\nval loss: 0.010133355855941772\nval DSC: 0.986585259437561\nTime: 2.106511116027832\n\nEpoch 338/400\ntrain loss: 0.008718149583847796\ntrain DSC: 0.988980770111084\nval loss: 0.010245105624198914\nval DSC: 0.9866952896118164\nTime: 2.109846353530884\n\nEpoch 339/400\ntrain loss: 0.008739553514074107\ntrain DSC: 0.9889323115348816\nval loss: 0.010227882862091064\nval DSC: 0.9862810373306274\nTime: 2.076305866241455\n\nEpoch 340/400\ntrain loss: 0.008687442443409904\ntrain DSC: 0.9889714121818542\nval loss: 0.010254544019699097\nval DSC: 0.985913872718811\nTime: 2.1082234382629395\n\nEpoch 341/400\ntrain loss: 0.008735935218998642\ntrain DSC: 0.9889256358146667\nval loss: 0.010179203748703004\nval DSC: 0.9864636659622192\nTime: 2.129814386367798\n\nEpoch 342/400\ntrain loss: 0.008737556269911469\ntrain DSC: 0.9889459013938904\nval loss: 0.010290813446044923\nval DSC: 0.9861984252929688\nTime: 2.1492879390716553\n\nEpoch 343/400\ntrain loss: 0.008762818868042992\ntrain DSC: 0.9889647960662842\nval loss: 0.010255527496337891\nval DSC: 0.9862837791442871\nTime: 2.079634189605713\n\nEpoch 344/400\ntrain loss: 0.008726417041215741\ntrain DSC: 0.9889726042747498\nval loss: 0.010212084650993348\nval DSC: 0.9862744212150574\nTime: 2.0353195667266846\n\nEpoch 345/400\ntrain loss: 0.008698444874560247\ntrain DSC: 0.9890144467353821\nval loss: 0.010269513726234436\nval DSC: 0.9863268733024597\nTime: 2.0913314819335938\n\nEpoch 346/400\ntrain loss: 0.008774552188935827\ntrain DSC: 0.9888986349105835\nval loss: 0.010125479102134705\nval DSC: 0.9864808917045593\nTime: 2.0929150581359863\n\nEpoch 347/400\ntrain loss: 0.008652010902029569\ntrain DSC: 0.989077091217041\nval loss: 0.01019783616065979\nval DSC: 0.9864640235900879\nTime: 2.030879020690918\n\nEpoch 348/400\ntrain loss: 0.008679377250984067\ntrain DSC: 0.9889844059944153\nval loss: 0.01017766296863556\nval DSC: 0.9865093231201172\nTime: 2.110105037689209\n\nEpoch 349/400\ntrain loss: 0.008707368960146044\ntrain DSC: 0.9889454245567322\nval loss: 0.010291767120361329\nval DSC: 0.9866341352462769\nTime: 2.0710859298706055\n\nEpoch 350/400\ntrain loss: 0.008731509818405401\ntrain DSC: 0.9889057874679565\nval loss: 0.010163956880569458\nval DSC: 0.9864562153816223\nTime: 2.034919261932373\n\nEpoch 351/400\ntrain loss: 0.008672251075994774\ntrain DSC: 0.9890009164810181\nval loss: 0.010204741358757019\nval DSC: 0.9862785339355469\nTime: 2.0544707775115967\n\nEpoch 352/400\ntrain loss: 0.008683162634489967\ntrain DSC: 0.988973081111908\nval loss: 0.01023203730583191\nval DSC: 0.9865911602973938\nTime: 2.0607120990753174\n\nEpoch 353/400\ntrain loss: 0.008666232961123108\ntrain DSC: 0.9890357851982117\nval loss: 0.010180288553237915\nval DSC: 0.9865099787712097\nTime: 2.102649688720703\n\nEpoch 354/400\ntrain loss: 0.008639355174830703\ntrain DSC: 0.9890801906585693\nval loss: 0.010173022747039795\nval DSC: 0.9864532351493835\nTime: 2.050640344619751\n\nEpoch 355/400\ntrain loss: 0.008736873259309863\ntrain DSC: 0.9888989925384521\nval loss: 0.010195472836494445\nval DSC: 0.9865962862968445\nTime: 2.214200019836426\n\nEpoch 356/400\ntrain loss: 0.008684034230279141\ntrain DSC: 0.9890191555023193\nval loss: 0.010198023915290833\nval DSC: 0.986285388469696\nTime: 2.136115074157715\n\nEpoch 357/400\ntrain loss: 0.008781405745959673\ntrain DSC: 0.9888144135475159\nval loss: 0.01026727557182312\nval DSC: 0.986345112323761\nTime: 2.0958945751190186\n\nEpoch 358/400\ntrain loss: 0.008691349967581327\ntrain DSC: 0.9889436364173889\nval loss: 0.010172882676124572\nval DSC: 0.9865714907646179\nTime: 2.123950719833374\n\nEpoch 359/400\ntrain loss: 0.008688141088016698\ntrain DSC: 0.9890023469924927\nval loss: 0.010230314731597901\nval DSC: 0.9862659573554993\nTime: 2.0711469650268555\n\nEpoch 360/400\ntrain loss: 0.008669469200196813\ntrain DSC: 0.9890510439872742\nval loss: 0.010163280367851257\nval DSC: 0.9864401817321777\nTime: 2.2004284858703613\n\nEpoch 361/400\ntrain loss: 0.008684001985143443\ntrain DSC: 0.9889981746673584\nval loss: 0.010130825638771056\nval DSC: 0.9863384366035461\nTime: 2.2404890060424805\n\nEpoch 362/400\ntrain loss: 0.008697440389726982\ntrain DSC: 0.9889553785324097\nval loss: 0.010302424430847168\nval DSC: 0.9864053726196289\nTime: 2.1426305770874023\n\nEpoch 363/400\ntrain loss: 0.008605136245977684\ntrain DSC: 0.9890506267547607\nval loss: 0.0101491779088974\nval DSC: 0.9864702224731445\nTime: 2.02388334274292\n\nEpoch 364/400\ntrain loss: 0.00862003252154491\ntrain DSC: 0.9890320301055908\nval loss: 0.01016974151134491\nval DSC: 0.9863301515579224\nTime: 2.0915708541870117\n\nEpoch 365/400\ntrain loss: 0.00863178343069358\ntrain DSC: 0.9890620708465576\nval loss: 0.010152137279510498\nval DSC: 0.9863397479057312\nTime: 2.2563395500183105\n\nEpoch 366/400\ntrain loss: 0.008633371259345383\ntrain DSC: 0.9890389442443848\nval loss: 0.010217288136482238\nval DSC: 0.9861868023872375\nTime: 2.1302714347839355\n\nEpoch 367/400\ntrain loss: 0.008620469296564822\ntrain DSC: 0.9891107082366943\nval loss: 0.010142272710800171\nval DSC: 0.9865385890007019\nTime: 2.104966163635254\n\nEpoch 368/400\ntrain loss: 0.008603909953695829\ntrain DSC: 0.9890871644020081\nval loss: 0.010157856345176696\nval DSC: 0.9864307641983032\nTime: 2.0688607692718506\n\nEpoch 369/400\ntrain loss: 0.008628639041400347\ntrain DSC: 0.9890142679214478\nval loss: 0.010194233059883118\nval DSC: 0.9863556623458862\nTime: 2.0783538818359375\n\nEpoch 370/400\ntrain loss: 0.008611589181618612\ntrain DSC: 0.9890367388725281\nval loss: 0.010237985849380493\nval DSC: 0.986049473285675\nTime: 2.0740044116973877\n\nEpoch 371/400\ntrain loss: 0.008631864532095487\ntrain DSC: 0.9891299605369568\nval loss: 0.01022985577583313\nval DSC: 0.9861488342285156\nTime: 2.1497626304626465\n\nEpoch 372/400\ntrain loss: 0.008606712349125595\ntrain DSC: 0.989037811756134\nval loss: 0.010170048475265503\nval DSC: 0.9865558743476868\nTime: 2.094930648803711\n\nEpoch 373/400\ntrain loss: 0.008611504171715408\ntrain DSC: 0.9890661835670471\nval loss: 0.010269221663475037\nval DSC: 0.9861776232719421\nTime: 2.153432846069336\n\nEpoch 374/400\ntrain loss: 0.008608189762615766\ntrain DSC: 0.9890595078468323\nval loss: 0.010190621018409729\nval DSC: 0.9865883588790894\nTime: 2.092348575592041\n\nEpoch 375/400\ntrain loss: 0.008617849623570676\ntrain DSC: 0.9890303015708923\nval loss: 0.010152846574783325\nval DSC: 0.9863921403884888\nTime: 2.0981972217559814\n\nEpoch 376/400\ntrain loss: 0.008544109883855601\ntrain DSC: 0.9891843795776367\nval loss: 0.010125741362571716\nval DSC: 0.9865347743034363\nTime: 2.0804812908172607\n\nEpoch 377/400\ntrain loss: 0.008609516698806012\ntrain DSC: 0.9890854358673096\nval loss: 0.010193121433258057\nval DSC: 0.9864137768745422\nTime: 2.0351192951202393\n\nEpoch 378/400\ntrain loss: 0.008590717784693985\ntrain DSC: 0.9890240430831909\nval loss: 0.010141226649284362\nval DSC: 0.9863235354423523\nTime: 2.1218435764312744\n\nEpoch 379/400\ntrain loss: 0.008591285494507337\ntrain DSC: 0.9891408681869507\nval loss: 0.01013982594013214\nval DSC: 0.986545205116272\nTime: 2.0918593406677246\n\nEpoch 380/400\ntrain loss: 0.008577918420072462\ntrain DSC: 0.9890666007995605\nval loss: 0.010120820999145509\nval DSC: 0.9865708351135254\nTime: 2.1690704822540283\n\nEpoch 381/400\ntrain loss: 0.008569433063757225\ntrain DSC: 0.9891296625137329\nval loss: 0.010117664933204651\nval DSC: 0.9864034652709961\nTime: 2.07106614112854\n\nEpoch 382/400\ntrain loss: 0.008588256406002357\ntrain DSC: 0.9890824556350708\nval loss: 0.01014726161956787\nval DSC: 0.986380934715271\nTime: 2.119785785675049\n\nEpoch 383/400\ntrain loss: 0.008614118959082932\ntrain DSC: 0.9890578389167786\nval loss: 0.010211113095283508\nval DSC: 0.9861198663711548\nTime: 2.062771797180176\n\nEpoch 384/400\ntrain loss: 0.008573176430874184\ntrain DSC: 0.9890817403793335\nval loss: 0.0101394385099411\nval DSC: 0.986284613609314\nTime: 2.0627377033233643\n\nEpoch 385/400\ntrain loss: 0.008559096054952652\ntrain DSC: 0.9891049861907959\nval loss: 0.010102686285972596\nval DSC: 0.9864782094955444\nTime: 2.0876026153564453\n\nEpoch 386/400\ntrain loss: 0.008590897575753634\ntrain DSC: 0.9890649318695068\nval loss: 0.010140034556388854\nval DSC: 0.9864178895950317\nTime: 2.0738935470581055\n\nEpoch 387/400\ntrain loss: 0.008556275094141726\ntrain DSC: 0.9891562461853027\nval loss: 0.010135224461555481\nval DSC: 0.9863254427909851\nTime: 2.228118658065796\n\nEpoch 388/400\ntrain loss: 0.00857415355619837\ntrain DSC: 0.989122211933136\nval loss: 0.010138484835624694\nval DSC: 0.9862722158432007\nTime: 2.093611240386963\n\nEpoch 389/400\ntrain loss: 0.00858167546694396\ntrain DSC: 0.9890859127044678\nval loss: 0.010186594724655152\nval DSC: 0.9863886833190918\nTime: 2.0614051818847656\n\nEpoch 390/400\ntrain loss: 0.008553390620184726\ntrain DSC: 0.9891029596328735\nval loss: 0.010164672136306762\nval DSC: 0.9863554239273071\nTime: 2.1580374240875244\n\nEpoch 391/400\ntrain loss: 0.008596155487123083\ntrain DSC: 0.9890071153640747\nval loss: 0.01019742488861084\nval DSC: 0.986286461353302\nTime: 2.1214585304260254\n\nEpoch 392/400\ntrain loss: 0.008518240490897756\ntrain DSC: 0.9891653656959534\nval loss: 0.010116484761238099\nval DSC: 0.9865055084228516\nTime: 2.067476272583008\n\nEpoch 393/400\ntrain loss: 0.008527391269558766\ntrain DSC: 0.9891471266746521\nval loss: 0.010166263580322266\nval DSC: 0.986274242401123\nTime: 2.0801522731781006\n\nEpoch 394/400\ntrain loss: 0.008565997491117383\ntrain DSC: 0.989098072052002\nval loss: 0.010224953293800354\nval DSC: 0.9866595268249512\nTime: 2.1367807388305664\n\nEpoch 395/400\ntrain loss: 0.008563179461682429\ntrain DSC: 0.9890955090522766\nval loss: 0.010133057832717896\nval DSC: 0.9863179922103882\nTime: 2.075679063796997\n\nEpoch 396/400\ntrain loss: 0.008550255025019412\ntrain DSC: 0.989065945148468\nval loss: 0.010199502110481262\nval DSC: 0.9861832857131958\nTime: 2.0950849056243896\n\nEpoch 397/400\ntrain loss: 0.008578661035318843\ntrain DSC: 0.9890847206115723\nval loss: 0.010164794325828553\nval DSC: 0.9866010546684265\nTime: 2.0722570419311523\n\nEpoch 398/400\ntrain loss: 0.008535599122281934\ntrain DSC: 0.9891163110733032\nval loss: 0.01018143892288208\nval DSC: 0.9864673614501953\nTime: 2.1093616485595703\n\nEpoch 399/400\ntrain loss: 0.008538719083442062\ntrain DSC: 0.9891473650932312\nval loss: 0.010213831067085266\nval DSC: 0.9862021207809448\nTime: 2.0731372833251953\n\nEpoch 400/400\ntrain loss: 0.008512105120987188\ntrain DSC: 0.9891387820243835\nval loss: 0.010117673873901367\nval DSC: 0.9864247441291809\nTime: 2.0751376152038574\n\nTraining complete, model saved. Best model after epoch 400\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676777480536
        }
      },
      "id": "1953e3b1-46e1-4a87-9a3d-b7def234dfa9"
    },
    {
      "cell_type": "code",
      "source": [
        "del decoder\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676776643959
        }
      },
      "id": "7c0442a1-2b0e-4652-ac09-76fdee163e92"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9e895368-b239-4928-a255-0c3f7ad4bbb6"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "shapeworks"
    },
    "kernelspec": {
      "name": "shapeworks",
      "language": "python",
      "display_name": "Python (shapeworks)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}