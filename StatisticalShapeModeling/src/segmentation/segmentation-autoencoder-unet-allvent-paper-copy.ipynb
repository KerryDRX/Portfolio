{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import scipy.ndimage as ndimage\n",
        "import nrrd\n",
        "import torchio as tio\n",
        "import monai\n",
        "import nibabel as nib\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1682912476079
        },
        "id": "8246ca62",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "id": "8246ca62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b2836ef1-c66c-4d38-9b47-a0b62c33be9f"
    },
    {
      "cell_type": "code",
      "source": [
        "city = 'Beijing_Zang'\n",
        "\n",
        "modes = ['train', 'test']\n",
        "total_size = 197\n",
        "train_size, test_size = 158, 39\n",
        "num_classes = 3\n",
        "\n",
        "image_dir = f'../dataset/{city}/MRI'\n",
        "image_arti_dir = f'../dataset/{city}/MRI_arti'\n",
        "label_dir = f'../dataset/{city}/Segmentation'\n",
        "autoencoder_wm_dir = f'../results/SCAE_WM_temp/best_autoencoder.torch'\n",
        "\n",
        "model_dir = f'../results/{city}_unet_autoencoder_gmwm'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682912484645
        }
      },
      "id": "52a47ac5-f5b3-4059-9dd7-05a4214e1f03"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "random_state = np.random.RandomState(seed=seed)\n",
        "perm = random_state.permutation(total_size)\n",
        "perm = {\n",
        "    'train': perm[:train_size],\n",
        "    'test': perm[-test_size:],\n",
        "}\n",
        "\n",
        "def get_subjects(mode):\n",
        "    subjects = []\n",
        "    image_src = image_dir #image_dir if mode == 'train' else image_arti_dir\n",
        "    image_paths = [sorted(glob(f'{image_src}/*.nii.gz'))[i] for i in perm[mode]]\n",
        "    for image_path in tqdm(image_paths, desc=mode):\n",
        "        fn = image_path.split('/')[-1]\n",
        "        label_path = f'{label_dir}/{fn}'\n",
        "        subject = tio.Subject(\n",
        "            image=tio.ScalarImage(image_path),\n",
        "            label=tio.LabelMap(label_path),\n",
        "        )\n",
        "        subjects.append(subject)\n",
        "    return subjects\n",
        "\n",
        "def get_transform():\n",
        "    resample = tio.Compose([\n",
        "        tio.Resample(2),\n",
        "        tio.CropOrPad((96, 128, 128)),\n",
        "    ])\n",
        "    signal = tio.Compose([ \n",
        "        tio.RescaleIntensity(percentiles=(0.1, 99.9), out_min_max=(0, 1)),\n",
        "    ])\n",
        "    spatial = tio.Compose([\n",
        "        tio.RandomAffine(translation=1),\n",
        "    ])\n",
        "    remapping = dict()\n",
        "    for i in range(139):\n",
        "        # remapping[i] = 1 if (3<=i<=11 or 19<=i<=20 or 25<=i<=32 or 35<=i) else 0\n",
        "        # remapping[i] = 1 if i in {12, 13, 16, 17} else 0\n",
        "        remapping[i] = 1 if (3<=i<=11 or 19<=i<=20 or 25<=i<=32 or 35<=i) else 2 if i in {12, 13, 16, 17} else 0\n",
        "    remapping = tio.RemapLabels(remapping)\n",
        "\n",
        "    onehot = tio.OneHot(num_classes=num_classes)\n",
        "    transform = {\n",
        "        'train': tio.Compose([\n",
        "            resample,\n",
        "            spatial,\n",
        "            signal,\n",
        "            remapping,\n",
        "            onehot,\n",
        "        ]),\n",
        "        'test': tio.Compose([\n",
        "            resample,\n",
        "            signal,\n",
        "            remapping,\n",
        "            onehot,\n",
        "        ]),\n",
        "    }\n",
        "    return transform\n",
        "\n",
        "def get_dataloader(transform):\n",
        "    dataloader = dict()\n",
        "    for mode in modes:\n",
        "        dataloader[mode] = torch.utils.data.DataLoader(\n",
        "            tio.SubjectsDataset(\n",
        "                subjects[mode], \n",
        "                transform=transform[mode]\n",
        "            ),\n",
        "            batch_size=1, \n",
        "            num_workers=os.cpu_count(),\n",
        "            shuffle=(mode == 'train'),\n",
        "        )\n",
        "    return dataloader\n",
        "\n",
        "subjects = {mode: get_subjects(mode) for mode in modes}\n",
        "transform = get_transform()\n",
        "dataloaders = get_dataloader(transform)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "train: 100%|██████████| 158/158 [00:01<00:00, 81.80it/s]\ntest: 100%|██████████| 39/39 [00:00<00:00, 87.30it/s]\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1682912611337
        }
      },
      "id": "fe9e2ac9"
    },
    {
      "cell_type": "code",
      "source": [
        "tio.__version__"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "'0.18.86'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682912671249
        }
      },
      "id": "ccfa7230-e7d3-4797-9db1-7244e3b06cf6"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "'1.13.1'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682912730211
        }
      },
      "id": "344a7342-be0e-48e4-8fdd-6663e5ccd23f"
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Python 3.8.5\r\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682912698821
        }
      },
      "id": "7d0599d5-1969-4436-b882-038066865041"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e247b1c8-c347-4528-8d76-8e1981fa3770"
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution(in_channels, out_channels, stride):\n",
        "    return torch.nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "\n",
        "def deconvolution(in_channels, out_channels, stride):\n",
        "    return torch.nn.ConvTranspose3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, output_padding=1)\n",
        "\n",
        "def normalization(channel):\n",
        "    return torch.nn.BatchNorm3d(channel)\n",
        "\n",
        "def activation():\n",
        "    return torch.nn.PReLU()\n",
        "\n",
        "def pooling(kernel_size):\n",
        "    return torch.nn.MaxPool3d(kernel_size=kernel_size)\n",
        "\n",
        "def upsampling(scale_factor):\n",
        "    return torch.nn.Upsample(scale_factor=scale_factor, mode='trilinear', align_corners=True)\n",
        "\n",
        "class Autoencoder(torch.nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            convolution(in_channels=1, out_channels=channels[0], stride=2),\n",
        "            normalization(channels[0]),\n",
        "            activation(),\n",
        "            \n",
        "            convolution(in_channels=channels[0], out_channels=channels[1], stride=2),\n",
        "            normalization(channels[1]),\n",
        "            activation(),\n",
        "            \n",
        "            convolution(in_channels=channels[1], out_channels=channels[2], stride=2),\n",
        "            normalization(channels[2]),\n",
        "            activation(),\n",
        "\n",
        "            convolution(in_channels=channels[2], out_channels=channels[3], stride=2),\n",
        "            normalization(channels[3]),\n",
        "            activation(),\n",
        "        )\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            deconvolution(in_channels=channels[3], out_channels=channels[2], stride=2),\n",
        "            normalization(channels[2]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[2], out_channels=channels[1], stride=2),\n",
        "            normalization(channels[1]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[1], out_channels=channels[0], stride=2),\n",
        "            normalization(channels[0]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[0], out_channels=1, stride=2),\n",
        "            normalization(1),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "autoencoder_wm = Autoencoder(channels=[64,128,256,512]).to(device)\n",
        "autoencoder_wm.load_state_dict(torch.load(autoencoder_wm_dir))\n",
        "for param in autoencoder_wm.parameters():\n",
        "    param.requires_grad = False"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678376568709
        }
      },
      "id": "15f056d2-eb79-40bd-8ff0-6a66544a31f2"
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.segmentation = monai.networks.nets.UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=4,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            norm=monai.networks.layers.Norm.BATCH,\n",
        "            dropout=0.3,\n",
        "        )\n",
        "        self.conv3d = torch.nn.Conv3d(in_channels=4, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, image):\n",
        "        prob4 = self.segmentation(image).sigmoid()\n",
        "        prob1 = self.conv3d(prob4).sigmoid()\n",
        "        shape = autoencoder.encoder(prob1)\n",
        "        recon = autoencoder.decoder(shape)\n",
        "        return prob4, prob1, shape, recon"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678158032399
        }
      },
      "id": "570a53f5-d51b-4320-9d01-c8a12a1f005d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9689e3b3-ea5d-426d-b3cb-091f185d8133"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(model):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.names = ['loss', 'loss_segm', 'loss_shape', 'loss_recon']\n",
        "    def log(self, mode, epoch, values):\n",
        "        for name, value in zip(self.names, values):\n",
        "            self.metrics[(mode, epoch, name)].append(value.item())\n",
        "    def show(self, mode, epoch):\n",
        "        print()\n",
        "        for name in self.names:\n",
        "            mean = np.mean(self.metrics[(mode, epoch, name)])\n",
        "            print(f'{mode} {name}: {mean}')\n",
        "            if name == 'loss':\n",
        "                mean_loss = mean\n",
        "        return mean_loss\n",
        "'''\n",
        "def predict(dataloaders):\n",
        "    mode = 'test'\n",
        "    model = Model().to(device)\n",
        "    model.load_state_dict(torch.load(f'{model_dir}/best_model.torch'))\n",
        "    model.eval()\n",
        "\n",
        "    for i, subject in enumerate(tqdm(dataloaders[mode])):\n",
        "        image = subject['image'][tio.DATA].to(device)\n",
        "        label = subject['label'][tio.DATA].to(device)\n",
        "        prob, mask, shape, recon = model(image)\n",
        "        \n",
        "        nrrd.write(f'{model_dir}/true{i}.nrrd', label[0, 0].detach().cpu().numpy())\n",
        "        nrrd.write(f'{model_dir}/pred{i}.nrrd', mask[0, 0].detach().cpu().numpy())\n",
        "        nrrd.write(f'{model_dir}/recon{i}.nrrd', recon[0, 0].detach().cpu().numpy())\n",
        "\n",
        "    !rm file.zip\n",
        "    !zip -r file.zip $model_dir\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "'''\n",
        "def train(model, lambdas, n_epochs, dataloaders, learning_rate):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\n",
        "\n",
        "    loss_dice = monai.losses.DiceLoss()\n",
        "    loss_mse = torch.nn.MSELoss()\n",
        "    \n",
        "    metric = monai.metrics.DiceMetric(reduction='mean_batch')\n",
        "    metrics = Metrics()\n",
        "    best_val_loss = np.Inf\n",
        "\n",
        "    tol, tol50 = 0, 0\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        t0 = time.time()\n",
        "        print(f'\\nEpoch {epoch}/{n_epochs}')\n",
        "        for mode in modes:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            for subject in dataloaders[mode]:\n",
        "                image = subject['image'][tio.DATA].to(device)\n",
        "                label = subject['label'][tio.DATA].to(device).float()\n",
        "                label1, label4 = 1 - label[:, :1], label[:, 1:]\n",
        "                \n",
        "                prob4, prob1, shape, recon = model(image)\n",
        "                loss_segm4 = loss_dice(prob4, label4)\n",
        "                loss_segm1 = loss_dice(prob1, label1)\n",
        "                loss_segm = loss_segm4 + loss_segm1\n",
        "                loss_shape = loss_mse(shape, autoencoder.encoder(label1))\n",
        "                loss_recon = torch.nn.ReLU()(loss_dice(recon, label1) - 0.0517497765712249)\n",
        "\n",
        "                loss = lambdas[0] * loss_segm + lambdas[1] * loss_shape + lambdas[2] * loss_recon\n",
        "\n",
        "                if mode == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                metric((prob4 > 0.5).float(), label4)\n",
        "                metrics.log(mode, epoch, [loss, loss_segm, loss_shape, loss_recon])\n",
        "\n",
        "            dsc = metric.aggregate().tolist()\n",
        "            metric.reset()\n",
        "            mean_loss = metrics.show(mode, epoch)\n",
        "            print(f'{mode} unet DSC: {dsc}')\n",
        "        \n",
        "        if mean_loss <= best_val_loss:\n",
        "            best_val_loss = mean_loss\n",
        "            best_epoch = epoch\n",
        "            tol = 0\n",
        "            tol50 = 0\n",
        "        else:\n",
        "            tol += 1\n",
        "            tol50 += 1\n",
        "        print(f'Best val loss: {best_val_loss}')\n",
        "        \n",
        "        if tol == 10:\n",
        "            scheduler.step()\n",
        "            print('Validation loss stopped to decrease for 10 epochs (LR /= 5).')\n",
        "            tol = 0\n",
        "        \n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "\n",
        "        if tol50 == 50:\n",
        "            print('Validation loss stopped to decrease for 50 epochs. Training terminated.')\n",
        "            break\n",
        "    \n",
        "    print(f'Best epoch: {best_epoch}')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678158032740
        }
      },
      "id": "86731a57-1ac6-4dfd-b48a-53fad0257e88"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0, 0)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/250\n\ntrain loss: 1.9797150122968457\ntrain loss_segm: 1.9797150122968457\ntrain loss_shape: 0.9787265115146395\ntrain loss_recon: 0.9399794027020659\ntrain unet DSC: [0.07165322452783585, 0.07009866088628769, 9.672767191659659e-06, 0.004965457133948803]\n\ntest loss: 1.8971243760524652\ntest loss_segm: 1.8971243760524652\ntest loss_shape: 1.0499779964104676\ntest loss_recon: 0.9148685611211337\ntest unet DSC: [0.3220730125904083, 0.3492332994937897, 1.029572194966022e-05, 0.010743088088929653]\nBest val loss: 1.8971243760524652\nTime: 58.37938618659973\n\n\nEpoch 2/250\n\ntrain loss: 1.7802003584330595\ntrain loss_segm: 1.7802003584330595\ntrain loss_shape: 0.9393068714232384\ntrain loss_recon: 0.9120554244970973\ntrain unet DSC: [0.3479003310203552, 0.3744296431541443, 1.28859528558678e-05, 0.2490081489086151]\n\ntest loss: 1.6428748980546608\ntest loss_segm: 1.6428748980546608\ntest loss_shape: 0.8552038639019697\ntest loss_recon: 0.9094769771282489\ntest unet DSC: [0.3800884485244751, 0.46923828125, 1.990803866647184e-05, 0.590435802936554]\nBest val loss: 1.6428748980546608\nTime: 58.152554750442505\n\n\nEpoch 3/250\n\ntrain loss: 1.6560611558865896\ntrain loss_segm: 1.6560611558865896\ntrain loss_shape: 0.7783858259267444\ntrain loss_recon: 0.9086658905578565\ntrain unet DSC: [0.3926343619823456, 0.42801356315612793, 2.878956001950428e-05, 0.5721860527992249]\n\ntest loss: 1.635107037348625\ntest loss_segm: 1.635107037348625\ntest loss_shape: 0.7243545605586126\ntest loss_recon: 0.8983349265196384\ntest unet DSC: [0.37680158019065857, 0.46622392535209656, 3.337029556860216e-05, 0.6052488684654236]\nBest val loss: 1.635107037348625\nTime: 56.96838068962097\n\n\nEpoch 4/250\n\ntrain loss: 1.624157350274581\ntrain loss_segm: 1.624157350274581\ntrain loss_shape: 0.6288827261592769\ntrain loss_recon: 0.900651515284671\ntrain unet DSC: [0.4161209762096405, 0.4657021164894104, 5.416594649432227e-05, 0.6183554530143738]\n\ntest loss: 1.6447730492322872\ntest loss_segm: 1.6447730492322872\ntest loss_shape: 0.5976442304941324\ntest loss_recon: 0.8900107053609995\ntest unet DSC: [0.3879910111427307, 0.44148963689804077, 4.9496262363391e-05, 0.567633330821991]\nBest val loss: 1.635107037348625\nTime: 57.120018005371094\n\n\nEpoch 5/250\n\ntrain loss: 1.6104478775700437\ntrain loss_segm: 1.6104478775700437\ntrain loss_shape: 0.5269534785913516\ntrain loss_recon: 0.8919205907024915\ntrain unet DSC: [0.429627001285553, 0.4756453335285187, 6.628850678680465e-05, 0.6345088481903076]\n\ntest loss: 1.612083878272619\ntest loss_segm: 1.612083878272619\ntest loss_shape: 0.45833271512618434\ntest loss_recon: 0.8715894191693037\ntest unet DSC: [0.412585973739624, 0.49844634532928467, 0.00010040832421509549, 0.6067337393760681]\nBest val loss: 1.612083878272619\nTime: 57.05611968040466\n\n\nEpoch 6/250\n\ntrain loss: 1.598082107833669\ntrain loss_segm: 1.598082107833669\ntrain loss_shape: 0.40975489495675776\ntrain loss_recon: 0.8708105758775638\ntrain unet DSC: [0.4392355978488922, 0.4863879978656769, 8.65342008182779e-05, 0.6363548636436462]\n\ntest loss: 1.581410933763553\ntest loss_segm: 1.581410933763553\ntest loss_shape: 0.31532843793049836\ntest loss_recon: 0.844339343217703\ntest unet DSC: [0.43150100111961365, 0.5181549191474915, 7.712789374636486e-05, 0.6547847390174866]\nBest val loss: 1.581410933763553\nTime: 57.779871702194214\n\n\nEpoch 7/250\n\ntrain loss: 1.579213382322577\ntrain loss_segm: 1.579213382322577\ntrain loss_shape: 0.26652180063950864\ntrain loss_recon: 0.8316914974888668\ntrain unet DSC: [0.4417404532432556, 0.4932212233543396, 9.216285980073735e-05, 0.6483321189880371]\n\ntest loss: 1.5875348616869023\ntest loss_segm: 1.5875348616869023\ntest loss_shape: 0.15885621156447974\ntest loss_recon: 0.6362167547146479\ntest unet DSC: [0.43834924697875977, 0.5132107138633728, 0.00010287266195518896, 0.5596811175346375]\nBest val loss: 1.581410933763553\nTime: 56.95952486991882\n\n\nEpoch 8/250\n\ntrain loss: 1.5496536629109443\ntrain loss_segm: 1.5496536629109443\ntrain loss_shape: 0.1635454846192387\ntrain loss_recon: 0.7688727903215191\ntrain unet DSC: [0.4434869587421417, 0.49584612250328064, 0.00010912984726019204, 0.6580173969268799]\n\ntest loss: 1.5341352163217006\ntest loss_segm: 1.5341352163217006\ntest loss_shape: 0.08457767886993213\ntest loss_recon: 0.35611047280522495\ntest unet DSC: [0.43916985392570496, 0.5435358881950378, 0.00013714794476982206, 0.6013177037239075]\nBest val loss: 1.5341352163217006\nTime: 56.762688636779785\n\n\nEpoch 9/250\n\ntrain loss: 1.5026850006248378\ntrain loss_segm: 1.5026850006248378\ntrain loss_shape: 0.09332500744752492\ntrain loss_recon: 0.5626208724666245\ntrain unet DSC: [0.4419381320476532, 0.49606773257255554, 0.00012202002108097076, 0.6616105437278748]\n\ntest loss: 1.4465383566342866\ntest loss_segm: 1.4465383566342866\ntest loss_shape: 0.06733886286234245\ntest loss_recon: 0.2998831847157234\ntest unet DSC: [0.45260366797447205, 0.5371928811073303, 0.00012042268645018339, 0.6541528105735779]\nBest val loss: 1.4465383566342866\nTime: 56.34107995033264\n\n\nEpoch 10/250\n\ntrain loss: 1.4161765145350107\ntrain loss_segm: 1.4161765145350107\ntrain loss_shape: 0.06454058802580531\ntrain loss_recon: 0.2683007349319096\ntrain unet DSC: [0.4341811239719391, 0.4939865469932556, 0.0001367262884741649, 0.6718189120292664]\n\ntest loss: 1.334634019778325\ntest loss_segm: 1.334634019778325\ntest loss_shape: 0.0645994353466309\ntest loss_recon: 0.2666913830699065\ntest unet DSC: [0.4022595286369324, 0.49949967861175537, 0.0001224940933752805, 0.7148655652999878]\nBest val loss: 1.334634019778325\nTime: 56.80784845352173\n\n\nEpoch 11/250\n\ntrain loss: 1.3192840346807166\ntrain loss_segm: 1.3192840346807166\ntrain loss_shape: 0.05653785908288216\ntrain loss_recon: 0.21843108827177482\ntrain unet DSC: [0.4160381853580475, 0.485963374376297, 0.0001358179870294407, 0.6631928086280823]\n\ntest loss: 1.2055969421680157\ntest loss_segm: 1.2055969421680157\ntest loss_shape: 0.04615518737297792\ntest loss_recon: 0.16879436755791688\ntest unet DSC: [0.43067261576652527, 0.552051842212677, 0.00011640360753517598, 0.7217103242874146]\nBest val loss: 1.2055969421680157\nTime: 56.567779302597046\n\n\nEpoch 12/250\n\ntrain loss: 1.225422774689107\ntrain loss_segm: 1.225422774689107\ntrain loss_shape: 0.053867954265656354\ntrain loss_recon: 0.21662698648398435\ntrain unet DSC: [0.4067502021789551, 0.4900840222835541, 0.00014178710989654064, 0.6717949509620667]\n\ntest loss: 1.1410420047931182\ntest loss_segm: 1.1410420047931182\ntest loss_shape: 0.04936599841293616\ntest loss_recon: 0.174440330801866\ntest unet DSC: [0.44528672099113464, 0.5508171916007996, 0.0001449946576030925, 0.680519700050354]\nBest val loss: 1.1410420047931182\nTime: 56.38256812095642\n\n\nEpoch 13/250\n\ntrain loss: 1.1446452782123904\ntrain loss_segm: 1.1446452782123904\ntrain loss_shape: 0.05129425466956594\ntrain loss_recon: 0.21147124808800372\ntrain unet DSC: [0.4066833257675171, 0.4952040910720825, 0.0001328695798292756, 0.6751002073287964]\n\ntest loss: 1.0399641532164354\ntest loss_segm: 1.0399641532164354\ntest loss_shape: 0.04439446258430298\ntest loss_recon: 0.16699355086072898\ntest unet DSC: [0.4214248061180115, 0.5410534739494324, 0.00010473570000613108, 0.7288852334022522]\nBest val loss: 1.0399641532164354\nTime: 56.54397010803223\n\n\nEpoch 14/250\n\ntrain loss: 1.0959341571300845\ntrain loss_segm: 1.0959341571300845\ntrain loss_shape: 0.04990970519005875\ntrain loss_recon: 0.21108732155606716\ntrain unet DSC: [0.4025881290435791, 0.49770379066467285, 0.00011806638212874532, 0.6788927316665649]\n\ntest loss: 1.0364143527471101\ntest loss_segm: 1.0364143527471101\ntest loss_shape: 0.04866100823841034\ntest loss_recon: 0.17827119200657576\ntest unet DSC: [0.4308691620826721, 0.5445287227630615, 0.00011891544272657484, 0.6792287826538086]\nBest val loss: 1.0364143527471101\nTime: 56.82400631904602\n\n\nEpoch 15/250\n\ntrain loss: 1.05099432000631\ntrain loss_segm: 1.05099432000631\ntrain loss_shape: 0.048533667397649985\ntrain loss_recon: 0.20678399512662163\ntrain unet DSC: [0.4081727862358093, 0.5021620392799377, 0.00011608960630837828, 0.6792945861816406]\n\ntest loss: 0.9966565416409419\ntest loss_segm: 0.9966565416409419\ntest loss_shape: 0.04831181146586553\ntest loss_recon: 0.1833187330227632\ntest unet DSC: [0.43319952487945557, 0.5484855771064758, 0.0001399920874973759, 0.6955552697181702]\nBest val loss: 0.9966565416409419\nTime: 56.30643129348755\n\n\nEpoch 16/250\n\ntrain loss: 1.0287180014803439\ntrain loss_segm: 1.0287180014803439\ntrain loss_shape: 0.04803294231054149\ntrain loss_recon: 0.20577435223739357\ntrain unet DSC: [0.4082391560077667, 0.5040945410728455, 0.00011126430763397366, 0.6804398894309998]\n\ntest loss: 0.9140965220255729\ntest loss_segm: 0.9140965220255729\ntest loss_shape: 0.0385190989726629\ntest loss_recon: 0.1458123410359407\ntest unet DSC: [0.4529021680355072, 0.5722655057907104, 0.00012195855379104614, 0.7381200790405273]\nBest val loss: 0.9140965220255729\nTime: 56.350823640823364\n\n\nEpoch 17/250\n\ntrain loss: 1.0008137388319909\ntrain loss_segm: 1.0008137388319909\ntrain loss_shape: 0.04794857190049524\ntrain loss_recon: 0.2053558465234841\ntrain unet DSC: [0.41212671995162964, 0.5054260492324829, 0.00011962986900471151, 0.6812633275985718]\n\ntest loss: 0.9227114763015356\ntest loss_segm: 0.9227114763015356\ntest loss_shape: 0.042910365817638546\ntest loss_recon: 0.15978816762948647\ntest unet DSC: [0.4459879994392395, 0.5604456663131714, 0.00012048122880514711, 0.7118591070175171]\nBest val loss: 0.9140965220255729\nTime: 56.172399044036865\n\n\nEpoch 18/250\n\ntrain loss: 0.9772575161879575\ntrain loss_segm: 0.9772575161879575\ntrain loss_shape: 0.04655325521208063\ntrain loss_recon: 0.19993227173255967\ntrain unet DSC: [0.41366100311279297, 0.5097841620445251, 0.00011080763943027705, 0.6867243051528931]\n\ntest loss: 0.9369915357002845\ntest loss_segm: 0.9369915357002845\ntest loss_shape: 0.04518728493115841\ntest loss_recon: 0.1680127114821703\ntest unet DSC: [0.448537141084671, 0.5677826404571533, 0.00012539311137516052, 0.6384515762329102]\nBest val loss: 0.9140965220255729\nTime: 56.87827730178833\n\n\nEpoch 19/250\n\ntrain loss: 0.9631657381600971\ntrain loss_segm: 0.9631657381600971\ntrain loss_shape: 0.045181308425019814\ntrain loss_recon: 0.1987131444525115\ntrain unet DSC: [0.4160042405128479, 0.5106444358825684, 0.00010828077211044729, 0.6855898499488831]\n\ntest loss: 0.9028403682586474\ntest loss_segm: 0.9028403682586474\ntest loss_shape: 0.04111380235124857\ntest loss_recon: 0.16023927277479416\ntest unet DSC: [0.44672316312789917, 0.5761316418647766, 0.00011285419168416411, 0.6837372779846191]\nBest val loss: 0.9028403682586474\nTime: 56.6069769859314\n\n\nEpoch 20/250\n\ntrain loss: 0.9488390653948241\ntrain loss_segm: 0.9488390653948241\ntrain loss_shape: 0.04462199025091868\ntrain loss_recon: 0.1971289666393135\ntrain unet DSC: [0.41829028725624084, 0.5133728981018066, 0.000112551060738042, 0.6842064261436462]\n\ntest loss: 0.8586755180970217\ntest loss_segm: 0.8586755180970217\ntest loss_shape: 0.0380012992865\ntest loss_recon: 0.1460240139411046\ntest unet DSC: [0.44809791445732117, 0.5825390815734863, 0.00010460869816597551, 0.7388176321983337]\nBest val loss: 0.8586755180970217\nTime: 56.644774198532104\n\n\nEpoch 21/250\n\ntrain loss: 0.9424691000316716\ntrain loss_segm: 0.9424691000316716\ntrain loss_shape: 0.0448952271143281\ntrain loss_recon: 0.1986566816892805\ntrain unet DSC: [0.41523727774620056, 0.5126540660858154, 0.00010962990927509964, 0.6936371922492981]\n\ntest loss: 0.851375651665223\ntest loss_segm: 0.851375651665223\ntest loss_shape: 0.03739320200223189\ntest loss_recon: 0.14616355911279336\ntest unet DSC: [0.4459622800350189, 0.5719607472419739, 9.982047777157277e-05, 0.7266891598701477]\nBest val loss: 0.851375651665223\nTime: 56.91229605674744\n\n\nEpoch 22/250\n\ntrain loss: 0.9265098530280439\ntrain loss_segm: 0.9265098530280439\ntrain loss_shape: 0.04338665071969168\ntrain loss_recon: 0.19485511894844756\ntrain unet DSC: [0.4193713963031769, 0.5161100625991821, 0.00011011481547029689, 0.6951663494110107]\n\ntest loss: 0.8525004218786191\ntest loss_segm: 0.8525004218786191\ntest loss_shape: 0.03935784727143936\ntest loss_recon: 0.1533384804542248\ntest unet DSC: [0.4413107931613922, 0.5589945316314697, 9.47659500525333e-05, 0.7284035682678223]\nBest val loss: 0.851375651665223\nTime: 56.49159622192383\n\n\nEpoch 23/250\n\ntrain loss: 0.9185119158105005\ntrain loss_segm: 0.9185119158105005\ntrain loss_shape: 0.042540917161239096\ntrain loss_recon: 0.19297464314518095\ntrain unet DSC: [0.42045485973358154, 0.5196300745010376, 0.00010531002772040665, 0.6945324540138245]\n\ntest loss: 0.9226549344185071\ntest loss_segm: 0.9226549344185071\ntest loss_shape: 0.04982067520419756\ntest loss_recon: 0.18970548609892526\ntest unet DSC: [0.43657058477401733, 0.5587655305862427, 0.00012883360614068806, 0.6497937440872192]\nBest val loss: 0.851375651665223\nTime: 56.9610538482666\n\n\nEpoch 24/250\n\ntrain loss: 0.9112562835216522\ntrain loss_segm: 0.9112562835216522\ntrain loss_shape: 0.04200706377483999\ntrain loss_recon: 0.18932244200495224\ntrain unet DSC: [0.42371928691864014, 0.5248425602912903, 0.00010730510257417336, 0.6888313293457031]\n\ntest loss: 0.8116338344720694\ntest loss_segm: 0.8116338344720694\ntest loss_shape: 0.033789873170929074\ntest loss_recon: 0.1363977631315207\ntest unet DSC: [0.4529825448989868, 0.5787531733512878, 8.414458716288209e-05, 0.7471197247505188]\nBest val loss: 0.8116338344720694\nTime: 56.6053946018219\n\n\nEpoch 25/250\n\ntrain loss: 0.9062542534327205\ntrain loss_segm: 0.9062542534327205\ntrain loss_shape: 0.04155356036145476\ntrain loss_recon: 0.18936508384686482\ntrain unet DSC: [0.42275309562683105, 0.5263930559158325, 9.757222869666293e-05, 0.695476770401001]\n\ntest loss: 0.8092183394309802\ntest loss_segm: 0.8092183394309802\ntest loss_shape: 0.03535619970315542\ntest loss_recon: 0.13946283704195267\ntest unet DSC: [0.4521987736225128, 0.5890783667564392, 9.152945858659223e-05, 0.7480566501617432]\nBest val loss: 0.8092183394309802\nTime: 56.96304488182068\n\n\nEpoch 26/250\n\ntrain loss: 0.9000538255594954\ntrain loss_segm: 0.9000538255594954\ntrain loss_shape: 0.04098355856170006\ntrain loss_recon: 0.18998736605236802\ntrain unet DSC: [0.4257083237171173, 0.5318832993507385, 0.00010167347500100732, 0.6954206824302673]\n\ntest loss: 0.8122100172898709\ntest loss_segm: 0.8122100172898709\ntest loss_shape: 0.034415561658067584\ntest loss_recon: 0.14129169247089288\ntest unet DSC: [0.45023345947265625, 0.5859220027923584, 7.823575288057327e-05, 0.7367987632751465]\nBest val loss: 0.8092183394309802\nTime: 56.386205196380615\n\n\nEpoch 27/250\n\ntrain loss: 0.8959249861632721\ntrain loss_segm: 0.8959249861632721\ntrain loss_shape: 0.040670953797105745\ntrain loss_recon: 0.18654245279635054\ntrain unet DSC: [0.42717328667640686, 0.5376712679862976, 9.806517482502386e-05, 0.6946320533752441]\n\ntest loss: 0.7817047650997455\ntest loss_segm: 0.7817047650997455\ntest loss_shape: 0.02970521132915448\ntest loss_recon: 0.12764204102448928\ntest unet DSC: [0.4612727165222168, 0.6001706123352051, 9.483470057602972e-05, 0.7617902159690857]\nBest val loss: 0.7817047650997455\nTime: 56.54307413101196\n\n\nEpoch 28/250\n\ntrain loss: 0.8957203762440742\ntrain loss_segm: 0.8957203762440742\ntrain loss_shape: 0.04120857756632038\ntrain loss_recon: 0.18913929588809797\ntrain unet DSC: [0.42806321382522583, 0.5394228100776672, 9.586286614648998e-05, 0.6964813470840454]\n\ntest loss: 0.7940267324447632\ntest loss_segm: 0.7940267324447632\ntest loss_shape: 0.030914664411774047\ntest loss_recon: 0.13391916043101212\ntest unet DSC: [0.4573713541030884, 0.587610125541687, 7.464805821655318e-05, 0.7387816309928894]\nBest val loss: 0.7817047650997455\nTime: 56.388741970062256\n\n\nEpoch 29/250\n\ntrain loss: 0.8847702843478963\ntrain loss_segm: 0.8847702843478963\ntrain loss_shape: 0.03976217334311974\ntrain loss_recon: 0.18286286690567113\ntrain unet DSC: [0.438770055770874, 0.5463412404060364, 9.323658014182001e-05, 0.6947133541107178]\n\ntest loss: 0.766404743377979\ntest loss_segm: 0.766404743377979\ntest loss_shape: 0.028170448799545948\ntest loss_recon: 0.12206783919380261\ntest unet DSC: [0.4679456353187561, 0.6138486862182617, 7.161488611018285e-05, 0.7686069011688232]\nBest val loss: 0.766404743377979\nTime: 56.61133623123169\n\n\nEpoch 30/250\n\ntrain loss: 0.8802543814423718\ntrain loss_segm: 0.8802543814423718\ntrain loss_shape: 0.039514173069803774\ntrain loss_recon: 0.18355475960275794\ntrain unet DSC: [0.44364890456199646, 0.5496779084205627, 9.867491462500766e-05, 0.6946130990982056]\n\ntest loss: 0.7667154853160565\ntest loss_segm: 0.7667154853160565\ntest loss_shape: 0.028626147132271376\ntest loss_recon: 0.12464310992986728\ntest unet DSC: [0.46873125433921814, 0.6058822870254517, 6.292734906310216e-05, 0.7607176303863525]\nBest val loss: 0.766404743377979\nTime: 56.29239869117737\n\n\nEpoch 31/250\n\ntrain loss: 0.8794016973881782\ntrain loss_segm: 0.8794016973881782\ntrain loss_shape: 0.03962856682041024\ntrain loss_recon: 0.18375020257279842\ntrain unet DSC: [0.4423529803752899, 0.5498063564300537, 8.555076055927202e-05, 0.6986644864082336]\n\ntest loss: 0.8191903218244895\ntest loss_segm: 0.8191903218244895\ntest loss_shape: 0.03679193885853657\ntest loss_recon: 0.14669284119437903\ntest unet DSC: [0.470701664686203, 0.595413863658905, 7.210695912363008e-05, 0.7077540159225464]\nBest val loss: 0.766404743377979\nTime: 57.108678340911865\n\n\nEpoch 32/250\n\ntrain loss: 0.8807921266253991\ntrain loss_segm: 0.8807921266253991\ntrain loss_shape: 0.03945291662423671\ntrain loss_recon: 0.18385543759110606\ntrain unet DSC: [0.4449211657047272, 0.5520617961883545, 8.216527930926532e-05, 0.6919140219688416]\n\ntest loss: 0.8173556297253339\ntest loss_segm: 0.8173556297253339\ntest loss_shape: 0.03630938075291805\ntest loss_recon: 0.14779090699859154\ntest unet DSC: [0.4596858024597168, 0.5996219515800476, 8.611720113549381e-05, 0.7029728889465332]\nBest val loss: 0.766404743377979\nTime: 56.91691970825195\n\n\nEpoch 33/250\n\ntrain loss: 0.8723316811308076\ntrain loss_segm: 0.8723316811308076\ntrain loss_shape: 0.03847617153785651\ntrain loss_recon: 0.18169059151712852\ntrain unet DSC: [0.45119571685791016, 0.5566478371620178, 8.39668427943252e-05, 0.700667679309845]\n\ntest loss: 0.7620617823723035\ntest loss_segm: 0.7620617823723035\ntest loss_shape: 0.02848738245666027\ntest loss_recon: 0.1238351994409011\ntest unet DSC: [0.4709864854812622, 0.5999982953071594, 5.824758409289643e-05, 0.7630043029785156]\nBest val loss: 0.7620617823723035\nTime: 57.22264337539673\n\n\nEpoch 34/250\n\ntrain loss: 0.8706573817548873\ntrain loss_segm: 0.8706573817548873\ntrain loss_shape: 0.03862858245361455\ntrain loss_recon: 0.1806984956883177\ntrain unet DSC: [0.4536430239677429, 0.5638386011123657, 8.510088082402945e-05, 0.6932888031005859]\n\ntest loss: 0.7529621934279417\ntest loss_segm: 0.7529621934279417\ntest loss_shape: 0.025825392741423387\ntest loss_recon: 0.11892129901127937\ntest unet DSC: [0.47747117280960083, 0.6116431951522827, 5.048062666901387e-05, 0.7605364918708801]\nBest val loss: 0.7529621934279417\nTime: 56.417898416519165\n\n\nEpoch 35/250\n\ntrain loss: 0.8643747594537614\ntrain loss_segm: 0.8643747594537614\ntrain loss_shape: 0.03758849804820139\ntrain loss_recon: 0.17754006414096565\ntrain unet DSC: [0.4589119553565979, 0.5687761306762695, 8.433083712588996e-05, 0.6969749331474304]\n\ntest loss: 0.7667580506740472\ntest loss_segm: 0.7667580506740472\ntest loss_shape: 0.029383369219990876\ntest loss_recon: 0.12823849715865576\ntest unet DSC: [0.478426069021225, 0.6256856918334961, 6.879657303215936e-05, 0.7556589841842651]\nBest val loss: 0.7529621934279417\nTime: 57.83086657524109\n\n\nEpoch 36/250\n\ntrain loss: 0.8592357465737983\ntrain loss_segm: 0.8592357465737983\ntrain loss_shape: 0.03714022452978394\ntrain loss_recon: 0.17532909369166894\ntrain unet DSC: [0.4595308303833008, 0.5726625323295593, 7.85548982094042e-05, 0.7010055780410767]\n\ntest loss: 0.844720701376597\ntest loss_segm: 0.844720701376597\ntest loss_shape: 0.03921342087097657\ntest loss_recon: 0.15570413961242408\ntest unet DSC: [0.4732884168624878, 0.5828580856323242, 6.936540739843622e-05, 0.6211543679237366]\nBest val loss: 0.7529621934279417\nTime: 57.173516273498535\n\n\nEpoch 37/250\n\ntrain loss: 0.857269948796381\ntrain loss_segm: 0.857269948796381\ntrain loss_shape: 0.037058614500904384\ntrain loss_recon: 0.1774106884870348\ntrain unet DSC: [0.4627920091152191, 0.5819724202156067, 7.852062117308378e-05, 0.698785126209259]\n\ntest loss: 0.7436805673134632\ntest loss_segm: 0.7436805673134632\ntest loss_shape: 0.026418308560282756\ntest loss_recon: 0.11510881972618592\ntest unet DSC: [0.49200209975242615, 0.6415926218032837, 5.6052522268146276e-05, 0.755653440952301]\nBest val loss: 0.7436805673134632\nTime: 57.31253695487976\n\n\nEpoch 38/250\n\ntrain loss: 0.8475261998327472\ntrain loss_segm: 0.8475261998327472\ntrain loss_shape: 0.036697265031877195\ntrain loss_recon: 0.1731470759156384\ntrain unet DSC: [0.46692541241645813, 0.5938044786453247, 7.746796472929418e-05, 0.7070828080177307]\n\ntest loss: 0.7474009150113815\ntest loss_segm: 0.7474009150113815\ntest loss_shape: 0.027349046407601774\ntest loss_recon: 0.11615572793361469\ntest unet DSC: [0.4866095781326294, 0.6400217413902283, 4.5929933548904955e-05, 0.7470232248306274]\nBest val loss: 0.7436805673134632\nTime: 57.48116421699524\n\n\nEpoch 39/250\n\ntrain loss: 0.8527108902418161\ntrain loss_segm: 0.8527108902418161\ntrain loss_shape: 0.03704008487159315\ntrain loss_recon: 0.17599982485363755\ntrain unet DSC: [0.4712620973587036, 0.6032057404518127, 7.493048178730533e-05, 0.6947616338729858]\n\ntest loss: 0.7676661366071457\ntest loss_segm: 0.7676661366071457\ntest loss_shape: 0.0300361833606775\ntest loss_recon: 0.1250503302957767\ntest unet DSC: [0.4939761161804199, 0.6512952446937561, 6.300248060142621e-05, 0.7198765277862549]\nBest val loss: 0.7436805673134632\nTime: 56.694772481918335\n\n\nEpoch 40/250\n\ntrain loss: 0.8456623124170907\ntrain loss_segm: 0.8456623124170907\ntrain loss_shape: 0.03638297592914557\ntrain loss_recon: 0.17530693783413007\ntrain unet DSC: [0.47395890951156616, 0.6390495300292969, 7.126350828912109e-05, 0.6973472833633423]\n\ntest loss: 0.72621362331586\ntest loss_segm: 0.72621362331586\ntest loss_shape: 0.02640499900548886\ntest loss_recon: 0.11990164440029706\ntest unet DSC: [0.5057200193405151, 0.7817923426628113, 5.1154383982066065e-05, 0.7595686316490173]\nBest val loss: 0.72621362331586\nTime: 57.81405472755432\n\n\nEpoch 41/250\n\ntrain loss: 0.8320502895343153\ntrain loss_segm: 0.8320502895343153\ntrain loss_shape: 0.0359887785360783\ntrain loss_recon: 0.17517165792516515\ntrain unet DSC: [0.4852534532546997, 0.6951910853385925, 6.913650577189401e-05, 0.7000573873519897]\n\ntest loss: 0.7141796136513735\ntest loss_segm: 0.7141796136513735\ntest loss_shape: 0.025861712029347055\ntest loss_recon: 0.12117716698692395\ntest unet DSC: [0.509493350982666, 0.7826784253120422, 4.1522511310176924e-05, 0.7664854526519775]\nBest val loss: 0.7141796136513735\nTime: 57.137455701828\n\n\nEpoch 42/250\n\ntrain loss: 0.8270834940898267\ntrain loss_segm: 0.8270834940898267\ntrain loss_shape: 0.036249528544707386\ntrain loss_recon: 0.1788691008770013\ntrain unet DSC: [0.49857667088508606, 0.7094337344169617, 6.611867138417438e-05, 0.6954663395881653]\n\ntest loss: 0.6877828255677835\ntest loss_segm: 0.6877828255677835\ntest loss_shape: 0.023477633125506915\ntest loss_recon: 0.11209154816774222\ntest unet DSC: [0.5369686484336853, 0.8107458353042603, 3.7959118344588205e-05, 0.7791846394538879]\nBest val loss: 0.6877828255677835\nTime: 56.76954889297485\n\n\nEpoch 43/250\n\ntrain loss: 0.8170585338073441\ntrain loss_segm: 0.8170585338073441\ntrain loss_shape: 0.03603277620682611\ntrain loss_recon: 0.17950564415394504\ntrain unet DSC: [0.5388852953910828, 0.7132323980331421, 6.749036401743069e-05, 0.6964977979660034]\n\ntest loss: 0.7483871670869681\ntest loss_segm: 0.7483871670869681\ntest loss_shape: 0.032924241362473905\ntest loss_recon: 0.1427591343720754\ntest unet DSC: [0.5780966281890869, 0.7637191414833069, 5.186992711969651e-05, 0.710852324962616]\nBest val loss: 0.6877828255677835\nTime: 57.31280517578125\n\n\nEpoch 44/250\n\ntrain loss: 0.8051549784744843\ntrain loss_segm: 0.8051549784744843\ntrain loss_shape: 0.03637339651018758\ntrain loss_recon: 0.18255219674563106\ntrain unet DSC: [0.6119987368583679, 0.7150033116340637, 6.491662497865036e-05, 0.6945810317993164]\n\ntest loss: 0.6722643329547002\ntest loss_segm: 0.6722643329547002\ntest loss_shape: 0.02527951979293273\ntest loss_recon: 0.12320046346539106\ntest unet DSC: [0.7147404551506042, 0.7982903718948364, 4.6028268116060644e-05, 0.7539219856262207]\nBest val loss: 0.6722643329547002\nTime: 56.74574851989746\n\n\nEpoch 45/250\n\ntrain loss: 0.7933240132995799\ntrain loss_segm: 0.7933240132995799\ntrain loss_shape: 0.03712524843838396\ntrain loss_recon: 0.18453365507759625\ntrain unet DSC: [0.6475061178207397, 0.7193922400474548, 6.491825479315594e-05, 0.6928592920303345]\n\ntest loss: 0.653153060338436\ntest loss_segm: 0.653153060338436\ntest loss_shape: 0.025334398906964522\ntest loss_recon: 0.12211952100579555\ntest unet DSC: [0.7364847660064697, 0.8092886209487915, 3.831876892945729e-05, 0.76017826795578]\nBest val loss: 0.653153060338436\nTime: 57.41894197463989\n\n\nEpoch 46/250\n\ntrain loss: 0.780339517925359\ntrain loss_segm: 0.780339517925359\ntrain loss_shape: 0.036575370692188226\ntrain loss_recon: 0.1831522947059402\ntrain unet DSC: [0.6633395552635193, 0.7209232449531555, 6.645631219726056e-05, 0.6977218389511108]\n\ntest loss: 0.6481969738617922\ntest loss_segm: 0.6481969738617922\ntest loss_shape: 0.026245353838954218\ntest loss_recon: 0.12728204243840316\ntest unet DSC: [0.7310566306114197, 0.8085256814956665, 3.1070532713783905e-05, 0.7669030427932739]\nBest val loss: 0.6481969738617922\nTime: 57.087470054626465\n\n\nEpoch 47/250\n\ntrain loss: 0.7751636331594443\ntrain loss_segm: 0.7751636331594443\ntrain loss_shape: 0.03684256177489893\ntrain loss_recon: 0.1855989607829082\ntrain unet DSC: [0.6649506092071533, 0.7252368330955505, 6.403027509804815e-05, 0.6998606324195862]\n\ntest loss: 0.6506395003734491\ntest loss_segm: 0.6506395003734491\ntest loss_shape: 0.02654681521921586\ntest loss_recon: 0.12858430640055582\ntest unet DSC: [0.7525043487548828, 0.8085780739784241, 5.261958722257987e-05, 0.7619128227233887]\nBest val loss: 0.6481969738617922\nTime: 57.24115777015686\n\n\nEpoch 48/250\n\ntrain loss: 0.7694567506071888\ntrain loss_segm: 0.7694567506071888\ntrain loss_shape: 0.0365719034371874\ntrain loss_recon: 0.1840373393666895\ntrain unet DSC: [0.670197606086731, 0.7260116338729858, 6.959315942367539e-05, 0.7017915844917297]\n\ntest loss: 0.6318016655934162\ntest loss_segm: 0.6318016655934162\ntest loss_shape: 0.02468864464511474\ntest loss_recon: 0.12398613215639041\ntest unet DSC: [0.7484896779060364, 0.813671886920929, 3.387875767657533e-05, 0.7685966491699219]\nBest val loss: 0.6318016655934162\nTime: 57.12485194206238\n\n\nEpoch 49/250\n\ntrain loss: 0.7667207955559597\ntrain loss_segm: 0.7667207955559597\ntrain loss_shape: 0.036315030925258805\ntrain loss_recon: 0.18537867477140094\ntrain unet DSC: [0.6749150156974792, 0.7270312309265137, 6.717162614222616e-05, 0.6971591711044312]\n\ntest loss: 0.6180894756928469\ntest loss_segm: 0.6180894756928469\ntest loss_shape: 0.023400422782661058\ntest loss_recon: 0.11768952766672158\ntest unet DSC: [0.7503566145896912, 0.8281241059303284, 3.374681546119973e-05, 0.7783952355384827]\nBest val loss: 0.6180894756928469\nTime: 57.13440132141113\n\n\nEpoch 50/250\n\ntrain loss: 0.7610219588762597\ntrain loss_segm: 0.7610219588762597\ntrain loss_shape: 0.036090654969404015\ntrain loss_recon: 0.18410766068138654\ntrain unet DSC: [0.6757096648216248, 0.7302981019020081, 6.559802568517625e-05, 0.7045278549194336]\n\ntest loss: 0.6182939807573954\ntest loss_segm: 0.6182939807573954\ntest loss_shape: 0.023617501848210126\ntest loss_recon: 0.11681744169730407\ntest unet DSC: [0.7417812347412109, 0.829993724822998, 3.080707756453194e-05, 0.7727036476135254]\nBest val loss: 0.6180894756928469\nTime: 56.79865550994873\n\n\nEpoch 51/250\n\ntrain loss: 0.7577517473999458\ntrain loss_segm: 0.7577517473999458\ntrain loss_shape: 0.03613689109164325\ntrain loss_recon: 0.1844329479374463\ntrain unet DSC: [0.6830063462257385, 0.7329512238502502, 6.788790778955445e-05, 0.6978194713592529]\n\ntest loss: 0.6172520640568856\ntest loss_segm: 0.6172520640568856\ntest loss_shape: 0.023592293047561094\ntest loss_recon: 0.11956533627250256\ntest unet DSC: [0.7510290741920471, 0.8308717608451843, 4.037549297208898e-05, 0.7647187113761902]\nBest val loss: 0.6172520640568856\nTime: 56.85414719581604\n\n\nEpoch 52/250\n\ntrain loss: 0.7580018266092373\ntrain loss_segm: 0.7580018266092373\ntrain loss_shape: 0.035984837190731415\ntrain loss_recon: 0.18385728948478458\ntrain unet DSC: [0.6780750751495361, 0.7323287725448608, 6.533779378514737e-05, 0.6975919604301453]\n\ntest loss: 0.6322241822878519\ntest loss_segm: 0.6322241822878519\ntest loss_shape: 0.02544897553534844\ntest loss_recon: 0.12464596932897201\ntest unet DSC: [0.7394986152648926, 0.8179341554641724, 3.524798376020044e-05, 0.7528377771377563]\nBest val loss: 0.6172520640568856\nTime: 57.42247557640076\n\n\nEpoch 53/250\n\ntrain loss: 0.7497162867950488\ntrain loss_segm: 0.7497162867950488\ntrain loss_shape: 0.035769438394640064\ntrain loss_recon: 0.18118114760027657\ntrain unet DSC: [0.6811781525611877, 0.7379637360572815, 6.621852662647143e-05, 0.7060622572898865]\n\ntest loss: 0.6148415528810941\ntest loss_segm: 0.6148415528810941\ntest loss_shape: 0.023312437634628553\ntest loss_recon: 0.1209467965631913\ntest unet DSC: [0.7576401829719543, 0.8227227926254272, 3.83215447072871e-05, 0.765772819519043]\nBest val loss: 0.6148415528810941\nTime: 56.09894371032715\n\n\nEpoch 54/250\n\ntrain loss: 0.7519365126573587\ntrain loss_segm: 0.7519365126573587\ntrain loss_shape: 0.035528720332946205\ntrain loss_recon: 0.1843708750992259\ntrain unet DSC: [0.681108295917511, 0.7358550429344177, 6.401674909284338e-05, 0.6985655426979065]\n\ntest loss: 0.6269355660829788\ntest loss_segm: 0.6269355660829788\ntest loss_shape: 0.02612360991919652\ntest loss_recon: 0.12186152612169583\ntest unet DSC: [0.7475157380104065, 0.81999272108078, 3.6486406315816566e-05, 0.7535895109176636]\nBest val loss: 0.6148415528810941\nTime: 56.301419734954834\n\n\nEpoch 55/250\n\ntrain loss: 0.7442165484911278\ntrain loss_segm: 0.7442165484911278\ntrain loss_shape: 0.03514997678797079\ntrain loss_recon: 0.1808312038266206\ntrain unet DSC: [0.6833744645118713, 0.7370932698249817, 6.830142228864133e-05, 0.7067174315452576]\n\ntest loss: 0.6247982634947851\ntest loss_segm: 0.6247982634947851\ntest loss_shape: 0.026058313245765675\ntest loss_recon: 0.12330091563172829\ntest unet DSC: [0.7594371438026428, 0.8188774585723877, 3.789013499044813e-05, 0.7544334530830383]\nBest val loss: 0.6148415528810941\nTime: 57.05823016166687\n\n\nEpoch 56/250\n\ntrain loss: 0.7455910757372651\ntrain loss_segm: 0.7455910757372651\ntrain loss_shape: 0.03509236025753655\ntrain loss_recon: 0.1823850337274467\ntrain unet DSC: [0.684227705001831, 0.7380253076553345, 6.468300853157416e-05, 0.7006178498268127]\n\ntest loss: 0.6064778963724772\ntest loss_segm: 0.6064778963724772\ntest loss_shape: 0.024051338147658568\ntest loss_recon: 0.11826306371352611\ntest unet DSC: [0.7685311436653137, 0.8310164213180542, 4.363857806310989e-05, 0.7688881754875183]\nBest val loss: 0.6064778963724772\nTime: 56.748820304870605\n\n\nEpoch 57/250\n\ntrain loss: 0.7429608664935148\ntrain loss_segm: 0.7429608664935148\ntrain loss_shape: 0.03509042542899334\ntrain loss_recon: 0.18236675849066505\ntrain unet DSC: [0.6872632503509521, 0.738528847694397, 6.363913416862488e-05, 0.7046522498130798]\n\ntest loss: 0.617560802361904\ntest loss_segm: 0.617560802361904\ntest loss_shape: 0.024428220107578315\ntest loss_recon: 0.12774484518628854\ntest unet DSC: [0.749390184879303, 0.8220937848091125, 2.9338129024836235e-05, 0.7603920102119446]\nBest val loss: 0.6064778963724772\nTime: 56.81884479522705\n\n\nEpoch 58/250\n\ntrain loss: 0.7406958239742473\ntrain loss_segm: 0.7406958239742473\ntrain loss_shape: 0.03516441034270993\ntrain loss_recon: 0.180806266921985\ntrain unet DSC: [0.6852523684501648, 0.7384023666381836, 6.265340925892815e-05, 0.7059733271598816]\n\ntest loss: 0.6540638391788189\ntest loss_segm: 0.6540638391788189\ntest loss_shape: 0.03023370737448717\ntest loss_recon: 0.13608183721319222\ntest unet DSC: [0.7393848299980164, 0.7997432947158813, 4.3863234168384224e-05, 0.7322574853897095]\nBest val loss: 0.6064778963724772\nTime: 56.38058876991272\n\n\nEpoch 59/250\n\ntrain loss: 0.7400343146505235\ntrain loss_segm: 0.7400343146505235\ntrain loss_shape: 0.035056973807513714\ntrain loss_recon: 0.18060122892449174\ntrain unet DSC: [0.68959641456604, 0.7399091720581055, 6.706485146423802e-05, 0.7016851902008057]\n\ntest loss: 0.6159156469198374\ntest loss_segm: 0.6159156469198374\ntest loss_shape: 0.025914746288878795\ntest loss_recon: 0.12174593896055833\ntest unet DSC: [0.754767119884491, 0.8228468298912048, 4.180667383479886e-05, 0.7637660503387451]\nBest val loss: 0.6064778963724772\nTime: 56.51075863838196\n\n\nEpoch 60/250\n\ntrain loss: 0.7392226699032362\ntrain loss_segm: 0.7392226699032362\ntrain loss_shape: 0.03523449600921779\ntrain loss_recon: 0.18183946807550477\ntrain unet DSC: [0.6857091784477234, 0.7403049468994141, 6.581579509656876e-05, 0.7048749327659607]\n\ntest loss: 0.5965579801645035\ntest loss_segm: 0.5965579801645035\ntest loss_shape: 0.02219343309601148\ntest loss_recon: 0.1162853965965601\ntest unet DSC: [0.7705925107002258, 0.8360910415649414, 4.7458659537369385e-05, 0.771134078502655]\nBest val loss: 0.5965579801645035\nTime: 57.529228925704956\n\n\nEpoch 61/250\n\ntrain loss: 0.7369887123379526\ntrain loss_segm: 0.7369887123379526\ntrain loss_shape: 0.03523334214770341\ntrain loss_recon: 0.1810515820980072\ntrain unet DSC: [0.6873925924301147, 0.7402805089950562, 7.026963430689648e-05, 0.7071839570999146]\n\ntest loss: 0.6193122749145215\ntest loss_segm: 0.6193122749145215\ntest loss_shape: 0.026190170660041846\ntest loss_recon: 0.12510814068791193\ntest unet DSC: [0.7556830048561096, 0.8198072910308838, 4.847003219765611e-05, 0.7629436254501343]\nBest val loss: 0.5965579801645035\nTime: 57.041847229003906\n\n\nEpoch 62/250\n\ntrain loss: 0.7344127719915365\ntrain loss_segm: 0.7344127719915365\ntrain loss_shape: 0.03483605429624455\ntrain loss_recon: 0.18019230239376238\ntrain unet DSC: [0.6920516490936279, 0.7425932288169861, 6.645456596743315e-05, 0.7039100527763367]\n\ntest loss: 0.6026053016002362\ntest loss_segm: 0.6026053016002362\ntest loss_shape: 0.0235413055246075\ntest loss_recon: 0.11651606246446952\ntest unet DSC: [0.7647948861122131, 0.8296516537666321, 3.7205125408945605e-05, 0.7615262866020203]\nBest val loss: 0.5965579801645035\nTime: 56.857502698898315\n\n\nEpoch 63/250\n\ntrain loss: 0.7343006232116795\ntrain loss_segm: 0.7343006232116795\ntrain loss_shape: 0.0348870776169285\ntrain loss_recon: 0.18032254024019725\ntrain unet DSC: [0.6905335783958435, 0.7423754930496216, 6.504743942059577e-05, 0.7046293020248413]\n\ntest loss: 0.6617156351223971\ntest loss_segm: 0.6617156351223971\ntest loss_shape: 0.030810348498515595\ntest loss_recon: 0.1414435270887155\ntest unet DSC: [0.7371536493301392, 0.7973284125328064, 5.5585725931450725e-05, 0.7155085206031799]\nBest val loss: 0.5965579801645035\nTime: 56.7508327960968\n\n\nEpoch 64/250\n\ntrain loss: 0.7342205202277703\ntrain loss_segm: 0.7342205202277703\ntrain loss_shape: 0.03489352496270137\ntrain loss_recon: 0.1811383518425724\ntrain unet DSC: [0.6919887661933899, 0.7423641085624695, 6.670213042525575e-05, 0.7035747766494751]\n\ntest loss: 0.638786493967741\ntest loss_segm: 0.638786493967741\ntest loss_shape: 0.028559905835069142\ntest loss_recon: 0.13199078988952515\ntest unet DSC: [0.7460639476776123, 0.81160968542099, 4.700042336480692e-05, 0.7333913445472717]\nBest val loss: 0.5965579801645035\nTime: 57.644328117370605\n\n\nEpoch 65/250\n\ntrain loss: 0.7336803961403763\ntrain loss_segm: 0.7336803961403763\ntrain loss_shape: 0.034852605478107174\ntrain loss_recon: 0.18196575881182392\ntrain unet DSC: [0.692990243434906, 0.7405945062637329, 6.718779332004488e-05, 0.7028862237930298]\n\ntest loss: 0.6371399148916587\ntest loss_segm: 0.6371399148916587\ntest loss_shape: 0.02836781658996374\ntest loss_recon: 0.1332403645874598\ntest unet DSC: [0.7390574216842651, 0.8152473568916321, 4.5988064812263474e-05, 0.739885687828064]\nBest val loss: 0.5965579801645035\nTime: 57.332136154174805\n\n\nEpoch 66/250\n\ntrain loss: 0.7297267065018038\ntrain loss_segm: 0.7297267065018038\ntrain loss_shape: 0.034679916616576385\ntrain loss_recon: 0.17991310914483252\ntrain unet DSC: [0.6894027590751648, 0.7440471649169922, 6.395210948539898e-05, 0.709467351436615]\n\ntest loss: 0.6167653791415386\ntest loss_segm: 0.6167653791415386\ntest loss_shape: 0.025834106529752415\ntest loss_recon: 0.12475925473830639\ntest unet DSC: [0.7522739768028259, 0.8217042088508606, 4.7618199459975585e-05, 0.762930691242218]\nBest val loss: 0.5965579801645035\nTime: 57.54719924926758\n\n\nEpoch 67/250\n\ntrain loss: 0.7304122727128524\ntrain loss_segm: 0.7304122727128524\ntrain loss_shape: 0.034426421198192274\ntrain loss_recon: 0.1796271010667463\ntrain unet DSC: [0.688607394695282, 0.742680013179779, 6.147378735477105e-05, 0.7107649445533752]\n\ntest loss: 0.5938747884371341\ntest loss_segm: 0.5938747884371341\ntest loss_shape: 0.02239441682990545\ntest loss_recon: 0.12111631026252723\ntest unet DSC: [0.7640907168388367, 0.8319529294967651, 2.9205706596258096e-05, 0.770686686038971]\nBest val loss: 0.5938747884371341\nTime: 56.86218047142029\n\n\nEpoch 68/250\n\ntrain loss: 0.7321415641639806\ntrain loss_segm: 0.7321415641639806\ntrain loss_shape: 0.03455504613539463\ntrain loss_recon: 0.182326386152189\ntrain unet DSC: [0.6913019418716431, 0.7434123754501343, 6.500663585029542e-05, 0.7035624384880066]\n\ntest loss: 0.5774485766887665\ntest loss_segm: 0.5774485766887665\ntest loss_shape: 0.02107176122566064\ntest loss_recon: 0.1120298312833676\ntest unet DSC: [0.7799472212791443, 0.845963180065155, 3.80534547730349e-05, 0.7796338200569153]\nBest val loss: 0.5774485766887665\nTime: 56.70654106140137\n\n\nEpoch 69/250\n\ntrain loss: 0.7273063508770133\ntrain loss_segm: 0.7273063508770133\ntrain loss_shape: 0.03442385548702146\ntrain loss_recon: 0.17960459569209739\ntrain unet DSC: [0.6953943967819214, 0.7461422085762024, 6.758302333764732e-05, 0.7037227749824524]\n\ntest loss: 0.6021478519989893\ntest loss_segm: 0.6021478519989893\ntest loss_shape: 0.024628111352332126\ntest loss_recon: 0.1204923784885651\ntest unet DSC: [0.7633950710296631, 0.8296622037887573, 3.5049120924668387e-05, 0.7678280472755432]\nBest val loss: 0.5774485766887665\nTime: 57.069942235946655\n\n\nEpoch 70/250\n\ntrain loss: 0.7306725024422512\ntrain loss_segm: 0.7306725024422512\ntrain loss_shape: 0.03462266839474817\ntrain loss_recon: 0.18094439332998252\ntrain unet DSC: [0.6935830116271973, 0.7455981969833374, 6.465634942287579e-05, 0.700105607509613]\n\ntest loss: 0.6136409388138697\ntest loss_segm: 0.6136409388138697\ntest loss_shape: 0.025370688488086064\ntest loss_recon: 0.1246639836866122\ntest unet DSC: [0.7546117901802063, 0.8237655758857727, 3.997814565082081e-05, 0.7525798678398132]\nBest val loss: 0.5774485766887665\nTime: 56.535698890686035\n\n\nEpoch 71/250\n\ntrain loss: 0.7236368833463404\ntrain loss_segm: 0.7236368833463404\ntrain loss_shape: 0.03396137652895118\ntrain loss_recon: 0.17764230826987495\ntrain unet DSC: [0.6959989070892334, 0.746983528137207, 6.601632048841566e-05, 0.7088853120803833]\n\ntest loss: 0.5975621075202258\ntest loss_segm: 0.5975621075202258\ntest loss_shape: 0.023227123854060967\ntest loss_recon: 0.12139946317825562\ntest unet DSC: [0.7636449337005615, 0.8340391516685486, 3.792592906393111e-05, 0.7644619941711426]\nBest val loss: 0.5774485766887665\nTime: 56.90600252151489\n\n\nEpoch 72/250\n\ntrain loss: 0.7253225694728803\ntrain loss_segm: 0.7253225694728803\ntrain loss_shape: 0.034143900321914424\ntrain loss_recon: 0.17877020026686824\ntrain unet DSC: [0.6968453526496887, 0.7458900809288025, 6.158083124319091e-05, 0.7040285468101501]\n\ntest loss: 0.5976005234779456\ntest loss_segm: 0.5976005234779456\ntest loss_shape: 0.02362065790937497\ntest loss_recon: 0.12045179364772943\ntest unet DSC: [0.7670159339904785, 0.8330615162849426, 3.909844235749915e-05, 0.7679799199104309]\nBest val loss: 0.5774485766887665\nTime: 56.788015365600586\n\n\nEpoch 73/250\n\ntrain loss: 0.7305856128282184\ntrain loss_segm: 0.7305856128282184\ntrain loss_shape: 0.034555589509066904\ntrain loss_recon: 0.18320110638307618\ntrain unet DSC: [0.6922634243965149, 0.7431073784828186, 6.763081910321489e-05, 0.7043634057044983]\n\ntest loss: 0.5859945462300227\ntest loss_segm: 0.5859945462300227\ntest loss_shape: 0.022627715713893756\ntest loss_recon: 0.1156268695799204\ntest unet DSC: [0.7690346837043762, 0.8420906066894531, 3.622447184170596e-05, 0.7727231383323669]\nBest val loss: 0.5774485766887665\nTime: 56.34798789024353\n\n\nEpoch 74/250\n\ntrain loss: 0.7264406594294536\ntrain loss_segm: 0.7264406594294536\ntrain loss_shape: 0.03403339939356982\ntrain loss_recon: 0.1793157894022857\ntrain unet DSC: [0.6916323900222778, 0.7457337975502014, 6.0820759244961664e-05, 0.7049915194511414]\n\ntest loss: 0.5873515835175147\ntest loss_segm: 0.5873515835175147\ntest loss_shape: 0.0229182485729838\ntest loss_recon: 0.1156299430399369\ntest unet DSC: [0.7586886882781982, 0.8396104574203491, 4.242922295816243e-05, 0.7786046266555786]\nBest val loss: 0.5774485766887665\nTime: 56.904844522476196\n\n\nEpoch 75/250\n\ntrain loss: 0.729387428941606\ntrain loss_segm: 0.729387428941606\ntrain loss_shape: 0.03412661683757471\ntrain loss_recon: 0.18213212942775292\ntrain unet DSC: [0.6949685215950012, 0.7442076206207275, 6.253388710319996e-05, 0.7014862895011902]\n\ntest loss: 0.6551767587661743\ntest loss_segm: 0.6551767587661743\ntest loss_shape: 0.03303741790258732\ntest loss_recon: 0.14353749776879945\ntest unet DSC: [0.7319576740264893, 0.8034029006958008, 5.3751806262880564e-05, 0.7282088994979858]\nBest val loss: 0.5774485766887665\nTime: 56.49653124809265\n\n\nEpoch 76/250\n\ntrain loss: 0.7283385697799393\ntrain loss_segm: 0.7283385697799393\ntrain loss_shape: 0.034270099317065525\ntrain loss_recon: 0.1805641646438007\ntrain unet DSC: [0.6933489441871643, 0.7438418865203857, 6.515422865049914e-05, 0.7020736336708069]\n\ntest loss: 0.5736552866605612\ntest loss_segm: 0.5736552866605612\ntest loss_shape: 0.02123863149720889\ntest loss_recon: 0.11348620506051259\ntest unet DSC: [0.7796075940132141, 0.8449229001998901, 3.596472379285842e-05, 0.7819328308105469]\nBest val loss: 0.5736552866605612\nTime: 56.683907985687256\n\n\nEpoch 77/250\n\ntrain loss: 0.7242127132566669\ntrain loss_segm: 0.7242127132566669\ntrain loss_shape: 0.03378949879043842\ntrain loss_recon: 0.1793840094646321\ntrain unet DSC: [0.6954109072685242, 0.7466202974319458, 6.142396159702912e-05, 0.7053806185722351]\n\ntest loss: 0.6050100991359124\ntest loss_segm: 0.6050100991359124\ntest loss_shape: 0.02607758314563678\ntest loss_recon: 0.12363954786306773\ntest unet DSC: [0.7581028938293457, 0.8284763693809509, 4.729289503302425e-05, 0.7701432704925537]\nBest val loss: 0.5736552866605612\nTime: 56.944563150405884\n\n\nEpoch 78/250\n\ntrain loss: 0.7250976781301861\ntrain loss_segm: 0.7250976781301861\ntrain loss_shape: 0.03420407030307039\ntrain loss_recon: 0.18033209660007984\ntrain unet DSC: [0.6948836445808411, 0.7456127405166626, 6.078795559005812e-05, 0.7071849703788757]\n\ntest loss: 0.602490192804581\ntest loss_segm: 0.602490192804581\ntest loss_shape: 0.02542525740006031\ntest loss_recon: 0.12320418025438602\ntest unet DSC: [0.7623608708381653, 0.8243521451950073, 3.779339385800995e-05, 0.7747706770896912]\nBest val loss: 0.5736552866605612\nTime: 56.731335401535034\n\n\nEpoch 79/250\n\ntrain loss: 0.7213787255407889\ntrain loss_segm: 0.7213787255407889\ntrain loss_shape: 0.033877523439122904\ntrain loss_recon: 0.1790279052868674\ntrain unet DSC: [0.7016568183898926, 0.7479203343391418, 6.587959796888754e-05, 0.7083805203437805]\n\ntest loss: 0.5937420206192212\ntest loss_segm: 0.5937420206192212\ntest loss_shape: 0.023581827704149943\ntest loss_recon: 0.1203942339007671\ntest unet DSC: [0.7704687714576721, 0.8330836296081543, 4.399719909997657e-05, 0.7697398066520691]\nBest val loss: 0.5736552866605612\nTime: 56.30619835853577\n\n\nEpoch 80/250\n\ntrain loss: 0.7189573347568512\ntrain loss_segm: 0.7189573347568512\ntrain loss_shape: 0.03361396098957409\ntrain loss_recon: 0.17756681559206564\ntrain unet DSC: [0.7005183696746826, 0.7510464191436768, 5.866312494617887e-05, 0.7056716680526733]\n\ntest loss: 0.5894877589665927\ntest loss_segm: 0.5894877589665927\ntest loss_shape: 0.022304528392851353\ntest loss_recon: 0.12293873755977704\ntest unet DSC: [0.7649981379508972, 0.8343153595924377, 3.2311239920090884e-05, 0.7698627710342407]\nBest val loss: 0.5736552866605612\nTime: 56.580475091934204\n\n\nEpoch 81/250\n\ntrain loss: 0.7221956830235976\ntrain loss_segm: 0.7221956830235976\ntrain loss_shape: 0.03393541783377339\ntrain loss_recon: 0.18048777723614173\ntrain unet DSC: [0.6987438201904297, 0.7465853095054626, 6.717893847962841e-05, 0.7080348134040833]\n\ntest loss: 0.6291792805378253\ntest loss_segm: 0.6291792805378253\ntest loss_shape: 0.02836100809658185\ntest loss_recon: 0.13252871091931295\ntest unet DSC: [0.7462231516838074, 0.8183811902999878, 4.543308386928402e-05, 0.7394062280654907]\nBest val loss: 0.5736552866605612\nTime: 56.4026095867157\n\n\nEpoch 82/250\n\ntrain loss: 0.7199025991596754\ntrain loss_segm: 0.7199025991596754\ntrain loss_shape: 0.03378245205017207\ntrain loss_recon: 0.17795487644174432\ntrain unet DSC: [0.6999281048774719, 0.7499887943267822, 6.143255450297147e-05, 0.7036513090133667]\n\ntest loss: 0.5751581474756583\ntest loss_segm: 0.5751581474756583\ntest loss_shape: 0.02107528642488596\ntest loss_recon: 0.11332728408085994\ntest unet DSC: [0.7781460881233215, 0.8416552543640137, 3.6423010897124186e-05, 0.7853154540061951]\nBest val loss: 0.5736552866605612\nTime: 56.38725805282593\n\n\nEpoch 83/250\n\ntrain loss: 0.7187837637677977\ntrain loss_segm: 0.7187837637677977\ntrain loss_shape: 0.03392569562773916\ntrain loss_recon: 0.1777763278046741\ntrain unet DSC: [0.6989208459854126, 0.7495198249816895, 6.369328912114725e-05, 0.7098331451416016]\n\ntest loss: 0.6344047089417776\ntest loss_segm: 0.6344047089417776\ntest loss_shape: 0.028661568075991593\ntest loss_recon: 0.13456303588090798\ntest unet DSC: [0.7507739067077637, 0.8108811378479004, 5.0522554374765605e-05, 0.7308393716812134]\nBest val loss: 0.5736552866605612\nTime: 56.80189299583435\n\n\nEpoch 84/250\n\ntrain loss: 0.7221118905876256\ntrain loss_segm: 0.7221118905876256\ntrain loss_shape: 0.033561415663814244\ntrain loss_recon: 0.17998384043008467\ntrain unet DSC: [0.6982347369194031, 0.7460846900939941, 6.254603067645803e-05, 0.7076319456100464]\n\ntest loss: 0.5827829104203445\ntest loss_segm: 0.5827829104203445\ntest loss_shape: 0.021986594972893212\ntest loss_recon: 0.11490857247740795\ntest unet DSC: [0.76766037940979, 0.8400848507881165, 3.645237666205503e-05, 0.7736223340034485]\nBest val loss: 0.5736552866605612\nTime: 56.93549823760986\n\n\nEpoch 85/250\n\ntrain loss: 0.7214859637278545\ntrain loss_segm: 0.7214859637278545\ntrain loss_shape: 0.033805404459656796\ntrain loss_recon: 0.17981012156115303\ntrain unet DSC: [0.6990627646446228, 0.7467913031578064, 5.945526936557144e-05, 0.7081261873245239]\n\ntest loss: 0.573049858594552\ntest loss_segm: 0.573049858594552\ntest loss_shape: 0.021407864008767482\ntest loss_recon: 0.11208093538880348\ntest unet DSC: [0.7818313241004944, 0.8421923518180847, 3.521314647514373e-05, 0.7782583236694336]\nBest val loss: 0.573049858594552\nTime: 56.736889362335205\n\n\nEpoch 86/250\n\ntrain loss: 0.7175778576844856\ntrain loss_segm: 0.7175778576844856\ntrain loss_shape: 0.03338760388661412\ntrain loss_recon: 0.17687012585280817\ntrain unet DSC: [0.6985980868339539, 0.7511482238769531, 6.057991777197458e-05, 0.7084466218948364]\n\ntest loss: 0.574872279778505\ntest loss_segm: 0.574872279778505\ntest loss_shape: 0.021017892477221977\ntest loss_recon: 0.11157911184888619\ntest unet DSC: [0.7676377296447754, 0.8438531756401062, 3.119679604424164e-05, 0.7798436880111694]\nBest val loss: 0.573049858594552\nTime: 56.48926281929016\n\n\nEpoch 87/250\n\ntrain loss: 0.7210773776603651\ntrain loss_segm: 0.7210773776603651\ntrain loss_shape: 0.03377375776490456\ntrain loss_recon: 0.17907239355241197\ntrain unet DSC: [0.6978142261505127, 0.7460505962371826, 6.14024029346183e-05, 0.7084671258926392]\n\ntest loss: 0.5794562124288999\ntest loss_segm: 0.5794562124288999\ntest loss_shape: 0.02155246404119027\ntest loss_recon: 0.11601616088778545\ntest unet DSC: [0.7666014432907104, 0.8422847390174866, 3.2798128813738e-05, 0.7767385840415955]\nBest val loss: 0.573049858594552\nTime: 56.92628312110901\n\n\nEpoch 88/250\n\ntrain loss: 0.7196902670437777\ntrain loss_segm: 0.7196902670437777\ntrain loss_shape: 0.0333855979828329\ntrain loss_recon: 0.1796260452534579\ntrain unet DSC: [0.6982438564300537, 0.7469607591629028, 6.062733882572502e-05, 0.7094008326530457]\n\ntest loss: 0.5757823105041797\ntest loss_segm: 0.5757823105041797\ntest loss_shape: 0.02132162396819928\ntest loss_recon: 0.11287905380893977\ntest unet DSC: [0.7763948440551758, 0.8409533500671387, 3.720500535564497e-05, 0.7785115838050842]\nBest val loss: 0.573049858594552\nTime: 56.13968515396118\n\n\nEpoch 89/250\n\ntrain loss: 0.715297703878789\ntrain loss_segm: 0.715297703878789\ntrain loss_shape: 0.033024277042927624\ntrain loss_recon: 0.17705865939961205\ntrain unet DSC: [0.7017828226089478, 0.7534281015396118, 6.08999325777404e-05, 0.7053661346435547]\n\ntest loss: 0.6045939754217099\ntest loss_segm: 0.6045939754217099\ntest loss_shape: 0.025966609517733257\ntest loss_recon: 0.1230030910900006\ntest unet DSC: [0.7557053565979004, 0.8310319781303406, 4.0600360080134124e-05, 0.7576609253883362]\nBest val loss: 0.573049858594552\nTime: 56.43937611579895\n\n\nEpoch 90/250\n\ntrain loss: 0.7222359187995331\ntrain loss_segm: 0.7222359187995331\ntrain loss_shape: 0.033606375744448434\ntrain loss_recon: 0.18052445389801944\ntrain unet DSC: [0.6987996101379395, 0.7484905123710632, 6.076721183490008e-05, 0.7021698951721191]\n\ntest loss: 0.5901806301031357\ntest loss_segm: 0.5901806301031357\ntest loss_shape: 0.02336590352635353\ntest loss_recon: 0.11934432884057362\ntest unet DSC: [0.7674980163574219, 0.8296858072280884, 3.6780995287699625e-05, 0.7731720805168152]\nBest val loss: 0.573049858594552\nTime: 56.79609680175781\n\n\nEpoch 91/250\n\ntrain loss: 0.7211622116686423\ntrain loss_segm: 0.7211622116686423\ntrain loss_shape: 0.03338830200130049\ntrain loss_recon: 0.1801473537955103\ntrain unet DSC: [0.6982186436653137, 0.7472172975540161, 6.0732392739737406e-05, 0.7070221304893494]\n\ntest loss: 0.632570282006875\ntest loss_segm: 0.632570282006875\ntest loss_shape: 0.029971993576066617\ntest loss_recon: 0.13309563982945222\ntest unet DSC: [0.7455069422721863, 0.8054467439651489, 5.055542351328768e-05, 0.7467077970504761]\nBest val loss: 0.573049858594552\nTime: 56.520986557006836\n\n\nEpoch 92/250\n\ntrain loss: 0.7194411656524562\ntrain loss_segm: 0.7194411656524562\ntrain loss_shape: 0.03370052772939582\ntrain loss_recon: 0.17907039557076707\ntrain unet DSC: [0.6990320682525635, 0.7517879009246826, 6.520565511891618e-05, 0.7046777606010437]\n\ntest loss: 0.6832731931637495\ntest loss_segm: 0.6832731931637495\ntest loss_shape: 0.03631956163698282\ntest loss_recon: 0.154341354010961\ntest unet DSC: [0.7221572995185852, 0.7868039011955261, 5.8185549278277904e-05, 0.6890133619308472]\nBest val loss: 0.573049858594552\nTime: 57.34729290008545\n\n\nEpoch 93/250\n\ntrain loss: 0.7185797325417965\ntrain loss_segm: 0.7185797325417965\ntrain loss_shape: 0.03366279882626443\ntrain loss_recon: 0.17865499297652063\ntrain unet DSC: [0.7000267505645752, 0.7503286600112915, 5.931523992330767e-05, 0.7048459649085999]\n\ntest loss: 0.5878186623255411\ntest loss_segm: 0.5878186623255411\ntest loss_shape: 0.02315759092855912\ntest loss_recon: 0.11755648436836708\ntest unet DSC: [0.7742550373077393, 0.8308291435241699, 3.9751997974235564e-05, 0.7729018926620483]\nBest val loss: 0.573049858594552\nTime: 57.28762459754944\n\n\nEpoch 94/250\n\ntrain loss: 0.717518274165407\ntrain loss_segm: 0.717518274165407\ntrain loss_shape: 0.03326652873354622\ntrain loss_recon: 0.1786902451628371\ntrain unet DSC: [0.7018027305603027, 0.7484366297721863, 6.077187572373077e-05, 0.7086148262023926]\n\ntest loss: 0.5784755746523539\ntest loss_segm: 0.5784755746523539\ntest loss_shape: 0.022123778501573283\ntest loss_recon: 0.12015885792863674\ntest unet DSC: [0.7752379775047302, 0.8358076214790344, 2.5906163500621915e-05, 0.7790158987045288]\nBest val loss: 0.573049858594552\nTime: 57.074800968170166\n\n\nEpoch 95/250\n\ntrain loss: 0.7168274525600143\ntrain loss_segm: 0.7168274525600143\ntrain loss_shape: 0.03324102707018581\ntrain loss_recon: 0.17790681199182437\ntrain unet DSC: [0.7007615566253662, 0.7514117956161499, 6.089635280659422e-05, 0.7048110961914062]\n\ntest loss: 0.5757081615619171\ntest loss_segm: 0.5757081615619171\ntest loss_shape: 0.022012809100441445\ntest loss_recon: 0.11336124048401149\ntest unet DSC: [0.7776187062263489, 0.8396164178848267, 4.046917092637159e-05, 0.7857025265693665]\nBest val loss: 0.573049858594552\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.87801432609558\n\n\nEpoch 96/250\n\ntrain loss: 0.7134104438974888\ntrain loss_segm: 0.7134104438974888\ntrain loss_shape: 0.03276835150922401\ntrain loss_recon: 0.1761332515485679\ntrain unet DSC: [0.7044411897659302, 0.7509833574295044, 6.301212124526501e-05, 0.7103061676025391]\n\ntest loss: 0.5708716213703156\ntest loss_segm: 0.5708716213703156\ntest loss_shape: 0.02101512387012824\ntest loss_recon: 0.1137329438366951\ntest unet DSC: [0.7794604301452637, 0.8437167406082153, 3.524607382132672e-05, 0.7839831113815308]\nBest val loss: 0.5708716213703156\nTime: 56.683319091796875\n\n\nEpoch 97/250\n\ntrain loss: 0.7124369329289545\ntrain loss_segm: 0.7124369329289545\ntrain loss_shape: 0.03260139243889459\ntrain loss_recon: 0.17680052224593826\ntrain unet DSC: [0.7034391164779663, 0.7536473870277405, 6.143346399767324e-05, 0.7111128568649292]\n\ntest loss: 0.5762563286683499\ntest loss_segm: 0.5762563286683499\ntest loss_shape: 0.021851322709176786\ntest loss_recon: 0.11539043782231136\ntest unet DSC: [0.7764790058135986, 0.8409585356712341, 3.782497515203431e-05, 0.7816320061683655]\nBest val loss: 0.5708716213703156\nTime: 57.5607328414917\n\n\nEpoch 98/250\n\ntrain loss: 0.7089786491816557\ntrain loss_segm: 0.7089786491816557\ntrain loss_shape: 0.032392484253814705\ntrain loss_recon: 0.17589133072503005\ntrain unet DSC: [0.7048259973526001, 0.7543583512306213, 5.8143341448158026e-05, 0.7159212231636047]\n\ntest loss: 0.5858414081426767\ntest loss_segm: 0.5858414081426767\ntest loss_shape: 0.0230822124494574\ntest loss_recon: 0.11841492421734028\ntest unet DSC: [0.7699368000030518, 0.8347995281219482, 3.9294041926041245e-05, 0.7772693037986755]\nBest val loss: 0.5708716213703156\nTime: 60.91224956512451\n\n\nEpoch 99/250\n\ntrain loss: 0.70542487577547\ntrain loss_segm: 0.70542487577547\ntrain loss_shape: 0.03190392476235387\ntrain loss_recon: 0.17422945901185652\ntrain unet DSC: [0.7082658410072327, 0.7567539215087891, 6.103112536948174e-05, 0.7156023383140564]\n\ntest loss: 0.5722076082840944\ntest loss_segm: 0.5722076082840944\ntest loss_shape: 0.02127735787190688\ntest loss_recon: 0.11293515286002404\ntest unet DSC: [0.7772132754325867, 0.8431967496871948, 3.446063419687562e-05, 0.7831369042396545]\nBest val loss: 0.5708716213703156\nTime: 57.05245542526245\n\n\nEpoch 100/250\n\ntrain loss: 0.7107417877716354\ntrain loss_segm: 0.7107417877716354\ntrain loss_shape: 0.032349452600354635\ntrain loss_recon: 0.17602094923016393\ntrain unet DSC: [0.7042344212532043, 0.7551281452178955, 5.885558857698925e-05, 0.7116752862930298]\n\ntest loss: 0.6174423832159776\ntest loss_segm: 0.6174423832159776\ntest loss_shape: 0.027889082948557842\ntest loss_recon: 0.12937819298643333\ntest unet DSC: [0.755369246006012, 0.8125613331794739, 4.356802674010396e-05, 0.7587636113166809]\nBest val loss: 0.5708716213703156\nTime: 56.8391478061676\n\n\nEpoch 101/250\n\ntrain loss: 0.708094625156137\ntrain loss_segm: 0.708094625156137\ntrain loss_shape: 0.03242972387071652\ntrain loss_recon: 0.1749896760794181\ntrain unet DSC: [0.7057040929794312, 0.756625771522522, 6.084987035137601e-05, 0.7129654884338379]\n\ntest loss: 0.5744576209630722\ntest loss_segm: 0.5744576209630722\ntest loss_shape: 0.021733058783679437\ntest loss_recon: 0.11387363563363369\ntest unet DSC: [0.7771610617637634, 0.8418711423873901, 3.612595537560992e-05, 0.7841348648071289]\nBest val loss: 0.5708716213703156\nTime: 56.53848218917847\n\n\nEpoch 102/250\n\ntrain loss: 0.7115628277953667\ntrain loss_segm: 0.7115628277953667\ntrain loss_shape: 0.032040669582784176\ntrain loss_recon: 0.17704852208306518\ntrain unet DSC: [0.7047844529151917, 0.7539570927619934, 5.7547687902115285e-05, 0.7096560597419739]\n\ntest loss: 0.5709481835365295\ntest loss_segm: 0.5709481835365295\ntest loss_shape: 0.02087908933082452\ntest loss_recon: 0.11466307431841508\ntest unet DSC: [0.7785595655441284, 0.8445567488670349, 3.32539711962454e-05, 0.7842226028442383]\nBest val loss: 0.5708716213703156\nTime: 56.628767013549805\n\n\nEpoch 103/250\n\ntrain loss: 0.7092331991165499\ntrain loss_segm: 0.7092331991165499\ntrain loss_shape: 0.032171665502217\ntrain loss_recon: 0.17548396806173686\ntrain unet DSC: [0.7052158117294312, 0.7566211819648743, 5.8580921177053824e-05, 0.7095658183097839]\n\ntest loss: 0.5678626749760065\ntest loss_segm: 0.5678626749760065\ntest loss_shape: 0.020465834758793697\ntest loss_recon: 0.11325650709943894\ntest unet DSC: [0.7785437703132629, 0.8469049334526062, 3.243763785576448e-05, 0.785029411315918]\nBest val loss: 0.5678626749760065\nTime: 56.65004777908325\n\n\nEpoch 104/250\n\ntrain loss: 0.704749772443047\ntrain loss_segm: 0.704749772443047\ntrain loss_shape: 0.03197811837484942\ntrain loss_recon: 0.17338759933091416\ntrain unet DSC: [0.7081314325332642, 0.7591168284416199, 6.0934238717891276e-05, 0.7146833539009094]\n\ntest loss: 0.5891934113624768\ntest loss_segm: 0.5891934113624768\ntest loss_shape: 0.023306745916413955\ntest loss_recon: 0.12107031878370506\ntest unet DSC: [0.7652654647827148, 0.8347564339637756, 3.8379996112780645e-05, 0.7773686647415161]\nBest val loss: 0.5678626749760065\nTime: 56.6636803150177\n\n\nEpoch 105/250\n\ntrain loss: 0.7059927480884746\ntrain loss_segm: 0.7059927480884746\ntrain loss_shape: 0.03168621727654451\ntrain loss_recon: 0.17474995657235762\ntrain unet DSC: [0.7077585458755493, 0.7584583163261414, 5.5047112255124375e-05, 0.7117015719413757]\n\ntest loss: 0.578840964115583\ntest loss_segm: 0.578840964115583\ntest loss_shape: 0.02202996430106652\ntest loss_recon: 0.11645265831015049\ntest unet DSC: [0.774276852607727, 0.8391256332397461, 3.7302346754586324e-05, 0.7815844416618347]\nBest val loss: 0.5678626749760065\nTime: 56.64503741264343\n\n\nEpoch 106/250\n\ntrain loss: 0.7014104063752331\ntrain loss_segm: 0.7014104063752331\ntrain loss_shape: 0.03140829270116136\ntrain loss_recon: 0.17099582356742665\ntrain unet DSC: [0.710212767124176, 0.7587854862213135, 5.696099833585322e-05, 0.7186689972877502]\n\ntest loss: 0.5661552976339291\ntest loss_segm: 0.5661552976339291\ntest loss_shape: 0.020287899013895255\ntest loss_recon: 0.11222274095202103\ntest unet DSC: [0.7823823094367981, 0.8479663133621216, 3.295974966022186e-05, 0.7866918444633484]\nBest val loss: 0.5661552976339291\nTime: 56.821346044540405\n\n\nEpoch 107/250\n\ntrain loss: 0.7076581094083907\ntrain loss_segm: 0.7076581094083907\ntrain loss_shape: 0.031889271674842776\ntrain loss_recon: 0.1742564410537104\ntrain unet DSC: [0.7094791531562805, 0.7577798962593079, 6.089495218475349e-05, 0.7057031393051147]\n\ntest loss: 0.568818640250426\ntest loss_segm: 0.568818640250426\ntest loss_shape: 0.020489398007973645\ntest loss_recon: 0.11336512663043462\ntest unet DSC: [0.7787754535675049, 0.8464639186859131, 3.119670873275027e-05, 0.7818415760993958]\nBest val loss: 0.5661552976339291\nTime: 57.030394077301025\n\n\nEpoch 108/250\n\ntrain loss: 0.7077889702742612\ntrain loss_segm: 0.7077889702742612\ntrain loss_shape: 0.03209202726006131\ntrain loss_recon: 0.17525376549249963\ntrain unet DSC: [0.7061027884483337, 0.7578593492507935, 5.7314169680466875e-05, 0.7124132513999939]\n\ntest loss: 0.5722949099846375\ntest loss_segm: 0.5722949099846375\ntest loss_shape: 0.02147349801201087\ntest loss_recon: 0.11333698903520902\ntest unet DSC: [0.7750154137611389, 0.8443610072135925, 3.4526532544987276e-05, 0.785679280757904]\nBest val loss: 0.5661552976339291\nTime: 56.50217962265015\n\n\nEpoch 109/250\n\ntrain loss: 0.7077582448343688\ntrain loss_segm: 0.7077582448343688\ntrain loss_shape: 0.03194777284383397\ntrain loss_recon: 0.1742062633739242\ntrain unet DSC: [0.707527756690979, 0.7558690309524536, 5.688863166142255e-05, 0.7100925445556641]\n\ntest loss: 0.5657206223561213\ntest loss_segm: 0.5657206223561213\ntest loss_shape: 0.020401162525209095\ntest loss_recon: 0.11241525172805175\ntest unet DSC: [0.7828437685966492, 0.846160888671875, 3.495242344797589e-05, 0.7859215140342712]\nBest val loss: 0.5657206223561213\nTime: 56.81811738014221\n\n\nEpoch 110/250\n\ntrain loss: 0.7088709198221376\ntrain loss_segm: 0.7088709198221376\ntrain loss_shape: 0.03198260592320298\ntrain loss_recon: 0.17659718769637844\ntrain unet DSC: [0.7033402919769287, 0.7554343938827515, 5.8860583521891385e-05, 0.7156233191490173]\n\ntest loss: 0.5623901280073019\ntest loss_segm: 0.5623901280073019\ntest loss_shape: 0.020071844522578593\ntest loss_recon: 0.11180063155599129\ntest unet DSC: [0.7827082872390747, 0.8502436876296997, 3.263311737100594e-05, 0.7872211337089539]\nBest val loss: 0.5623901280073019\nTime: 56.96989297866821\n\n\nEpoch 111/250\n\ntrain loss: 0.7093652041652535\ntrain loss_segm: 0.7093652041652535\ntrain loss_shape: 0.03223131944814438\ntrain loss_recon: 0.17726744456758983\ntrain unet DSC: [0.7042111754417419, 0.7551776766777039, 5.867087747901678e-05, 0.7139639854431152]\n\ntest loss: 0.5650577079027127\ntest loss_segm: 0.5650577079027127\ntest loss_shape: 0.020201077851920556\ntest loss_recon: 0.11287092770903538\ntest unet DSC: [0.7809728384017944, 0.8476249575614929, 3.237253986299038e-05, 0.7900025248527527]\nBest val loss: 0.5623901280073019\nTime: 56.47400259971619\n\n\nEpoch 112/250\n\ntrain loss: 0.7091340309456934\ntrain loss_segm: 0.7091340309456934\ntrain loss_shape: 0.03162010414879533\ntrain loss_recon: 0.17658465063270135\ntrain unet DSC: [0.7074704766273499, 0.7553197741508484, 5.6495016906410456e-05, 0.7107479572296143]\n\ntest loss: 0.5734886641685779\ntest loss_segm: 0.5734886641685779\ntest loss_shape: 0.021215636545839984\ntest loss_recon: 0.11594545640624486\ntest unet DSC: [0.7757124900817871, 0.8436768651008606, 3.482175452518277e-05, 0.7855793833732605]\nBest val loss: 0.5623901280073019\nTime: 56.61433720588684\n\n\nEpoch 113/250\n\ntrain loss: 0.7071924352947669\ntrain loss_segm: 0.7071924352947669\ntrain loss_shape: 0.03193417879976804\ntrain loss_recon: 0.17591556963286822\ntrain unet DSC: [0.7089773416519165, 0.7570505142211914, 5.676365617546253e-05, 0.7105767130851746]\n\ntest loss: 0.5745411683351566\ntest loss_segm: 0.5745411683351566\ntest loss_shape: 0.021583113079078686\ntest loss_recon: 0.11553788748689187\ntest unet DSC: [0.7745249271392822, 0.8427830338478088, 3.5409106203587726e-05, 0.783812940120697]\nBest val loss: 0.5623901280073019\nTime: 57.19554281234741\n\n\nEpoch 114/250\n\ntrain loss: 0.7074851816213583\ntrain loss_segm: 0.7074851816213583\ntrain loss_shape: 0.03186511053833403\ntrain loss_recon: 0.17434564242257347\ntrain unet DSC: [0.7070298790931702, 0.7562109231948853, 5.699353278032504e-05, 0.711410403251648]\n\ntest loss: 0.5700479615957309\ntest loss_segm: 0.5700479615957309\ntest loss_shape: 0.020807738654697552\ntest loss_recon: 0.11522972956299782\ntest unet DSC: [0.7781383991241455, 0.8459725975990295, 3.423384259804152e-05, 0.7849647998809814]\nBest val loss: 0.5623901280073019\nTime: 56.56604599952698\n\n\nEpoch 115/250\n\ntrain loss: 0.7018371718593791\ntrain loss_segm: 0.7018371718593791\ntrain loss_shape: 0.03139632003075337\ntrain loss_recon: 0.17257008069678198\ntrain unet DSC: [0.7113732695579529, 0.7602015733718872, 5.889834210393019e-05, 0.7138845324516296]\n\ntest loss: 0.5692963401476542\ntest loss_segm: 0.5692963401476542\ntest loss_shape: 0.020371561010296527\ntest loss_recon: 0.11504656754625149\ntest unet DSC: [0.7782250642776489, 0.8470483422279358, 3.1164963729679585e-05, 0.7820948362350464]\nBest val loss: 0.5623901280073019\nTime: 56.03754806518555\n\n\nEpoch 116/250\n\ntrain loss: 0.7048860795890228\ntrain loss_segm: 0.7048860795890228\ntrain loss_shape: 0.03177916482562506\ntrain loss_recon: 0.17513563182157807\ntrain unet DSC: [0.7060128450393677, 0.7585938572883606, 5.907479862798937e-05, 0.7167032957077026]\n\ntest loss: 0.5852756859400333\ntest loss_segm: 0.5852756859400333\ntest loss_shape: 0.02311177895619319\ntest loss_recon: 0.11908623146323058\ntest unet DSC: [0.7705782651901245, 0.8352773785591125, 4.148167499806732e-05, 0.7774766683578491]\nBest val loss: 0.5623901280073019\nTime: 57.081254720687866\n\n\nEpoch 117/250\n\ntrain loss: 0.7061591284184516\ntrain loss_segm: 0.7061591284184516\ntrain loss_shape: 0.03180606649080409\ntrain loss_recon: 0.17513682995038696\ntrain unet DSC: [0.7083905935287476, 0.7587764859199524, 5.736896491725929e-05, 0.71165531873703]\n\ntest loss: 0.5682482184507908\ntest loss_segm: 0.5682482184507908\ntest loss_shape: 0.020583107804831784\ntest loss_recon: 0.11421787805664234\ntest unet DSC: [0.7783921957015991, 0.8485937714576721, 3.446356640779413e-05, 0.7843195796012878]\nBest val loss: 0.5623901280073019\nTime: 57.00774264335632\n\n\nEpoch 118/250\n\ntrain loss: 0.704987295066254\ntrain loss_segm: 0.704987295066254\ntrain loss_shape: 0.03145844403277092\ntrain loss_recon: 0.17425083453896678\ntrain unet DSC: [0.7102086544036865, 0.7588106393814087, 5.999032873660326e-05, 0.7099247574806213]\n\ntest loss: 0.5721375185709733\ntest loss_segm: 0.5721375185709733\ntest loss_shape: 0.021545102580999717\ntest loss_recon: 0.11492931183714133\ntest unet DSC: [0.7770744562149048, 0.844199538230896, 3.736877624760382e-05, 0.7850707173347473]\nBest val loss: 0.5623901280073019\nTime: 56.331218957901\n\n\nEpoch 119/250\n\ntrain loss: 0.7108604368529742\ntrain loss_segm: 0.7108604368529742\ntrain loss_shape: 0.031442594773407224\ntrain loss_recon: 0.17677652486894704\ntrain unet DSC: [0.7056571245193481, 0.7561389803886414, 5.831438465975225e-05, 0.7043789625167847]\n\ntest loss: 0.6006665504895724\ntest loss_segm: 0.6006665504895724\ntest loss_shape: 0.02516746112646965\ntest loss_recon: 0.1244183819836531\ntest unet DSC: [0.7618077993392944, 0.8285894989967346, 4.2199233575956896e-05, 0.765356183052063]\nBest val loss: 0.5623901280073019\nTime: 57.70054650306702\n\n\nEpoch 120/250\n\ntrain loss: 0.7010799008834211\ntrain loss_segm: 0.7010799008834211\ntrain loss_shape: 0.031440226384733296\ntrain loss_recon: 0.17232279828454874\ntrain unet DSC: [0.7106496095657349, 0.7604325413703918, 5.610740481643006e-05, 0.7167381644248962]\n\ntest loss: 0.5767415104768215\ntest loss_segm: 0.5767415104768215\ntest loss_shape: 0.0221720869676807\ntest loss_recon: 0.11630945280194283\ntest unet DSC: [0.7746378779411316, 0.8419877886772156, 3.6814195482293144e-05, 0.7831690907478333]\nBest val loss: 0.5623901280073019\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.841031074523926\n\n\nEpoch 121/250\n\ntrain loss: 0.7055947550489933\ntrain loss_segm: 0.7055947550489933\ntrain loss_shape: 0.032006512985482245\ntrain loss_recon: 0.17378711426937127\ntrain unet DSC: [0.7107298374176025, 0.7573558688163757, 5.733196303481236e-05, 0.7076677680015564]\n\ntest loss: 0.569217731555303\ntest loss_segm: 0.569217731555303\ntest loss_shape: 0.021146981976926327\ntest loss_recon: 0.11305091119347474\ntest unet DSC: [0.7785605192184448, 0.8466255068778992, 3.5213615774409845e-05, 0.7850230932235718]\nBest val loss: 0.5623901280073019\nTime: 56.48950481414795\n\n\nEpoch 122/250\n\ntrain loss: 0.7002569840678686\ntrain loss_segm: 0.7002569840678686\ntrain loss_shape: 0.03169511832721249\ntrain loss_recon: 0.17308829114123991\ntrain unet DSC: [0.713150143623352, 0.7596606612205505, 6.024386311764829e-05, 0.7187310457229614]\n\ntest loss: 0.5670151099180564\ntest loss_segm: 0.5670151099180564\ntest loss_shape: 0.020794297687900372\ntest loss_recon: 0.11210090791185696\ntest unet DSC: [0.7812073826789856, 0.847794234752655, 3.6225770600140095e-05, 0.7869058847427368]\nBest val loss: 0.5623901280073019\nTime: 56.16653347015381\n\n\nEpoch 123/250\n\ntrain loss: 0.7023182047318809\ntrain loss_segm: 0.7023182047318809\ntrain loss_shape: 0.03180713833698744\ntrain loss_recon: 0.17291150298676913\ntrain unet DSC: [0.7117831110954285, 0.7594802379608154, 5.939437687629834e-05, 0.7141178250312805]\n\ntest loss: 0.5990904049995618\ntest loss_segm: 0.5990904049995618\ntest loss_shape: 0.025226436268824797\ntest loss_recon: 0.1236590539606718\ntest unet DSC: [0.7633927464485168, 0.828340470790863, 4.396148870000616e-05, 0.7695066928863525]\nBest val loss: 0.5623901280073019\nTime: 56.93284893035889\n\n\nEpoch 124/250\n\ntrain loss: 0.7114670023887972\ntrain loss_segm: 0.7114670023887972\ntrain loss_shape: 0.03185114932918473\ntrain loss_recon: 0.1773931819804107\ntrain unet DSC: [0.7042762637138367, 0.7560357451438904, 5.740262713516131e-05, 0.7077397108078003]\n\ntest loss: 0.5662089570974692\ntest loss_segm: 0.5662089570974692\ntest loss_shape: 0.02062233582807657\ntest loss_recon: 0.11217487880434746\ntest unet DSC: [0.7823742628097534, 0.8480839729309082, 3.648675556178205e-05, 0.7864586710929871]\nBest val loss: 0.5623901280073019\nTime: 56.70425796508789\n\n\nEpoch 125/250\n\ntrain loss: 0.703702246840996\ntrain loss_segm: 0.703702246840996\ntrain loss_shape: 0.03183588411517536\ntrain loss_recon: 0.17428641191011743\ntrain unet DSC: [0.710482656955719, 0.7581388354301453, 5.869921005796641e-05, 0.7166528105735779]\n\ntest loss: 0.5674300247277969\ntest loss_segm: 0.5674300247277969\ntest loss_shape: 0.0206601680853428\ntest loss_recon: 0.11359910848431098\ntest unet DSC: [0.7811769247055054, 0.8477070927619934, 3.469149305601604e-05, 0.7866560220718384]\nBest val loss: 0.5623901280073019\nTime: 56.66203188896179\n\n\nEpoch 126/250\n\ntrain loss: 0.70291198922109\ntrain loss_segm: 0.70291198922109\ntrain loss_shape: 0.031501814569759215\ntrain loss_recon: 0.17365459399887279\ntrain unet DSC: [0.7115126848220825, 0.759723961353302, 6.277082138694823e-05, 0.7113795876502991]\n\ntest loss: 0.5740837126206129\ntest loss_segm: 0.5740837126206129\ntest loss_shape: 0.021781698108101502\ntest loss_recon: 0.11551087588453904\ntest unet DSC: [0.7758548259735107, 0.8442164659500122, 3.776050652959384e-05, 0.7842435836791992]\nBest val loss: 0.5623901280073019\nTime: 56.52349925041199\n\n\nEpoch 127/250\n\ntrain loss: 0.7067055766341053\ntrain loss_segm: 0.7067055766341053\ntrain loss_shape: 0.03149006203335675\ntrain loss_recon: 0.1755457969405983\ntrain unet DSC: [0.7090867161750793, 0.7541444301605225, 5.891832915949635e-05, 0.715390145778656]\n\ntest loss: 0.5706367645508204\ntest loss_segm: 0.5706367645508204\ntest loss_shape: 0.0213225549325729\ntest loss_recon: 0.1140154992731718\ntest unet DSC: [0.7786552309989929, 0.8458494544029236, 3.714006015798077e-05, 0.7855454683303833]\nBest val loss: 0.5623901280073019\nTime: 56.867873191833496\n\n\nEpoch 128/250\n\ntrain loss: 0.702492468719241\ntrain loss_segm: 0.702492468719241\ntrain loss_shape: 0.03153409714562983\ntrain loss_recon: 0.17319649797451647\ntrain unet DSC: [0.7081735730171204, 0.7599285244941711, 5.938824688200839e-05, 0.7161811590194702]\n\ntest loss: 0.5818276451184199\ntest loss_segm: 0.5818276451184199\ntest loss_shape: 0.022760574276057575\ntest loss_recon: 0.11723997759131286\ntest unet DSC: [0.7718321681022644, 0.8384170532226562, 3.9718284824630246e-05, 0.7804385423660278]\nBest val loss: 0.5623901280073019\nTime: 57.12383055686951\n\n\nEpoch 129/250\n\ntrain loss: 0.7066583637195297\ntrain loss_segm: 0.7066583637195297\ntrain loss_shape: 0.03132708125476596\ntrain loss_recon: 0.17463448898324482\ntrain unet DSC: [0.7060334086418152, 0.7572449445724487, 5.5642416555201635e-05, 0.7143027186393738]\n\ntest loss: 0.5648522086632557\ntest loss_segm: 0.5648522086632557\ntest loss_shape: 0.020125372956196468\ntest loss_recon: 0.11301953431505424\ntest unet DSC: [0.7810961604118347, 0.8502136468887329, 3.2111620384966955e-05, 0.7846143841743469]\nBest val loss: 0.5623901280073019\nTime: 57.0750846862793\n\n\nEpoch 130/250\n\ntrain loss: 0.7038568865649307\ntrain loss_segm: 0.7038568865649307\ntrain loss_shape: 0.03136695847151023\ntrain loss_recon: 0.1744994395707227\ntrain unet DSC: [0.7077930569648743, 0.7582167387008667, 5.961226634099148e-05, 0.71775883436203]\n\ntest loss: 0.5699721635916294\ntest loss_segm: 0.5699721635916294\ntest loss_shape: 0.021167690077653296\ntest loss_recon: 0.11366462506926976\ntest unet DSC: [0.7795155644416809, 0.8459605574607849, 3.684618786792271e-05, 0.7852807641029358]\nBest val loss: 0.5623901280073019\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.57926821708679\n\n\nEpoch 131/250\n\ntrain loss: 0.7020514897907837\ntrain loss_segm: 0.7020514897907837\ntrain loss_shape: 0.031132940026118033\ntrain loss_recon: 0.17237345638531673\ntrain unet DSC: [0.7119945287704468, 0.758564829826355, 5.5921806051628664e-05, 0.7136666178703308]\n\ntest loss: 0.5654454643909748\ntest loss_segm: 0.5654454643909748\ntest loss_shape: 0.020444366722725905\ntest loss_recon: 0.11258119258743066\ntest unet DSC: [0.7826359868049622, 0.8487091064453125, 3.5278888390166685e-05, 0.7870202660560608]\nBest val loss: 0.5623901280073019\nTime: 57.168843030929565\n\n\nEpoch 132/250\n\ntrain loss: 0.7023300962357582\ntrain loss_segm: 0.7023300962357582\ntrain loss_shape: 0.031806402568575705\ntrain loss_recon: 0.17324694142311434\ntrain unet DSC: [0.71284419298172, 0.7591014504432678, 6.168484833324328e-05, 0.7136019468307495]\n\ntest loss: 0.5692489200677627\ntest loss_segm: 0.5692489200677627\ntest loss_shape: 0.021052253551972218\ntest loss_recon: 0.11349349707747117\ntest unet DSC: [0.7794154286384583, 0.8464422821998596, 3.700962770381011e-05, 0.7866822481155396]\nBest val loss: 0.5623901280073019\nTime: 56.653958797454834\n\n\nEpoch 133/250\n\ntrain loss: 0.7049279775046096\ntrain loss_segm: 0.7049279775046096\ntrain loss_shape: 0.031378357258589964\ntrain loss_recon: 0.17444204727682885\ntrain unet DSC: [0.710651695728302, 0.758820652961731, 5.816699922434054e-05, 0.7097947597503662]\n\ntest loss: 0.565286778486692\ntest loss_segm: 0.565286778486692\ntest loss_shape: 0.020322750250880536\ntest loss_recon: 0.11247392103840144\ntest unet DSC: [0.7820977568626404, 0.849058210849762, 3.576899689505808e-05, 0.7867783904075623]\nBest val loss: 0.5623901280073019\nTime: 57.2315776348114\n\n\nEpoch 134/250\n\ntrain loss: 0.7045113859297354\ntrain loss_segm: 0.7045113859297354\ntrain loss_shape: 0.03110189905649499\ntrain loss_recon: 0.1739516758088824\ntrain unet DSC: [0.7094680070877075, 0.7592541575431824, 5.756385871791281e-05, 0.7105207443237305]\n\ntest loss: 0.5709289854917771\ntest loss_segm: 0.5709289854917771\ntest loss_shape: 0.021367478017241526\ntest loss_recon: 0.11389208250702956\ntest unet DSC: [0.7783904075622559, 0.845439612865448, 3.7955865991534665e-05, 0.7855513095855713]\nBest val loss: 0.5623901280073019\nTime: 57.18775534629822\n\n\nEpoch 135/250\n\ntrain loss: 0.7051556525351126\ntrain loss_segm: 0.7051556525351126\ntrain loss_shape: 0.031180602675185926\ntrain loss_recon: 0.1750402850440786\ntrain unet DSC: [0.7074393630027771, 0.757983922958374, 5.620836964226328e-05, 0.7160546183586121]\n\ntest loss: 0.5642749269803365\ntest loss_segm: 0.5642749269803365\ntest loss_shape: 0.02019951227479256\ntest loss_recon: 0.11226205250773674\ntest unet DSC: [0.7830462455749512, 0.8495367765426636, 3.5181168641429394e-05, 0.7871200442314148]\nBest val loss: 0.5623901280073019\nTime: 56.920408725738525\n\n\nEpoch 136/250\n\ntrain loss: 0.7082414359231538\ntrain loss_segm: 0.7082414359231538\ntrain loss_shape: 0.03133108990290497\ntrain loss_recon: 0.1753967894783503\ntrain unet DSC: [0.7070971727371216, 0.7568372488021851, 5.671145117958076e-05, 0.7090685963630676]\n\ntest loss: 0.5672255693337857\ntest loss_segm: 0.5672255693337857\ntest loss_shape: 0.02057384054821271\ntest loss_recon: 0.11370459189399695\ntest unet DSC: [0.7799676060676575, 0.8480404615402222, 3.472406751825474e-05, 0.7854523658752441]\nBest val loss: 0.5623901280073019\nTime: 56.939313650131226\n\n\nEpoch 137/250\n\ntrain loss: 0.7025854919530168\ntrain loss_segm: 0.7025854919530168\ntrain loss_shape: 0.031467168155727504\ntrain loss_recon: 0.173914143556281\ntrain unet DSC: [0.7100607752799988, 0.7596896886825562, 6.024233516654931e-05, 0.7158660888671875]\n\ntest loss: 0.5735777196211692\ntest loss_segm: 0.5735777196211692\ntest loss_shape: 0.021646943134375107\ntest loss_recon: 0.11531472158355591\ntest unet DSC: [0.7767097353935242, 0.8434844613075256, 3.805414598900825e-05, 0.7846729755401611]\nBest val loss: 0.5623901280073019\nTime: 57.056307315826416\n\n\nEpoch 138/250\n\ntrain loss: 0.7030301588245585\ntrain loss_segm: 0.7030301588245585\ntrain loss_shape: 0.031420381445013267\ntrain loss_recon: 0.17310927327297912\ntrain unet DSC: [0.7095731496810913, 0.7590999603271484, 5.966272146906704e-05, 0.7140856981277466]\n\ntest loss: 0.5637001395225525\ntest loss_segm: 0.5637001395225525\ntest loss_shape: 0.020093494333708897\ntest loss_recon: 0.1124119612459953\ntest unet DSC: [0.7823248505592346, 0.8500785827636719, 3.260129960835911e-05, 0.7864242196083069]\nBest val loss: 0.5623901280073019\nTime: 56.778372287750244\n\n\nEpoch 139/250\n\ntrain loss: 0.704933963621719\ntrain loss_segm: 0.704933963621719\ntrain loss_shape: 0.03180063789403891\ntrain loss_recon: 0.1747627457297301\ntrain unet DSC: [0.7077773809432983, 0.7577695250511169, 5.8249504945706576e-05, 0.7152227163314819]\n\ntest loss: 0.5699357382762127\ntest loss_segm: 0.5699357382762127\ntest loss_shape: 0.021114268865531836\ntest loss_recon: 0.11419320822908328\ntest unet DSC: [0.7784213423728943, 0.8463177680969238, 3.5964843846159056e-05, 0.7865923643112183]\nBest val loss: 0.5623901280073019\nTime: 57.21809387207031\n\n\nEpoch 140/250\n\ntrain loss: 0.7041912689993653\ntrain loss_segm: 0.7041912689993653\ntrain loss_shape: 0.031790828367671636\ntrain loss_recon: 0.17521106414025342\ntrain unet DSC: [0.7080100178718567, 0.7566149234771729, 5.9999067161697894e-05, 0.7203693985939026]\n\ntest loss: 0.5634524707610791\ntest loss_segm: 0.5634524707610791\ntest loss_shape: 0.020067230726663884\ntest loss_recon: 0.11211293696975097\ntest unet DSC: [0.7823915481567383, 0.8504959344863892, 3.36462544510141e-05, 0.7869318723678589]\nBest val loss: 0.5623901280073019\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.805758476257324\n\n\nEpoch 141/250\n\ntrain loss: 0.7057009045081802\ntrain loss_segm: 0.7057009045081802\ntrain loss_shape: 0.03137814245316424\ntrain loss_recon: 0.17391718263867534\ntrain unet DSC: [0.707991898059845, 0.7576531171798706, 5.916888767387718e-05, 0.7101538777351379]\n\ntest loss: 0.5669345022776188\ntest loss_segm: 0.5669345022776188\ntest loss_shape: 0.02073428349999281\ntest loss_recon: 0.11271117694484882\ntest unet DSC: [0.7810634970664978, 0.8474952578544617, 3.625827594078146e-05, 0.786691427230835]\nBest val loss: 0.5623901280073019\nTime: 56.630072832107544\n\n\nEpoch 142/250\n\ntrain loss: 0.7056092502950113\ntrain loss_segm: 0.7056092502950113\ntrain loss_shape: 0.031352222920690155\ntrain loss_recon: 0.17489531391029117\ntrain unet DSC: [0.7093244194984436, 0.757691502571106, 5.77862374484539e-05, 0.7121461033821106]\n\ntest loss: 0.5673417708812616\ntest loss_segm: 0.5673417708812616\ntest loss_shape: 0.02065184139288389\ntest loss_recon: 0.11347393567363422\ntest unet DSC: [0.780808687210083, 0.8472056984901428, 3.4691125620156527e-05, 0.78663569688797]\nBest val loss: 0.5623901280073019\nTime: 57.29460048675537\n\n\nEpoch 143/250\n\ntrain loss: 0.7026209774651105\ntrain loss_segm: 0.7026209774651105\ntrain loss_shape: 0.03168464011123663\ntrain loss_recon: 0.17416515870939328\ntrain unet DSC: [0.7089695930480957, 0.7604942321777344, 5.8775949582923204e-05, 0.7141721844673157]\n\ntest loss: 0.602211092527096\ntest loss_segm: 0.602211092527096\ntest loss_shape: 0.025756326098090563\ntest loss_recon: 0.12542869752416244\ntest unet DSC: [0.7620084881782532, 0.8255961537361145, 4.52020758530125e-05, 0.7671509385108948]\nBest val loss: 0.5623901280073019\nTime: 57.1636860370636\n\n\nEpoch 144/250\n\ntrain loss: 0.7058790427974507\ntrain loss_segm: 0.7058790427974507\ntrain loss_shape: 0.03158134201989521\ntrain loss_recon: 0.17551562041515792\ntrain unet DSC: [0.7092763185501099, 0.755843460559845, 5.6287532061105594e-05, 0.7146602869033813]\n\ntest loss: 0.5713485632187281\ntest loss_segm: 0.5713485632187281\ntest loss_shape: 0.0213693684587876\ntest loss_recon: 0.11447847300232986\ntest unet DSC: [0.778151273727417, 0.8455080986022949, 3.691182064358145e-05, 0.7847175002098083]\nBest val loss: 0.5623901280073019\nTime: 56.82803511619568\n\n\nEpoch 145/250\n\ntrain loss: 0.6993577495405946\ntrain loss_segm: 0.6993577495405946\ntrain loss_shape: 0.03096116586482223\ntrain loss_recon: 0.1712951007523114\ntrain unet DSC: [0.7127948999404907, 0.7598339915275574, 5.834192052134313e-05, 0.7174137830734253]\n\ntest loss: 0.5694400140872369\ntest loss_segm: 0.5694400140872369\ntest loss_shape: 0.021181798826616544\ntest loss_recon: 0.11314889970116127\ntest unet DSC: [0.7786264419555664, 0.8464766144752502, 3.743374327314086e-05, 0.7865549325942993]\nBest val loss: 0.5623901280073019\nTime: 57.196977853775024\n\n\nEpoch 146/250\n\ntrain loss: 0.7025611768417721\ntrain loss_segm: 0.7025611768417721\ntrain loss_shape: 0.03133379178758286\ntrain loss_recon: 0.17282887220477003\ntrain unet DSC: [0.7116636037826538, 0.7599905729293823, 5.797688208986074e-05, 0.7113540172576904]\n\ntest loss: 0.5681588122477899\ntest loss_segm: 0.5681588122477899\ntest loss_shape: 0.020870309729033556\ntest loss_recon: 0.11315678738248654\ntest unet DSC: [0.7798547744750977, 0.846843421459198, 3.6715722671942785e-05, 0.7871655821800232]\nBest val loss: 0.5623901280073019\nTime: 57.538959980010986\n\n\nEpoch 147/250\n\ntrain loss: 0.7024775562407095\ntrain loss_segm: 0.7024775562407095\ntrain loss_shape: 0.03131089549323049\ntrain loss_recon: 0.17317419963641256\ntrain unet DSC: [0.7109077572822571, 0.760775625705719, 5.8846773754339665e-05, 0.7109710574150085]\n\ntest loss: 0.564509996236899\ntest loss_segm: 0.564509996236899\ntest loss_shape: 0.020122717301814984\ntest loss_recon: 0.11276737762949406\ntest unet DSC: [0.7809683680534363, 0.8502972722053528, 3.273229958722368e-05, 0.785301685333252]\nBest val loss: 0.5623901280073019\nTime: 56.48694682121277\n\n\nEpoch 148/250\n\ntrain loss: 0.70560030507136\ntrain loss_segm: 0.70560030507136\ntrain loss_shape: 0.03148939077376942\ntrain loss_recon: 0.17458136455167697\ntrain unet DSC: [0.7097870111465454, 0.7563098073005676, 5.812860035803169e-05, 0.7108070254325867]\n\ntest loss: 0.5629299237177923\ntest loss_segm: 0.5629299237177923\ntest loss_shape: 0.020052815047212135\ntest loss_recon: 0.11171052614465737\ntest unet DSC: [0.7830765843391418, 0.8502699136734009, 3.43319152307231e-05, 0.7874007821083069]\nBest val loss: 0.5623901280073019\nTime: 57.147340297698975\n\n\nEpoch 149/250\n\ntrain loss: 0.7026854048046884\ntrain loss_segm: 0.7026854048046884\ntrain loss_shape: 0.03144304673600046\ntrain loss_recon: 0.17417399971922742\ntrain unet DSC: [0.7082495093345642, 0.7592346668243408, 5.9124300605617464e-05, 0.7169386744499207]\n\ntest loss: 0.5673384582385038\ntest loss_segm: 0.5673384582385038\ntest loss_shape: 0.020577733906415794\ntest loss_recon: 0.11368216067934647\ntest unet DSC: [0.7792836427688599, 0.8480772376060486, 3.3874963264679536e-05, 0.7867689728736877]\nBest val loss: 0.5623901280073019\nTime: 57.11374640464783\n\n\nEpoch 150/250\n\ntrain loss: 0.705180274534829\ntrain loss_segm: 0.705180274534829\ntrain loss_shape: 0.03145129022577518\ntrain loss_recon: 0.17476497977217542\ntrain unet DSC: [0.7097963094711304, 0.7562113404273987, 5.776656689704396e-05, 0.7135666012763977]\n\ntest loss: 0.5707104481183566\ntest loss_segm: 0.5707104481183566\ntest loss_shape: 0.021267831516571533\ntest loss_recon: 0.11423275361840542\ntest unet DSC: [0.7783453464508057, 0.8456631898880005, 3.655238469946198e-05, 0.785274088382721]\nBest val loss: 0.5623901280073019\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.02149701118469\n\n\nEpoch 151/250\n\ntrain loss: 0.7043938927258118\ntrain loss_segm: 0.7043938927258118\ntrain loss_shape: 0.0318013648797251\ntrain loss_recon: 0.17403878516788723\ntrain unet DSC: [0.7085928916931152, 0.759146511554718, 5.8951674873242155e-05, 0.7115132808685303]\n\ntest loss: 0.5645584448789939\ntest loss_segm: 0.5645584448789939\ntest loss_shape: 0.020171361569410715\ntest loss_recon: 0.11264982657172741\ntest unet DSC: [0.7814955115318298, 0.8498454689979553, 3.325477518956177e-05, 0.7856278419494629]\nBest val loss: 0.5623901280073019\nTime: 57.20325303077698\n\n\nEpoch 152/250\n\ntrain loss: 0.701714188614978\ntrain loss_segm: 0.701714188614978\ntrain loss_shape: 0.0314428328854751\ntrain loss_recon: 0.17245005948256842\ntrain unet DSC: [0.7104895710945129, 0.7601948976516724, 5.652085746987723e-05, 0.7147843837738037]\n\ntest loss: 0.5724208156267802\ntest loss_segm: 0.5724208156267802\ntest loss_shape: 0.0214539613479223\ntest loss_recon: 0.11447579232163918\ntest unet DSC: [0.7782351970672607, 0.8437269330024719, 3.83802289434243e-05, 0.7847583293914795]\nBest val loss: 0.5623901280073019\nTime: 56.7761664390564\n\n\nEpoch 153/250\n\ntrain loss: 0.7037380662145494\ntrain loss_segm: 0.7037380662145494\ntrain loss_shape: 0.03148446229722681\ntrain loss_recon: 0.17341264346732368\ntrain unet DSC: [0.7097664475440979, 0.7595203518867493, 5.780090214102529e-05, 0.7122206091880798]\n\ntest loss: 0.5892201608572251\ntest loss_segm: 0.5892201608572251\ntest loss_shape: 0.02366073950169942\ntest loss_recon: 0.12021967644492786\ntest unet DSC: [0.7674590945243835, 0.8342549204826355, 4.1611838241806254e-05, 0.776255190372467]\nBest val loss: 0.5623901280073019\nTime: 70.02854990959167\n\n\nEpoch 154/250\n\ntrain loss: 0.7073585194877431\ntrain loss_segm: 0.7073585194877431\ntrain loss_shape: 0.03169291775343539\ntrain loss_recon: 0.17672891292390944\ntrain unet DSC: [0.7069409489631653, 0.7561485171318054, 5.778810736956075e-05, 0.7141550779342651]\n\ntest loss: 0.5689825927599882\ntest loss_segm: 0.5689825927599882\ntest loss_shape: 0.02107714720739004\ntest loss_recon: 0.11334116136034329\ntest unet DSC: [0.7801377177238464, 0.8464665412902832, 3.707478754222393e-05, 0.7849563360214233]\nBest val loss: 0.5623901280073019\nTime: 57.86290526390076\n\n\nEpoch 155/250\n\ntrain loss: 0.7020426053035108\ntrain loss_segm: 0.7020426053035108\ntrain loss_shape: 0.031419243321671515\ntrain loss_recon: 0.17340605951185467\ntrain unet DSC: [0.7100691199302673, 0.7573461532592773, 5.830032023368403e-05, 0.7188636064529419]\n\ntest loss: 0.566703295860535\ntest loss_segm: 0.566703295860535\ntest loss_shape: 0.020536807485115834\ntest loss_recon: 0.11324030991930228\ntest unet DSC: [0.7806711196899414, 0.8481713533401489, 3.4821907320292667e-05, 0.7860381007194519]\nBest val loss: 0.5623901280073019\nTime: 56.54901456832886\n\n\nEpoch 156/250\n\ntrain loss: 0.7012664287150661\ntrain loss_segm: 0.7012664287150661\ntrain loss_shape: 0.03136284023381864\ntrain loss_recon: 0.17329056476113164\ntrain unet DSC: [0.70890212059021, 0.7595353126525879, 5.908310413360596e-05, 0.7183414697647095]\n\ntest loss: 0.5717863494004959\ntest loss_segm: 0.5717863494004959\ntest loss_shape: 0.021430075717851136\ntest loss_recon: 0.11454511252351296\ntest unet DSC: [0.7779817581176758, 0.8448897004127502, 3.746659785974771e-05, 0.7848206758499146]\nBest val loss: 0.5623901280073019\nTime: 58.61729955673218\n\n\nEpoch 157/250\n\ntrain loss: 0.7012203064145921\ntrain loss_segm: 0.7012203064145921\ntrain loss_shape: 0.03128097268835276\ntrain loss_recon: 0.1735611988962451\ntrain unet DSC: [0.7123370170593262, 0.7611673474311829, 5.840509038534947e-05, 0.7148725986480713]\n\ntest loss: 0.5635974300213349\ntest loss_segm: 0.5635974300213349\ntest loss_shape: 0.019961669587363035\ntest loss_recon: 0.11257260493361033\ntest unet DSC: [0.7818046808242798, 0.8506078720092773, 3.178498809575103e-05, 0.7851219177246094]\nBest val loss: 0.5623901280073019\nTime: 57.0255606174469\n\n\nEpoch 158/250\n\ntrain loss: 0.7043808596798137\ntrain loss_segm: 0.7043808596798137\ntrain loss_shape: 0.031548716068928\ntrain loss_recon: 0.17488124447910092\ntrain unet DSC: [0.7091042995452881, 0.7581885457038879, 5.647550642606802e-05, 0.713840663433075]\n\ntest loss: 0.5681669834332589\ntest loss_segm: 0.5681669834332589\ntest loss_shape: 0.02096869858602683\ntest loss_recon: 0.11282572026054065\ntest unet DSC: [0.7800410389900208, 0.8471815586090088, 3.6911238566972315e-05, 0.7868125438690186]\nBest val loss: 0.5623901280073019\nTime: 57.42473793029785\n\n\nEpoch 159/250\n\ntrain loss: 0.7015904969052423\ntrain loss_segm: 0.7015904969052423\ntrain loss_shape: 0.031191551161906386\ntrain loss_recon: 0.1728891715218749\ntrain unet DSC: [0.7083551287651062, 0.7606235146522522, 5.508719550562091e-05, 0.7188035249710083]\n\ntest loss: 0.5672385310515379\ntest loss_segm: 0.5672385310515379\ntest loss_shape: 0.020757817042370636\ntest loss_recon: 0.11263749337731263\ntest unet DSC: [0.7797966003417969, 0.8478075861930847, 3.6291043215896934e-05, 0.7874250411987305]\nBest val loss: 0.5623901280073019\nTime: 56.778844118118286\n\n\nEpoch 160/250\n\ntrain loss: 0.7040259770200222\ntrain loss_segm: 0.7040259770200222\ntrain loss_shape: 0.0311217030769662\ntrain loss_recon: 0.17384251405166673\ntrain unet DSC: [0.708785891532898, 0.7596251368522644, 5.4455675126519054e-05, 0.7114864587783813]\n\ntest loss: 0.5734257186070467\ntest loss_segm: 0.5734257186070467\ntest loss_shape: 0.021551642399758864\ntest loss_recon: 0.11503785648024999\ntest unet DSC: [0.7778339982032776, 0.8428356647491455, 3.811908027273603e-05, 0.7840956449508667]\nBest val loss: 0.5623901280073019\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.441060304641724\n\nValidation loss stopped to decrease for 50 epochs. Training terminated.\nBest epoch: 110\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678167152419
        }
      },
      "id": "df70ab72-64aa-46eb-ac49-f0f998b0d028"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.1, 0.1)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/250\n\ntrain loss: 2.1087590772894362\ntrain loss_segm: 1.995371024819869\ntrain loss_shape: 0.19677230866649484\ntrain loss_recon: 0.9371084167232996\ntrain unet DSC: [0.0003391536301933229, 0.0005034269415773451, 4.100931255379692e-05, 0.0006307260482572019]\n\ntest loss: 2.100265710781782\ntest loss_segm: 1.995046520844484\ntest loss_shape: 0.11667958838053238\ntest loss_recon: 0.9355125778760666\ntest unet DSC: [0.0001986196730285883, 0.0004923303495161235, 2.2826770873507485e-06, 0.0008464180282317102]\nBest val loss: 2.100265710781782\nTime: 57.82144379615784\n\n\nEpoch 2/250\n\ntrain loss: 2.0908771496784837\ntrain loss_segm: 1.9949659780610967\ntrain loss_shape: 0.16490313292870037\ntrain loss_recon: 0.7942084360726273\ntrain unet DSC: [0.0004432617861311883, 0.0004593630146700889, 3.7360787246143445e-05, 0.0012333305785432458]\n\ntest loss: 2.0738546603765244\ntest loss_segm: 1.9949707709825957\ntest loss_shape: 0.16495221127302218\ntest loss_recon: 0.6238865210459783\ntest unet DSC: [0.0006603985675610602, 0.0007340951124206185, 7.387788355117664e-05, 0.0014524018624797463]\nBest val loss: 2.0738546603765244\nTime: 57.72028470039368\n\n\nEpoch 3/250\n\ntrain loss: 2.067141406143768\ntrain loss_segm: 1.9950357585013667\ntrain loss_shape: 0.15576149872209452\ntrain loss_recon: 0.5652949968470803\ntrain unet DSC: [0.0007491302094422281, 0.0008533471846021712, 8.5902247519698e-05, 0.001505688764154911]\n\ntest loss: 2.058266358497815\ntest loss_segm: 1.9945410123238196\ntest loss_shape: 0.13540803793913278\ntest loss_recon: 0.5018455890508798\ntest unet DSC: [0.0006837959517724812, 0.0007838285528123379, 8.093647920759395e-05, 0.0018139275489374995]\nBest val loss: 2.058266358497815\nTime: 58.10917377471924\n\n\nEpoch 4/250\n\ntrain loss: 2.0542468949209285\ntrain loss_segm: 1.9946941846533666\ntrain loss_shape: 0.13372351870506624\ntrain loss_recon: 0.46180371355406846\ntrain unet DSC: [0.0008392477175220847, 0.0010056090541183949, 0.00010075260070152581, 0.001605311525054276]\n\ntest loss: 2.0412101929004374\ntest loss_segm: 1.994664525374388\ntest loss_shape: 0.09955514795505084\ntest loss_recon: 0.36590163829999095\ntest unet DSC: [0.000815375882666558, 0.0009635451715439558, 0.00011110959167126566, 0.0017257931176573038]\nBest val loss: 2.0412101929004374\nTime: 58.50507831573486\n\n\nEpoch 5/250\n\ntrain loss: 2.044830577282966\ntrain loss_segm: 1.9945138489143759\ntrain loss_shape: 0.11249317497580866\ntrain loss_recon: 0.3906740714100343\ntrain unet DSC: [0.0008586145122535527, 0.0010803358163684607, 0.00010803548502735794, 0.0017302355263382196]\n\ntest loss: 2.034819627419496\ntest loss_segm: 1.9941863494041638\ntest loss_shape: 0.0889140887138171\ntest loss_recon: 0.3174185424278944\ntest unet DSC: [0.0007915314636193216, 0.0009781161788851023, 9.488352225162089e-05, 0.001841076067648828]\nBest val loss: 2.034819627419496\nTime: 57.61682987213135\n\n\nEpoch 6/250\n\ntrain loss: 2.0394126493719558\ntrain loss_segm: 1.9942720834212968\ntrain loss_shape: 0.10371011712505848\ntrain loss_recon: 0.34769552229325984\ntrain unet DSC: [0.0008255880675278604, 0.0010640183463692665, 0.00010563326941337436, 0.0019002992194145918]\n\ntest loss: 2.0271802804408927\ntest loss_segm: 1.9939833971170278\ntest loss_shape: 0.07873410578721608\ntest loss_recon: 0.25323485449338573\ntest unet DSC: [0.0007694601663388312, 0.0009683558018878102, 0.00010594310151645914, 0.001961727859452367]\nBest val loss: 2.0271802804408927\nTime: 57.81716823577881\n\n\nEpoch 7/250\n\ntrain loss: 2.035047525092016\ntrain loss_segm: 1.993910179108004\ntrain loss_shape: 0.09745739103306698\ntrain loss_recon: 0.31391607300390173\ntrain unet DSC: [0.0008201611344702542, 0.0010008164681494236, 0.00010404257045593113, 0.002072612289339304]\n\ntest loss: 2.02770909284934\ntest loss_segm: 1.9936185402747912\ntest loss_shape: 0.07958366129642878\ntest loss_recon: 0.26132185222246707\ntest unet DSC: [0.0007687087636440992, 0.000912594492547214, 0.00010021404887083918, 0.002167360158637166]\nBest val loss: 2.0271802804408927\nTime: 57.59053874015808\n\n\nEpoch 8/250\n\ntrain loss: 2.0323154428337196\ntrain loss_segm: 1.9936274244815488\ntrain loss_shape: 0.09271471942716007\ntrain loss_recon: 0.2941655727881419\ntrain unet DSC: [0.0007874133880250156, 0.0009441819274798036, 0.00010675005614757538, 0.002121984027326107]\n\ntest loss: 2.026115038456061\ntest loss_segm: 1.9928499123989007\ntest loss_shape: 0.08462097209233505\ntest loss_recon: 0.24803037750415313\ntest unet DSC: [0.0006781936390325427, 0.0007545606349594891, 9.368727478431538e-05, 0.0022606332786381245]\nBest val loss: 2.026115038456061\nTime: 57.853941440582275\n\n\nEpoch 9/250\n\ntrain loss: 2.0291511348531217\ntrain loss_segm: 1.9930708106560042\ntrain loss_shape: 0.08842868735141392\ntrain loss_recon: 0.2723745220824133\ntrain unet DSC: [0.0007930245483294129, 0.0009237154154106975, 0.00011215962877031416, 0.002221230184659362]\n\ntest loss: 2.0202932418921056\ntest loss_segm: 1.9927059198037171\ntest loss_shape: 0.07523542986466335\ntest loss_recon: 0.2006376970272798\ntest unet DSC: [0.000678346143104136, 0.0007879391778260469, 9.32211842155084e-05, 0.0022674687206745148]\nBest val loss: 2.0202932418921056\nTime: 57.9775071144104\n\n\nEpoch 10/250\n\ntrain loss: 2.0260032581377634\ntrain loss_segm: 1.9927884083759935\ntrain loss_shape: 0.08470001802602901\ntrain loss_recon: 0.24744845021374617\ntrain unet DSC: [0.0007347887731157243, 0.0008569646743126214, 0.00011038868979085237, 0.002261101733893156]\n\ntest loss: 2.018695978017954\ntest loss_segm: 1.9919583461223505\ntest loss_shape: 0.0693839508562516\ntest loss_recon: 0.19799234049442488\ntest unet DSC: [0.0006456270348280668, 0.0007058099145069718, 9.4172268291004e-05, 0.002343685133382678]\nBest val loss: 2.018695978017954\nTime: 57.926275968551636\n\n\nEpoch 11/250\n\ntrain loss: 2.025887923150123\ntrain loss_segm: 1.9921070224122157\ntrain loss_shape: 0.08473528138821639\ntrain loss_recon: 0.25307374343841893\ntrain unet DSC: [0.0007241258281283081, 0.0008413185132667422, 0.00011095230001956224, 0.0022560199722647667]\n\ntest loss: 2.013094052290305\ntest loss_segm: 1.9912936565203545\ntest loss_shape: 0.06961802469614224\ntest loss_recon: 0.1483859348182495\ntest unet DSC: [0.0006086757057346404, 0.0006911127711646259, 9.036341361934319e-05, 0.0023021462839096785]\nBest val loss: 2.013094052290305\nTime: 57.9605987071991\n\n\nEpoch 12/250\n\ntrain loss: 2.0228009291842013\ntrain loss_segm: 1.9915283246885371\ntrain loss_shape: 0.08187084263072739\ntrain loss_recon: 0.23085515285971797\ntrain unet DSC: [0.0006869134376756847, 0.0008079400868155062, 0.00010989229485858232, 0.002361477352678776]\n\ntest loss: 2.0124500011786437\ntest loss_segm: 1.990217468677423\ntest loss_shape: 0.06603567798932393\ntest loss_recon: 0.15628945684203735\ntest unet DSC: [0.0005524805164895952, 0.0006487412611022592, 9.239736391464248e-05, 0.002715249080210924]\nBest val loss: 2.0124500011786437\nTime: 58.12259316444397\n\n\nEpoch 13/250\n\ntrain loss: 2.0198743886585477\ntrain loss_segm: 1.990825416166571\ntrain loss_shape: 0.08004367441127572\ntrain loss_recon: 0.2104462029624589\ntrain unet DSC: [0.0006199521012604237, 0.0007540254155173898, 9.980873437598348e-05, 0.0025285170413553715]\n\ntest loss: 2.010322766426282\ntest loss_segm: 1.9896557667316535\ntest loss_shape: 0.06761309848381923\ntest loss_recon: 0.13905718540533996\ntest unet DSC: [0.0005270238034427166, 0.0006555618601851165, 8.274800347862765e-05, 0.0027209913823753595]\nBest val loss: 2.010322766426282\nTime: 57.93542551994324\n\n\nEpoch 14/250\n\ntrain loss: 2.020202222504193\ntrain loss_segm: 1.9892342384857467\ntrain loss_shape: 0.08103715873594526\ntrain loss_recon: 0.22864265428690972\ntrain unet DSC: [0.0006245023105293512, 0.0007761672022752464, 0.00010296593973180279, 0.0029057811480015516]\n\ntest loss: 2.0165084325350247\ntest loss_segm: 1.9891194380246675\ntest loss_shape: 0.0723896300754486\ntest loss_recon: 0.20150035820328271\ntest unet DSC: [0.0005421495297923684, 0.0006557021988555789, 7.791470852680504e-05, 0.002619348233565688]\nBest val loss: 2.010322766426282\nTime: 57.94341230392456\n\n\nEpoch 15/250\n\ntrain loss: 2.016851007938385\ntrain loss_segm: 1.989131120941307\ntrain loss_shape: 0.07829477364503884\ntrain loss_recon: 0.1989042328882821\ntrain unet DSC: [0.0005676178843714297, 0.000720982498023659, 0.0001022048236336559, 0.0028999741189181805]\n\ntest loss: 2.0066505456582093\ntest loss_segm: 1.9878371159235637\ntest loss_shape: 0.06580869280374967\ntest loss_recon: 0.1223256470492253\ntest unet DSC: [0.0005163320456631482, 0.0006341204280033708, 9.387084719492123e-05, 0.003223428037017584]\nBest val loss: 2.0066505456582093\nTime: 58.25078320503235\n\n\nEpoch 16/250\n\ntrain loss: 2.013619104518166\ntrain loss_segm: 1.9881368762330165\ntrain loss_shape: 0.07717256108888343\ntrain loss_recon: 0.17764976689049713\ntrain unet DSC: [0.0005530891357921064, 0.0006757163209840655, 9.642780059948564e-05, 0.0031565038952976465]\n\ntest loss: 2.002851412846492\ntest loss_segm: 1.985898772875468\ntest loss_shape: 0.06495148516618289\ntest loss_recon: 0.10457500557486828\ntest unet DSC: [0.000475209963042289, 0.0005825330154038966, 8.458512456854805e-05, 0.003476195503026247]\nBest val loss: 2.002851412846492\nTime: 57.73928260803223\n\n\nEpoch 17/250\n\ntrain loss: 2.01138788612583\ntrain loss_segm: 1.9867414937743657\ntrain loss_shape: 0.07797863515966301\ntrain loss_recon: 0.16848540947407106\ntrain unet DSC: [0.000549463729839772, 0.0006921119638718665, 9.715167107060552e-05, 0.003429664531722665]\n\ntest loss: 2.0016591518353195\ntest loss_segm: 1.984723170598348\ntest loss_shape: 0.06641849397848813\ntest loss_recon: 0.10294130377662487\ntest unet DSC: [0.00047672586515545845, 0.000564898073207587, 7.748793723294511e-05, 0.0036052411887794733]\nBest val loss: 2.0016591518353195\nTime: 57.951575756073\n\n\nEpoch 18/250\n\ntrain loss: 2.0089006499399114\ntrain loss_segm: 1.9851679696312434\ntrain loss_shape: 0.07762459917819198\ntrain loss_recon: 0.15970221998853773\ntrain unet DSC: [0.0005530324415303767, 0.0006854297243990004, 9.115965804085135e-05, 0.003657500958070159]\n\ntest loss: 2.0014785589315953\ntest loss_segm: 1.9834562142690022\ntest loss_shape: 0.0681655580798785\ntest loss_recon: 0.11205779933012448\ntest unet DSC: [0.00048274514847435057, 0.000550420256331563, 6.862526061013341e-05, 0.003947642166167498]\nBest val loss: 2.0014785589315953\nTime: 57.94271922111511\n\n\nEpoch 19/250\n\ntrain loss: 2.00833635692355\ntrain loss_segm: 1.9845125841189035\ntrain loss_shape: 0.07800045107267325\ntrain loss_recon: 0.16023733353690256\ntrain unet DSC: [0.000543291971553117, 0.0006564186769537628, 8.834806067170575e-05, 0.0036084402818232775]\n\ntest loss: 1.998369913834792\ntest loss_segm: 1.9820185838601527\ntest loss_shape: 0.06512213536562064\ntest loss_recon: 0.09839113496052913\ntest unet DSC: [0.00045459152897819877, 0.000547920644748956, 6.666042463621125e-05, 0.0038917239289730787]\nBest val loss: 1.998369913834792\nTime: 58.023754596710205\n\n\nEpoch 20/250\n\ntrain loss: 2.0063734167738807\ntrain loss_segm: 1.9834346914593177\ntrain loss_shape: 0.0773972910652055\ntrain loss_recon: 0.15198990815802466\ntrain unet DSC: [0.0005272586713545024, 0.0006427259650081396, 8.467572479275987e-05, 0.0037759565748274326]\n\ntest loss: 1.9974319140116374\ntest loss_segm: 1.9808317331167369\ntest loss_shape: 0.06522795223654845\ntest loss_recon: 0.1007739238632031\ntest unet DSC: [0.00045677387970499694, 0.0005439432570710778, 6.413892697310075e-05, 0.00395341869443655]\nBest val loss: 1.9974319140116374\nTime: 57.64511561393738\n\n\nEpoch 21/250\n\ntrain loss: 2.0046539993225774\ntrain loss_segm: 1.9819703532170645\ntrain loss_shape: 0.07688987962430037\ntrain loss_recon: 0.1499465984539895\ntrain unet DSC: [0.0005164319300092757, 0.0006276565254665911, 8.141173020703718e-05, 0.0038359048776328564]\n\ntest loss: 1.9991225370993981\ntest loss_segm: 1.9799255774571345\ntest loss_shape: 0.06908121227453916\ntest loss_recon: 0.12288855321896382\ntest unet DSC: [0.00045953699736855924, 0.0005398588255047798, 5.790506475022994e-05, 0.003966502845287323]\nBest val loss: 1.9974319140116374\nTime: 57.43549704551697\n\n\nEpoch 22/250\n\ntrain loss: 2.003650985186613\ntrain loss_segm: 1.9805848772012735\ntrain loss_shape: 0.07598534320728688\ntrain loss_recon: 0.1546756825822441\ntrain unet DSC: [0.000514090177603066, 0.0006206660182215273, 7.609925523865968e-05, 0.003963918425142765]\n\ntest loss: 1.9935393730799358\ntest loss_segm: 1.9775025110978346\ntest loss_shape: 0.0622491697088266\ntest loss_recon: 0.09811942317546943\ntest unet DSC: [0.0004218737594783306, 0.0005188517388887703, 5.5522741604363546e-05, 0.004088687710464001]\nBest val loss: 1.9935393730799358\nTime: 57.63527584075928\n\n\nEpoch 23/250\n\ntrain loss: 2.0021425296988666\ntrain loss_segm: 1.9797558958017374\ntrain loss_shape: 0.07462847678438772\ntrain loss_recon: 0.1492378359684084\ntrain unet DSC: [0.0004924436798319221, 0.0005931241903454065, 6.986172229517251e-05, 0.003980871755629778]\n\ntest loss: 1.9897024509234307\ntest loss_segm: 1.9755885570477216\ntest loss_shape: 0.0626933759985826\ntest loss_recon: 0.07844550353594315\ntest unet DSC: [0.0004232977225910872, 0.0004884181544184685, 5.501261694007553e-05, 0.0044500925578176975]\nBest val loss: 1.9897024509234307\nTime: 57.624805212020874\n\n\nEpoch 24/250\n\ntrain loss: 2.0001931190490723\ntrain loss_segm: 1.9772890693024745\ntrain loss_shape: 0.07389815913253947\ntrain loss_recon: 0.15514231383611884\ntrain unet DSC: [0.0004891701973974705, 0.0005823109531775117, 7.318452117033303e-05, 0.004553948063403368]\n\ntest loss: 1.9888260945295677\ntest loss_segm: 1.974046270052592\ntest loss_shape: 0.0630172569400225\ntest loss_recon: 0.0847809961399971\ntest unet DSC: [0.00042364263208583, 0.0005009573651477695, 6.503947952296585e-05, 0.00507157389074564]\nBest val loss: 1.9888260945295677\nTime: 58.9403350353241\n\n\nEpoch 25/250\n\ntrain loss: 1.9971555079085916\ntrain loss_segm: 1.9740959745419175\ntrain loss_shape: 0.07249353382783601\ntrain loss_recon: 0.1581017569273333\ntrain unet DSC: [0.0004613540368154645, 0.0005652788677252829, 6.860944995423779e-05, 0.006661680061370134]\n\ntest loss: 1.987607668607663\ntest loss_segm: 1.9703105688095093\ntest loss_shape: 0.05981371561304117\ntest loss_recon: 0.1131572972696561\ntest unet DSC: [0.0004237456596456468, 0.00048752003931440413, 4.870401608059183e-05, 0.008880970068275928]\nBest val loss: 1.987607668607663\nTime: 58.09925651550293\n\n\nEpoch 26/250\n\ntrain loss: 1.992946973329858\ntrain loss_segm: 1.9692770295505282\ntrain loss_shape: 0.07128232043189338\ntrain loss_recon: 0.16541701206300832\ntrain unet DSC: [0.0004434450820554048, 0.0005366846453398466, 6.438899436034262e-05, 0.009174673818051815]\n\ntest loss: 1.982741661560841\ntest loss_segm: 1.9641381685550396\ntest loss_shape: 0.057318681898789525\ntest loss_recon: 0.12871622456571993\ntest unet DSC: [0.0003573210851754993, 0.00042783035314641893, 5.003076512366533e-05, 0.01022137701511383]\nBest val loss: 1.982741661560841\nTime: 58.90650224685669\n\n\nEpoch 27/250\n\ntrain loss: 1.9891688801065277\ntrain loss_segm: 1.9646665491635287\ntrain loss_shape: 0.0695098392146675\ntrain loss_recon: 0.17551344378462322\ntrain unet DSC: [0.00039820699021220207, 0.0004995418130420148, 6.215299799805507e-05, 0.013839432038366795]\n\ntest loss: 1.9759562932527983\ntest loss_segm: 1.9558283610221667\ntest loss_shape: 0.051599128888203546\ntest loss_recon: 0.1496802143370494\ntest unet DSC: [0.0002991167129948735, 0.00039419851964339614, 5.01226750202477e-05, 0.015914084389805794]\nBest val loss: 1.9759562932527983\nTime: 57.39680743217468\n\n\nEpoch 28/250\n\ntrain loss: 1.982772028898891\ntrain loss_segm: 1.957033344461948\ntrain loss_shape: 0.06874209802739228\ntrain loss_recon: 0.18864480493283725\ntrain unet DSC: [0.0003729353775270283, 0.0004559500084724277, 6.096718061598949e-05, 0.015525834634900093]\n\ntest loss: 1.9681242520992572\ntest loss_segm: 1.9482326874366174\ntest loss_shape: 0.05213353238426722\ntest loss_recon: 0.1467822450093734\ntest unet DSC: [0.0002800451766233891, 0.0003439222928136587, 4.513998283073306e-05, 0.01626908965408802]\nBest val loss: 1.9681242520992572\nTime: 58.633952140808105\n\n\nEpoch 29/250\n\ntrain loss: 1.97620550062083\ntrain loss_segm: 1.9505648099923436\ntrain loss_shape: 0.07134802707859987\ntrain loss_recon: 0.18505888123395323\ntrain unet DSC: [0.000352347589796409, 0.00042540367576293647, 5.688821693183854e-05, 0.015826379880309105]\n\ntest loss: 1.9637341713294005\ntest loss_segm: 1.9449984629948933\ntest loss_shape: 0.05704043079645206\ntest loss_recon: 0.13031669285817024\ntest unet DSC: [0.00032045552507042885, 0.00036904963781125844, 4.381315011414699e-05, 0.016919227316975594]\nBest val loss: 1.9637341713294005\nTime: 63.34868359565735\n\n\nEpoch 30/250\n\ntrain loss: 1.9717248519764672\ntrain loss_segm: 1.9453583835046502\ntrain loss_shape: 0.07495497478337228\ntrain loss_recon: 0.1887096862626981\ntrain unet DSC: [0.0003289489832241088, 0.0003863071615342051, 5.41661684110295e-05, 0.015931755304336548]\n\ntest loss: 1.954701542854309\ntest loss_segm: 1.9359632149720802\ntest loss_shape: 0.056334084138656274\ntest loss_recon: 0.1310493298448049\ntest unet DSC: [0.0002583818859420717, 0.0003040255978703499, 4.0307848394149914e-05, 0.016744155436754227]\nBest val loss: 1.954701542854309\nTime: 63.123746395111084\n\n\nEpoch 31/250\n\ntrain loss: 1.9687501161913328\ntrain loss_segm: 1.93997208377983\ntrain loss_shape: 0.08200345226103746\ntrain loss_recon: 0.20577683629868906\ntrain unet DSC: [0.0003234643954783678, 0.0003769274626392871, 5.570160647039302e-05, 0.01564873941242695]\n\ntest loss: 1.9501000489944067\ntest loss_segm: 1.9311384054330678\ntest loss_shape: 0.06242939084768295\ntest loss_recon: 0.12718700378751144\ntest unet DSC: [0.0002656722499523312, 0.00030657785828225315, 4.082583109266125e-05, 0.017036961391568184]\nBest val loss: 1.9501000489944067\nTime: 59.42227578163147\n\n\nEpoch 32/250\n\ntrain loss: 1.960904014261463\ntrain loss_segm: 1.9329689062094386\ntrain loss_shape: 0.08251044302707232\ntrain loss_recon: 0.19684076158306266\ntrain unet DSC: [0.00031701207626610994, 0.0003667364071588963, 5.078781396150589e-05, 0.01642783358693123]\n\ntest loss: 1.9464228703425481\ntest loss_segm: 1.925185509217091\ntest loss_shape: 0.06329811555452836\ntest loss_recon: 0.14907549684628463\ntest unet DSC: [0.00025832775281742215, 0.0002809915167745203, 3.6466382880462334e-05, 0.017168426886200905]\nBest val loss: 1.9464228703425481\nTime: 58.8228964805603\n\n\nEpoch 33/250\n\ntrain loss: 1.9570403944087933\ntrain loss_segm: 1.9283364430258545\ntrain loss_shape: 0.08266127703687813\ntrain loss_recon: 0.20437828262772742\ntrain unet DSC: [0.000302102358546108, 0.0003449950891081244, 4.756814450956881e-05, 0.016633935272693634]\n\ntest loss: 1.9431109581238184\ntest loss_segm: 1.9204047306990013\ntest loss_shape: 0.0670047992697129\ntest loss_recon: 0.1600574454626976\ntest unet DSC: [0.0002506549353711307, 0.00028511762502603233, 3.4179596696048975e-05, 0.016931887716054916]\nBest val loss: 1.9431109581238184\nTime: 58.260756731033325\n\n\nEpoch 34/250\n\ntrain loss: 1.9537213829499256\ntrain loss_segm: 1.9241517118260831\ntrain loss_shape: 0.084726024560536\ntrain loss_recon: 0.21097074939480312\ntrain unet DSC: [0.0002863768604584038, 0.00032510486198589206, 4.3763386202044785e-05, 0.016466358676552773]\n\ntest loss: 1.9374800217457306\ntest loss_segm: 1.9144632143852038\ntest loss_shape: 0.06290233144775415\ntest loss_recon: 0.16726569602122673\ntest unet DSC: [0.00022904970683157444, 0.0002626186760608107, 3.150457632727921e-05, 0.01701502688229084]\nBest val loss: 1.9374800217457306\nTime: 58.56818461418152\n\n\nEpoch 35/250\n\ntrain loss: 1.9496015559268902\ntrain loss_segm: 1.9182548017441472\ntrain loss_shape: 0.08893556572213958\ntrain loss_recon: 0.22453201194352743\ntrain unet DSC: [0.0002836851344909519, 0.00031136671896092594, 4.1560037061572075e-05, 0.016577117145061493]\n\ntest loss: 1.933317007162632\ntest loss_segm: 1.9084508969233587\ntest loss_shape: 0.06807594278301948\ntest loss_recon: 0.18058515320985746\ntest unet DSC: [0.00022878956224303693, 0.00026880024233832955, 2.9489610824384727e-05, 0.017237428575754166]\nBest val loss: 1.933317007162632\nTime: 57.71757936477661\n\n\nEpoch 36/250\n\ntrain loss: 1.9402804404874392\ntrain loss_segm: 1.9049964411349236\ntrain loss_shape: 0.09661329768692391\ntrain loss_recon: 0.25622668532253823\ntrain unet DSC: [0.0002635630953591317, 0.00029268357320688665, 3.9704587834421545e-05, 0.021127214655280113]\n\ntest loss: 1.7873392166235509\ntest loss_segm: 1.753507531606234\ntest loss_shape: 0.08032744005322456\ntest loss_recon: 0.25798946886490554\ntest unet DSC: [0.00022108685516286641, 0.0002517222601454705, 3.043427932425402e-05, 0.0856318324804306]\nBest val loss: 1.7873392166235509\nTime: 58.03069019317627\n\n\nEpoch 37/250\n\ntrain loss: 1.6599702925621709\ntrain loss_segm: 1.623342825642115\ntrain loss_shape: 0.0976938014567087\ntrain loss_recon: 0.26858084997798826\ntrain unet DSC: [0.00028449753881432116, 0.0003222209634259343, 4.376101787784137e-05, 0.10885598510503769]\n\ntest loss: 1.423112725600218\ntest loss_segm: 1.4021939619993553\ntest loss_shape: 0.03207750513385504\ntest loss_recon: 0.1771101122483229\ntest unet DSC: [0.00021687682601623237, 0.0002599636500235647, 3.9293579902732745e-05, 0.11893543601036072]\nBest val loss: 1.423112725600218\nTime: 57.95411777496338\n\n\nEpoch 38/250\n\ntrain loss: 1.4342738721944108\ntrain loss_segm: 1.409473820577694\ntrain loss_shape: 0.04003773696862067\ntrain loss_recon: 0.207962817029108\ntrain unet DSC: [0.0003281701938249171, 0.0003653945750556886, 5.4775089665781707e-05, 0.1352117508649826]\n\ntest loss: 1.342151165008545\ntest loss_segm: 1.323188332410959\ntest loss_shape: 0.02832667500926898\ntest loss_recon: 0.16130170684594375\ntest unet DSC: [0.0002268876414746046, 0.00027695376775227487, 3.9081314753275365e-05, 0.1400454193353653]\nBest val loss: 1.342151165008545\nTime: 58.84212040901184\n\n\nEpoch 39/250\n\ntrain loss: 1.387579296208635\ntrain loss_segm: 1.3630938145178784\ntrain loss_shape: 0.03927599762483865\ntrain loss_recon: 0.20557879873468907\ntrain unet DSC: [0.00032660647411830723, 0.0003647593839559704, 5.491729825735092e-05, 0.14716725051403046]\n\ntest loss: 1.301050400122618\ntest loss_segm: 1.2834530946535943\ntest loss_shape: 0.025773824646304816\ntest loss_recon: 0.15019933879375458\ntest unet DSC: [0.0002413850015727803, 0.0002728406398091465, 3.9667826058575884e-05, 0.1528472602367401]\nBest val loss: 1.301050400122618\nTime: 57.8313148021698\n\n\nEpoch 40/250\n\ntrain loss: 1.3634066898611528\ntrain loss_segm: 1.339428961277008\ntrain loss_shape: 0.0380042651550302\ntrain loss_recon: 0.20177302396372904\ntrain unet DSC: [0.0003286839637439698, 0.0003638816415332258, 5.3937863413011655e-05, 0.15309324860572815]\n\ntest loss: 1.281071629279699\ntest loss_segm: 1.2638097940347133\ntest loss_shape: 0.026627282683665935\ntest loss_recon: 0.14599110606389168\ntest unet DSC: [0.000248314521741122, 0.0002802723611239344, 3.684211696963757e-05, 0.15930111706256866]\nBest val loss: 1.281071629279699\nTime: 58.90485668182373\n\n\nEpoch 41/250\n\ntrain loss: 1.3447901184045816\ntrain loss_segm: 1.3210767341565481\ntrain loss_shape: 0.037356472043674205\ntrain loss_recon: 0.1997774105660523\ntrain unet DSC: [0.0003364049771334976, 0.0003686046111397445, 5.501701889443211e-05, 0.15741753578186035]\n\ntest loss: 1.2763584240888939\ntest loss_segm: 1.2582372885483961\ntest loss_shape: 0.02736121602356434\ntest loss_recon: 0.15385007743652052\ntest unet DSC: [0.00022261287085711956, 0.00026192821678705513, 3.8031685107853264e-05, 0.1513880044221878]\nBest val loss: 1.2763584240888939\nTime: 57.746726751327515\n\n\nEpoch 42/250\n\ntrain loss: 1.33469329302824\ntrain loss_segm: 1.3111454761480983\ntrain loss_shape: 0.036745121348885044\ntrain loss_recon: 0.198733036653905\ntrain unet DSC: [0.0003303615376353264, 0.00036720355274155736, 5.2586176025215536e-05, 0.15961478650569916]\n\ntest loss: 1.2696144855939424\ntest loss_segm: 1.251350155243507\ntest loss_shape: 0.027947298513773162\ntest loss_recon: 0.1546960587684925\ntest unet DSC: [0.0002432739274809137, 0.00028823866159655154, 4.0571423596702516e-05, 0.15847700834274292]\nBest val loss: 1.2696144855939424\nTime: 57.70528841018677\n\n\nEpoch 43/250\n\ntrain loss: 1.3249912790105314\ntrain loss_segm: 1.301768318761753\ntrain loss_shape: 0.03621937955812186\ntrain loss_recon: 0.19601022895378403\ntrain unet DSC: [0.0003163664077874273, 0.00034993642475456, 5.1025028369622305e-05, 0.1641392558813095]\n\ntest loss: 1.270840021280142\ntest loss_segm: 1.2516610958637335\ntest loss_shape: 0.029909403851399056\ntest loss_recon: 0.16187992691993713\ntest unet DSC: [0.0002550235076341778, 0.0003051453677471727, 4.059155980939977e-05, 0.1620704084634781]\nBest val loss: 1.2696144855939424\nTime: 58.567389488220215\n\n\nEpoch 44/250\n\ntrain loss: 1.3210814127439185\ntrain loss_segm: 1.2976471623287926\ntrain loss_shape: 0.03622689616831043\ntrain loss_recon: 0.1981156066625933\ntrain unet DSC: [0.000329189992044121, 0.0003651890147011727, 5.267173037282191e-05, 0.16522452235221863]\n\ntest loss: 1.243671022928678\ntest loss_segm: 1.2269755082252698\ntest loss_shape: 0.02612354453557577\ntest loss_recon: 0.14083159046295363\ntest unet DSC: [0.0002561454602982849, 0.0002855434431694448, 4.030434138257988e-05, 0.1693985015153885]\nBest val loss: 1.243671022928678\nTime: 57.34335207939148\n\n\nEpoch 45/250\n\ntrain loss: 1.3154649583599236\ntrain loss_segm: 1.2922160678272006\ntrain loss_shape: 0.035712719164034235\ntrain loss_recon: 0.19677623380211334\ntrain unet DSC: [0.00031090446282178164, 0.0003562677593436092, 5.1674858696060255e-05, 0.168334499001503]\n\ntest loss: 1.2346744139989216\ntest loss_segm: 1.218540595127986\ntest loss_shape: 0.023946518221726783\ntest loss_recon: 0.1373916623684076\ntest unet DSC: [0.000230585370445624, 0.0002794802712742239, 3.890340667567216e-05, 0.1648043543100357]\nBest val loss: 1.2346744139989216\nTime: 57.42832136154175\n\n\nEpoch 46/250\n\ntrain loss: 1.3099984134299845\ntrain loss_segm: 1.286865849283677\ntrain loss_shape: 0.03561488686341651\ntrain loss_recon: 0.19571072702543646\ntrain unet DSC: [0.00032282949541695416, 0.0003618379414547235, 5.316303577274084e-05, 0.17035315930843353]\n\ntest loss: 1.2309344487312512\ntest loss_segm: 1.2147284012574415\ntest loss_shape: 0.023363802677545793\ntest loss_recon: 0.13869669689581945\ntest unet DSC: [0.0002265420916955918, 0.00027894240338355303, 3.2068764994619414e-05, 0.16620026528835297]\nBest val loss: 1.2309344487312512\nTime: 57.32902526855469\n\n\nEpoch 47/250\n\ntrain loss: 1.3051119605197181\ntrain loss_segm: 1.282204105884214\ntrain loss_shape: 0.03550592793411092\ntrain loss_recon: 0.1935726773889759\ntrain unet DSC: [0.0003303021949250251, 0.00036850597825832665, 4.9019108701031655e-05, 0.17240507900714874]\n\ntest loss: 1.238328958169008\ntest loss_segm: 1.2210336740200336\ntest loss_shape: 0.028586197931032915\ntest loss_recon: 0.14436669915150374\ntest unet DSC: [0.0002846165734808892, 0.0003245934203732759, 4.4700736907543615e-05, 0.17811468243598938]\nBest val loss: 1.2309344487312512\nTime: 57.607117891311646\n\n\nEpoch 48/250\n\ntrain loss: 1.302712394466883\ntrain loss_segm: 1.2797387468663952\ntrain loss_shape: 0.035317493696944625\ntrain loss_recon: 0.19441897205159633\ntrain unet DSC: [0.0003338527458254248, 0.0003617621259763837, 5.2490071539068595e-05, 0.17399714887142181]\n\ntest loss: 1.2381192537454457\ntest loss_segm: 1.2206856195743268\ntest loss_shape: 0.02840444531578284\ntest loss_recon: 0.14593177346082833\ntest unet DSC: [0.000286048132693395, 0.00031863024923950434, 3.819602352450602e-05, 0.18233120441436768]\nBest val loss: 1.2309344487312512\nTime: 56.85975122451782\n\n\nEpoch 49/250\n\ntrain loss: 1.3021969765047483\ntrain loss_segm: 1.2790411605110652\ntrain loss_shape: 0.035076875322132925\ntrain loss_recon: 0.19648134859302377\ntrain unet DSC: [0.0003227943670935929, 0.00035727364593185484, 5.195643097977154e-05, 0.17345240712165833]\n\ntest loss: 1.224638431500166\ntest loss_segm: 1.2083337612641163\ntest loss_shape: 0.02468480905279135\ntest loss_recon: 0.13836195300786924\ntest unet DSC: [0.00025782614829950035, 0.00030176746076904237, 4.1121758840745315e-05, 0.17468219995498657]\nBest val loss: 1.224638431500166\nTime: 57.44202208518982\n\n\nEpoch 50/250\n\ntrain loss: 1.2986891073516653\ntrain loss_segm: 1.2757687870460221\ntrain loss_shape: 0.03510403226400855\ntrain loss_recon: 0.19409917917432665\ntrain unet DSC: [0.00033419104875065386, 0.00037599902134388685, 5.465170761453919e-05, 0.1737203598022461]\n\ntest loss: 1.2153114538926344\ntest loss_segm: 1.1997724526967757\ntest loss_shape: 0.02286566335421342\ntest loss_recon: 0.13252446170036608\ntest unet DSC: [0.00023756649170536548, 0.0002844874979928136, 3.540617763064802e-05, 0.17301994562149048]\nBest val loss: 1.2153114538926344\nTime: 57.32671284675598\n\n\nEpoch 51/250\n\ntrain loss: 1.2964106217215332\ntrain loss_segm: 1.27360339783415\ntrain loss_shape: 0.0347434671336337\ntrain loss_recon: 0.19332879791154137\ntrain unet DSC: [0.0003321732801850885, 0.0003692122409120202, 4.908553455607034e-05, 0.1772639900445938]\n\ntest loss: 1.2206155887016883\ntest loss_segm: 1.204361875851949\ntest loss_shape: 0.024915459112097055\ntest loss_recon: 0.1376216694330558\ntest unet DSC: [0.00025563346571289003, 0.00029827459366060793, 4.072000228916295e-05, 0.17837436497211456]\nBest val loss: 1.2153114538926344\nTime: 57.383882999420166\n\n\nEpoch 52/250\n\ntrain loss: 1.292902925346471\ntrain loss_segm: 1.270173590394515\ntrain loss_shape: 0.03441150987497236\ntrain loss_recon: 0.1928818397506883\ntrain unet DSC: [0.0003186956746503711, 0.00035877968184649944, 5.072055137134157e-05, 0.17994509637355804]\n\ntest loss: 1.2254256713084686\ntest loss_segm: 1.208591525371258\ntest loss_shape: 0.025797111412080433\ntest loss_recon: 0.1425443941201919\ntest unet DSC: [0.00025039323372766376, 0.0003038817667402327, 3.75542622350622e-05, 0.17529693245887756]\nBest val loss: 1.2153114538926344\nTime: 57.692192792892456\n\n\nEpoch 53/250\n\ntrain loss: 1.2916845317128338\ntrain loss_segm: 1.2689848927002918\ntrain loss_shape: 0.0344435616125223\ntrain loss_recon: 0.19255290255893634\ntrain unet DSC: [0.00032714533153921366, 0.00036819095839746296, 5.3126117563806474e-05, 0.1803288459777832]\n\ntest loss: 1.2504376264718862\ntest loss_segm: 1.2309378110445464\ntest loss_shape: 0.03178853106995424\ntest loss_recon: 0.16320965305352822\ntest unet DSC: [0.000277590355835855, 0.00034065026557072997, 3.796499004238285e-05, 0.1795452982187271]\nBest val loss: 1.2153114538926344\nTime: 56.98142075538635\n\n\nEpoch 54/250\n\ntrain loss: 1.291367167913461\ntrain loss_segm: 1.268697370456744\ntrain loss_shape: 0.033972475441950784\ntrain loss_recon: 0.19272544074662123\ntrain unet DSC: [0.0003213666204828769, 0.00034636069904081523, 5.0117061618948355e-05, 0.17970573902130127]\n\ntest loss: 1.2130944453752959\ntest loss_segm: 1.1971672498262846\ntest loss_shape: 0.022483595193196565\ntest loss_recon: 0.13678835179561225\ntest unet DSC: [0.00022569127031601965, 0.0002685847575776279, 3.8650239730486646e-05, 0.17028416693210602]\nBest val loss: 1.2130944453752959\nTime: 57.21195983886719\n\n\nEpoch 55/250\n\ntrain loss: 1.2895577346222311\ntrain loss_segm: 1.2670026662983471\ntrain loss_shape: 0.034005726899810226\ntrain loss_recon: 0.19154499046787432\ntrain unet DSC: [0.0003248079738114029, 0.00035739995655603707, 4.948853165842593e-05, 0.18063299357891083]\n\ntest loss: 1.2164536042091174\ntest loss_segm: 1.2000825435687335\ntest loss_shape: 0.023230584266667183\ntest loss_recon: 0.14048001590447548\ntest unet DSC: [0.00022561066725756973, 0.0002727303944993764, 3.494462725939229e-05, 0.17105735838413239]\nBest val loss: 1.2130944453752959\nTime: 57.42642545700073\n\n\nEpoch 56/250\n\ntrain loss: 1.2871935563751413\ntrain loss_segm: 1.2647024732601793\ntrain loss_shape: 0.03408443368971348\ntrain loss_recon: 0.19082639413543895\ntrain unet DSC: [0.0003333595232106745, 0.00037308529135771096, 4.887135582976043e-05, 0.18456532061100006]\n\ntest loss: 1.211810787518819\ntest loss_segm: 1.1958890817104242\ntest loss_shape: 0.02352001164586116\ntest loss_recon: 0.13569702017001617\ntest unet DSC: [0.0002473409695085138, 0.00028102577198296785, 3.8598140235990286e-05, 0.1791360229253769]\nBest val loss: 1.211810787518819\nTime: 57.607630491256714\n\n\nEpoch 57/250\n\ntrain loss: 1.2872241478932054\ntrain loss_segm: 1.2648154014273534\ntrain loss_shape: 0.03375047711726231\ntrain loss_recon: 0.19033702840156194\ntrain unet DSC: [0.00032632882357575, 0.00036147425998933613, 4.846322553930804e-05, 0.1836652159690857]\n\ntest loss: 1.1976888454877412\ntest loss_segm: 1.183031366421626\ntest loss_shape: 0.021419949781818267\ntest loss_recon: 0.1251549006272585\ntest unet DSC: [0.0002492812054697424, 0.0002818521752487868, 3.5004217352252454e-05, 0.19197283685207367]\nBest val loss: 1.1976888454877412\nTime: 57.04884600639343\n\n\nEpoch 58/250\n\ntrain loss: 1.284742378735844\ntrain loss_segm: 1.2623468305491194\ntrain loss_shape: 0.03362484061736849\ntrain loss_recon: 0.19033066975542262\ntrain unet DSC: [0.0003335024230182171, 0.0003649576392490417, 5.179385334486142e-05, 0.18620307743549347]\n\ntest loss: 1.2098031288538225\ntest loss_segm: 1.19373274460817\ntest loss_shape: 0.024549502210739333\ntest loss_recon: 0.13615413888906822\ntest unet DSC: [0.00027335446793586016, 0.000294836558168754, 3.9154467231128365e-05, 0.18973146378993988]\nBest val loss: 1.1976888454877412\nTime: 56.92306971549988\n\n\nEpoch 59/250\n\ntrain loss: 1.2823774558079393\ntrain loss_segm: 1.2601509230046333\ntrain loss_shape: 0.033330085545872586\ntrain loss_recon: 0.1889352094900759\ntrain unet DSC: [0.0003335365909151733, 0.00036521535366773605, 4.9394780944567174e-05, 0.18711009621620178]\n\ntest loss: 1.212098708519569\ntest loss_segm: 1.1954806523445325\ntest loss_shape: 0.02470785680298622\ntest loss_recon: 0.14147277711293635\ntest unet DSC: [0.00022719595290254802, 0.0002718222967814654, 3.448947973083705e-05, 0.17849604785442352]\nBest val loss: 1.1976888454877412\nTime: 57.44790840148926\n\n\nEpoch 60/250\n\ntrain loss: 1.2842475017414818\ntrain loss_segm: 1.261737978156609\ntrain loss_shape: 0.0335523573257312\ntrain loss_recon: 0.19154290845499763\ntrain unet DSC: [0.000332461524521932, 0.00037151333526708186, 4.7838409955147654e-05, 0.1887296736240387]\n\ntest loss: 1.214030146598816\ntest loss_segm: 1.1974360514909794\ntest loss_shape: 0.02355117852298113\ntest loss_recon: 0.14238987404566544\ntest unet DSC: [0.00022951756545808166, 0.00026620583957992494, 3.546873267623596e-05, 0.17640526592731476]\nBest val loss: 1.1976888454877412\nTime: 57.701560497283936\n\n\nEpoch 61/250\n\ntrain loss: 1.281149841562102\ntrain loss_segm: 1.2591083457198324\ntrain loss_shape: 0.03295091998209305\ntrain loss_recon: 0.18746405160879787\ntrain unet DSC: [0.00031738460529595613, 0.0003499790618661791, 4.668368637794629e-05, 0.18853464722633362]\n\ntest loss: 1.2020824261200733\ntest loss_segm: 1.1864243165040627\ntest loss_shape: 0.022205564790429212\ntest loss_recon: 0.13437551183578295\ntest unet DSC: [0.0002521306450944394, 0.0002897466765716672, 3.787906825891696e-05, 0.18905994296073914]\nBest val loss: 1.1976888454877412\nTime: 57.367173194885254\n\n\nEpoch 62/250\n\ntrain loss: 1.2803924476044088\ntrain loss_segm: 1.2581886400150348\ntrain loss_shape: 0.03327159635439704\ntrain loss_recon: 0.1887664906209028\ntrain unet DSC: [0.0003362125135026872, 0.00037249960587359965, 4.788879959960468e-05, 0.19215917587280273]\n\ntest loss: 1.2167648352109468\ntest loss_segm: 1.199804113461421\ntest loss_shape: 0.024726024709450893\ntest loss_recon: 0.14488133329611558\ntest unet DSC: [0.0002454471541568637, 0.0002792837331071496, 3.3563515899004415e-05, 0.18291564285755157]\nBest val loss: 1.1976888454877412\nTime: 57.49575614929199\n\n\nEpoch 63/250\n\ntrain loss: 1.281522438495974\ntrain loss_segm: 1.2591982280151754\ntrain loss_shape: 0.03300682945719248\ntrain loss_recon: 0.19023529477889026\ntrain unet DSC: [0.0003239262441638857, 0.0003577359893824905, 4.742896999232471e-05, 0.19178441166877747]\n\ntest loss: 1.207884458395151\ntest loss_segm: 1.191644466840304\ntest loss_shape: 0.022857680080983885\ntest loss_recon: 0.13954235422305572\ntest unet DSC: [0.00023884254915174097, 0.0002736214082688093, 3.5913893952965736e-05, 0.18623124063014984]\nBest val loss: 1.1976888454877412\nTime: 57.790295362472534\n\n\nEpoch 64/250\n\ntrain loss: 1.2783316411549532\ntrain loss_segm: 1.256051328363298\ntrain loss_shape: 0.03309346539781818\ntrain loss_recon: 0.18970968263058724\ntrain unet DSC: [0.0003253459290135652, 0.0003711340541485697, 4.8160814912989736e-05, 0.19736850261688232]\n\ntest loss: 1.216403169509692\ntest loss_segm: 1.1992529049897804\ntest loss_shape: 0.02619050357204217\ntest loss_recon: 0.1453121556685521\ntest unet DSC: [0.0002676785516086966, 0.0002993499510921538, 4.022648499812931e-05, 0.1948465257883072]\nBest val loss: 1.1976888454877412\nTime: 57.604140281677246\n\n\nEpoch 65/250\n\ntrain loss: 1.2806776837457585\ntrain loss_segm: 1.2583882242818423\ntrain loss_shape: 0.032991020409744\ntrain loss_recon: 0.18990356654306\ntrain unet DSC: [0.0003265443374402821, 0.0003668872523121536, 4.913130760542117e-05, 0.19950014352798462]\n\ntest loss: 1.2182441369081154\ntest loss_segm: 1.2010517028661876\ntest loss_shape: 0.023984523824392222\ntest loss_recon: 0.14793982146642146\ntest unet DSC: [0.00022848081425763667, 0.00026226555928587914, 3.424941314733587e-05, 0.1774575561285019]\nBest val loss: 1.1976888454877412\nTime: 58.50518250465393\n\n\nEpoch 66/250\n\ntrain loss: 1.2775315001041074\ntrain loss_segm: 1.2554250057739547\ntrain loss_shape: 0.03251055197766687\ntrain loss_recon: 0.1885544372887551\ntrain unet DSC: [0.0003315116045996547, 0.0003702696121763438, 4.904277011519298e-05, 0.20231150090694427]\n\ntest loss: 1.1872864839358208\ntest loss_segm: 1.172815753863408\ntest loss_shape: 0.020050029867352583\ntest loss_recon: 0.12465733786424\ntest unet DSC: [0.00024021121498662978, 0.0002839978551492095, 3.633387677837163e-05, 0.202737495303154]\nBest val loss: 1.1872864839358208\nTime: 57.3387291431427\n\n\nEpoch 67/250\n\ntrain loss: 1.2762290963643714\ntrain loss_segm: 1.254213346710688\ntrain loss_shape: 0.032452728045231936\ntrain loss_recon: 0.1877047619487666\ntrain unet DSC: [0.00032674797694198787, 0.0003611283318605274, 4.769817678607069e-05, 0.20777994394302368]\n\ntest loss: 1.19554930466872\ntest loss_segm: 1.1803316917174902\ntest loss_shape: 0.0210489460434287\ntest loss_recon: 0.13112728144877997\ntest unet DSC: [0.00023773756402079016, 0.0002736752212513238, 3.7771922507090494e-05, 0.20478668808937073]\nBest val loss: 1.1872864839358208\nTime: 57.25148582458496\n\n\nEpoch 68/250\n\ntrain loss: 1.2772371512425096\ntrain loss_segm: 1.2549515773978415\ntrain loss_shape: 0.032712934220421916\ntrain loss_recon: 0.19014284118444105\ntrain unet DSC: [0.0003250590816605836, 0.00036042381543666124, 4.7520403313683346e-05, 0.21349011361598969]\n\ntest loss: 1.1947931723716931\ntest loss_segm: 1.1795735848255646\ntest loss_shape: 0.020987037784205034\ntest loss_recon: 0.13120888020747748\ntest unet DSC: [0.00024194752040784806, 0.00028473351267166436, 3.345720688230358e-05, 0.20712696015834808]\nBest val loss: 1.1872864839358208\nTime: 57.164759397506714\n\n\nEpoch 69/250\n\ntrain loss: 1.2727423393273656\ntrain loss_segm: 1.250982416581504\ntrain loss_shape: 0.03188420478490334\ntrain loss_recon: 0.18571500914006295\ntrain unet DSC: [0.00032058381475508213, 0.00035654971725307405, 4.860670378548093e-05, 0.22504031658172607]\n\ntest loss: 1.1961408792397914\ntest loss_segm: 1.1807675911830022\ntest loss_shape: 0.0209737161699778\ntest loss_recon: 0.13275910379030767\ntest unet DSC: [0.00022852382971905172, 0.00026664347387850285, 3.294098860351369e-05, 0.21223339438438416]\nBest val loss: 1.1872864839358208\nTime: 57.87115144729614\n\n\nEpoch 70/250\n\ntrain loss: 1.26758180766166\ntrain loss_segm: 1.2457211138326911\ntrain loss_shape: 0.03211424744959119\ntrain loss_recon: 0.1864926893311211\ntrain unet DSC: [0.0003306910512037575, 0.0003628115518949926, 4.769102451973595e-05, 0.2845246493816376]\n\ntest loss: 1.1389864744284215\ntest loss_segm: 1.1220281093548505\ntest loss_shape: 0.022974279613640063\ntest loss_recon: 0.1466092226596979\ntest unet DSC: [0.00022324026213027537, 0.0002640039601828903, 3.4142733056796715e-05, 0.7353013753890991]\nBest val loss: 1.1389864744284215\nTime: 57.09735465049744\n\n\nEpoch 71/250\n\ntrain loss: 1.2104069681107243\ntrain loss_segm: 1.1871513294268259\ntrain loss_shape: 0.033922501544975024\ntrain loss_recon: 0.19863387753691855\ntrain unet DSC: [0.0003248301218263805, 0.0003697994980029762, 4.8758320190245286e-05, 0.6806303858757019]\n\ntest loss: 1.1233282028100429\ntest loss_segm: 1.1063964122380965\ntest loss_shape: 0.023994603146536227\ntest loss_recon: 0.14532343011636\ntest unet DSC: [0.0002513220242690295, 0.0003008237399626523, 3.7280831747921184e-05, 0.7277933955192566]\nBest val loss: 1.1233282028100429\nTime: 57.492921113967896\n\n\nEpoch 72/250\n\ntrain loss: 1.1869799049594734\ntrain loss_segm: 1.1639367055289354\ntrain loss_shape: 0.03360559457700841\ntrain loss_recon: 0.19682643109861808\ntrain unet DSC: [0.00033290876308456063, 0.0003809181216638535, 4.898856423096731e-05, 0.7007572650909424]\n\ntest loss: 1.091040429396507\ntest loss_segm: 1.0748739043871562\ntest loss_shape: 0.02259431965649128\ntest loss_recon: 0.13907092121931222\ntest unet DSC: [0.0002504624717403203, 0.000292915414320305, 3.8063444662839174e-05, 0.7655757665634155]\nBest val loss: 1.091040429396507\nTime: 57.56745219230652\n\n\nEpoch 73/250\n\ntrain loss: 1.1790717281872714\ntrain loss_segm: 1.15609953675089\ntrain loss_shape: 0.03339645433935184\ntrain loss_recon: 0.19632542265366904\ntrain unet DSC: [0.000325834087561816, 0.00036847995943389833, 4.791925675817765e-05, 0.6995029449462891]\n\ntest loss: 1.0826025742750902\ntest loss_segm: 1.0661141337492528\ntest loss_shape: 0.022393927407952454\ntest loss_recon: 0.1424905294791246\ntest unet DSC: [0.00023878391948528588, 0.0002958398254122585, 3.849345011985861e-05, 0.7810657024383545]\nBest val loss: 1.0826025742750902\nTime: 57.453564405441284\n\n\nEpoch 74/250\n\ntrain loss: 1.1738875832738755\ntrain loss_segm: 1.1508352839494054\ntrain loss_shape: 0.03339389280144927\ntrain loss_recon: 0.1971290999952751\ntrain unet DSC: [0.00033919347333721817, 0.00038340408354997635, 5.058145688963123e-05, 0.7040311694145203]\n\ntest loss: 1.0864119560290606\ntest loss_segm: 1.069583148528368\ntest loss_shape: 0.02331262404242387\ntest loss_recon: 0.1449754964082669\ntest unet DSC: [0.00023831416910979897, 0.0002901534317061305, 3.590577398426831e-05, 0.7703428864479065]\nBest val loss: 1.0826025742750902\nTime: 57.73328995704651\n\n\nEpoch 75/250\n\ntrain loss: 1.1707088924661468\ntrain loss_segm: 1.1473832658574552\ntrain loss_shape: 0.033531389328875116\ntrain loss_recon: 0.19972488293542137\ntrain unet DSC: [0.0003392331418581307, 0.0003826385654974729, 4.751141386805102e-05, 0.7031175494194031]\n\ntest loss: 1.0732448987471752\ntest loss_segm: 1.0567565667323577\ntest loss_shape: 0.02278800003039531\ntest loss_recon: 0.14209520549346238\ntest unet DSC: [0.00025371668743900955, 0.00028871107497252524, 3.56782584276516e-05, 0.7828385829925537]\nBest val loss: 1.0732448987471752\nTime: 57.25430989265442\n\n\nEpoch 76/250\n\ntrain loss: 1.1647214700904074\ntrain loss_segm: 1.1416867277290248\ntrain loss_shape: 0.03313735411561365\ntrain loss_recon: 0.19721007347106934\ntrain unet DSC: [0.0003325910947751254, 0.0003808279288932681, 4.7777866711840034e-05, 0.7048125267028809]\n\ntest loss: 1.0777938885566516\ntest loss_segm: 1.0609067036555364\ntest loss_shape: 0.023098945904236574\ntest loss_recon: 0.14577305355133155\ntest unet DSC: [0.00023127160966396332, 0.00027537011192180216, 3.173145523760468e-05, 0.7630011439323425]\nBest val loss: 1.0732448987471752\nTime: 58.5516676902771\n\n\nEpoch 77/250\n\ntrain loss: 1.1602855513367472\ntrain loss_segm: 1.1373470176624347\ntrain loss_shape: 0.03311175620772793\ntrain loss_recon: 0.1962735628402686\ntrain unet DSC: [0.00034255889477208257, 0.00039143720641732216, 4.905835885438137e-05, 0.7085392475128174]\n\ntest loss: 1.0705264531649077\ntest loss_segm: 1.0539910395940144\ntest loss_shape: 0.022520778103707693\ntest loss_recon: 0.14283335285309035\ntest unet DSC: [0.00025377541896887124, 0.0003028951759915799, 3.625838508014567e-05, 0.7767408490180969]\nBest val loss: 1.0705264531649077\nTime: 58.606985569000244\n\n\nEpoch 78/250\n\ntrain loss: 1.160757900793341\ntrain loss_segm: 1.1375234157224245\ntrain loss_shape: 0.033477512164678\ntrain loss_recon: 0.19886737486606912\ntrain unet DSC: [0.00033513488597236574, 0.0003821784630417824, 4.696933683590032e-05, 0.7084985375404358]\n\ntest loss: 1.0639290366417322\ntest loss_segm: 1.0475443876706636\ntest loss_shape: 0.022186240778328516\ntest loss_recon: 0.14166035751501718\ntest unet DSC: [0.0002458430244587362, 0.00028642869438044727, 3.5322133044246584e-05, 0.7741411924362183]\nBest val loss: 1.0639290366417322\nTime: 57.92673087120056\n\n\nEpoch 79/250\n\ntrain loss: 1.1587811580187157\ntrain loss_segm: 1.1357010384149189\ntrain loss_shape: 0.03290576557334088\ntrain loss_recon: 0.19789540173509454\ntrain unet DSC: [0.0003440856817178428, 0.0003810988855548203, 4.910506686428562e-05, 0.705813467502594]\n\ntest loss: 1.0730100808999476\ntest loss_segm: 1.0561975683921423\ntest loss_shape: 0.024285550611332442\ntest loss_recon: 0.1438394655019809\ntest unet DSC: [0.0002750503772404045, 0.0003282493562437594, 4.141678800806403e-05, 0.7701844573020935]\nBest val loss: 1.0639290366417322\nTime: 57.64044785499573\n\n\nEpoch 80/250\n\ntrain loss: 1.1526190774350227\ntrain loss_segm: 1.1296739178367807\ntrain loss_shape: 0.032948528345721435\ntrain loss_recon: 0.1965030333663844\ntrain unet DSC: [0.00034664501436054707, 0.0003854864917229861, 4.9759641115088016e-05, 0.7171029448509216]\n\ntest loss: 1.0633650467945979\ntest loss_segm: 1.0470170837182264\ntest loss_shape: 0.022374041593418673\ntest loss_recon: 0.14110558231671652\ntest unet DSC: [0.00025496521266177297, 0.00030755362240597606, 4.020833148388192e-05, 0.7776041030883789]\nBest val loss: 1.0633650467945979\nTime: 57.671974897384644\n\n\nEpoch 81/250\n\ntrain loss: 1.1542725910114338\ntrain loss_segm: 1.1313183881059479\ntrain loss_shape: 0.0329259609732824\ntrain loss_recon: 0.19661605848541744\ntrain unet DSC: [0.0003466201014816761, 0.00039513653609901667, 5.118639091961086e-05, 0.7089099884033203]\n\ntest loss: 1.0624008117577968\ntest loss_segm: 1.045611783480033\ntest loss_shape: 0.022342657718138818\ntest loss_recon: 0.1455476945027327\ntest unet DSC: [0.0002497313544154167, 0.000291478936560452, 3.776822632062249e-05, 0.7715317010879517]\nBest val loss: 1.0624008117577968\nTime: 57.23079872131348\n\n\nEpoch 82/250\n\ntrain loss: 1.1493974841093715\ntrain loss_segm: 1.1264321721052821\ntrain loss_shape: 0.03282482801830467\ntrain loss_recon: 0.19682831354910815\ntrain unet DSC: [0.0003502330800984055, 0.00039016714436002076, 5.023739140597172e-05, 0.7161064743995667]\n\ntest loss: 1.064663141201704\ntest loss_segm: 1.0479137668242822\ntest loss_shape: 0.02261469644518235\ntest loss_recon: 0.14487908016412687\ntest unet DSC: [0.0002341677900403738, 0.00028850106173194945, 3.596942769945599e-05, 0.7640726566314697]\nBest val loss: 1.0624008117577968\nTime: 57.913068771362305\n\n\nEpoch 83/250\n\ntrain loss: 1.147928475579129\ntrain loss_segm: 1.1250477523743352\ntrain loss_shape: 0.032942183692998527\ntrain loss_recon: 0.1958651433262644\ntrain unet DSC: [0.00036237601307220757, 0.00040590573917143047, 5.0914310122607276e-05, 0.7156960368156433]\n\ntest loss: 1.0615762762534313\ntest loss_segm: 1.044917853978964\ntest loss_shape: 0.0221042642608667\ntest loss_recon: 0.14447991244303873\ntest unet DSC: [0.00024499004939571023, 0.0002790560247376561, 3.6227109376341105e-05, 0.771297037601471]\nBest val loss: 1.0615762762534313\nTime: 57.28434729576111\n\n\nEpoch 84/250\n\ntrain loss: 1.150960507272165\ntrain loss_segm: 1.1278194928471046\ntrain loss_shape: 0.03327985976740152\ntrain loss_recon: 0.19813034079874617\ntrain unet DSC: [0.00035188402398489416, 0.0003897932474501431, 5.0694128731265664e-05, 0.7116479277610779]\n\ntest loss: 1.0485520103038886\ntest loss_segm: 1.032474866280189\ntest loss_shape: 0.021054422315687705\ntest loss_recon: 0.1397170351865964\ntest unet DSC: [0.0002441967953927815, 0.00028668614686466753, 3.684886542032473e-05, 0.7924293279647827]\nBest val loss: 1.0485520103038886\nTime: 57.44598150253296\n\n\nEpoch 85/250\n\ntrain loss: 1.1485358016400398\ntrain loss_segm: 1.1254887543147123\ntrain loss_shape: 0.03304325891776553\ntrain loss_recon: 0.19742726553467255\ntrain unet DSC: [0.0003475559060461819, 0.00038535427302122116, 5.0635284424060956e-05, 0.7127187252044678]\n\ntest loss: 1.066730219584245\ntest loss_segm: 1.0493850004978669\ntest loss_shape: 0.023145149008203775\ntest loss_recon: 0.15030718652101663\ntest unet DSC: [0.0002411296300124377, 0.00028487684903666377, 3.443355672061443e-05, 0.7576672434806824]\nBest val loss: 1.0485520103038886\nTime: 57.76218605041504\n\n\nEpoch 86/250\n\ntrain loss: 1.1452411684808852\ntrain loss_segm: 1.1224633421324477\ntrain loss_shape: 0.032606044135799137\ntrain loss_recon: 0.19517222754185712\ntrain unet DSC: [0.000356825883500278, 0.000392741960240528, 5.181803498999216e-05, 0.7126227021217346]\n\ntest loss: 1.0451622223242736\ntest loss_segm: 1.029349582317548\ntest loss_shape: 0.020542258874346048\ntest loss_recon: 0.13758408297330904\ntest unet DSC: [0.00025097286561504006, 0.0002869036979973316, 3.495333658065647e-05, 0.7867382764816284]\nBest val loss: 1.0451622223242736\nTime: 57.539510011672974\n\n\nEpoch 87/250\n\ntrain loss: 1.1406383205063735\ntrain loss_segm: 1.1183502659013\ntrain loss_shape: 0.03247520243866912\ntrain loss_recon: 0.19040532957149459\ntrain unet DSC: [0.00036604143679142, 0.0004009305848740041, 5.2768358727917075e-05, 0.7140594124794006]\n\ntest loss: 1.0663403914524958\ntest loss_segm: 1.049634847885523\ntest loss_shape: 0.02219935566282425\ntest loss_recon: 0.14485623018863875\ntest unet DSC: [0.0002326785324839875, 0.00027436012169346213, 3.583843135857023e-05, 0.7555990815162659]\nBest val loss: 1.0451622223242736\nTime: 57.651252031326294\n\n\nEpoch 88/250\n\ntrain loss: 1.1365541325339787\ntrain loss_segm: 1.1144884304155278\ntrain loss_shape: 0.031980858879941926\ntrain loss_recon: 0.18867611554981786\ntrain unet DSC: [0.0003468090435490012, 0.0003998338943347335, 5.089515252620913e-05, 0.7187097668647766]\n\ntest loss: 1.0421569912861555\ntest loss_segm: 1.0268176946884546\ntest loss_shape: 0.02057955120331966\ntest loss_recon: 0.13281341584829184\ntest unet DSC: [0.0002533576625864953, 0.00028211591416038573, 3.8773268897784874e-05, 0.7838795781135559]\nBest val loss: 1.0421569912861555\nTime: 57.65539216995239\n\n\nEpoch 89/250\n\ntrain loss: 1.10473530918737\ntrain loss_segm: 1.084558475621139\ntrain loss_shape: 0.030583535297479057\ntrain loss_recon: 0.1711848258406301\ntrain unet DSC: [0.0002528986369725317, 0.00029590673511847854, 4.1159459215123206e-05, 0.712063729763031]\n\ntest loss: 0.9950340374922141\ntest loss_segm: 0.9820077220598856\ntest loss_shape: 0.019330743270424697\ntest loss_recon: 0.11093247634096023\ntest unet DSC: [0.00011087011080235243, 0.00014039568486623466, 2.3570884877699427e-05, 0.7831225991249084]\nBest val loss: 0.9950340374922141\nTime: 57.33464002609253\n\n\nEpoch 90/250\n\ntrain loss: 1.0913305810735197\ntrain loss_segm: 1.0718056446389308\ntrain loss_shape: 0.030021547398801092\ntrain loss_recon: 0.1652278399354295\ntrain unet DSC: [0.00021419695985969156, 0.0002690773399081081, 3.650792132248171e-05, 0.7114829421043396]\n\ntest loss: 0.988019802631476\ntest loss_segm: 0.9756562067912176\ntest loss_shape: 0.018439555588441018\ntest loss_recon: 0.1051964279359732\ntest unet DSC: [0.00011202536552445963, 0.000146946229506284, 2.510361809981987e-05, 0.7860591411590576]\nBest val loss: 0.988019802631476\nTime: 57.71696496009827\n\n\nEpoch 91/250\n\ntrain loss: 1.0901117441774923\ntrain loss_segm: 1.0704838724075993\ntrain loss_shape: 0.03007880116140918\ntrain loss_recon: 0.16619993576520606\ntrain unet DSC: [0.00021394839859567583, 0.000265745009528473, 3.757227386813611e-05, 0.7147194743156433]\n\ntest loss: 0.9959876201091669\ntest loss_segm: 0.9829751467093443\ntest loss_shape: 0.018950657226527348\ntest loss_recon: 0.11117401422980504\ntest unet DSC: [8.804161188891158e-05, 0.00013344749459065497, 2.0729099560412578e-05, 0.7740057706832886]\nBest val loss: 0.988019802631476\nTime: 57.29013538360596\n\n\nEpoch 92/250\n\ntrain loss: 1.0873523433751697\ntrain loss_segm: 1.0679950397225875\ntrain loss_shape: 0.029367421800860122\ntrain loss_recon: 0.16420564008287236\ntrain unet DSC: [0.0002002357505261898, 0.00024834455689415336, 3.7500100006582215e-05, 0.7160854935646057]\n\ntest loss: 0.9965281838025802\ntest loss_segm: 0.9831491036292834\ntest loss_shape: 0.020176138275135785\ntest loss_recon: 0.11361475097827423\ntest unet DSC: [0.00010456138988956809, 0.0001437266473658383, 2.4029855921980925e-05, 0.7802633047103882]\nBest val loss: 0.988019802631476\nTime: 57.429686546325684\n\n\nEpoch 93/250\n\ntrain loss: 1.0929185087922253\ntrain loss_segm: 1.073204741070542\ntrain loss_shape: 0.030180727108086967\ntrain loss_recon: 0.16695695370435715\ntrain unet DSC: [0.0002084357984131202, 0.00025803595781326294, 3.6989393265685067e-05, 0.7049945592880249]\n\ntest loss: 0.9987100133529077\ntest loss_segm: 0.9849823254805344\ntest loss_shape: 0.020066355999845725\ntest loss_recon: 0.11721057540331131\ntest unet DSC: [9.298705845139921e-05, 0.00012532423716038465, 2.0729887182824314e-05, 0.7802010774612427]\nBest val loss: 0.988019802631476\nTime: 57.66535520553589\n\n\nEpoch 94/250\n\ntrain loss: 1.091039408611346\ntrain loss_segm: 1.0713853424862971\ntrain loss_shape: 0.029877883759386176\ntrain loss_recon: 0.1666627638513529\ntrain unet DSC: [0.00020632341329474002, 0.00026141252601519227, 3.6910063499817625e-05, 0.7101505398750305]\n\ntest loss: 0.991097303537222\ntest loss_segm: 0.9779442778000464\ntest loss_shape: 0.01890877791895316\ntest loss_recon: 0.11262160616043286\ntest unet DSC: [8.896320650819689e-05, 0.00011866161366924644, 2.2005106075084768e-05, 0.7868363261222839]\nBest val loss: 0.988019802631476\nTime: 57.28859233856201\n\n\nEpoch 95/250\n\ntrain loss: 1.0906399379048166\ntrain loss_segm: 1.070915987597236\ntrain loss_shape: 0.029940978718237785\ntrain loss_recon: 0.16729855895796908\ntrain unet DSC: [0.00020993128418922424, 0.0002637931320350617, 3.595729504013434e-05, 0.7137419581413269]\n\ntest loss: 0.9956584114294785\ntest loss_segm: 0.9821558793385824\ntest loss_shape: 0.02055145453852721\ntest loss_recon: 0.11447381447905149\ntest unet DSC: [0.0001006597449304536, 0.00012300910020712763, 1.7170068531413563e-05, 0.7650843858718872]\nBest val loss: 0.988019802631476\nTime: 58.29018425941467\n\n\nEpoch 96/250\n\ntrain loss: 1.088427259197718\ntrain loss_segm: 1.0688967289803903\ntrain loss_shape: 0.02981996660038263\ntrain loss_recon: 0.1654853509573997\ntrain unet DSC: [0.00020263815531507134, 0.0002622237952891737, 3.3438362152082846e-05, 0.7109111547470093]\n\ntest loss: 0.9912297771527216\ntest loss_segm: 0.9781281764690692\ntest loss_shape: 0.018748010150515117\ntest loss_recon: 0.11226798565341876\ntest unet DSC: [9.306002903031185e-05, 0.00011970887135248631, 1.9030347175430506e-05, 0.7763610482215881]\nBest val loss: 0.988019802631476\nTime: 57.36748957633972\n\n\nEpoch 97/250\n\ntrain loss: 1.0874835550785065\ntrain loss_segm: 1.0678840207902691\ntrain loss_shape: 0.029598642117049122\ntrain loss_recon: 0.1663966940928109\ntrain unet DSC: [0.00020840714569203556, 0.0002551106736063957, 3.5450135328574106e-05, 0.7132118940353394]\n\ntest loss: 0.9992082913716634\ntest loss_segm: 0.9857012904607333\ntest loss_shape: 0.02043182476877402\ntest loss_recon: 0.11463817686606677\ntest unet DSC: [0.0001277057599509135, 0.00015715962217655033, 2.3305250579142012e-05, 0.7711018323898315]\nBest val loss: 0.988019802631476\nTime: 57.269660234451294\n\n\nEpoch 98/250\n\ntrain loss: 1.083583007884931\ntrain loss_segm: 1.0644004439251333\ntrain loss_shape: 0.0294563623994023\ntrain loss_recon: 0.162369289352924\ntrain unet DSC: [0.00022146264382172376, 0.00026976634399034083, 3.6632976843975484e-05, 0.7146984338760376]\n\ntest loss: 0.9930955095168872\ntest loss_segm: 0.9796935747831296\ntest loss_shape: 0.020133117978007365\ntest loss_recon: 0.11388634345852412\ntest unet DSC: [0.000117018076707609, 0.00013435535947792232, 2.2656407963950187e-05, 0.7843238115310669]\nBest val loss: 0.988019802631476\nTime: 57.21092367172241\n\n\nEpoch 99/250\n\ntrain loss: 1.084102877710439\ntrain loss_segm: 1.0647631464879723\ntrain loss_shape: 0.0294869496380981\ntrain loss_recon: 0.16391039734022528\ntrain unet DSC: [0.00021864565496798605, 0.0002692640700843185, 3.8471902371384203e-05, 0.7183727622032166]\n\ntest loss: 0.9962787521191132\ntest loss_segm: 0.9827250670164059\ntest loss_shape: 0.019245407758997038\ntest loss_recon: 0.11629151619779758\ntest unet DSC: [8.79567742231302e-05, 0.0001147446091636084, 2.2428675947594456e-05, 0.7714946269989014]\nBest val loss: 0.988019802631476\nTime: 57.37991523742676\n\n\nEpoch 100/250\n\ntrain loss: 1.0822076454192777\ntrain loss_segm: 1.0630130707463132\ntrain loss_shape: 0.029057941611714756\ntrain loss_recon: 0.1628878582504731\ntrain unet DSC: [0.000208559533348307, 0.0002633168187458068, 3.7195695767877623e-05, 0.7182009816169739]\n\ntest loss: 0.9993484631562845\ntest loss_segm: 0.985462425610958\ntest loss_shape: 0.01981297281021491\ntest loss_recon: 0.11904745453443283\ntest unet DSC: [8.735497976886109e-05, 0.00010287723125657067, 1.6053782019298524e-05, 0.7661629319190979]\nBest val loss: 0.988019802631476\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.75450801849365\n\n\nEpoch 101/250\n\ntrain loss: 1.0825311941436575\ntrain loss_segm: 1.0631720416153534\ntrain loss_shape: 0.029128004643547385\ntrain loss_recon: 0.16446355702001839\ntrain unet DSC: [0.00020516196673270315, 0.00025642808759585023, 3.398414628463797e-05, 0.7168018221855164]\n\ntest loss: 0.9890615191215124\ntest loss_segm: 0.9760910578263111\ntest loss_shape: 0.018500947202436436\ntest loss_recon: 0.11120366056760152\ntest unet DSC: [9.771426994120702e-05, 0.00011892745533259585, 1.8077324057230726e-05, 0.7777443528175354]\nBest val loss: 0.988019802631476\nTime: 58.17499327659607\n\n\nEpoch 102/250\n\ntrain loss: 1.0790860634061354\ntrain loss_segm: 1.0600263457509536\ntrain loss_shape: 0.028372924951718576\ntrain loss_recon: 0.16222422324781177\ntrain unet DSC: [0.0001988532894756645, 0.00025165878469124436, 3.4273351047886536e-05, 0.7174533605575562]\n\ntest loss: 0.9882779350647559\ntest loss_segm: 0.9753302931785583\ntest loss_shape: 0.01863668706172552\ntest loss_recon: 0.11083974822973594\ntest unet DSC: [0.00010329367796657607, 0.000122218974865973, 2.0037834474351257e-05, 0.7821415066719055]\nBest val loss: 0.988019802631476\nTime: 57.64344382286072\n\n\nEpoch 103/250\n\ntrain loss: 1.081418668544745\ntrain loss_segm: 1.0621372748024855\ntrain loss_shape: 0.028658138623437548\ntrain loss_recon: 0.16415582269524473\ntrain unet DSC: [0.0002117338590323925, 0.000257028586929664, 3.4149499697377905e-05, 0.718463122844696]\n\ntest loss: 0.9883299806179144\ntest loss_segm: 0.9753519403628814\ntest loss_shape: 0.01817059249449999\ntest loss_recon: 0.11160988150498806\ntest unet DSC: [9.216518810717389e-05, 0.00011536949750734493, 1.8700784494285472e-05, 0.7761873006820679]\nBest val loss: 0.988019802631476\nTime: 57.817296504974365\n\n\nEpoch 104/250\n\ntrain loss: 1.0816256339791455\ntrain loss_segm: 1.0623780492740342\ntrain loss_shape: 0.02861346689913469\ntrain loss_recon: 0.16386238278090198\ntrain unet DSC: [0.00020580612181220204, 0.0002559202257543802, 3.718531661434099e-05, 0.7137640714645386]\n\ntest loss: 0.9856771735044626\ntest loss_segm: 0.9728558445588137\ntest loss_shape: 0.017999863180403527\ntest loss_recon: 0.11021343332070571\ntest unet DSC: [9.454732207814232e-05, 0.00012023074668832123, 1.863408033386804e-05, 0.7809075713157654]\nBest val loss: 0.9856771735044626\nTime: 57.84594202041626\n\n\nEpoch 105/250\n\ntrain loss: 1.0795951953417138\ntrain loss_segm: 1.0605386838882784\ntrain loss_shape: 0.028559958974866174\ntrain loss_recon: 0.16200523059579391\ntrain unet DSC: [0.00020909708109684289, 0.000261571112787351, 3.674698746181093e-05, 0.721096396446228]\n\ntest loss: 0.9872743089993795\ntest loss_segm: 0.9743972222010294\ntest loss_shape: 0.0180591565485184\ntest loss_recon: 0.11071171592443417\ntest unet DSC: [9.418749687029049e-05, 0.00011406067642383277, 1.886257632577326e-05, 0.7801295518875122]\nBest val loss: 0.9856771735044626\nTime: 58.91507840156555\n\n\nEpoch 106/250\n\ntrain loss: 1.0797893951965283\ntrain loss_segm: 1.0606544779825815\ntrain loss_shape: 0.028619212121997452\ntrain loss_recon: 0.16272994208656535\ntrain unet DSC: [0.00020154760568402708, 0.0002564388560131192, 3.2356721931137145e-05, 0.7215920686721802]\n\ntest loss: 0.9799409646254319\ntest loss_segm: 0.9676005274821551\ntest loss_shape: 0.01715252214135268\ntest loss_recon: 0.1062519253255465\ntest unet DSC: [0.0001003242505248636, 0.00012042179150739685, 2.0267510990379378e-05, 0.7850558757781982]\nBest val loss: 0.9799409646254319\nTime: 58.01513123512268\n\n\nEpoch 107/250\n\ntrain loss: 1.0770575943626934\ntrain loss_segm: 1.0580083286460442\ntrain loss_shape: 0.028071921183197183\ntrain loss_recon: 0.16242079580603522\ntrain unet DSC: [0.00020388601114973426, 0.0002593033714219928, 3.4680419048527256e-05, 0.7252795696258545]\n\ntest loss: 0.9875506728123395\ntest loss_segm: 0.9745878852330722\ntest loss_shape: 0.018150633989045255\ntest loss_recon: 0.11147729479349576\ntest unet DSC: [9.559206228004768e-05, 0.00011464628914836794, 1.9843897462124005e-05, 0.7780545353889465]\nBest val loss: 0.9799409646254319\nTime: 57.53073835372925\n\n\nEpoch 108/250\n\ntrain loss: 1.0788912177085876\ntrain loss_segm: 1.0598396051533614\ntrain loss_shape: 0.028131121380513983\ntrain loss_recon: 0.16238497266003601\ntrain unet DSC: [0.0002036448277067393, 0.00025400437880307436, 3.336009831400588e-05, 0.7195820808410645]\n\ntest loss: 0.9796078220391885\ntest loss_segm: 0.9673797916143368\ntest loss_shape: 0.01727007484684388\ntest loss_recon: 0.10501025817715205\ntest unet DSC: [0.00010453040158608928, 0.00012590154074132442, 2.0429808500921354e-05, 0.7849568128585815]\nBest val loss: 0.9796078220391885\nTime: 57.75038766860962\n\n\nEpoch 109/250\n\ntrain loss: 1.0774647650084919\ntrain loss_segm: 1.0584269015094903\ntrain loss_shape: 0.028272222515337075\ntrain loss_recon: 0.16210642082121554\ntrain unet DSC: [0.0002067914028884843, 0.0002563390298746526, 3.4028824302367866e-05, 0.721396267414093]\n\ntest loss: 0.9809022759779905\ntest loss_segm: 0.9686091679793137\ntest loss_shape: 0.01765133562282874\ntest loss_recon: 0.10527972466288468\ntest unet DSC: [0.00011269313836237416, 0.00013167757424525917, 2.1900086721871048e-05, 0.7852370142936707]\nBest val loss: 0.9796078220391885\nTime: 57.62450361251831\n\n\nEpoch 110/250\n\ntrain loss: 1.0787318092358262\ntrain loss_segm: 1.0596700076815448\ntrain loss_shape: 0.02842490840703249\ntrain loss_recon: 0.1621930735020698\ntrain unet DSC: [0.0002052138006547466, 0.0002530566125642508, 3.39553807862103e-05, 0.7188109159469604]\n\ntest loss: 0.9876938126026056\ntest loss_segm: 0.9747067934427506\ntest loss_shape: 0.018075083716748618\ntest loss_recon: 0.11179518164732517\ntest unet DSC: [9.307732398156077e-05, 0.00011503924179123715, 1.9516821339493617e-05, 0.7776845097541809]\nBest val loss: 0.9796078220391885\nTime: 57.50008702278137\n\n\nEpoch 111/250\n\ntrain loss: 1.0770421952386446\ntrain loss_segm: 1.0579070392288739\ntrain loss_shape: 0.028325585247595098\ntrain loss_recon: 0.16302599458471884\ntrain unet DSC: [0.00020775278971996158, 0.0002588133793324232, 3.387483593542129e-05, 0.7267197370529175]\n\ntest loss: 0.9805652254667038\ntest loss_segm: 0.9681409276448764\ntest loss_shape: 0.01747098844498396\ntest loss_recon: 0.10677208589055599\ntest unet DSC: [0.0001035912282532081, 0.00012678318307735026, 2.1967105567455292e-05, 0.7865937948226929]\nBest val loss: 0.9796078220391885\nTime: 57.72647023200989\n\n\nEpoch 112/250\n\ntrain loss: 1.0785109098953536\ntrain loss_segm: 1.0595622089090226\ntrain loss_shape: 0.02786549576852895\ntrain loss_recon: 0.16162151890464976\ntrain unet DSC: [0.00020212744129821658, 0.0002514614607207477, 3.4579661587486044e-05, 0.7165057063102722]\n\ntest loss: 0.9852633262291933\ntest loss_segm: 0.9724835692307888\ntest loss_shape: 0.017827999491531115\ntest loss_recon: 0.10996958622947717\ntest unet DSC: [9.363416029373184e-05, 0.000123852034448646, 1.9680479454109445e-05, 0.7802664041519165]\nBest val loss: 0.9796078220391885\nTime: 57.17261004447937\n\n\nEpoch 113/250\n\ntrain loss: 1.0763957881474797\ntrain loss_segm: 1.0572811606564099\ntrain loss_shape: 0.028255066068112095\ntrain loss_recon: 0.16289120801736282\ntrain unet DSC: [0.0002068233588943258, 0.00025334703968837857, 3.476463461993262e-05, 0.7280988097190857]\n\ntest loss: 0.9855286677678426\ntest loss_segm: 0.9727992782225976\ntest loss_shape: 0.018027867811421554\ntest loss_recon: 0.10926615666502561\ntest unet DSC: [0.00010614038910716772, 0.00012880880967713892, 2.134614624083042e-05, 0.7799482941627502]\nBest val loss: 0.9796078220391885\nTime: 57.31923031806946\n\n\nEpoch 114/250\n\ntrain loss: 1.0753207523611528\ntrain loss_segm: 1.0563941182969492\ntrain loss_shape: 0.02823692437591432\ntrain loss_recon: 0.16102942569723613\ntrain unet DSC: [0.00020837222109548748, 0.00025125782121904194, 3.52763308910653e-05, 0.7277405858039856]\n\ntest loss: 0.9844144827280289\ntest loss_segm: 0.9717772603034973\ntest loss_shape: 0.017774310894310474\ntest loss_recon: 0.10859789995428844\ntest unet DSC: [0.00010440433834446594, 0.00012678503117058426, 2.0986019080737606e-05, 0.7812095880508423]\nBest val loss: 0.9796078220391885\nTime: 57.34684729576111\n\n\nEpoch 115/250\n\ntrain loss: 1.0796925478343722\ntrain loss_segm: 1.0604476577873472\ntrain loss_shape: 0.028461540554048894\ntrain loss_recon: 0.16398736127192462\ntrain unet DSC: [0.00020620273426175117, 0.0002572751836851239, 3.40554652211722e-05, 0.7205132842063904]\n\ntest loss: 0.9842982720106076\ntest loss_segm: 0.9716862715207614\ntest loss_shape: 0.017683625125732176\ntest loss_recon: 0.10843640804672852\ntest unet DSC: [0.00010205103171756491, 0.00011728653771569952, 2.0593293811543845e-05, 0.7799113392829895]\nBest val loss: 0.9796078220391885\nTime: 57.98516082763672\n\n\nEpoch 116/250\n\ntrain loss: 1.07943847586837\ntrain loss_segm: 1.0603691297995894\ntrain loss_shape: 0.028121828113364267\ntrain loss_recon: 0.16257160458760925\ntrain unet DSC: [0.00020248461805749685, 0.00024910029605962336, 3.459737854427658e-05, 0.7130086421966553]\n\ntest loss: 0.9811746034866724\ntest loss_segm: 0.9687522405233139\ntest loss_shape: 0.017408772252309017\ntest loss_recon: 0.10681483159080529\ntest unet DSC: [0.00010417679732199758, 0.0001283178135054186, 2.052818854281213e-05, 0.7817320823669434]\nBest val loss: 0.9796078220391885\nTime: 57.63861966133118\n\n\nEpoch 117/250\n\ntrain loss: 1.0793011656290368\ntrain loss_segm: 1.0602422907382627\ntrain loss_shape: 0.028038195888452893\ntrain loss_recon: 0.16255057652634156\ntrain unet DSC: [0.0002017463557422161, 0.0002573874080553651, 3.295119313406758e-05, 0.7157265543937683]\n\ntest loss: 0.9833217981534127\ntest loss_segm: 0.970659199433449\ntest loss_shape: 0.017693823609405603\ntest loss_recon: 0.1089321735004584\ntest unet DSC: [9.767862502485514e-05, 0.0001273718662559986, 2.1118255972396582e-05, 0.7830079197883606]\nBest val loss: 0.9796078220391885\nTime: 57.446890115737915\n\n\nEpoch 118/250\n\ntrain loss: 1.0819923436339898\ntrain loss_segm: 1.0626452395433112\ntrain loss_shape: 0.028378476542008074\ntrain loss_recon: 0.16509256523715543\ntrain unet DSC: [0.00020423611567821354, 0.0002459841198287904, 3.4493059501983225e-05, 0.7150129675865173]\n\ntest loss: 0.9823236816968673\ntest loss_segm: 0.9698383884552197\ntest loss_shape: 0.017350872453206625\ntest loss_recon: 0.10750201965371768\ntest unet DSC: [9.84893340501003e-05, 0.00012188987602712587, 2.03972576855449e-05, 0.7813554406166077]\nBest val loss: 0.9796078220391885\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.6157968044281\n\n\nEpoch 119/250\n\ntrain loss: 1.0769530463067791\ntrain loss_segm: 1.0579299704183507\ntrain loss_shape: 0.028201626000713697\ntrain loss_recon: 0.16202918791412552\ntrain unet DSC: [0.00020173781376797706, 0.00024899496929720044, 3.511684917612001e-05, 0.7211483716964722]\n\ntest loss: 0.9827498747752264\ntest loss_segm: 0.9702417560112782\ntest loss_shape: 0.01776073197237192\ntest loss_recon: 0.10732045626411071\ntest unet DSC: [0.00010553747415542603, 0.00012648980191443115, 2.1670466594514437e-05, 0.7840546369552612]\nBest val loss: 0.9796078220391885\nTime: 57.14355134963989\n\n\nEpoch 120/250\n\ntrain loss: 1.0748038661630848\ntrain loss_segm: 1.05596268139308\ntrain loss_shape: 0.028104133774302428\ntrain loss_recon: 0.16030770684061926\ntrain unet DSC: [0.00021115098206792027, 0.0002622190222609788, 3.607184044085443e-05, 0.7232292890548706]\n\ntest loss: 0.9816986796183463\ntest loss_segm: 0.9691533262913044\ntest loss_shape: 0.017392700609679405\ntest loss_recon: 0.10806079027362359\ntest unet DSC: [9.783944551600143e-05, 0.00011908308079000562, 2.0038232833030634e-05, 0.7840580344200134]\nBest val loss: 0.9796078220391885\nTime: 57.71725392341614\n\n\nEpoch 121/250\n\ntrain loss: 1.0747137563892557\ntrain loss_segm: 1.0558541047422192\ntrain loss_shape: 0.02793728034424631\ntrain loss_recon: 0.16065920901166486\ntrain unet DSC: [0.00020278252486605197, 0.00025315475068055093, 3.497776560834609e-05, 0.7235256433486938]\n\ntest loss: 0.9810243096107092\ntest loss_segm: 0.9685565645878131\ntest loss_shape: 0.01723589315914955\ntest loss_recon: 0.10744158054391544\ntest unet DSC: [9.78715397650376e-05, 0.00012250919826328754, 2.0528355889837258e-05, 0.7830384969711304]\nBest val loss: 0.9796078220391885\nTime: 58.43729782104492\n\n\nEpoch 122/250\n\ntrain loss: 1.0751556259921835\ntrain loss_segm: 1.0562504462048978\ntrain loss_shape: 0.027930337699908245\ntrain loss_recon: 0.1611214945022064\ntrain unet DSC: [0.00020619425049517304, 0.00025290815392509103, 3.349153121234849e-05, 0.7220791578292847]\n\ntest loss: 0.9847890673539578\ntest loss_segm: 0.9719936175224109\ntest loss_shape: 0.017693617524435885\ntest loss_recon: 0.11026102848924123\ntest unet DSC: [9.31736794882454e-05, 0.00011500652908580378, 1.9287545001134276e-05, 0.7812914252281189]\nBest val loss: 0.9796078220391885\nTime: 57.18257427215576\n\n\nEpoch 123/250\n\ntrain loss: 1.0755915336216553\ntrain loss_segm: 1.0566824025745634\ntrain loss_shape: 0.028074223088407065\ntrain loss_recon: 0.16101710849641998\ntrain unet DSC: [0.00020591120119206607, 0.00026252723182551563, 3.4039709134958684e-05, 0.7225552797317505]\n\ntest loss: 0.9797173906595279\ntest loss_segm: 0.9673275183408688\ntest loss_shape: 0.0171744194932473\ntest loss_recon: 0.10672427990879768\ntest unet DSC: [9.940892778104171e-05, 0.000121953860798385, 2.0626643163268454e-05, 0.7855936884880066]\nBest val loss: 0.9796078220391885\nTime: 59.381768226623535\n\n\nEpoch 124/250\n\ntrain loss: 1.0761630689796013\ntrain loss_segm: 1.0572012127200259\ntrain loss_shape: 0.027672910821164333\ntrain loss_recon: 0.16194569168588782\ntrain unet DSC: [0.00020095310173928738, 0.0002562009321991354, 3.187830952811055e-05, 0.7223625779151917]\n\ntest loss: 0.9818107225956061\ntest loss_segm: 0.9693413025293595\ntest loss_shape: 0.01744674287067774\ntest loss_recon: 0.10724753685868703\ntest unet DSC: [0.00010204773570876569, 0.00012371655611786991, 2.0690475139417686e-05, 0.7847996950149536]\nBest val loss: 0.9796078220391885\nTime: 57.392255544662476\n\n\nEpoch 125/250\n\ntrain loss: 1.0753435409521754\ntrain loss_segm: 1.056373117468025\ntrain loss_shape: 0.02802708297024799\ntrain loss_recon: 0.16167716344794894\ntrain unet DSC: [0.00020515287178568542, 0.00026181634166277945, 3.756897058337927e-05, 0.7196313738822937]\n\ntest loss: 0.9841382411810068\ntest loss_segm: 0.9714549290828216\ntest loss_shape: 0.017696954500980865\ntest loss_recon: 0.10913622006773949\ntest unet DSC: [9.894749382510781e-05, 0.00012081280874554068, 2.111623871314805e-05, 0.7825255393981934]\nBest val loss: 0.9796078220391885\nTime: 57.673181772232056\n\n\nEpoch 126/250\n\ntrain loss: 1.0764429440981225\ntrain loss_segm: 1.0574622339085689\ntrain loss_shape: 0.02788371715364577\ntrain loss_recon: 0.1619233698643084\ntrain unet DSC: [0.00020199830760248005, 0.0002476860536262393, 3.352678322698921e-05, 0.7217792272567749]\n\ntest loss: 0.9821995832981207\ntest loss_segm: 0.9696393410364786\ntest loss_shape: 0.017336829255024593\ntest loss_recon: 0.10826560100301719\ntest unet DSC: [9.735274943523109e-05, 0.00012114120909245685, 2.013635639741551e-05, 0.7823185324668884]\nBest val loss: 0.9796078220391885\nTime: 57.073423624038696\n\n\nEpoch 127/250\n\ntrain loss: 1.0749000510837459\ntrain loss_segm: 1.0560525433172154\ntrain loss_shape: 0.027759069612226153\ntrain loss_recon: 0.16071607174846944\ntrain unet DSC: [0.0002041647967416793, 0.00025306781753897667, 3.53623581759166e-05, 0.7230799794197083]\n\ntest loss: 0.982106381501907\ntest loss_segm: 0.9695852964352338\ntest loss_shape: 0.017598945265397046\ntest loss_recon: 0.10761193214700772\ntest unet DSC: [0.00010299649147782475, 0.00012466318730730563, 2.0985207811463624e-05, 0.784546971321106]\nBest val loss: 0.9796078220391885\nTime: 57.63813233375549\n\n\nEpoch 128/250\n\ntrain loss: 1.0777005481569073\ntrain loss_segm: 1.058605683000782\ntrain loss_shape: 0.027980518833840194\ntrain loss_recon: 0.16296808886094183\ntrain unet DSC: [0.0001985742710530758, 0.00024880378623493016, 3.388542609172873e-05, 0.7233825922012329]\n\ntest loss: 0.984001527994107\ntest loss_segm: 0.9712426799994248\ntest loss_shape: 0.017666152463509485\ntest loss_recon: 0.10992237476584239\ntest unet DSC: [9.565657092025504e-05, 0.00011650710803223774, 1.9320123101351783e-05, 0.781801700592041]\nBest val loss: 0.9796078220391885\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.44585919380188\n\n\nEpoch 129/250\n\ntrain loss: 1.0769027784655365\ntrain loss_segm: 1.0579030272326893\ntrain loss_shape: 0.027888507087113736\ntrain loss_recon: 0.16210901048741763\ntrain unet DSC: [0.00020022568060085177, 0.00025412195827811956, 3.265099803684279e-05, 0.7198056578636169]\n\ntest loss: 0.9844486086796491\ntest loss_segm: 0.971677439335065\ntest loss_shape: 0.017696053100128967\ntest loss_recon: 0.11001564992161897\ntest unet DSC: [9.565589425619692e-05, 0.00011621291196206585, 1.9417968360357918e-05, 0.780933678150177]\nBest val loss: 0.9796078220391885\nTime: 57.435407876968384\n\n\nEpoch 130/250\n\ntrain loss: 1.072586471144157\ntrain loss_segm: 1.0538472657716726\ntrain loss_shape: 0.02758169659896742\ntrain loss_recon: 0.1598103967129807\ntrain unet DSC: [0.00020075537031516433, 0.00024576327996328473, 3.358237154316157e-05, 0.7279376983642578]\n\ntest loss: 0.9836376309394836\ntest loss_segm: 0.9709380284333841\ntest loss_shape: 0.01758845028682397\ntest loss_recon: 0.1094075419390813\ntest unet DSC: [9.745088027557358e-05, 0.00011732237180694938, 1.9613766198744997e-05, 0.7828273177146912]\nBest val loss: 0.9796078220391885\nTime: 57.34713363647461\n\n\nEpoch 131/250\n\ntrain loss: 1.0757786525955684\ntrain loss_segm: 1.0567714594587494\ntrain loss_shape: 0.027952221334216338\ntrain loss_recon: 0.1621197127513116\ntrain unet DSC: [0.00020164372108411044, 0.0002445101854391396, 3.17636840918567e-05, 0.7272930145263672]\n\ntest loss: 0.9853595632773179\ntest loss_segm: 0.9724363088607788\ntest loss_shape: 0.017856354992359113\ntest loss_recon: 0.11137622986466457\ntest unet DSC: [9.376845991937444e-05, 0.00011317965982016176, 1.8275408365298063e-05, 0.779725968837738]\nBest val loss: 0.9796078220391885\nTime: 58.37677884101868\n\n\nEpoch 132/250\n\ntrain loss: 1.0777087841607347\ntrain loss_segm: 1.0586330434189568\ntrain loss_shape: 0.028280212036982368\ntrain loss_recon: 0.1624771561473608\ntrain unet DSC: [0.00020605226745828986, 0.000257406965829432, 3.2491832826053724e-05, 0.7205551266670227]\n\ntest loss: 0.9843636292677659\ntest loss_segm: 0.9715579656454233\ntest loss_shape: 0.017714616173926074\ntest loss_recon: 0.11034214124083519\ntest unet DSC: [9.562874038238078e-05, 0.00011546326277311891, 1.9026771042263135e-05, 0.7802940011024475]\nBest val loss: 0.9796078220391885\nTime: 57.75640296936035\n\n\nEpoch 133/250\n\ntrain loss: 1.0733267065090468\ntrain loss_segm: 1.054599711412116\ntrain loss_shape: 0.02774904966590148\ntrain loss_recon: 0.15952086186956002\ntrain unet DSC: [0.0002107791369780898, 0.0002577278355602175, 3.27550878864713e-05, 0.7233725786209106]\n\ntest loss: 0.9820282963606027\ntest loss_segm: 0.969539822676243\ntest loss_shape: 0.017543507883181937\ntest loss_recon: 0.1073412078504379\ntest unet DSC: [0.00010374560952186584, 0.0001228676992468536, 2.0657864297390915e-05, 0.7851142287254333]\nBest val loss: 0.9796078220391885\nTime: 57.04612588882446\n\n\nEpoch 134/250\n\ntrain loss: 1.0767339852791797\ntrain loss_segm: 1.0576965254318864\ntrain loss_shape: 0.02810480459180625\ntrain loss_recon: 0.1622697877214302\ntrain unet DSC: [0.0002050063048955053, 0.0002533460792619735, 3.440635555307381e-05, 0.7210662364959717]\n\ntest loss: 0.9818958319150485\ntest loss_segm: 0.969347996589465\ntest loss_shape: 0.017376920088934593\ntest loss_recon: 0.10810152928416546\ntest unet DSC: [9.780875552678481e-05, 0.00011820181680377573, 1.9613595213741064e-05, 0.7841505408287048]\nBest val loss: 0.9796078220391885\nTime: 57.52693462371826\n\n\nEpoch 135/250\n\ntrain loss: 1.0769531987135923\ntrain loss_segm: 1.0580485715896268\ntrain loss_shape: 0.027874471690458587\ntrain loss_recon: 0.16117183090765266\ntrain unet DSC: [0.00020410073921084404, 0.0002519136178307235, 3.270146044087596e-05, 0.7157667279243469]\n\ntest loss: 0.9837113374318832\ntest loss_segm: 0.9710363180209429\ntest loss_shape: 0.01759216437737147\ntest loss_recon: 0.10915798646135208\ntest unet DSC: [9.702358511276543e-05, 0.00011604840256040916, 1.997284016397316e-05, 0.7830691933631897]\nBest val loss: 0.9796078220391885\nTime: 57.3316969871521\n\n\nEpoch 136/250\n\ntrain loss: 1.0727588323098194\ntrain loss_segm: 1.0539696567420718\ntrain loss_shape: 0.027755673962963533\ntrain loss_recon: 0.16013613579016697\ntrain unet DSC: [0.00020487884467002004, 0.00025936707970686257, 3.350949555169791e-05, 0.7268920540809631]\n\ntest loss: 0.9843391928917322\ntest loss_segm: 0.9716180440707084\ntest loss_shape: 0.017660761705766886\ntest loss_recon: 0.10955071076750755\ntest unet DSC: [9.738387598190457e-05, 0.00011692992848111317, 1.9940302081522532e-05, 0.7815262079238892]\nBest val loss: 0.9796078220391885\nTime: 58.12537693977356\n\n\nEpoch 137/250\n\ntrain loss: 1.0746215476265437\ntrain loss_segm: 1.0557367492325698\ntrain loss_shape: 0.027771449765732772\ntrain loss_recon: 0.16107652619292465\ntrain unet DSC: [0.00020164491434115916, 0.0002529821649659425, 3.367698809597641e-05, 0.7225564122200012]\n\ntest loss: 0.980912249821883\ntest loss_segm: 0.9685019147701752\ntest loss_shape: 0.017340565506273355\ntest loss_recon: 0.10676277381105301\ntest unet DSC: [0.0001026686149998568, 0.0001232262293342501, 2.0560344637488015e-05, 0.786308765411377]\nBest val loss: 0.9796078220391885\nTime: 57.48026704788208\n\n\nEpoch 138/250\n\ntrain loss: 1.076034639455095\ntrain loss_segm: 1.057059927077233\ntrain loss_shape: 0.02774329445784605\ntrain loss_recon: 0.16200384481138067\ntrain unet DSC: [0.00020271044922992587, 0.0002555226383265108, 3.448699862929061e-05, 0.7198759913444519]\n\ntest loss: 0.9857755135267209\ntest loss_segm: 0.972886982636574\ntest loss_shape: 0.01784119349068556\ntest loss_recon: 0.11104419989845692\ntest unet DSC: [9.451538062421605e-05, 0.00011500641994643956, 1.9091401554760523e-05, 0.7791789174079895]\nBest val loss: 0.9796078220391885\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.51548433303833\n\n\nEpoch 139/250\n\ntrain loss: 1.074974452770209\ntrain loss_segm: 1.0560420008399818\ntrain loss_shape: 0.027741549866674823\ntrain loss_recon: 0.16158304752522631\ntrain unet DSC: [0.00019668271124828607, 0.00024786798167042434, 3.3130992960650474e-05, 0.7234336137771606]\n\ntest loss: 0.9840998435631777\ntest loss_segm: 0.97133030799719\ntest loss_shape: 0.01762469067500952\ntest loss_recon: 0.11007059317750809\ntest unet DSC: [9.425464668311179e-05, 0.00011481020192150027, 1.8862983779399656e-05, 0.7818493247032166]\nBest val loss: 0.9796078220391885\nTime: 57.23854064941406\n\n\nEpoch 140/250\n\ntrain loss: 1.0757334685023827\ntrain loss_segm: 1.0567603020728389\ntrain loss_shape: 0.027785074979632715\ntrain loss_recon: 0.1619466737714372\ntrain unet DSC: [0.00020634470274671912, 0.00025325847673229873, 3.4057058655889705e-05, 0.7250558137893677]\n\ntest loss: 0.9856988054055434\ntest loss_segm: 0.9727794038943756\ntest loss_shape: 0.01785887577212774\ntest loss_recon: 0.11133518671760192\ntest unet DSC: [9.318056254414842e-05, 0.00011415909830247983, 1.824261744332034e-05, 0.7787603139877319]\nBest val loss: 0.9796078220391885\nTime: 57.33431029319763\n\n\nEpoch 141/250\n\ntrain loss: 1.0765515196172497\ntrain loss_segm: 1.0576147414460968\ntrain loss_shape: 0.027908583006621163\ntrain loss_recon: 0.16145920649736742\ntrain unet DSC: [0.00020621108706109226, 0.0002489399048499763, 3.2861935324035585e-05, 0.7227616906166077]\n\ntest loss: 0.984002411365509\ntest loss_segm: 0.971280894218347\ntest loss_shape: 0.017621001252570212\ntest loss_recon: 0.10959423591311161\ntest unet DSC: [9.588268585503101e-05, 0.0001165386856882833, 1.9450397303444333e-05, 0.7822052836418152]\nBest val loss: 0.9796078220391885\nTime: 57.62045669555664\n\n\nEpoch 142/250\n\ntrain loss: 1.073671676312821\ntrain loss_segm: 1.0547969718522663\ntrain loss_shape: 0.027852692004717603\ntrain loss_recon: 0.16089436487306522\ntrain unet DSC: [0.00019824954506475478, 0.00024843603023327887, 3.146597373415716e-05, 0.7246397137641907]\n\ntest loss: 0.985187692519946\ntest loss_segm: 0.9723553840930645\ntest loss_shape: 0.017758770392109186\ntest loss_recon: 0.11056436111147587\ntest unet DSC: [9.513440454611555e-05, 0.00011507084127515554, 1.9581579181249253e-05, 0.7803138494491577]\nBest val loss: 0.9796078220391885\nTime: 57.34708023071289\n\n\nEpoch 143/250\n\ntrain loss: 1.077636813061147\ntrain loss_segm: 1.0584916457345215\ntrain loss_shape: 0.028195569250449727\ntrain loss_recon: 0.16325614912600456\ntrain unet DSC: [0.00020144766313023865, 0.00024982786271721125, 3.225647014915012e-05, 0.7219741344451904]\n\ntest loss: 0.98108532337042\ntest loss_segm: 0.9686208581313108\ntest loss_shape: 0.01730622520718055\ntest loss_recon: 0.10733846756510246\ntest unet DSC: [9.973250416805968e-05, 0.00012153020361438394, 2.0266576029825956e-05, 0.784938633441925]\nBest val loss: 0.9796078220391885\nTime: 57.71078705787659\n\n\nEpoch 144/250\n\ntrain loss: 1.07471100174928\ntrain loss_segm: 1.0559023992170262\ntrain loss_shape: 0.027546778144432776\ntrain loss_recon: 0.16053925353232062\ntrain unet DSC: [0.00020032197062391788, 0.00025122010265477, 3.256356285419315e-05, 0.719674825668335]\n\ntest loss: 0.9842438162901462\ntest loss_segm: 0.9715101627203134\ntest loss_shape: 0.017621127601999503\ntest loss_recon: 0.10971543355247913\ntest unet DSC: [9.650323772802949e-05, 0.00011689766688505188, 1.9711693312274292e-05, 0.7814419865608215]\nBest val loss: 0.9796078220391885\nTime: 58.64272952079773\n\n\nEpoch 145/250\n\ntrain loss: 1.0718097268026086\ntrain loss_segm: 1.053036882530285\ntrain loss_shape: 0.027590007621417695\ntrain loss_recon: 0.16013841710607463\ntrain unet DSC: [0.00020364295050967485, 0.0002505430893506855, 3.4948374377563596e-05, 0.729515790939331]\n\ntest loss: 0.9851556298060294\ntest loss_segm: 0.9722161996058929\ntest loss_shape: 0.017891760700597212\ntest loss_recon: 0.1115026222780729\ntest unet DSC: [9.32488837861456e-05, 0.00011337570322211832, 1.8210628695669584e-05, 0.7798628211021423]\nBest val loss: 0.9796078220391885\nTime: 58.03005337715149\n\n\nEpoch 146/250\n\ntrain loss: 1.0744929536233974\ntrain loss_segm: 1.0557082974458043\ntrain loss_shape: 0.02764066375933493\ntrain loss_recon: 0.1602059712138357\ntrain unet DSC: [0.0002033728378592059, 0.0002521022397559136, 3.492236282909289e-05, 0.7215824127197266]\n\ntest loss: 0.9835000267395606\ntest loss_segm: 0.9707585175832113\ntest loss_shape: 0.017579698767990638\ntest loss_recon: 0.10983532753128272\ntest unet DSC: [9.471386874793097e-05, 0.00011569129856070504, 1.8994081983692013e-05, 0.7818571925163269]\nBest val loss: 0.9796078220391885\nTime: 57.639496088027954\n\n\nEpoch 147/250\n\ntrain loss: 1.0747109300728086\ntrain loss_segm: 1.0557682223712341\ntrain loss_shape: 0.027791516843570185\ntrain loss_recon: 0.1616355710061668\ntrain unet DSC: [0.00020441188826225698, 0.0002545311872381717, 3.429174466873519e-05, 0.7240562438964844]\n\ntest loss: 0.9875429563033276\ntest loss_segm: 0.9744467108677595\ntest loss_shape: 0.018136186572985772\ntest loss_recon: 0.11282621830319747\ntest unet DSC: [9.230193245457485e-05, 0.00011285521031823009, 1.8079756046063267e-05, 0.7768831849098206]\nBest val loss: 0.9796078220391885\nTime: 58.21020817756653\n\n\nEpoch 148/250\n\ntrain loss: 1.0747921055630794\ntrain loss_segm: 1.0559816586820385\ntrain loss_shape: 0.027774107795727403\ntrain loss_recon: 0.1603304223405032\ntrain unet DSC: [0.00019866600632667542, 0.00024999745073728263, 3.229369758628309e-05, 0.7208381295204163]\n\ntest loss: 0.980478645899357\ntest loss_segm: 0.968051396883451\ntest loss_shape: 0.01720261748115986\ntest loss_recon: 0.10706987986579919\ntest unet DSC: [0.00010015812586061656, 0.00012205218808958307, 2.0332321582827717e-05, 0.7853472828865051]\nBest val loss: 0.9796078220391885\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.490190505981445\n\n\nEpoch 149/250\n\ntrain loss: 1.0735287564464762\ntrain loss_segm: 1.0548080549209933\ntrain loss_shape: 0.02758892590203617\ntrain loss_recon: 0.15961810842722277\ntrain unet DSC: [0.00020340287301223725, 0.0002548914635553956, 3.2604435546090826e-05, 0.7227153778076172]\n\ntest loss: 0.9840234074837122\ntest loss_segm: 0.971313687471243\ntest loss_shape: 0.017635217055869408\ntest loss_recon: 0.1094619411115463\ntest unet DSC: [9.790693002287298e-05, 0.00011843124229926616, 2.007105649681762e-05, 0.7808957695960999]\nBest val loss: 0.9796078220391885\nTime: 57.4032096862793\n\n\nEpoch 150/250\n\ntrain loss: 1.0762982772120946\ntrain loss_segm: 1.0572272690036628\ntrain loss_shape: 0.02789739940221174\ntrain loss_recon: 0.16281269059245346\ntrain unet DSC: [0.00020852894522249699, 0.000263691385043785, 3.378522887942381e-05, 0.7248300909996033]\n\ntest loss: 0.9816540556076245\ntest loss_segm: 0.9691893519499363\ntest loss_shape: 0.017429081985774714\ntest loss_recon: 0.10721783082072552\ntest unet DSC: [0.00010312618542229757, 0.00012387930473778397, 2.0690647943411022e-05, 0.784030556678772]\nBest val loss: 0.9796078220391885\nTime: 57.4911425113678\n\n\nEpoch 151/250\n\ntrain loss: 1.0747062880781633\ntrain loss_segm: 1.055751851087884\ntrain loss_shape: 0.02794744729807105\ntrain loss_recon: 0.16159695056797582\ntrain unet DSC: [0.0002057419769698754, 0.00025379154249094427, 3.501031460473314e-05, 0.7270886898040771]\n\ntest loss: 0.9823193397277441\ntest loss_segm: 0.9697358577679365\ntest loss_shape: 0.017415687394065734\ntest loss_recon: 0.1084191746627673\ntest unet DSC: [9.748154843691736e-05, 0.00011865905980812386, 1.9874933059327304e-05, 0.7829559445381165]\nBest val loss: 0.9796078220391885\nTime: 57.35641574859619\n\n\nEpoch 152/250\n\ntrain loss: 1.076518722727329\ntrain loss_segm: 1.0575047987925856\ntrain loss_shape: 0.027892746632517892\ntrain loss_recon: 0.16224650048379657\ntrain unet DSC: [0.0002022395929088816, 0.0002572696248535067, 3.371205093571916e-05, 0.7209009528160095]\n\ntest loss: 0.9856499036153158\ntest loss_segm: 0.9727545227759924\ntest loss_shape: 0.017782296842107408\ntest loss_recon: 0.11117158877925995\ntest unet DSC: [9.330821922048926e-05, 0.00011370125866960734, 1.8470802388037555e-05, 0.7798527479171753]\nBest val loss: 0.9796078220391885\nTime: 57.39432454109192\n\n\nEpoch 153/250\n\ntrain loss: 1.073679110294656\ntrain loss_segm: 1.0549151765394815\ntrain loss_shape: 0.027756960048705717\ntrain loss_recon: 0.15988239742626872\ntrain unet DSC: [0.0001981236709980294, 0.00025086189270950854, 3.1713545467937365e-05, 0.7233496308326721]\n\ntest loss: 0.9821408681380444\ntest loss_segm: 0.9695667058993609\ntest loss_shape: 0.017405001303324334\ntest loss_recon: 0.1083366100031596\ntest unet DSC: [9.803833381738514e-05, 0.00011980175622738898, 2.0136623788857833e-05, 0.7834301590919495]\nBest val loss: 0.9796078220391885\nTime: 57.137542963027954\n\n\nEpoch 154/250\n\ntrain loss: 1.0729192808459076\ntrain loss_segm: 1.0541129402721985\ntrain loss_shape: 0.028068258450661278\ntrain loss_recon: 0.15999522543500497\ntrain unet DSC: [0.00020762959320563823, 0.0002603189495857805, 3.4650562156457454e-05, 0.7251741290092468]\n\ntest loss: 0.982560354929704\ntest loss_segm: 0.9701045048542511\ntest loss_shape: 0.017757622501215875\ntest loss_recon: 0.10680095899181488\ntest unet DSC: [0.00010880183981498703, 0.00012818562390748411, 2.1604562789434567e-05, 0.7847234010696411]\nBest val loss: 0.9796078220391885\nTime: 57.807642221450806\n\n\nEpoch 155/250\n\ntrain loss: 1.0782017530519752\ntrain loss_segm: 1.059060635445993\ntrain loss_shape: 0.028267822016172017\ntrain loss_recon: 0.16314335669614846\ntrain unet DSC: [0.00020668907382059842, 0.0002610517549328506, 3.5658285924000666e-05, 0.7211687564849854]\n\ntest loss: 0.9821524925720997\ntest loss_segm: 0.9695919920236636\ntest loss_shape: 0.01740527167343176\ntest loss_recon: 0.10819981428675163\ntest unet DSC: [9.849271009443328e-05, 0.00012006209726678208, 2.016885446209926e-05, 0.7835472822189331]\nBest val loss: 0.9796078220391885\nTime: 58.59577465057373\n\n\nEpoch 156/250\n\ntrain loss: 1.0746033980122096\ntrain loss_segm: 1.055701117726821\ntrain loss_shape: 0.02807526688291868\ntrain loss_recon: 0.16094761058876786\ntrain unet DSC: [0.00020912384206894785, 0.00025682218256406486, 3.662566086859442e-05, 0.7261967658996582]\n\ntest loss: 0.9837192679062868\ntest loss_segm: 0.9710294268070123\ntest loss_shape: 0.017555610587199528\ntest loss_recon: 0.1093428464463124\ntest unet DSC: [9.715575288282707e-05, 0.00011755006562452763, 1.980948400159832e-05, 0.7814570665359497]\nBest val loss: 0.9796078220391885\nTime: 58.48516297340393\n\n\nEpoch 157/250\n\ntrain loss: 1.0790877534618861\ntrain loss_segm: 1.0599230403387094\ntrain loss_shape: 0.028099347471813613\ntrain loss_recon: 0.16354786482038378\ntrain unet DSC: [0.00020815477182623, 0.0002596702834125608, 3.21201587212272e-05, 0.7178703546524048]\n\ntest loss: 0.9834633989211841\ntest loss_segm: 0.970766041523371\ntest loss_shape: 0.017538007038335007\ntest loss_recon: 0.10943552555564122\ntest unet DSC: [9.578610479366034e-05, 0.00011637544230325148, 1.9483255528029986e-05, 0.782191812992096]\nBest val loss: 0.9796078220391885\nTime: 58.3160924911499\n\n\nEpoch 158/250\n\ntrain loss: 1.0755292248876789\ntrain loss_segm: 1.0566264436214785\ntrain loss_shape: 0.027926097400014914\ntrain loss_recon: 0.16110177812036836\ntrain unet DSC: [0.00020116982341278344, 0.00025379046564921737, 3.4125445381505415e-05, 0.7207576036453247]\n\ntest loss: 0.9838785957067441\ntest loss_segm: 0.9711806972821554\ntest loss_shape: 0.017587668978824064\ntest loss_recon: 0.10939132641905393\ntest unet DSC: [9.712269820738584e-05, 0.00011699496099026874, 1.9907482055714354e-05, 0.7817234396934509]\nBest val loss: 0.9796078220391885\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 58.642030477523804\n\nValidation loss stopped to decrease for 50 epochs. Training terminated.\nBest epoch: 108\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678176287489
        }
      },
      "id": "f324a1b2-f4d3-4798-9974-68e2a6c66c42"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.01, 0.1)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/250\n\ntrain loss: 2.0819810161107704\ntrain loss_segm: 1.9950085184242152\ntrain loss_shape: 0.4429322841423976\ntrain loss_recon: 0.8254318154310878\ntrain unet DSC: [0.0029797807801514864, 0.005523730535060167, 0.00018772351904772222, 0.00043243003892712295]\n\ntest loss: 2.0631533830593796\ntest loss_segm: 1.994207168236757\ntest loss_shape: 0.3576461122586177\ntest loss_recon: 0.6536975778066195\ntest unet DSC: [0.004681847989559174, 0.011594888754189014, 0.0001874046283774078, 0.0003627249097917229]\nBest val loss: 2.0631533830593796\nTime: 58.901795625686646\n\n\nEpoch 2/250\n\ntrain loss: 2.0470866387403466\ntrain loss_segm: 1.9843992628628695\ntrain loss_shape: 0.31069158357155474\ntrain loss_recon: 0.5958046203927149\ntrain unet DSC: [0.007809534668922424, 0.08312167227268219, 0.00018983325571753085, 0.00039585138438269496]\n\ntest loss: 1.9516642124224932\ntest loss_segm: 1.8990147205499501\ntest loss_shape: 0.2931606249931531\ntest loss_recon: 0.49717885102981174\ntest unet DSC: [0.01948983408510685, 0.45515626668930054, 0.0001935512264026329, 0.0003994391008745879]\nBest val loss: 1.9516642124224932\nTime: 58.32229709625244\n\n\nEpoch 3/250\n\ntrain loss: 1.919923151595683\ntrain loss_segm: 1.8614993216116218\ntrain loss_shape: 0.3075847061751764\ntrain loss_recon: 0.5534798099270349\ntrain unet DSC: [0.18257100880146027, 0.43388691544532776, 0.00018328601436223835, 0.00040705283754505217]\n\ntest loss: 1.8472867256555803\ntest loss_segm: 1.7802703625116594\ntest loss_shape: 0.23401714135438967\ntest loss_recon: 0.6467619018676953\ntest unet DSC: [0.40946289896965027, 0.47938820719718933, 0.00019876103033311665, 0.00037184011307545006]\nBest val loss: 1.8472867256555803\nTime: 57.99692964553833\n\n\nEpoch 4/250\n\ntrain loss: 1.8292242929905276\ntrain loss_segm: 1.77670230971107\ntrain loss_shape: 0.29627289066586315\ntrain loss_recon: 0.49559255679951436\ntrain unet DSC: [0.4148162305355072, 0.48467114567756653, 0.0001713872334221378, 0.0003617517068050802]\n\ntest loss: 1.8006701072057087\ntest loss_segm: 1.75232095290453\ntest loss_shape: 0.24666433991530004\ntest loss_recon: 0.4588251694654807\ntest unet DSC: [0.4556691646575928, 0.5362605452537537, 0.00020512435003183782, 0.0003178971237502992]\nBest val loss: 1.8006701072057087\nTime: 57.878201961517334\n\n\nEpoch 5/250\n\ntrain loss: 1.8110309529908095\ntrain loss_segm: 1.7648651562159574\ntrain loss_shape: 0.28421656806257706\ntrain loss_recon: 0.4332363197132002\ntrain unet DSC: [0.4386198818683624, 0.5015636682510376, 0.00017483989358879626, 0.00033359837834723294]\n\ntest loss: 1.8211538210893288\ntest loss_segm: 1.7609340349833171\ntest loss_shape: 0.2331107572103158\ntest loss_recon: 0.5788868123140091\ntest unet DSC: [0.44342896342277527, 0.5087677836418152, 0.00019478514150250703, 0.00031419057631865144]\nBest val loss: 1.8006701072057087\nTime: 57.47749376296997\n\n\nEpoch 6/250\n\ntrain loss: 1.802055191390122\ntrain loss_segm: 1.7598592555975612\ntrain loss_shape: 0.2754098499500299\ntrain loss_recon: 0.3944183472968355\ntrain unet DSC: [0.4505029618740082, 0.5081197023391724, 0.00016887379752006382, 0.0003162970533594489]\n\ntest loss: 1.8125203542220287\ntest loss_segm: 1.7545361213195019\ntest loss_shape: 0.23483965068291396\ntest loss_recon: 0.556358298430076\ntest unet DSC: [0.4609946310520172, 0.5185976028442383, 0.0001895505702123046, 0.00031730576301924884]\nBest val loss: 1.8006701072057087\nTime: 57.96160674095154\n\n\nEpoch 7/250\n\ntrain loss: 1.7980703522887411\ntrain loss_segm: 1.7577106303806547\ntrain loss_shape: 0.27007929160247873\ntrain loss_recon: 0.3765892745761932\ntrain unet DSC: [0.45540565252304077, 0.5107659697532654, 0.0001663322764215991, 0.00029320057365112007]\n\ntest loss: 1.7685648508560963\ntest loss_segm: 1.7325584888458252\ntest loss_shape: 0.24244475670349905\ntest loss_recon: 0.3358192142003622\ntest unet DSC: [0.49986138939857483, 0.5652557611465454, 0.0001758076687110588, 0.00027477688854560256]\nBest val loss: 1.7685648508560963\nTime: 57.71436619758606\n\n\nEpoch 8/250\n\ntrain loss: 1.7933984519560127\ntrain loss_segm: 1.7551543259922462\ntrain loss_shape: 0.27152930784829055\ntrain loss_recon: 0.3552882803391807\ntrain unet DSC: [0.459227979183197, 0.5153375267982483, 0.00016592786414548755, 0.00030940063879825175]\n\ntest loss: 1.7726411269261286\ntest loss_segm: 1.737267677600567\ntest loss_shape: 0.2500765793598615\ntest loss_recon: 0.328726793328921\ntest unet DSC: [0.4939322769641876, 0.5512691140174866, 0.00019036718003917485, 0.00027977890567854047]\nBest val loss: 1.7685648508560963\nTime: 57.800719022750854\n\n\nEpoch 9/250\n\ntrain loss: 1.7928728483900238\ntrain loss_segm: 1.7546078804173046\ntrain loss_shape: 0.27495941085905967\ntrain loss_recon: 0.35515380075460745\ntrain unet DSC: [0.46323439478874207, 0.5125949382781982, 0.0001708206400508061, 0.00029996290686540306]\n\ntest loss: 1.7674916707552397\ntest loss_segm: 1.7344428973320203\ntest loss_shape: 0.25154150296480227\ntest loss_recon: 0.3053335608580174\ntest unet DSC: [0.4968598186969757, 0.5584611892700195, 0.00018039417045656592, 0.00028327832114882767]\nBest val loss: 1.7674916707552397\nTime: 57.4490749835968\n\n\nEpoch 10/250\n\ntrain loss: 1.7866200087945672\ntrain loss_segm: 1.7523249893248836\ntrain loss_shape: 0.26719981139596505\ntrain loss_recon: 0.3162301516608347\ntrain unet DSC: [0.46597719192504883, 0.5172832608222961, 0.00016652955673635006, 0.0003071701794397086]\n\ntest loss: 1.792798250149458\ntest loss_segm: 1.7512443096209795\ntest loss_shape: 0.23458560842734116\ntest loss_recon: 0.39208077505612987\ntest unet DSC: [0.47135230898857117, 0.5153596997261047, 0.00018569848907645792, 0.00029924747650511563]\nBest val loss: 1.7674916707552397\nTime: 57.856324672698975\n\n\nEpoch 11/250\n\ntrain loss: 1.7716169576101666\ntrain loss_segm: 1.736792426320571\ntrain loss_shape: 0.27292047441005707\ntrain loss_recon: 0.32095331783536113\ntrain unet DSC: [0.5112214684486389, 0.5350342392921448, 0.00017606721667107195, 0.00031831779051572084]\n\ntest loss: 1.7455462186764448\ntest loss_segm: 1.7043418975976796\ntest loss_shape: 0.2509291309576768\ntest loss_recon: 0.3869503541634633\ntest unet DSC: [0.5844742059707642, 0.5944210290908813, 0.00019074617011938244, 0.0003153989091515541]\nBest val loss: 1.7455462186764448\nTime: 58.33006453514099\n\n\nEpoch 12/250\n\ntrain loss: 1.7260216919681695\ntrain loss_segm: 1.6879968469655966\ntrain loss_shape: 0.27558352637894545\ntrain loss_recon: 0.35269011359048796\ntrain unet DSC: [0.608027994632721, 0.6381070017814636, 0.00018099794397130609, 0.0003164031368214637]\n\ntest loss: 1.672415091441228\ntest loss_segm: 1.6376457306054921\ntest loss_shape: 0.2505295605231554\ntest loss_recon: 0.3226407567660014\ntest unet DSC: [0.7041311264038086, 0.7439416646957397, 0.00018839057884179056, 0.00028647889848798513]\nBest val loss: 1.672415091441228\nTime: 58.599018812179565\n\n\nEpoch 13/250\n\ntrain loss: 1.7061398331123063\ntrain loss_segm: 1.6697125963017911\ntrain loss_shape: 0.2687055834486515\ntrain loss_recon: 0.3374017681690711\ntrain unet DSC: [0.6421669125556946, 0.6763418912887573, 0.00017078820383176208, 0.0003064256743527949]\n\ntest loss: 1.6785958821956928\ntest loss_segm: 1.6434153960301325\ntest loss_shape: 0.23842773147118398\ntest loss_recon: 0.3279620011647542\ntest unet DSC: [0.6875618100166321, 0.7336729764938354, 0.000188440055353567, 0.000300941668683663]\nBest val loss: 1.672415091441228\nTime: 58.12710642814636\n\n\nEpoch 14/250\n\ntrain loss: 1.6933001890967163\ntrain loss_segm: 1.6594456924667842\ntrain loss_shape: 0.26332839125696617\ntrain loss_recon: 0.3122121965206122\ntrain unet DSC: [0.6612443327903748, 0.6960448026657104, 0.00018311667372472584, 0.00031819878495298326]\n\ntest loss: 1.6409059121058538\ntest loss_segm: 1.613801861420656\ntest loss_shape: 0.24178074262081048\ntest loss_recon: 0.24686242983891413\ntest unet DSC: [0.7495816349983215, 0.7893084287643433, 0.00018047174671664834, 0.0002865244459826499]\nBest val loss: 1.6409059121058538\nTime: 58.2352659702301\n\n\nEpoch 15/250\n\ntrain loss: 1.6845303524898578\ntrain loss_segm: 1.653119363362276\ntrain loss_shape: 0.2594469798893868\ntrain loss_recon: 0.288165225635601\ntrain unet DSC: [0.6762923002243042, 0.7053583860397339, 0.00017597789701540023, 0.000309031514916569]\n\ntest loss: 1.6395543691439507\ntest loss_segm: 1.6129248723005638\ntest loss_shape: 0.237489776351513\ntest loss_recon: 0.24254596539032766\ntest unet DSC: [0.749937891960144, 0.7913385033607483, 0.00017849626601673663, 0.0002989622298628092]\nBest val loss: 1.6395543691439507\nTime: 58.515686988830566\n\n\nEpoch 16/250\n\ntrain loss: 1.6838532090187073\ntrain loss_segm: 1.6529180150997789\ntrain loss_shape: 0.25870145461227323\ntrain loss_recon: 0.2834817847307724\ntrain unet DSC: [0.6766006350517273, 0.7042034268379211, 0.00017879574443213642, 0.0003199594502802938]\n\ntest loss: 1.635256012280782\ntest loss_segm: 1.6106586823096642\ntest loss_shape: 0.23577872147926918\ntest loss_recon: 0.22239535244611594\ntest unet DSC: [0.756397008895874, 0.7957321405410767, 0.00018580319010652602, 0.00030603056075051427]\nBest val loss: 1.635256012280782\nTime: 58.33902072906494\n\n\nEpoch 17/250\n\ntrain loss: 1.6784930923317052\ntrain loss_segm: 1.6485882793800741\ntrain loss_shape: 0.2531642453579963\ntrain loss_recon: 0.273731645998321\ntrain unet DSC: [0.6837659478187561, 0.7140714526176453, 0.0001736874255584553, 0.00031339761335402727]\n\ntest loss: 1.638113905221988\ntest loss_segm: 1.6146784592897465\ntest loss_shape: 0.22720315135442293\ntest loss_recon: 0.21163414667050043\ntest unet DSC: [0.7443680167198181, 0.7893701195716858, 0.00018282335076946765, 0.0002982844307553023]\nBest val loss: 1.635256012280782\nTime: 57.93332290649414\n\n\nEpoch 18/250\n\ntrain loss: 1.67380773719353\ntrain loss_segm: 1.645076620427868\ntrain loss_shape: 0.2491103791737858\ntrain loss_recon: 0.2624001369068894\ntrain unet DSC: [0.6899278163909912, 0.7206600308418274, 0.00018531864043325186, 0.00032387362443841994]\n\ntest loss: 1.6410989455687695\ntest loss_segm: 1.6173449357350667\ntest loss_shape: 0.22649942338466644\ntest loss_recon: 0.21489019548663726\ntest unet DSC: [0.7427054047584534, 0.7785704135894775, 0.00018076651031151414, 0.00030684968805871904]\nBest val loss: 1.635256012280782\nTime: 57.719276666641235\n\n\nEpoch 19/250\n\ntrain loss: 1.6725699524336224\ntrain loss_segm: 1.6447452253933195\ntrain loss_shape: 0.24585981336952764\ntrain loss_recon: 0.2536612834922875\ntrain unet DSC: [0.6928422451019287, 0.7172545790672302, 0.0001866418751887977, 0.00034002179745584726]\n\ntest loss: 1.6292919806945019\ntest loss_segm: 1.608333489833734\ntest loss_shape: 0.2221304816313279\ntest loss_recon: 0.18737182498742372\ntest unet DSC: [0.7589892745018005, 0.797156035900116, 0.0001876280439319089, 0.00030551684903912246]\nBest val loss: 1.6292919806945019\nTime: 57.633394956588745\n\n\nEpoch 20/250\n\ntrain loss: 1.6684025707124155\ntrain loss_segm: 1.6419015707848947\ntrain loss_shape: 0.24018269352897814\ntrain loss_recon: 0.2409917550939548\ntrain unet DSC: [0.6955338716506958, 0.7255699038505554, 0.00018579147581476718, 0.0003284997073933482]\n\ntest loss: 1.629145160699502\ntest loss_segm: 1.6074628921655507\ntest loss_shape: 0.21608636088860342\ntest loss_recon: 0.19521393321263483\ntest unet DSC: [0.753372848033905, 0.8040279746055603, 0.0001866428938228637, 0.0003050611703656614]\nBest val loss: 1.629145160699502\nTime: 57.692718267440796\n\n\nEpoch 21/250\n\ntrain loss: 1.6678299285188507\ntrain loss_segm: 1.641901459120497\ntrain loss_shape: 0.2357904958196833\ntrain loss_recon: 0.23570569963017596\ntrain unet DSC: [0.6967494487762451, 0.7225274443626404, 0.00018622307106852531, 0.0003338795795571059]\n\ntest loss: 1.6316479322237847\ntest loss_segm: 1.6102468814605322\ntest loss_shape: 0.21159654015149826\ntest loss_recon: 0.19285091967918935\ntest unet DSC: [0.7523370981216431, 0.7916361689567566, 0.0001872049324447289, 0.00032182270660996437]\nBest val loss: 1.629145160699502\nTime: 57.73044228553772\n\n\nEpoch 22/250\n\ntrain loss: 1.664428088483931\ntrain loss_segm: 1.639241480374638\ntrain loss_shape: 0.22985304439369636\ntrain loss_recon: 0.2288807349491723\ntrain unet DSC: [0.7005630135536194, 0.7280835509300232, 0.00019176866044290364, 0.00035752137773670256]\n\ntest loss: 1.6181020247630584\ntest loss_segm: 1.6002641121546428\ntest loss_shape: 0.2045172510238794\ntest loss_recon: 0.15792734080400223\ntest unet DSC: [0.772415280342102, 0.8116399645805359, 0.0001855226291809231, 0.0003180778003297746]\nBest val loss: 1.6181020247630584\nTime: 57.45123600959778\n\n\nEpoch 23/250\n\ntrain loss: 1.6642899882944324\ntrain loss_segm: 1.6395115527925612\ntrain loss_shape: 0.22501856088638306\ntrain loss_recon: 0.22528250996447816\ntrain unet DSC: [0.7007756233215332, 0.7260377407073975, 0.0001754222612362355, 0.00034420774318277836]\n\ntest loss: 1.6154321768344977\ntest loss_segm: 1.5980980732502081\ntest loss_shape: 0.19991067968882048\ntest loss_recon: 0.15334990467780676\ntest unet DSC: [0.7740722298622131, 0.8150616884231567, 0.00017618972924537957, 0.0003170184791088104]\nBest val loss: 1.6154321768344977\nTime: 57.35015892982483\n\n\nEpoch 24/250\n\ntrain loss: 1.661284949960588\ntrain loss_segm: 1.6378451585769653\ntrain loss_shape: 0.22065370878841303\ntrain loss_recon: 0.21233254246696642\ntrain unet DSC: [0.7017403841018677, 0.7298417687416077, 0.0001825068611651659, 0.00035048616700805724]\n\ntest loss: 1.6208713665986672\ntest loss_segm: 1.6033879151711097\ntest loss_shape: 0.19703895655962136\ntest loss_recon: 0.15513067874006736\ntest unet DSC: [0.7663435339927673, 0.8008381128311157, 0.00018008382176049054, 0.00033921501017175615]\nBest val loss: 1.6154321768344977\nTime: 58.04450750350952\n\n\nEpoch 25/250\n\ntrain loss: 1.659024035628838\ntrain loss_segm: 1.6363048312030262\ntrain loss_shape: 0.21445851150570036\ntrain loss_recon: 0.20574620318940923\ntrain unet DSC: [0.7040238976478577, 0.7322254180908203, 0.0001793777191778645, 0.0003542561025824398]\n\ntest loss: 1.6116792085843208\ntest loss_segm: 1.595184176396101\ntest loss_shape: 0.18892816167611343\ntest loss_recon: 0.14605743313829103\ntest unet DSC: [0.7811204195022583, 0.8170031309127808, 0.0001727504568407312, 0.0003334422945044935]\nBest val loss: 1.6116792085843208\nTime: 58.38935351371765\n\n\nEpoch 26/250\n\ntrain loss: 1.65875056800963\ntrain loss_segm: 1.6365934633001495\ntrain loss_shape: 0.20988349908892112\ntrain loss_recon: 0.20058267943183833\ntrain unet DSC: [0.7033625245094299, 0.7298879623413086, 0.00018115089915227145, 0.0003558571625035256]\n\ntest loss: 1.6183789173762004\ntest loss_segm: 1.6028034197978485\ntest loss_shape: 0.18579912720582423\ntest loss_recon: 0.13717507389493477\ntest unet DSC: [0.7676299214363098, 0.7996112108230591, 0.00018024601740762591, 0.0003412947990000248]\nBest val loss: 1.6116792085843208\nTime: 58.37794208526611\n\n\nEpoch 27/250\n\ntrain loss: 1.6563571397262284\ntrain loss_segm: 1.6349699467043333\ntrain loss_shape: 0.2027250993289525\ntrain loss_recon: 0.19359941101526912\ntrain unet DSC: [0.7047750949859619, 0.7332898378372192, 0.00017345172818750143, 0.0003509972302708775]\n\ntest loss: 1.606867573200128\ntest loss_segm: 1.5929636771862323\ntest loss_shape: 0.18398040036360422\ntest loss_recon: 0.12064089024296173\ntest unet DSC: [0.7833986282348633, 0.8189159035682678, 0.00017484645650256425, 0.0003370190679561347]\nBest val loss: 1.606867573200128\nTime: 64.3955934047699\n\n\nEpoch 28/250\n\ntrain loss: 1.655426999436149\ntrain loss_segm: 1.6345528160469442\ntrain loss_shape: 0.19770075220473204\ntrain loss_recon: 0.1889717525130586\ntrain unet DSC: [0.7057627439498901, 0.7317166924476624, 0.00017661995661910623, 0.0003489326627459377]\n\ntest loss: 1.6088819198119335\ntest loss_segm: 1.5951521152105086\ntest loss_shape: 0.17214645789219782\ntest loss_recon: 0.12008330923242447\ntest unet DSC: [0.7748755812644958, 0.8158132433891296, 0.00017762347124516964, 0.0003418338019400835]\nBest val loss: 1.606867573200128\nTime: 57.89079260826111\n\n\nEpoch 29/250\n\ntrain loss: 1.6510636202896698\ntrain loss_segm: 1.6308558620984042\ntrain loss_shape: 0.18864206385008897\ntrain loss_recon: 0.18321335808480088\ntrain unet DSC: [0.7094866037368774, 0.7382394075393677, 0.00017790736455935985, 0.0003663924289867282]\n\ntest loss: 1.6079907295031426\ntest loss_segm: 1.594130033101791\ntest loss_shape: 0.16532252327753946\ntest loss_recon: 0.12207474912970494\ntest unet DSC: [0.7740774750709534, 0.8159966468811035, 0.00017823583038989455, 0.00034589655115269125]\nBest val loss: 1.606867573200128\nTime: 57.71790790557861\n\n\nEpoch 30/250\n\ntrain loss: 1.6530122689053983\ntrain loss_segm: 1.6329987237725077\ntrain loss_shape: 0.18155504668815226\ntrain loss_recon: 0.18197995096539396\ntrain unet DSC: [0.7055605053901672, 0.7323166131973267, 0.0001703697635093704, 0.0003493498370517045]\n\ntest loss: 1.603095803505335\ntest loss_segm: 1.590704349371103\ntest loss_shape: 0.15640774006262803\ntest loss_recon: 0.10827387429964848\ntest unet DSC: [0.7813246250152588, 0.8190284371376038, 0.00017184694297611713, 0.0003413854574318975]\nBest val loss: 1.603095803505335\nTime: 58.44851803779602\n\n\nEpoch 31/250\n\ntrain loss: 1.6463674729383444\ntrain loss_segm: 1.6273273131515407\ntrain loss_shape: 0.1695282786047157\ntrain loss_recon: 0.17344878750699985\ntrain unet DSC: [0.7163707613945007, 0.7379140853881836, 0.00018487525812815875, 0.0003668941499199718]\n\ntest loss: 1.6014436666782086\ntest loss_segm: 1.5879457424848507\ntest loss_shape: 0.14643698510451195\ntest loss_recon: 0.12033567940577483\ntest unet DSC: [0.779593288898468, 0.8257951140403748, 0.00017906360153574497, 0.00034445253550074995]\nBest val loss: 1.6014436666782086\nTime: 57.59819054603577\n\n\nEpoch 32/250\n\ntrain loss: 1.6465847711019879\ntrain loss_segm: 1.6278298285943043\ntrain loss_shape: 0.16101526134187663\ntrain loss_recon: 0.1714479072561747\ntrain unet DSC: [0.7131252288818359, 0.7343823313713074, 0.00018108579388353974, 0.00037138847983442247]\n\ntest loss: 1.6062554793480115\ntest loss_segm: 1.5948780255439954\ntest loss_shape: 0.14179139450574532\ntest loss_recon: 0.09959540764490764\ntest unet DSC: [0.7718234062194824, 0.7998342514038086, 0.00018248974811285734, 0.0003501696919556707]\nBest val loss: 1.6014436666782086\nTime: 58.55786371231079\n\n\nEpoch 33/250\n\ntrain loss: 1.642388653906086\ntrain loss_segm: 1.6244066863120357\ntrain loss_shape: 0.14961870844605601\ntrain loss_recon: 0.16485786284732667\ntrain unet DSC: [0.7165337800979614, 0.7375319600105286, 0.0001874377194326371, 0.0003776105586439371]\n\ntest loss: 1.5923685782994978\ntest loss_segm: 1.5807490073717558\ntest loss_shape: 0.1239445413916539\ntest loss_recon: 0.10380135581661494\ntest unet DSC: [0.7893431782722473, 0.8264579772949219, 0.00018330365128349513, 0.00035066381678916514]\nBest val loss: 1.5923685782994978\nTime: 57.93191647529602\n\n\nEpoch 34/250\n\ntrain loss: 1.6395403946502298\ntrain loss_segm: 1.621622859677182\ntrain loss_shape: 0.13587319506685944\ntrain loss_recon: 0.1655880508213481\ntrain unet DSC: [0.7176183462142944, 0.7379428744316101, 0.0001827865489758551, 0.00039218919118866324]\n\ntest loss: 1.592824563001975\ntest loss_segm: 1.5804971364828257\ntest loss_shape: 0.11172027255480106\ntest loss_recon: 0.11210237080470109\ntest unet DSC: [0.7884435653686523, 0.8176087737083435, 0.0001845096703618765, 0.0003617583424784243]\nBest val loss: 1.5923685782994978\nTime: 59.23015880584717\n\n\nEpoch 35/250\n\ntrain loss: 1.634273177460779\ntrain loss_segm: 1.6171994488450545\ntrain loss_shape: 0.12530473784743984\ntrain loss_recon: 0.15820681926193117\ntrain unet DSC: [0.7187981009483337, 0.7429324984550476, 0.0001935494365170598, 0.00038697681156918406]\n\ntest loss: 1.590343652627407\ntest loss_segm: 1.57964433156527\ntest loss_shape: 0.10594899952411652\ntest loss_recon: 0.09639828164990132\ntest unet DSC: [0.7823720574378967, 0.8143225908279419, 0.0001949928409885615, 0.00036575685953721404]\nBest val loss: 1.590343652627407\nTime: 57.659621715545654\n\n\nEpoch 36/250\n\ntrain loss: 1.6281646580635747\ntrain loss_segm: 1.6117568566829343\ntrain loss_shape: 0.11527648563437824\ntrain loss_recon: 0.1525504329253601\ntrain unet DSC: [0.7233632206916809, 0.7440212368965149, 0.000196458087884821, 0.0003902262542396784]\n\ntest loss: 1.5780293116202722\ntest loss_segm: 1.5678458305505605\ntest loss_shape: 0.09369805378791614\ntest loss_recon: 0.0924650567273299\ntest unet DSC: [0.7949625849723816, 0.8273316621780396, 0.0001893725129775703, 0.0003513975243549794]\nBest val loss: 1.5780293116202722\nTime: 57.8813362121582\n\n\nEpoch 37/250\n\ntrain loss: 1.6238251630263993\ntrain loss_segm: 1.6076715731922584\ntrain loss_shape: 0.10741464086348497\ntrain loss_recon: 0.15079442185314396\ntrain unet DSC: [0.720657467842102, 0.7395763993263245, 0.00021021798602305353, 0.00039691448910161853]\n\ntest loss: 1.562679037069663\ntest loss_segm: 1.5532211340390718\ntest loss_shape: 0.08438944338988034\ntest loss_recon: 0.08614015741608082\ntest unet DSC: [0.80980384349823, 0.8381436467170715, 0.0002065232110908255, 0.0003533087146934122]\nBest val loss: 1.562679037069663\nTime: 58.23162817955017\n\n\nEpoch 38/250\n\ntrain loss: 1.6144176540495474\ntrain loss_segm: 1.5988835239712196\ntrain loss_shape: 0.10075392533989647\ntrain loss_recon: 0.14526590195637715\ntrain unet DSC: [0.7229065895080566, 0.7424930334091187, 0.00022336476831696928, 0.00040014833211898804]\n\ntest loss: 1.5561658755326881\ntest loss_segm: 1.5454021417177641\ntest loss_shape: 0.07859489102012072\ntest loss_recon: 0.09977794295320144\ntest unet DSC: [0.8004290461540222, 0.8325850367546082, 0.00023361132480204105, 0.0003789672045968473]\nBest val loss: 1.5561658755326881\nTime: 57.527562618255615\n\n\nEpoch 39/250\n\ntrain loss: 1.6017873219296903\ntrain loss_segm: 1.585508981837502\ntrain loss_shape: 0.09352976104975501\ntrain loss_recon: 0.15343038933469524\ntrain unet DSC: [0.722615122795105, 0.74085932970047, 0.00026107337907887995, 0.00045031291665509343]\n\ntest loss: 1.5379234063319671\ntest loss_segm: 1.5272586131707215\ntest loss_shape: 0.07473846735098423\ntest loss_recon: 0.0991741676743214\ntest unet DSC: [0.7917693853378296, 0.8291722536087036, 0.00027191266417503357, 0.00042555315303616226]\nBest val loss: 1.5379234063319671\nTime: 58.12886357307434\n\n\nEpoch 40/250\n\ntrain loss: 1.5740436805954463\ntrain loss_segm: 1.5557567296148855\ntrain loss_shape: 0.08231543692984158\ntrain loss_recon: 0.17463795189993292\ntrain unet DSC: [0.7216199040412903, 0.7303901314735413, 0.0003602847864385694, 0.0007958061178214848]\n\ntest loss: 1.497278210444328\ntest loss_segm: 1.4831987222035725\ntest loss_shape: 0.06023641255421516\ntest loss_recon: 0.1347712168517785\ntest unet DSC: [0.7840076684951782, 0.7932671904563904, 0.0002857595100067556, 0.0011755884625017643]\nBest val loss: 1.497278210444328\nTime: 58.26588439941406\n\n\nEpoch 41/250\n\ntrain loss: 1.4565793509724774\ntrain loss_segm: 1.4376959061320824\ntrain loss_shape: 0.05080497459378801\ntrain loss_recon: 0.18375395643937437\ntrain unet DSC: [0.6313650608062744, 0.5761539340019226, 0.017207792028784752, 0.05420451983809471]\n\ntest loss: 1.3380229503680499\ntest loss_segm: 1.3234147567015428\ntest loss_shape: 0.03828021291738901\ntest loss_recon: 0.14225403868999237\ntest unet DSC: [0.5300887227058411, 0.5628769993782043, 0.04018481820821762, 0.11700347810983658]\nBest val loss: 1.3380229503680499\nTime: 57.898396015167236\n\n\nEpoch 42/250\n\ntrain loss: 1.3373160422602786\ntrain loss_segm: 1.319554657121248\ntrain loss_shape: 0.03954068841294774\ntrain loss_recon: 0.1736598518453067\ntrain unet DSC: [0.4878639876842499, 0.5030956268310547, 0.04814160615205765, 0.14629143476486206]\n\ntest loss: 1.2164936310205705\ntest loss_segm: 1.203975368768741\ntest loss_shape: 0.027842413299740888\ntest loss_recon: 0.12239833443592756\ntest unet DSC: [0.45941025018692017, 0.5147022604942322, 0.048731058835983276, 0.22607949376106262]\nBest val loss: 1.2164936310205705\nTime: 58.057262659072876\n\n\nEpoch 43/250\n\ntrain loss: 1.217646963234189\ntrain loss_segm: 1.1993614140945146\ntrain loss_shape: 0.03933428475468219\ntrain loss_recon: 0.1789220899720735\ntrain unet DSC: [0.4512949585914612, 0.49416476488113403, 0.05363323166966438, 0.4626987874507904]\n\ntest loss: 1.066313963669997\ntest loss_segm: 1.0529658855536046\ntest loss_shape: 0.027091296532979377\ntest loss_recon: 0.1307715158431958\ntest unet DSC: [0.45024773478507996, 0.5104401707649231, 0.05156403407454491, 0.6420484185218811]\nBest val loss: 1.066313963669997\nTime: 59.07942771911621\n\n\nEpoch 44/250\n\ntrain loss: 1.1354200285446794\ntrain loss_segm: 1.1171263607242439\ntrain loss_shape: 0.0382758249422606\ntrain loss_recon: 0.17910914575751824\ntrain unet DSC: [0.45168256759643555, 0.49071386456489563, 0.053160831332206726, 0.5866536498069763]\n\ntest loss: 1.0086344159566438\ntest loss_segm: 0.9964077594952706\ntest loss_shape: 0.02566604679211592\ntest loss_recon: 0.11970002003587209\ntest unet DSC: [0.45459580421447754, 0.5198401212692261, 0.06002208963036537, 0.6697350144386292]\nBest val loss: 1.0086344159566438\nTime: 58.506171226501465\n\n\nEpoch 45/250\n\ntrain loss: 1.0924922635283651\ntrain loss_segm: 1.0745851789848715\ntrain loss_shape: 0.03728555687526359\ntrain loss_recon: 0.17534232047634035\ntrain unet DSC: [0.4518273174762726, 0.4955108165740967, 0.05787145718932152, 0.6055439114570618]\n\ntest loss: 0.9680874439386221\ntest loss_segm: 0.9561981956164042\ntest loss_shape: 0.02474626851005432\ntest loss_recon: 0.11641785960931045\ntest unet DSC: [0.4642289876937866, 0.5272245407104492, 0.06170886754989624, 0.6783026456832886]\nBest val loss: 0.9680874439386221\nTime: 58.79668664932251\n\n\nEpoch 46/250\n\ntrain loss: 1.0615709363659727\ntrain loss_segm: 1.0439677317685718\ntrain loss_shape: 0.036174343509858924\ntrain loss_recon: 0.17241460537608666\ntrain unet DSC: [0.45392531156539917, 0.499932199716568, 0.0619119256734848, 0.6110886931419373]\n\ntest loss: 0.9405548939338098\ntest loss_segm: 0.928961306046217\ntest loss_shape: 0.026675605644973423\ntest loss_recon: 0.11326839020236945\ntest unet DSC: [0.4773673117160797, 0.5334194898605347, 0.06714872270822525, 0.6853347420692444]\nBest val loss: 0.9405548939338098\nTime: 57.79511737823486\n\n\nEpoch 47/250\n\ntrain loss: 1.0370113129102732\ntrain loss_segm: 1.0193279589278788\ntrain loss_shape: 0.03631850574872916\ntrain loss_recon: 0.17320176902451093\ntrain unet DSC: [0.4591260850429535, 0.4991593062877655, 0.06491522490978241, 0.6198490262031555]\n\ntest loss: 0.9156454129096789\ntest loss_segm: 0.9040838373013032\ntest loss_shape: 0.025270936604684744\ntest loss_recon: 0.11308873845980717\ntest unet DSC: [0.47284629940986633, 0.5355056524276733, 0.06973955035209656, 0.686329185962677]\nBest val loss: 0.9156454129096789\nTime: 57.609559059143066\n\n\nEpoch 48/250\n\ntrain loss: 1.0120936986766285\ntrain loss_segm: 0.994557497244847\ntrain loss_shape: 0.035685164936452726\ntrain loss_recon: 0.17179352026196976\ntrain unet DSC: [0.46001771092414856, 0.5032567977905273, 0.06840252131223679, 0.6237221360206604]\n\ntest loss: 0.9231489912057534\ntest loss_segm: 0.9104382991790771\ntest loss_shape: 0.03127071624382948\ntest loss_recon: 0.12397984463052872\ntest unet DSC: [0.4760421812534332, 0.5523253679275513, 0.07721146941184998, 0.6575102210044861]\nBest val loss: 0.9156454129096789\nTime: 57.606316804885864\n\n\nEpoch 49/250\n\ntrain loss: 0.9903937193411815\ntrain loss_segm: 0.9730955958366394\ntrain loss_shape: 0.03549458670040852\ntrain loss_recon: 0.16943185021982918\ntrain unet DSC: [0.46294528245925903, 0.5051881074905396, 0.07066696137189865, 0.6335147023200989]\n\ntest loss: 0.8732039469939011\ntest loss_segm: 0.8620914144393725\ntest loss_shape: 0.02351969248877886\ntest loss_recon: 0.10877333524135444\ntest unet DSC: [0.4783582389354706, 0.5358984470367432, 0.07172069698572159, 0.6954164505004883]\nBest val loss: 0.8732039469939011\nTime: 57.542044162750244\n\n\nEpoch 50/250\n\ntrain loss: 0.9739952042132993\ntrain loss_segm: 0.9570596286013157\ntrain loss_shape: 0.03462714536846438\ntrain loss_recon: 0.16589310171106195\ntrain unet DSC: [0.47010281682014465, 0.5093907117843628, 0.07151179760694504, 0.6309987306594849]\n\ntest loss: 0.8549149005840986\ntest loss_segm: 0.8442866832782061\ntest loss_shape: 0.02171000150533823\ntest loss_recon: 0.10411124905714622\ntest unet DSC: [0.4842062294483185, 0.5371764302253723, 0.07112419605255127, 0.6858629584312439]\nBest val loss: 0.8549149005840986\nTime: 57.68803930282593\n\n\nEpoch 51/250\n\ntrain loss: 0.9593429203274884\ntrain loss_segm: 0.9424925288067588\ntrain loss_shape: 0.03455464288450872\ntrain loss_recon: 0.16504848088267482\ntrain unet DSC: [0.47189292311668396, 0.5111207365989685, 0.07381348311901093, 0.6379125714302063]\n\ntest loss: 0.8284862362421476\ntest loss_segm: 0.8184525569279989\ntest loss_shape: 0.020848554845612783\ntest loss_recon: 0.098251962604431\ntest unet DSC: [0.5041290521621704, 0.5543015599250793, 0.0773225724697113, 0.6993321776390076]\nBest val loss: 0.8284862362421476\nTime: 57.720144271850586\n\n\nEpoch 52/250\n\ntrain loss: 0.9517798563347587\ntrain loss_segm: 0.9346008817606335\ntrain loss_shape: 0.034682020981194855\ntrain loss_recon: 0.16832152678619458\ntrain unet DSC: [0.4766511917114258, 0.508979320526123, 0.07467491179704666, 0.6391742825508118]\n\ntest loss: 0.8272233773500491\ntest loss_segm: 0.8166335301521497\ntest loss_shape: 0.023032323600581057\ntest loss_recon: 0.10359526081727101\ntest unet DSC: [0.5071384906768799, 0.5510385632514954, 0.07766597718000412, 0.7017130851745605]\nBest val loss: 0.8272233773500491\nTime: 57.65672731399536\n\n\nEpoch 53/250\n\ntrain loss: 0.9409703778315194\ntrain loss_segm: 0.9240194891072526\ntrain loss_shape: 0.034410128888638716\ntrain loss_recon: 0.16606792376105545\ntrain unet DSC: [0.48241621255874634, 0.5104953646659851, 0.07648059725761414, 0.6421295404434204]\n\ntest loss: 0.8099168034700247\ntest loss_segm: 0.799639995281513\ntest loss_shape: 0.021473299998503465\ntest loss_recon: 0.10062079494580245\ntest unet DSC: [0.513554036617279, 0.5555424094200134, 0.07932447642087936, 0.7021412253379822]\nBest val loss: 0.8099168034700247\nTime: 57.42409110069275\n\n\nEpoch 54/250\n\ntrain loss: 0.92695156871518\ntrain loss_segm: 0.910139539573766\ntrain loss_shape: 0.03398779296327995\ntrain loss_recon: 0.16472153396263153\ntrain unet DSC: [0.4892381727695465, 0.5169965624809265, 0.07518681138753891, 0.6436975598335266]\n\ntest loss: 0.8148749210895636\ntest loss_segm: 0.8041862769004626\ntest loss_shape: 0.023171217801670235\ntest loss_recon: 0.10456929890773235\ntest unet DSC: [0.5174564123153687, 0.5529419183731079, 0.07794981449842453, 0.6883957982063293]\nBest val loss: 0.8099168034700247\nTime: 57.30259704589844\n\n\nEpoch 55/250\n\ntrain loss: 0.9210425094713138\ntrain loss_segm: 0.9042285627956632\ntrain loss_shape: 0.034104776993110965\ntrain loss_recon: 0.16472896226222003\ntrain unet DSC: [0.49551352858543396, 0.5166004300117493, 0.077614925801754, 0.649944543838501]\n\ntest loss: 0.8096575202086033\ntest loss_segm: 0.7985480473591731\ntest loss_shape: 0.022165405444609813\ntest loss_recon: 0.10887819624099976\ntest unet DSC: [0.5091218948364258, 0.542264997959137, 0.07215505838394165, 0.6981818079948425]\nBest val loss: 0.8096575202086033\nTime: 58.19292211532593\n\n\nEpoch 56/250\n\ntrain loss: 0.9067733687690541\ntrain loss_segm: 0.890272483795504\ntrain loss_shape: 0.03338076590407121\ntrain loss_recon: 0.16167075104445597\ntrain unet DSC: [0.5112833976745605, 0.5182874798774719, 0.07648378610610962, 0.6462031602859497]\n\ntest loss: 0.7952396136063796\ntest loss_segm: 0.784655939310025\ntest loss_shape: 0.02362702013208316\ntest loss_recon: 0.103474005483664\ntest unet DSC: [0.5318580269813538, 0.554580569267273, 0.07832899689674377, 0.7067329287528992]\nBest val loss: 0.7952396136063796\nTime: 57.73029279708862\n\n\nEpoch 57/250\n\ntrain loss: 0.9062045752247677\ntrain loss_segm: 0.8896689886533762\ntrain loss_shape: 0.0336834055760616\ntrain loss_recon: 0.16198752424384974\ntrain unet DSC: [0.5253567695617676, 0.5199486613273621, 0.07614485919475555, 0.6410524845123291]\n\ntest loss: 0.7743873382226015\ntest loss_segm: 0.764512314246251\ntest loss_shape: 0.020745194015594628\ntest loss_recon: 0.09667577288853817\ntest unet DSC: [0.5583474636077881, 0.5667071342468262, 0.07643009722232819, 0.7045626640319824]\nBest val loss: 0.7743873382226015\nTime: 57.26484203338623\n\n\nEpoch 58/250\n\ntrain loss: 0.895271869022635\ntrain loss_segm: 0.8787260327158095\ntrain loss_shape: 0.033486339452240284\ntrain loss_recon: 0.16210974187036103\ntrain unet DSC: [0.548172116279602, 0.523194432258606, 0.07847714424133301, 0.6427512168884277]\n\ntest loss: 0.7719019468014057\ntest loss_segm: 0.7617221765029125\ntest loss_shape: 0.020950472770402066\ntest loss_recon: 0.099702664770377\ntest unet DSC: [0.5838043093681335, 0.5612990260124207, 0.07462476193904877, 0.7046586275100708]\nBest val loss: 0.7719019468014057\nTime: 57.79285717010498\n\n\nEpoch 59/250\n\ntrain loss: 0.8879324307170096\ntrain loss_segm: 0.8709812579275686\ntrain loss_shape: 0.033935025853069525\ntrain loss_recon: 0.16611823618789262\ntrain unet DSC: [0.6226049661636353, 0.5271956920623779, 0.07724334299564362, 0.6405359506607056]\n\ntest loss: 0.7478090570523188\ntest loss_segm: 0.7367521753677955\ntest loss_shape: 0.02268619706424383\ntest loss_recon: 0.10830024839975895\ntest unet DSC: [0.772789716720581, 0.5706850290298462, 0.07906223833560944, 0.7020430564880371]\nBest val loss: 0.7478090570523188\nTime: 57.42411422729492\n\n\nEpoch 60/250\n\ntrain loss: 0.867139830619474\ntrain loss_segm: 0.8497549163389809\ntrain loss_shape: 0.034052423083612435\ntrain loss_recon: 0.17044392067797576\ntrain unet DSC: [0.7025547027587891, 0.5377381443977356, 0.07846881449222565, 0.6359540820121765]\n\ntest loss: 0.7217404979925889\ntest loss_segm: 0.710889431146475\ntest loss_shape: 0.021353651769459248\ntest loss_recon: 0.1063752779020713\ntest unet DSC: [0.7966769337654114, 0.5882336497306824, 0.07749457657337189, 0.7067845463752747]\nBest val loss: 0.7217404979925889\nTime: 57.894168853759766\n\n\nEpoch 61/250\n\ntrain loss: 0.8488262122190451\ntrain loss_segm: 0.8314835199072391\ntrain loss_shape: 0.03415134887482169\ntrain loss_recon: 0.17001178087312965\ntrain unet DSC: [0.7100231051445007, 0.5663129687309265, 0.0801064595580101, 0.6415179967880249]\n\ntest loss: 0.7027438298249856\ntest loss_segm: 0.6917438782178439\ntest loss_shape: 0.021144344662435543\ntest loss_recon: 0.10788506651536012\ntest unet DSC: [0.8034319877624512, 0.6357703804969788, 0.07824049890041351, 0.7112153172492981]\nBest val loss: 0.7027438298249856\nTime: 57.792126417160034\n\n\nEpoch 62/250\n\ntrain loss: 0.8362087064151522\ntrain loss_segm: 0.8184513921224619\ntrain loss_shape: 0.034296001001155076\ntrain loss_recon: 0.17414357494327087\ntrain unet DSC: [0.7151630520820618, 0.620750904083252, 0.07920502126216888, 0.6431133151054382]\n\ntest loss: 0.7150837503946744\ntest loss_segm: 0.7028221793663807\ntest loss_shape: 0.026451358834329326\ntest loss_recon: 0.1199706420302391\ntest unet DSC: [0.7803515195846558, 0.6833063960075378, 0.07934155315160751, 0.7000795006752014]\nBest val loss: 0.7027438298249856\nTime: 57.37683963775635\n\n\nEpoch 63/250\n\ntrain loss: 0.8198963939389096\ntrain loss_segm: 0.8021492641183394\ntrain loss_shape: 0.033911640897298916\ntrain loss_recon: 0.174080142209047\ntrain unet DSC: [0.7166721224784851, 0.6453603506088257, 0.08033975213766098, 0.6495271325111389]\n\ntest loss: 0.6778944791891636\ntest loss_segm: 0.6663867617264773\ntest loss_shape: 0.021572687472097386\ntest loss_recon: 0.11291994889959311\ntest unet DSC: [0.8059187531471252, 0.7025304436683655, 0.07595915347337723, 0.7059836387634277]\nBest val loss: 0.6778944791891636\nTime: 57.6779670715332\n\n\nEpoch 64/250\n\ntrain loss: 0.8118652731557435\ntrain loss_segm: 0.7940179483045505\ntrain loss_shape: 0.034033984287724466\ntrain loss_recon: 0.1750698747891414\ntrain unet DSC: [0.7206767797470093, 0.6532561182975769, 0.07945015281438828, 0.6452943086624146]\n\ntest loss: 0.6733833643106314\ntest loss_segm: 0.6618383205853976\ntest loss_shape: 0.021799093685471095\ntest loss_recon: 0.11327056319285662\ntest unet DSC: [0.7937543988227844, 0.7197765707969666, 0.07652577012777328, 0.7066613435745239]\nBest val loss: 0.6733833643106314\nTime: 58.987220287323\n\n\nEpoch 65/250\n\ntrain loss: 0.8070323139051848\ntrain loss_segm: 0.7891710200641728\ntrain loss_shape: 0.03417540835570308\ntrain loss_recon: 0.17519540273690526\ntrain unet DSC: [0.7184496521949768, 0.6583225131034851, 0.0787288174033165, 0.645365297794342]\n\ntest loss: 0.6587105072461642\ntest loss_segm: 0.6475158578310257\ntest loss_shape: 0.02152304055216985\ntest loss_recon: 0.10979415285281646\ntest unet DSC: [0.7966378331184387, 0.7297086715698242, 0.08059798181056976, 0.717514157295227]\nBest val loss: 0.6587105072461642\nTime: 57.27738547325134\n\n\nEpoch 66/250\n\ntrain loss: 0.794976387220093\ntrain loss_segm: 0.7774352850038794\ntrain loss_shape: 0.03406056577834902\ntrain loss_recon: 0.17200498952518536\ntrain unet DSC: [0.7250977754592896, 0.6599387526512146, 0.08094633370637894, 0.6422152519226074]\n\ntest loss: 0.6632639016860571\ntest loss_segm: 0.6513448143616701\ntest loss_shape: 0.02178452514971678\ntest loss_recon: 0.11701244192245679\ntest unet DSC: [0.7947285771369934, 0.7226048707962036, 0.07639341801404953, 0.7063195705413818]\nBest val loss: 0.6587105072461642\nTime: 57.4974148273468\n\n\nEpoch 67/250\n\ntrain loss: 0.7918409229833868\ntrain loss_segm: 0.7740806824044336\ntrain loss_shape: 0.03401951911517336\ntrain loss_recon: 0.1742004803087138\ntrain unet DSC: [0.7267524600028992, 0.6614762544631958, 0.08153125643730164, 0.6430383324623108]\n\ntest loss: 0.6331976537521069\ntest loss_segm: 0.6226779367679205\ntest loss_shape: 0.019990790277146377\ntest loss_recon: 0.10319811946306473\ntest unet DSC: [0.8150215148925781, 0.7429367303848267, 0.08410152047872543, 0.7248365879058838]\nBest val loss: 0.6331976537521069\nTime: 57.21551012992859\n\n\nEpoch 68/250\n\ntrain loss: 0.7877619870101349\ntrain loss_segm: 0.7700782025916667\ntrain loss_shape: 0.03394057941210421\ntrain loss_recon: 0.1734438245243664\ntrain unet DSC: [0.7250760197639465, 0.6638802886009216, 0.08197248727083206, 0.6445080041885376]\n\ntest loss: 0.6500949974243457\ntest loss_segm: 0.6387393298821572\ntest loss_shape: 0.021104628196320474\ntest loss_recon: 0.11144626675507961\ntest unet DSC: [0.8063347935676575, 0.7200616002082825, 0.08073141425848007, 0.7000548839569092]\nBest val loss: 0.6331976537521069\nTime: 57.93431854248047\n\n\nEpoch 69/250\n\ntrain loss: 0.7820573898055886\ntrain loss_segm: 0.7645733065997498\ntrain loss_shape: 0.03367892248390973\ntrain loss_recon: 0.1714729451680485\ntrain unet DSC: [0.7279041409492493, 0.6637637615203857, 0.08164893835783005, 0.6460434198379517]\n\ntest loss: 0.6505384949537424\ntest loss_segm: 0.638835187141712\ntest loss_shape: 0.0229729342346008\ntest loss_recon: 0.11473576304240105\ntest unet DSC: [0.7996740341186523, 0.7239924073219299, 0.07950104773044586, 0.7117842435836792]\nBest val loss: 0.6331976537521069\nTime: 58.017722368240356\n\n\nEpoch 70/250\n\ntrain loss: 0.779546385324454\ntrain loss_segm: 0.7618154509912564\ntrain loss_shape: 0.03355978062682891\ntrain loss_recon: 0.17395334325353556\ntrain unet DSC: [0.7265834808349609, 0.662947416305542, 0.08232592046260834, 0.6481400728225708]\n\ntest loss: 0.6405392495485452\ntest loss_segm: 0.6288911172976861\ntest loss_shape: 0.021342991994550593\ntest loss_recon: 0.11434705202014019\ntest unet DSC: [0.8019303679466248, 0.7307901978492737, 0.08019151538610458, 0.7115667462348938]\nBest val loss: 0.6331976537521069\nTime: 57.740713357925415\n\n\nEpoch 71/250\n\ntrain loss: 0.7757549365110035\ntrain loss_segm: 0.7580454851253123\ntrain loss_shape: 0.03363517741377972\ntrain loss_recon: 0.1737309914035133\ntrain unet DSC: [0.7283887267112732, 0.6616998314857483, 0.0835137590765953, 0.651692807674408]\n\ntest loss: 0.6405127682746985\ntest loss_segm: 0.6290709582658914\ntest loss_shape: 0.021440805198672492\ntest loss_recon: 0.11227398012311031\ntest unet DSC: [0.7983696460723877, 0.7275672554969788, 0.08002708107233047, 0.7085666060447693]\nBest val loss: 0.6331976537521069\nTime: 57.4032416343689\n\n\nEpoch 72/250\n\ntrain loss: 0.7780576555789271\ntrain loss_segm: 0.7601335388195666\ntrain loss_shape: 0.03369188329935828\ntrain loss_recon: 0.17587201429318777\ntrain unet DSC: [0.7236039042472839, 0.6627094149589539, 0.08234450221061707, 0.6518166065216064]\n\ntest loss: 0.6290026773244907\ntest loss_segm: 0.6178446358595139\ntest loss_shape: 0.02164228917218936\ntest loss_recon: 0.1094161660816425\ntest unet DSC: [0.80782151222229, 0.7391203045845032, 0.0833355262875557, 0.7167456150054932]\nBest val loss: 0.6290026773244907\nTime: 58.01175856590271\n\n\nEpoch 73/250\n\ntrain loss: 0.7670572830151908\ntrain loss_segm: 0.7496111453333988\ntrain loss_shape: 0.033284664672764044\ntrain loss_recon: 0.17113290661120717\ntrain unet DSC: [0.7299677729606628, 0.6682258248329163, 0.08255062997341156, 0.6513472199440002]\n\ntest loss: 0.6236473520596822\ntest loss_segm: 0.6126867578579829\ntest loss_shape: 0.020548584369512703\ntest loss_recon: 0.10755112308722276\ntest unet DSC: [0.8128505349159241, 0.7364352941513062, 0.08186973631381989, 0.7084270119667053]\nBest val loss: 0.6236473520596822\nTime: 57.74778962135315\n\n\nEpoch 74/250\n\ntrain loss: 0.7670134419127356\ntrain loss_segm: 0.7495158559913877\ntrain loss_shape: 0.03335379652348878\ntrain loss_recon: 0.1716404894861994\ntrain unet DSC: [0.7279363870620728, 0.6721029877662659, 0.08287593722343445, 0.650691568851471]\n\ntest loss: 0.6084398764830369\ntest loss_segm: 0.5980912974247565\ntest loss_shape: 0.019426024327866543\ntest loss_recon: 0.1015431308784546\ntest unet DSC: [0.8187376856803894, 0.7437824606895447, 0.08551624417304993, 0.7263357639312744]\nBest val loss: 0.6084398764830369\nTime: 58.04991626739502\n\n\nEpoch 75/250\n\ntrain loss: 0.7631232266939139\ntrain loss_segm: 0.745567383645456\ntrain loss_shape: 0.03343859645007532\ntrain loss_recon: 0.1722145474787\ntrain unet DSC: [0.7320623993873596, 0.6662755012512207, 0.08311020582914352, 0.6538272500038147]\n\ntest loss: 0.6302664081255595\ntest loss_segm: 0.6188557950349954\ntest loss_shape: 0.022097932915083874\ntest loss_recon: 0.11189636282431774\ntest unet DSC: [0.8033265471458435, 0.7340254187583923, 0.0801904946565628, 0.7077529430389404]\nBest val loss: 0.6084398764830369\nTime: 57.475669384002686\n\n\nEpoch 76/250\n\ntrain loss: 0.7622682007053231\ntrain loss_segm: 0.7448042042647736\ntrain loss_shape: 0.033534988360125806\ntrain loss_recon: 0.1712864959919\ntrain unet DSC: [0.7272714972496033, 0.6709700226783752, 0.0832596942782402, 0.6492591500282288]\n\ntest loss: 0.6186957046007499\ntest loss_segm: 0.6078388828497666\ntest loss_shape: 0.019626238096791964\ntest loss_recon: 0.10660561919212341\ntest unet DSC: [0.8112033009529114, 0.7288565039634705, 0.08272998780012131, 0.7127196192741394]\nBest val loss: 0.6084398764830369\nTime: 57.56478500366211\n\n\nEpoch 77/250\n\ntrain loss: 0.7602815330028534\ntrain loss_segm: 0.7428253666509556\ntrain loss_shape: 0.03308545410208687\ntrain loss_recon: 0.17125314087430132\ntrain unet DSC: [0.7293277978897095, 0.6689515113830566, 0.0831514298915863, 0.6533837914466858]\n\ntest loss: 0.6142154381825373\ntest loss_segm: 0.6033047452951089\ntest loss_shape: 0.020537273456844\ntest loss_recon: 0.10705314309169085\ntest unet DSC: [0.813133716583252, 0.7380315065383911, 0.08397749066352844, 0.7171286344528198]\nBest val loss: 0.6084398764830369\nTime: 57.289947748184204\n\n\nEpoch 78/250\n\ntrain loss: 0.7589668891852415\ntrain loss_segm: 0.7413766470891011\ntrain loss_shape: 0.03340186482837683\ntrain loss_recon: 0.1725622591904447\ntrain unet DSC: [0.7301875352859497, 0.6708320379257202, 0.08412571251392365, 0.6551709771156311]\n\ntest loss: 0.617206749243614\ntest loss_segm: 0.6061413983503977\ntest loss_shape: 0.020097583412933044\ntest loss_recon: 0.10864383373887111\ntest unet DSC: [0.8096069693565369, 0.7339975237846375, 0.08071066439151764, 0.7084937691688538]\nBest val loss: 0.6084398764830369\nTime: 57.30296850204468\n\n\nEpoch 79/250\n\ntrain loss: 0.7544613952123667\ntrain loss_segm: 0.7371166816240624\ntrain loss_shape: 0.03326209316242345\ntrain loss_recon: 0.17012092461691627\ntrain unet DSC: [0.7351607084274292, 0.6719976663589478, 0.08417519181966782, 0.649836540222168]\n\ntest loss: 0.6278214378234668\ntest loss_segm: 0.6161071597001492\ntest loss_shape: 0.02192830440039054\ntest loss_recon: 0.11495000343674268\ntest unet DSC: [0.8015069365501404, 0.732435941696167, 0.0807768926024437, 0.7071598768234253]\nBest val loss: 0.6084398764830369\nTime: 57.2302463054657\n\n\nEpoch 80/250\n\ntrain loss: 0.75205830758131\ntrain loss_segm: 0.7346563769292228\ntrain loss_shape: 0.033100313294820397\ntrain loss_recon: 0.1707092779158037\ntrain unet DSC: [0.7325345873832703, 0.6743654608726501, 0.08373700827360153, 0.6552592515945435]\n\ntest loss: 0.6338544182288341\ntest loss_segm: 0.6219100722899804\ntest loss_shape: 0.02359935373832018\ntest loss_recon: 0.117083529344736\ntest unet DSC: [0.7955953478813171, 0.7352279424667358, 0.08296284824609756, 0.7010629177093506]\nBest val loss: 0.6084398764830369\nTime: 57.92063355445862\n\n\nEpoch 81/250\n\ntrain loss: 0.7536139823967898\ntrain loss_segm: 0.7360996207858943\ntrain loss_shape: 0.03291605639306805\ntrain loss_recon: 0.17185204051717928\ntrain unet DSC: [0.7311224341392517, 0.6697102189064026, 0.08501002192497253, 0.6563127040863037]\n\ntest loss: 0.6211704955651209\ntest loss_segm: 0.6099335169180845\ntest loss_shape: 0.02188850946437854\ntest loss_recon: 0.11018099053165852\ntest unet DSC: [0.8076328635215759, 0.7329574227333069, 0.0845089703798294, 0.7088986039161682]\nBest val loss: 0.6084398764830369\nTime: 57.75248169898987\n\n\nEpoch 82/250\n\ntrain loss: 0.7501872517640078\ntrain loss_segm: 0.732714604350585\ntrain loss_shape: 0.03274624075599109\ntrain loss_recon: 0.17145188652639148\ntrain unet DSC: [0.7317697405815125, 0.6721348166465759, 0.08460967987775803, 0.6580349206924438]\n\ntest loss: 0.6045083411228962\ntest loss_segm: 0.5937184041891342\ntest loss_shape: 0.0194431704063064\ntest loss_recon: 0.1059550483448383\ntest unet DSC: [0.8200093507766724, 0.7420068383216858, 0.08517073094844818, 0.7139575481414795]\nBest val loss: 0.6045083411228962\nTime: 57.84286117553711\n\n\nEpoch 83/250\n\ntrain loss: 0.7498903417889076\ntrain loss_segm: 0.7325245417371581\ntrain loss_shape: 0.03260320241126833\ntrain loss_recon: 0.1703977227399621\ntrain unet DSC: [0.7322183847427368, 0.6753939390182495, 0.08399319648742676, 0.6505903005599976]\n\ntest loss: 0.6105521160822648\ntest loss_segm: 0.5994822688591785\ntest loss_shape: 0.021499521385591764\ntest loss_recon: 0.108548559821569\ntest unet DSC: [0.8084049820899963, 0.745861828327179, 0.08446727693080902, 0.7181879878044128]\nBest val loss: 0.6045083411228962\nTime: 58.509716749191284\n\n\nEpoch 84/250\n\ntrain loss: 0.7489831462691102\ntrain loss_segm: 0.7315544116346142\ntrain loss_shape: 0.032793793438261824\ntrain loss_recon: 0.17100799074278603\ntrain unet DSC: [0.7330427765846252, 0.6805241107940674, 0.08420983701944351, 0.6488898396492004]\n\ntest loss: 0.6019251881501614\ntest loss_segm: 0.591302073154694\ntest loss_shape: 0.019521226772131063\ntest loss_recon: 0.10427909573683372\ntest unet DSC: [0.8155167698860168, 0.7534065246582031, 0.0832294300198555, 0.7102828621864319]\nBest val loss: 0.6019251881501614\nTime: 57.54696726799011\n\n\nEpoch 85/250\n\ntrain loss: 0.7477239170406438\ntrain loss_segm: 0.7302293136150022\ntrain loss_shape: 0.032500355056475236\ntrain loss_recon: 0.17169600979814045\ntrain unet DSC: [0.7345057129859924, 0.6785865426063538, 0.08557569235563278, 0.6490848660469055]\n\ntest loss: 0.6239566122874235\ntest loss_segm: 0.612094677411593\ntest loss_shape: 0.02238449401771411\ntest loss_recon: 0.11638090052665809\ntest unet DSC: [0.8009240031242371, 0.7366664409637451, 0.08256165683269501, 0.7044121623039246]\nBest val loss: 0.6019251881501614\nTime: 57.548587799072266\n\n\nEpoch 86/250\n\ntrain loss: 0.7467372168468523\ntrain loss_segm: 0.729230241684974\ntrain loss_shape: 0.03268153434972974\ntrain loss_recon: 0.17180161913738975\ntrain unet DSC: [0.7324158549308777, 0.6823277473449707, 0.0847812220454216, 0.6557050347328186]\n\ntest loss: 0.6304918993742038\ntest loss_segm: 0.6185570099414923\ntest loss_shape: 0.024721831107177794\ntest loss_recon: 0.11687665528211838\ntest unet DSC: [0.7957603931427002, 0.740349531173706, 0.08556652069091797, 0.7004722356796265]\nBest val loss: 0.6019251881501614\nTime: 57.577836990356445\n\n\nEpoch 87/250\n\ntrain loss: 0.7443839432317999\ntrain loss_segm: 0.7269614082348498\ntrain loss_shape: 0.032637674007800564\ntrain loss_recon: 0.17096160635163513\ntrain unet DSC: [0.7318992018699646, 0.6839892268180847, 0.08477231860160828, 0.6579610705375671]\n\ntest loss: 0.6034623178151938\ntest loss_segm: 0.5925749356930072\ntest loss_shape: 0.019913455662436973\ntest loss_recon: 0.10688244618284397\ntest unet DSC: [0.8061236143112183, 0.7543899416923523, 0.08160605281591415, 0.718015730381012]\nBest val loss: 0.6019251881501614\nTime: 57.737207889556885\n\n\nEpoch 88/250\n\ntrain loss: 0.7407601603978797\ntrain loss_segm: 0.7234860545472254\ntrain loss_shape: 0.032233651621337935\ntrain loss_recon: 0.16951772891267947\ntrain unet DSC: [0.736288845539093, 0.6908184885978699, 0.08505386859178543, 0.6551961898803711]\n\ntest loss: 0.6040549446374942\ntest loss_segm: 0.5929214663994617\ntest loss_shape: 0.020476070877451163\ntest loss_recon: 0.1092871496310601\ntest unet DSC: [0.8087794184684753, 0.7542386054992676, 0.0826326310634613, 0.720097541809082]\nBest val loss: 0.6019251881501614\nTime: 57.280027866363525\n\n\nEpoch 89/250\n\ntrain loss: 0.7398616490484793\ntrain loss_segm: 0.7225855268255065\ntrain loss_shape: 0.03247325158901984\ntrain loss_recon: 0.16951390129478672\ntrain unet DSC: [0.7326227426528931, 0.6996445655822754, 0.0837923064827919, 0.6535416841506958]\n\ntest loss: 0.6023530104221442\ntest loss_segm: 0.5911187904003339\ntest loss_shape: 0.021127099816042643\ntest loss_recon: 0.11022948731596653\ntest unet DSC: [0.8068417310714722, 0.765673041343689, 0.08298501372337341, 0.7205640077590942]\nBest val loss: 0.6019251881501614\nTime: 57.268192768096924\n\n\nEpoch 90/250\n\ntrain loss: 0.7381292087367818\ntrain loss_segm: 0.7206569608253769\ntrain loss_shape: 0.03251155075628923\ntrain loss_recon: 0.17147136130664922\ntrain unet DSC: [0.7326187491416931, 0.7042251825332642, 0.08581195026636124, 0.6610856652259827]\n\ntest loss: 0.5907450165504065\ntest loss_segm: 0.5800549326798855\ntest loss_shape: 0.019528190605342388\ntest loss_recon: 0.10494801574028455\ntest unet DSC: [0.814323365688324, 0.7923686504364014, 0.08454284816980362, 0.7111780643463135]\nBest val loss: 0.5907450165504065\nTime: 57.32272744178772\n\n\nEpoch 91/250\n\ntrain loss: 0.7360914802249474\ntrain loss_segm: 0.7186474822744539\ntrain loss_shape: 0.032379725028442434\ntrain loss_recon: 0.17120205111141445\ntrain unet DSC: [0.7373335361480713, 0.7113409638404846, 0.08429380506277084, 0.65174400806427]\n\ntest loss: 0.6241848613971319\ntest loss_segm: 0.6120096888297644\ntest loss_shape: 0.026455475805470578\ntest loss_recon: 0.11910619586706161\ntest unet DSC: [0.7894067168235779, 0.7792787551879883, 0.08654479682445526, 0.711300790309906]\nBest val loss: 0.5907450165504065\nTime: 57.57829523086548\n\n\nEpoch 92/250\n\ntrain loss: 0.7359888300865511\ntrain loss_segm: 0.7185133944583845\ntrain loss_shape: 0.0327090879524999\ntrain loss_recon: 0.17148344963788986\ntrain unet DSC: [0.7332343459129333, 0.7138921022415161, 0.08574014902114868, 0.655170738697052]\n\ntest loss: 0.612920846694555\ntest loss_segm: 0.6010225124848194\ntest loss_shape: 0.024206801938513916\ntest loss_recon: 0.11656267425188652\ntest unet DSC: [0.7890629172325134, 0.785703182220459, 0.0821576938033104, 0.721572995185852]\nBest val loss: 0.5907450165504065\nTime: 57.95960235595703\n\n\nEpoch 93/250\n\ntrain loss: 0.7282149542736102\ntrain loss_segm: 0.7109430398367629\ntrain loss_shape: 0.032499853663052185\ntrain loss_recon: 0.16946915074994293\ntrain unet DSC: [0.7382060885429382, 0.7188268303871155, 0.08553485572338104, 0.6599755883216858]\n\ntest loss: 0.6198181662804041\ntest loss_segm: 0.6077064115267533\ntest loss_shape: 0.025361838201299693\ntest loss_recon: 0.11858134353772187\ntest unet DSC: [0.7941628694534302, 0.7809993028640747, 0.08427979052066803, 0.7009465098381042]\nBest val loss: 0.5907450165504065\nTime: 57.494245529174805\n\n\nEpoch 94/250\n\ntrain loss: 0.7351609838159778\ntrain loss_segm: 0.7176460133323187\ntrain loss_shape: 0.03254423429600046\ntrain loss_recon: 0.1718953062273279\ntrain unet DSC: [0.7344123721122742, 0.7130400538444519, 0.08477997779846191, 0.6501158475875854]\n\ntest loss: 0.5772863848087115\ntest loss_segm: 0.5666691347574576\ntest loss_shape: 0.01968958193006424\ntest loss_recon: 0.1042035107429211\ntest unet DSC: [0.8198890686035156, 0.8054153919219971, 0.08711234480142593, 0.7294652462005615]\nBest val loss: 0.5772863848087115\nTime: 57.65891695022583\n\n\nEpoch 95/250\n\ntrain loss: 0.7310411077511462\ntrain loss_segm: 0.71365119309365\ntrain loss_shape: 0.03219302814406685\ntrain loss_recon: 0.17067986019427264\ntrain unet DSC: [0.7342013120651245, 0.7138640880584717, 0.08661369979381561, 0.6596121788024902]\n\ntest loss: 0.583141273412949\ntest loss_segm: 0.5722035353000348\ntest loss_shape: 0.019908942067279264\ntest loss_recon: 0.10738648913609676\ntest unet DSC: [0.8164669871330261, 0.7974122166633606, 0.08540986478328705, 0.7234417796134949]\nBest val loss: 0.5772863848087115\nTime: 57.48282337188721\n\n\nEpoch 96/250\n\ntrain loss: 0.7235410515266129\ntrain loss_segm: 0.7064973072537893\ntrain loss_shape: 0.031942249110699455\ntrain loss_recon: 0.16724323981170414\ntrain unet DSC: [0.7375330328941345, 0.7204672694206238, 0.0856025293469429, 0.6606507897377014]\n\ntest loss: 0.5773910612632067\ntest loss_segm: 0.5665469918495569\ntest loss_shape: 0.018990151966229465\ntest loss_recon: 0.10654166961709659\ntest unet DSC: [0.8174311518669128, 0.8017932176589966, 0.0854576900601387, 0.7214471101760864]\nBest val loss: 0.5772863848087115\nTime: 57.85925483703613\n\n\nEpoch 97/250\n\ntrain loss: 0.7272063618219351\ntrain loss_segm: 0.7099200592765326\ntrain loss_shape: 0.032238611527070214\ntrain loss_recon: 0.16963918252459056\ntrain unet DSC: [0.7362756133079529, 0.717888593673706, 0.08447360247373581, 0.6571789383888245]\n\ntest loss: 0.5878324470458887\ntest loss_segm: 0.5763611205113239\ntest loss_shape: 0.02090815380693246\ntest loss_recon: 0.11262246192647861\ntest unet DSC: [0.8076461553573608, 0.8031296730041504, 0.08100838214159012, 0.7258897423744202]\nBest val loss: 0.5772863848087115\nTime: 57.232147216796875\n\n\nEpoch 98/250\n\ntrain loss: 0.7242664955084837\ntrain loss_segm: 0.7070350066015992\ntrain loss_shape: 0.03242077652506436\ntrain loss_recon: 0.1690728119279765\ntrain unet DSC: [0.738517701625824, 0.7196981906890869, 0.08628198504447937, 0.6560985445976257]\n\ntest loss: 0.5759703494035281\ntest loss_segm: 0.5654436090053656\ntest loss_shape: 0.019806708209216595\ntest loss_recon: 0.1032867594025074\ntest unet DSC: [0.8110234141349792, 0.8046791553497314, 0.08770538866519928, 0.7265884280204773]\nBest val loss: 0.5759703494035281\nTime: 58.281469106674194\n\n\nEpoch 99/250\n\ntrain loss: 0.7273023747190644\ntrain loss_segm: 0.7098972408831874\ntrain loss_shape: 0.032244801650979094\ntrain loss_recon: 0.1708268995715093\ntrain unet DSC: [0.7357431054115295, 0.7169193625450134, 0.08594298362731934, 0.6562060117721558]\n\ntest loss: 0.5942275753388038\ntest loss_segm: 0.5829225587539184\ntest loss_shape: 0.020683738546302684\ntest loss_recon: 0.11098180606197088\ntest unet DSC: [0.805924117565155, 0.7936745285987854, 0.08307170122861862, 0.6996208429336548]\nBest val loss: 0.5759703494035281\nTime: 57.75053834915161\n\n\nEpoch 100/250\n\ntrain loss: 0.7232828977741773\ntrain loss_segm: 0.7061944456794594\ntrain loss_shape: 0.031943003375884856\ntrain loss_recon: 0.16769025144697744\ntrain unet DSC: [0.7371877431869507, 0.7187010049819946, 0.08653111755847931, 0.6539028286933899]\n\ntest loss: 0.5776397700493152\ntest loss_segm: 0.5667835153066195\ntest loss_shape: 0.019183332793032512\ntest loss_recon: 0.10664420336102828\ntest unet DSC: [0.8170205354690552, 0.8016501069068909, 0.0843483954668045, 0.718692421913147]\nBest val loss: 0.5759703494035281\nTime: 57.2490930557251\n\n\nEpoch 101/250\n\ntrain loss: 0.7238256248492229\ntrain loss_segm: 0.7066176450705226\ntrain loss_shape: 0.03206983822857654\ntrain loss_recon: 0.1688728054301648\ntrain unet DSC: [0.736276388168335, 0.7198757529258728, 0.08583417534828186, 0.652462363243103]\n\ntest loss: 0.5772020702178662\ntest loss_segm: 0.5663224389919868\ntest loss_shape: 0.019162894919132575\ntest loss_recon: 0.10687997182592368\ntest unet DSC: [0.8202787637710571, 0.7950353026390076, 0.08364483714103699, 0.7214442491531372]\nBest val loss: 0.5759703494035281\nTime: 57.499470472335815\n\n\nEpoch 102/250\n\ntrain loss: 0.7262544994112812\ntrain loss_segm: 0.7089400785633281\ntrain loss_shape: 0.03210346508016692\ntrain loss_recon: 0.16993387430151807\ntrain unet DSC: [0.7357180714607239, 0.7175115942955017, 0.08554030954837799, 0.6521047949790955]\n\ntest loss: 0.592445453772178\ntest loss_segm: 0.580864334717775\ntest loss_shape: 0.020795057360560466\ntest loss_recon: 0.11373175193484013\ntest unet DSC: [0.8060961961746216, 0.7920767664909363, 0.08179119229316711, 0.7110732793807983]\nBest val loss: 0.5759703494035281\nTime: 57.23590397834778\n\n\nEpoch 103/250\n\ntrain loss: 0.7226191140428374\ntrain loss_segm: 0.7053021660333947\ntrain loss_shape: 0.03195805857075921\ntrain loss_recon: 0.16997369586289685\ntrain unet DSC: [0.7361258864402771, 0.7221630215644836, 0.08506359905004501, 0.6562131643295288]\n\ntest loss: 0.623805834696843\ntest loss_segm: 0.6112756652709765\ntest loss_shape: 0.027839761919891223\ntest loss_recon: 0.12251769197292817\ntest unet DSC: [0.7838073968887329, 0.7856301069259644, 0.08756621927022934, 0.7071138620376587]\nBest val loss: 0.5759703494035281\nTime: 57.48172950744629\n\n\nEpoch 104/250\n\ntrain loss: 0.7246700521511368\ntrain loss_segm: 0.7073953759066666\ntrain loss_shape: 0.032122878369556955\ntrain loss_recon: 0.16953447178194794\ntrain unet DSC: [0.7354534864425659, 0.7187293767929077, 0.08747664839029312, 0.6566356420516968]\n\ntest loss: 0.5952775379021963\ntest loss_segm: 0.5837427767423483\ntest loss_shape: 0.022324746522383813\ntest loss_recon: 0.113115111604715\ntest unet DSC: [0.8019955158233643, 0.7942407131195068, 0.0859723910689354, 0.712358295917511]\nBest val loss: 0.5759703494035281\nTime: 57.78877091407776\n\n\nEpoch 105/250\n\ntrain loss: 0.7218376956408536\ntrain loss_segm: 0.7044997332216818\ntrain loss_shape: 0.03196953748694704\ntrain loss_recon: 0.1701826748402813\ntrain unet DSC: [0.7366995215415955, 0.7191402316093445, 0.08579204976558685, 0.6607865691184998]\n\ntest loss: 0.5715101323066614\ntest loss_segm: 0.5606805857939597\ntest loss_shape: 0.018760365028029833\ntest loss_recon: 0.10641941581016932\ntest unet DSC: [0.8230854868888855, 0.8059654235839844, 0.08482560515403748, 0.7241566777229309]\nBest val loss: 0.5715101323066614\nTime: 57.604737758636475\n\n\nEpoch 106/250\n\ntrain loss: 0.7218489643139175\ntrain loss_segm: 0.7046810169763202\ntrain loss_shape: 0.031866390977196304\ntrain loss_recon: 0.16849284581368482\ntrain unet DSC: [0.7364457845687866, 0.7202427387237549, 0.08496508002281189, 0.6570072770118713]\n\ntest loss: 0.57510595367505\ntest loss_segm: 0.5638957092395196\ntest loss_shape: 0.01970174005971505\ntest loss_recon: 0.11013231436029458\ntest unet DSC: [0.8127005100250244, 0.8082228302955627, 0.08448974043130875, 0.7288629412651062]\nBest val loss: 0.5715101323066614\nTime: 57.70222020149231\n\n\nEpoch 107/250\n\ntrain loss: 0.7202344536781311\ntrain loss_segm: 0.7030180330517926\ntrain loss_shape: 0.03148062385712998\ntrain loss_recon: 0.16901616384334203\ntrain unet DSC: [0.7382060289382935, 0.7226255536079407, 0.08557125180959702, 0.6564539670944214]\n\ntest loss: 0.5712771094762362\ntest loss_segm: 0.560519950512128\ntest loss_shape: 0.01971186463458416\ntest loss_recon: 0.10560041140669431\ntest unet DSC: [0.8174253106117249, 0.8049044609069824, 0.08754824101924896, 0.7324575781822205]\nBest val loss: 0.5712771094762362\nTime: 57.73869204521179\n\n\nEpoch 108/250\n\ntrain loss: 0.7223209848132315\ntrain loss_segm: 0.7051064809666404\ntrain loss_shape: 0.031672361778401874\ntrain loss_recon: 0.1689777960882911\ntrain unet DSC: [0.737927258014679, 0.7183752059936523, 0.08538330346345901, 0.6524658203125]\n\ntest loss: 0.5767363699582907\ntest loss_segm: 0.5656369022833996\ntest loss_shape: 0.019313818727357265\ntest loss_recon: 0.10906332817215186\ntest unet DSC: [0.8166339993476868, 0.801638662815094, 0.0839020311832428, 0.7185074090957642]\nBest val loss: 0.5712771094762362\nTime: 57.26313853263855\n\n\nEpoch 109/250\n\ntrain loss: 0.71968339289291\ntrain loss_segm: 0.7024138532107389\ntrain loss_shape: 0.03184644307328176\ntrain loss_recon: 0.16951079817512368\ntrain unet DSC: [0.737878143787384, 0.7187715172767639, 0.086582712829113, 0.6592972874641418]\n\ntest loss: 0.5817699394164941\ntest loss_segm: 0.5705463450688583\ntest loss_shape: 0.020455784427049834\ntest loss_recon: 0.1101903786452917\ntest unet DSC: [0.804734468460083, 0.7995057702064514, 0.08497752994298935, 0.7276007533073425]\nBest val loss: 0.5712771094762362\nTime: 57.20807981491089\n\n\nEpoch 110/250\n\ntrain loss: 0.7197889338565778\ntrain loss_segm: 0.7025020213066777\ntrain loss_shape: 0.031792353591115416\ntrain loss_recon: 0.16968988242877436\ntrain unet DSC: [0.7374988198280334, 0.7186356782913208, 0.08646813780069351, 0.6598922610282898]\n\ntest loss: 0.5703294414740342\ntest loss_segm: 0.5595757999481299\ntest loss_shape: 0.01931837256042621\ntest loss_recon: 0.10560457007243083\ntest unet DSC: [0.8161793351173401, 0.8035534620285034, 0.08689028769731522, 0.730694591999054]\nBest val loss: 0.5703294414740342\nTime: 57.544471740722656\n\n\nEpoch 111/250\n\ntrain loss: 0.7138909757891788\ntrain loss_segm: 0.6968983275980889\ntrain loss_shape: 0.03134727439123996\ntrain loss_recon: 0.1667917768789243\ntrain unet DSC: [0.7398558855056763, 0.7221018671989441, 0.08847382664680481, 0.6620230674743652]\n\ntest loss: 0.5833371831820562\ntest loss_segm: 0.5718837724282191\ntest loss_shape: 0.019826459005857125\ntest loss_recon: 0.11255143172083756\ntest unet DSC: [0.8117860555648804, 0.795771062374115, 0.08304186165332794, 0.7162696123123169]\nBest val loss: 0.5703294414740342\nTime: 58.27398467063904\n\n\nEpoch 112/250\n\ntrain loss: 0.7176755636553221\ntrain loss_segm: 0.7003750235219545\ntrain loss_shape: 0.03194086721662102\ntrain loss_recon: 0.16981133620572997\ntrain unet DSC: [0.736644983291626, 0.720895528793335, 0.08579294383525848, 0.6626428365707397]\n\ntest loss: 0.5960069077137189\ntest loss_segm: 0.5840586217550131\ntest loss_shape: 0.02242000190875469\ntest loss_recon: 0.11724089477688839\ntest unet DSC: [0.8058607578277588, 0.7885074019432068, 0.08356903493404388, 0.709841787815094]\nBest val loss: 0.5703294414740342\nTime: 57.86434555053711\n\n\nEpoch 113/250\n\ntrain loss: 0.7166947243334372\ntrain loss_segm: 0.6995569995686978\ntrain loss_shape: 0.031758269601607624\ntrain loss_recon: 0.1682014488909818\ntrain unet DSC: [0.7396520376205444, 0.7210637331008911, 0.08640995621681213, 0.6581087708473206]\n\ntest loss: 0.5732782513667376\ntest loss_segm: 0.5622427593439053\ntest loss_shape: 0.019891120684452545\ntest loss_recon: 0.10836581780742376\ntest unet DSC: [0.8154674172401428, 0.8050028681755066, 0.08681633323431015, 0.7244510054588318]\nBest val loss: 0.5703294414740342\nTime: 58.264676570892334\n\n\nEpoch 114/250\n\ntrain loss: 0.7163844625406628\ntrain loss_segm: 0.6991962500010864\ntrain loss_shape: 0.031193495834175543\ntrain loss_recon: 0.16876279138311556\ntrain unet DSC: [0.7417186498641968, 0.7202663421630859, 0.08619925379753113, 0.6595395803451538]\n\ntest loss: 0.5833785220598563\ntest loss_segm: 0.5717989213955708\ntest loss_shape: 0.02008825447410345\ntest loss_recon: 0.11378718263063675\ntest unet DSC: [0.8118963837623596, 0.794379472732544, 0.08266700059175491, 0.7157697081565857]\nBest val loss: 0.5703294414740342\nTime: 57.558135986328125\n\n\nEpoch 115/250\n\ntrain loss: 0.7188769669472417\ntrain loss_segm: 0.7014881112153017\ntrain loss_shape: 0.03184738408632671\ntrain loss_recon: 0.1707038374829896\ntrain unet DSC: [0.7398141026496887, 0.7212388515472412, 0.0886353999376297, 0.6557556390762329]\n\ntest loss: 0.5759673920961527\ntest loss_segm: 0.5650293223368816\ntest loss_shape: 0.02076495596422599\ntest loss_recon: 0.10730421676849708\ntest unet DSC: [0.8143218755722046, 0.802325427532196, 0.08640895783901215, 0.7271084189414978]\nBest val loss: 0.5703294414740342\nTime: 57.396697759628296\n\n\nEpoch 116/250\n\ntrain loss: 0.7156777989260757\ntrain loss_segm: 0.6985364631761478\ntrain loss_shape: 0.031827841567087775\ntrain loss_recon: 0.16823057802040367\ntrain unet DSC: [0.7403539419174194, 0.720721423625946, 0.08788422495126724, 0.6595840454101562]\n\ntest loss: 0.5809880242897913\ntest loss_segm: 0.5698205591776432\ntest loss_shape: 0.02000675839968981\ntest loss_recon: 0.10967399456944221\ntest unet DSC: [0.810706377029419, 0.7922893166542053, 0.08494659513235092, 0.7211389541625977]\nBest val loss: 0.5703294414740342\nTime: 57.78220510482788\n\n\nEpoch 117/250\n\ntrain loss: 0.7167410054538823\ntrain loss_segm: 0.6994393135927901\ntrain loss_shape: 0.031638183053347126\ntrain loss_recon: 0.16985311745842802\ntrain unet DSC: [0.7361788749694824, 0.7223989963531494, 0.08771266043186188, 0.6621643304824829]\n\ntest loss: 0.5853445552862607\ntest loss_segm: 0.5739226203698379\ntest loss_shape: 0.020192523319751788\ntest loss_recon: 0.11220008516922975\ntest unet DSC: [0.8150219321250916, 0.7867397665977478, 0.08281677961349487, 0.716049075126648]\nBest val loss: 0.5703294414740342\nTime: 57.67074394226074\n\n\nEpoch 118/250\n\ntrain loss: 0.7123776527145241\ntrain loss_segm: 0.6953821118119397\ntrain loss_shape: 0.03101594183782611\ntrain loss_recon: 0.16685382420597952\ntrain unet DSC: [0.7385724186897278, 0.7243473529815674, 0.08735202252864838, 0.6637254953384399]\n\ntest loss: 0.5756146808465322\ntest loss_segm: 0.5643802262269534\ntest loss_shape: 0.019275985562648527\ntest loss_recon: 0.11041686397332412\ntest unet DSC: [0.81707763671875, 0.7988694906234741, 0.08357688784599304, 0.7212403416633606]\nBest val loss: 0.5703294414740342\nTime: 57.535303354263306\n\n\nEpoch 119/250\n\ntrain loss: 0.7215255695053294\ntrain loss_segm: 0.7040590228913706\ntrain loss_shape: 0.03154382396112137\ntrain loss_recon: 0.17151112279182748\ntrain unet DSC: [0.7347536683082581, 0.7182478904724121, 0.08615030348300934, 0.6585741639137268]\n\ntest loss: 0.5689756870269775\ntest loss_segm: 0.5579371597522345\ntest loss_shape: 0.01880224619824917\ntest loss_recon: 0.10850504652047768\ntest unet DSC: [0.8209416270256042, 0.8008834719657898, 0.08597882837057114, 0.7294925451278687]\nBest val loss: 0.5689756870269775\nTime: 57.90133237838745\n\n\nEpoch 120/250\n\ntrain loss: 0.7129689628564859\ntrain loss_segm: 0.6958771287640438\ntrain loss_shape: 0.031803422902203816\ntrain loss_recon: 0.16773801975989644\ntrain unet DSC: [0.7419102787971497, 0.7205381989479065, 0.08797616511583328, 0.6617394089698792]\n\ntest loss: 0.5756204991768568\ntest loss_segm: 0.5645766479846759\ntest loss_shape: 0.021035623736679554\ntest loss_recon: 0.108335025035418\ntest unet DSC: [0.8171621561050415, 0.8013441562652588, 0.0882166177034378, 0.7241352796554565]\nBest val loss: 0.5689756870269775\nTime: 57.66718888282776\n\n\nEpoch 121/250\n\ntrain loss: 0.7133330367033994\ntrain loss_segm: 0.6962458034859428\ntrain loss_shape: 0.03153826697151872\ntrain loss_recon: 0.1677185283101435\ntrain unet DSC: [0.7409018278121948, 0.7253657579421997, 0.08888254314661026, 0.6561727523803711]\n\ntest loss: 0.5867974544182802\ntest loss_segm: 0.5751805886244162\ntest loss_shape: 0.021327180358079765\ntest loss_recon: 0.11403590822831178\ntest unet DSC: [0.8042946457862854, 0.7965565919876099, 0.08277026563882828, 0.7252140045166016]\nBest val loss: 0.5689756870269775\nTime: 57.87432384490967\n\n\nEpoch 122/250\n\ntrain loss: 0.7127237025695511\ntrain loss_segm: 0.6955910426906392\ntrain loss_shape: 0.03103782118687147\ntrain loss_recon: 0.16822282819053794\ntrain unet DSC: [0.7420862317085266, 0.7234112620353699, 0.08563143759965897, 0.6606594920158386]\n\ntest loss: 0.5701388349899879\ntest loss_segm: 0.5592315686054719\ntest loss_shape: 0.018950522136993896\ntest loss_recon: 0.1071776083837717\ntest unet DSC: [0.8167871832847595, 0.803124189376831, 0.08405862748622894, 0.7271941304206848]\nBest val loss: 0.5689756870269775\nTime: 57.87964129447937\n\n\nEpoch 123/250\n\ntrain loss: 0.7121676297127446\ntrain loss_segm: 0.6950606307651424\ntrain loss_shape: 0.0314028435091995\ntrain loss_recon: 0.16792976045155827\ntrain unet DSC: [0.7397803068161011, 0.7217513918876648, 0.08698862791061401, 0.6655384302139282]\n\ntest loss: 0.5746573400803101\ntest loss_segm: 0.5635180328136835\ntest loss_shape: 0.020011510150745895\ntest loss_recon: 0.1093918873140445\ntest unet DSC: [0.8110758662223816, 0.8063287138938904, 0.08522062748670578, 0.7194141745567322]\nBest val loss: 0.5689756870269775\nTime: 58.004884481430054\n\n\nEpoch 124/250\n\ntrain loss: 0.7096359763718858\ntrain loss_segm: 0.6926402321344689\ntrain loss_shape: 0.03130824991230723\ntrain loss_recon: 0.16682664484162874\ntrain unet DSC: [0.7408944368362427, 0.7231637239456177, 0.08751695603132248, 0.6678045988082886]\n\ntest loss: 0.5734363381679242\ntest loss_segm: 0.5623455116382012\ntest loss_shape: 0.019417128859995268\ntest loss_recon: 0.10896649660590367\ntest unet DSC: [0.8135663270950317, 0.8008108735084534, 0.08302213251590729, 0.7311662435531616]\nBest val loss: 0.5689756870269775\nTime: 57.477686166763306\n\n\nEpoch 125/250\n\ntrain loss: 0.71087479704543\ntrain loss_segm: 0.6937442482272281\ntrain loss_shape: 0.0315520523729015\ntrain loss_recon: 0.16815030037225048\ntrain unet DSC: [0.7411899566650391, 0.7264154553413391, 0.08752226084470749, 0.6614811420440674]\n\ntest loss: 0.5701345136532416\ntest loss_segm: 0.5591849975096874\ntest loss_shape: 0.01923680233840759\ntest loss_recon: 0.10757147272427876\ntest unet DSC: [0.8158426284790039, 0.8026662468910217, 0.08517283946275711, 0.7276997566223145]\nBest val loss: 0.5689756870269775\nTime: 58.64929986000061\n\n\nEpoch 126/250\n\ntrain loss: 0.7125258909750588\ntrain loss_segm: 0.6954471235033832\ntrain loss_shape: 0.03137626726463248\ntrain loss_recon: 0.16765006527870516\ntrain unet DSC: [0.7398569583892822, 0.7235841155052185, 0.08602681010961533, 0.6626908779144287]\n\ntest loss: 0.583441327779721\ntest loss_segm: 0.5721297279382364\ntest loss_shape: 0.021101325010068905\ntest loss_recon: 0.11100590649323586\ntest unet DSC: [0.8042337894439697, 0.7941078543663025, 0.08649224787950516, 0.7260822653770447]\nBest val loss: 0.5689756870269775\nTime: 57.56753611564636\n\n\nEpoch 127/250\n\ntrain loss: 0.7158186560190176\ntrain loss_segm: 0.6986537681350226\ntrain loss_shape: 0.031662501064659676\ntrain loss_recon: 0.1684826717157907\ntrain unet DSC: [0.7408470511436462, 0.7220671772956848, 0.08843410760164261, 0.6541529297828674]\n\ntest loss: 0.5794535149366428\ntest loss_segm: 0.5680033373527038\ntest loss_shape: 0.01958850436867812\ntest loss_recon: 0.11254295257803722\ntest unet DSC: [0.80455082654953, 0.7958340048789978, 0.08291501551866531, 0.7266327142715454]\nBest val loss: 0.5689756870269775\nTime: 57.29845952987671\n\n\nEpoch 128/250\n\ntrain loss: 0.7094090165971201\ntrain loss_segm: 0.6924738201159465\ntrain loss_shape: 0.031428359578587584\ntrain loss_recon: 0.16620910481278656\ntrain unet DSC: [0.7408227324485779, 0.7240198254585266, 0.0892980545759201, 0.6645152568817139]\n\ntest loss: 0.5712198706773611\ntest loss_segm: 0.560132942138574\ntest loss_shape: 0.018500984718020145\ntest loss_recon: 0.10901914126215836\ntest unet DSC: [0.8158703446388245, 0.7990723848342896, 0.08463401347398758, 0.7281785011291504]\nBest val loss: 0.5689756870269775\nTime: 56.83758902549744\n\n\nEpoch 129/250\n\ntrain loss: 0.712331032451195\ntrain loss_segm: 0.6951287351077116\ntrain loss_shape: 0.031237789382568642\ntrain loss_recon: 0.16889923980718927\ntrain unet DSC: [0.7411014437675476, 0.7244664430618286, 0.08842727541923523, 0.6618601083755493]\n\ntest loss: 0.576489155109112\ntest loss_segm: 0.5652950283808585\ntest loss_shape: 0.02018542945958101\ntest loss_recon: 0.1099227782434378\ntest unet DSC: [0.8123511075973511, 0.7918413281440735, 0.08629688620567322, 0.7302746176719666]\nBest val loss: 0.5689756870269775\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.39380884170532\n\n\nEpoch 130/250\n\ntrain loss: 0.7065849372103244\ntrain loss_segm: 0.6896309611163561\ntrain loss_shape: 0.030881154209469693\ntrain loss_recon: 0.1664516271292409\ntrain unet DSC: [0.7450593113899231, 0.7249248623847961, 0.08730867505073547, 0.66133713722229]\n\ntest loss: 0.5759381017623804\ntest loss_segm: 0.5646838003244156\ntest loss_shape: 0.019963052386465747\ntest loss_recon: 0.1105467517597553\ntest unet DSC: [0.8131157159805298, 0.7988400459289551, 0.08482605218887329, 0.7248672842979431]\nBest val loss: 0.5689756870269775\nTime: 57.70775246620178\n\n\nEpoch 131/250\n\ntrain loss: 0.704640544290784\ntrain loss_segm: 0.6879049863996385\ntrain loss_shape: 0.030506923700435253\ntrain loss_recon: 0.1643048759691323\ntrain unet DSC: [0.7450284361839294, 0.7245286107063293, 0.0882607102394104, 0.6675481796264648]\n\ntest loss: 0.5695043488954886\ntest loss_segm: 0.5585160293640234\ntest loss_shape: 0.019223445835403908\ntest loss_recon: 0.10796085773752286\ntest unet DSC: [0.8173210024833679, 0.8010594844818115, 0.08522684127092361, 0.7297747135162354]\nBest val loss: 0.5689756870269775\nTime: 58.18988919258118\n\n\nEpoch 132/250\n\ntrain loss: 0.706747029401079\ntrain loss_segm: 0.6898193393327012\ntrain loss_shape: 0.030570107553437163\ntrain loss_recon: 0.16621992595588106\ntrain unet DSC: [0.7433043122291565, 0.7262403964996338, 0.08894774317741394, 0.663637638092041]\n\ntest loss: 0.5702201005740043\ntest loss_segm: 0.5593830125454144\ntest loss_shape: 0.019523048295806616\ntest loss_recon: 0.10641863082463925\ntest unet DSC: [0.8159336447715759, 0.8013738393783569, 0.08607743680477142, 0.7295125126838684]\nBest val loss: 0.5689756870269775\nTime: 57.43712782859802\n\n\nEpoch 133/250\n\ntrain loss: 0.7029083340982848\ntrain loss_segm: 0.6861205685742294\ntrain loss_shape: 0.030633188772333574\ntrain loss_recon: 0.16481437194573728\ntrain unet DSC: [0.745826244354248, 0.7282300591468811, 0.08839894086122513, 0.6683456897735596]\n\ntest loss: 0.5697198930459145\ntest loss_segm: 0.5587556316302373\ntest loss_shape: 0.019203963737266187\ntest loss_recon: 0.10772224047627205\ntest unet DSC: [0.8153006434440613, 0.8023951053619385, 0.08507700264453888, 0.7294737696647644]\nBest val loss: 0.5689756870269775\nTime: 58.09780192375183\n\n\nEpoch 134/250\n\ntrain loss: 0.7068108908737762\ntrain loss_segm: 0.6898930155778233\ntrain loss_shape: 0.03053774081076248\ntrain loss_recon: 0.16612498773426948\ntrain unet DSC: [0.7447390556335449, 0.7275885343551636, 0.08858902007341385, 0.6612660884857178]\n\ntest loss: 0.5615162933484102\ntest loss_segm: 0.5508233568607233\ntest loss_shape: 0.0184906647564509\ntest loss_recon: 0.10508031808795074\ntest unet DSC: [0.8238052725791931, 0.8085091710090637, 0.08690337091684341, 0.7315207719802856]\nBest val loss: 0.5615162933484102\nTime: 57.230268478393555\n\n\nEpoch 135/250\n\ntrain loss: 0.7058259069542342\ntrain loss_segm: 0.688923156336893\ntrain loss_shape: 0.030315137633322914\ntrain loss_recon: 0.16599602009393746\ntrain unet DSC: [0.745051920413971, 0.7272178530693054, 0.08729861676692963, 0.6667742729187012]\n\ntest loss: 0.5628913029646262\ntest loss_segm: 0.5522212248582107\ntest loss_shape: 0.018805164581116956\ntest loss_recon: 0.10482032262744048\ntest unet DSC: [0.8214970231056213, 0.8086354732513428, 0.08729901909828186, 0.7317895889282227]\nBest val loss: 0.5615162933484102\nTime: 57.190619230270386\n\n\nEpoch 136/250\n\ntrain loss: 0.7047858092981049\ntrain loss_segm: 0.6879428276155568\ntrain loss_shape: 0.030497927996742575\ntrain loss_recon: 0.16538000352020504\ntrain unet DSC: [0.7443035840988159, 0.7282232642173767, 0.08903009444475174, 0.6649023294448853]\n\ntest loss: 0.5655179849037757\ntest loss_segm: 0.5546500606414599\ntest loss_shape: 0.018774621427441254\ntest loss_recon: 0.10680175219208767\ntest unet DSC: [0.8200492858886719, 0.8044339418411255, 0.0861552432179451, 0.7325058579444885]\nBest val loss: 0.5615162933484102\nTime: 57.089890241622925\n\n\nEpoch 137/250\n\ntrain loss: 0.7019063585166689\ntrain loss_segm: 0.6852385292324839\ntrain loss_shape: 0.030521880446261244\ntrain loss_recon: 0.1636261080827894\ntrain unet DSC: [0.7460415363311768, 0.7293360233306885, 0.08987241983413696, 0.6676808595657349]\n\ntest loss: 0.5690602881786151\ntest loss_segm: 0.5581684173681797\ntest loss_shape: 0.019110522710551053\ntest loss_recon: 0.10700759606865737\ntest unet DSC: [0.8182632327079773, 0.7996424436569214, 0.08592944592237473, 0.7312746047973633]\nBest val loss: 0.5615162933484102\nTime: 57.19991374015808\n\n\nEpoch 138/250\n\ntrain loss: 0.706219066547442\ntrain loss_segm: 0.6893319638469552\ntrain loss_shape: 0.030107336201339582\ntrain loss_recon: 0.16586036131351808\ntrain unet DSC: [0.7451074719429016, 0.7259472012519836, 0.08942289650440216, 0.6653375029563904]\n\ntest loss: 0.5660082499186198\ntest loss_segm: 0.5553087317026578\ntest loss_shape: 0.018841763838934593\ntest loss_recon: 0.10511103740487343\ntest unet DSC: [0.8199903964996338, 0.8022983074188232, 0.08602891117334366, 0.7285391092300415]\nBest val loss: 0.5615162933484102\nTime: 57.31236481666565\n\n\nEpoch 139/250\n\ntrain loss: 0.7048982400682908\ntrain loss_segm: 0.6880696446080751\ntrain loss_shape: 0.03026993478400798\ntrain loss_recon: 0.16525897129049785\ntrain unet DSC: [0.7440904974937439, 0.7276753783226013, 0.08873835951089859, 0.6657319664955139]\n\ntest loss: 0.571247527996699\ntest loss_segm: 0.5603132461890196\ntest loss_shape: 0.019968082459691245\ntest loss_recon: 0.10734606830355449\ntest unet DSC: [0.8163026571273804, 0.8027395606040955, 0.08578997105360031, 0.7274577021598816]\nBest val loss: 0.5615162933484102\nTime: 57.316994190216064\n\n\nEpoch 140/250\n\ntrain loss: 0.7037119035479389\ntrain loss_segm: 0.6868669601935374\ntrain loss_shape: 0.029994463029352925\ntrain loss_recon: 0.16544998173095002\ntrain unet DSC: [0.7460368871688843, 0.7274725437164307, 0.08731567114591599, 0.6655633449554443]\n\ntest loss: 0.5635477717106159\ntest loss_segm: 0.552826620065249\ntest loss_shape: 0.01883683485002854\ntest loss_recon: 0.1053278729892694\ntest unet DSC: [0.8202911019325256, 0.8078791499137878, 0.0871078222990036, 0.7312555313110352]\nBest val loss: 0.5615162933484102\nTime: 57.434401512145996\n\n\nEpoch 141/250\n\ntrain loss: 0.7002533281905742\ntrain loss_segm: 0.6835980969893781\ntrain loss_shape: 0.030027832482245904\ntrain loss_recon: 0.163549536013905\ntrain unet DSC: [0.7481120824813843, 0.730808675289154, 0.08904504030942917, 0.6658772826194763]\n\ntest loss: 0.5659172022954012\ntest loss_segm: 0.5551718397018237\ntest loss_shape: 0.01877670257519453\ntest loss_recon: 0.10557597961563331\ntest unet DSC: [0.8202903270721436, 0.8026325702667236, 0.08621161431074142, 0.7292116284370422]\nBest val loss: 0.5615162933484102\nTime: 57.31044960021973\n\n\nEpoch 142/250\n\ntrain loss: 0.7035257540926149\ntrain loss_segm: 0.6867713890498197\ntrain loss_shape: 0.03000569408405808\ntrain loss_recon: 0.16454308602628828\ntrain unet DSC: [0.7447444796562195, 0.7285460829734802, 0.0897306576371193, 0.6637095212936401]\n\ntest loss: 0.5778212562585489\ntest loss_segm: 0.566652428645354\ntest loss_shape: 0.020586156883300878\ntest loss_recon: 0.1096296811906191\ntest unet DSC: [0.8124619722366333, 0.7966293692588806, 0.08594895154237747, 0.7258784770965576]\nBest val loss: 0.5615162933484102\nTime: 57.397446393966675\n\n\nEpoch 143/250\n\ntrain loss: 0.7042342596038987\ntrain loss_segm: 0.6874810019625893\ntrain loss_shape: 0.029936885213644446\ntrain loss_recon: 0.16453886590898037\ntrain unet DSC: [0.7454724907875061, 0.7279701232910156, 0.08926845341920853, 0.6631320118904114]\n\ntest loss: 0.5703724126021067\ntest loss_segm: 0.5593243187818772\ntest loss_shape: 0.01914596512245062\ntest loss_recon: 0.1085663293607724\ntest unet DSC: [0.815924882888794, 0.8021208643913269, 0.08510490506887436, 0.7258175611495972]\nBest val loss: 0.5615162933484102\nTime: 57.05177927017212\n\n\nEpoch 144/250\n\ntrain loss: 0.7012715249121944\ntrain loss_segm: 0.6845586182950418\ntrain loss_shape: 0.03015285796379741\ntrain loss_recon: 0.1641138316709784\ntrain unet DSC: [0.7484592199325562, 0.7272383570671082, 0.08885980397462845, 0.668675422668457]\n\ntest loss: 0.5664202288175241\ntest loss_segm: 0.5555690259505541\ntest loss_shape: 0.01882089715068921\ntest loss_recon: 0.10662999300238414\ntest unet DSC: [0.8173836469650269, 0.8050110340118408, 0.08583758771419525, 0.7307567000389099]\nBest val loss: 0.5615162933484102\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.41899394989014\n\n\nEpoch 145/250\n\ntrain loss: 0.7066810417778885\ntrain loss_segm: 0.6896266107317768\ntrain loss_shape: 0.030431335898045496\ntrain loss_recon: 0.16750122465287584\ntrain unet DSC: [0.7472915053367615, 0.7241116166114807, 0.08899220079183578, 0.663826048374176]\n\ntest loss: 0.5683216200425074\ntest loss_segm: 0.5574937829604516\ntest loss_shape: 0.019181236863518372\ntest loss_recon: 0.10636022849342762\ntest unet DSC: [0.817203938961029, 0.8037275671958923, 0.08585342019796371, 0.729825496673584]\nBest val loss: 0.5615162933484102\nTime: 57.89957070350647\n\n\nEpoch 146/250\n\ntrain loss: 0.7027007565845417\ntrain loss_segm: 0.6858413213793235\ntrain loss_shape: 0.03005082208568914\ntrain loss_recon: 0.16558925111931336\ntrain unet DSC: [0.7471185326576233, 0.7278758883476257, 0.08869094401597977, 0.666553258895874]\n\ntest loss: 0.5655626043295249\ntest loss_segm: 0.5547375465050722\ntest loss_shape: 0.018684067476827364\ntest loss_recon: 0.10638217217265031\ntest unet DSC: [0.8193835616111755, 0.8055910468101501, 0.08579728752374649, 0.7293568253517151]\nBest val loss: 0.5615162933484102\nTime: 57.47244739532471\n\n\nEpoch 147/250\n\ntrain loss: 0.7013754256163971\ntrain loss_segm: 0.6845987935232211\ntrain loss_shape: 0.02982548852981646\ntrain loss_recon: 0.16478382204246672\ntrain unet DSC: [0.7491416335105896, 0.728405773639679, 0.08769521862268448, 0.6658661365509033]\n\ntest loss: 0.5664463815016624\ntest loss_segm: 0.5555974894609207\ntest loss_shape: 0.018598000805538435\ntest loss_recon: 0.10662915548070884\ntest unet DSC: [0.8184323310852051, 0.8041883707046509, 0.08538395911455154, 0.7296477556228638]\nBest val loss: 0.5615162933484102\nTime: 57.14804291725159\n\n\nEpoch 148/250\n\ntrain loss: 0.7026909210636646\ntrain loss_segm: 0.6858584737475915\ntrain loss_shape: 0.030006794659774516\ntrain loss_recon: 0.16532383144750626\ntrain unet DSC: [0.7454400658607483, 0.7278308272361755, 0.0891401469707489, 0.6680825352668762]\n\ntest loss: 0.5640144990040705\ntest loss_segm: 0.5532476527568622\ntest loss_shape: 0.018438105543072406\ntest loss_recon: 0.10582470062833566\ntest unet DSC: [0.8206989765167236, 0.8059378862380981, 0.08580958843231201, 0.7296193242073059]\nBest val loss: 0.5615162933484102\nTime: 57.45151615142822\n\n\nEpoch 149/250\n\ntrain loss: 0.7020178124119963\ntrain loss_segm: 0.6851520251624191\ntrain loss_shape: 0.02993084568105921\ntrain loss_recon: 0.1656648173362394\ntrain unet DSC: [0.7485154271125793, 0.7278499603271484, 0.08913521468639374, 0.6682539582252502]\n\ntest loss: 0.5627806133184677\ntest loss_segm: 0.552061032026242\ntest loss_shape: 0.018273994732552614\ntest loss_recon: 0.10536837052458371\ntest unet DSC: [0.8209167718887329, 0.8068257570266724, 0.08615614473819733, 0.7304754853248596]\nBest val loss: 0.5615162933484102\nTime: 57.4109570980072\n\n\nEpoch 150/250\n\ntrain loss: 0.701869320643099\ntrain loss_segm: 0.6850293878513046\ntrain loss_shape: 0.029995420393592948\ntrain loss_recon: 0.16539979669489438\ntrain unet DSC: [0.7461114525794983, 0.7283474802970886, 0.08872707933187485, 0.6674169898033142]\n\ntest loss: 0.5690932518396622\ntest loss_segm: 0.558174423682384\ntest loss_shape: 0.01912063171561712\ntest loss_recon: 0.10727627546741413\ntest unet DSC: [0.8156996965408325, 0.8028411269187927, 0.08549653738737106, 0.729840874671936]\nBest val loss: 0.5615162933484102\nTime: 57.35971999168396\n\n\nEpoch 151/250\n\ntrain loss: 0.6947690263579164\ntrain loss_segm: 0.6783209624169748\ntrain loss_shape: 0.029498356253003018\ntrain loss_recon: 0.16153082581637782\ntrain unet DSC: [0.7505423426628113, 0.7313807606697083, 0.09035355597734451, 0.6700458526611328]\n\ntest loss: 0.5688399022970444\ntest loss_segm: 0.557937353849411\ntest loss_shape: 0.019495983082705583\ntest loss_recon: 0.10707595839332311\ntest unet DSC: [0.8163477182388306, 0.8039169311523438, 0.08635277301073074, 0.7293161153793335]\nBest val loss: 0.5615162933484102\nTime: 57.007511377334595\n\n\nEpoch 152/250\n\ntrain loss: 0.6973746741497064\ntrain loss_segm: 0.6808909296612197\ntrain loss_shape: 0.02974214032292366\ntrain loss_recon: 0.16186323280953155\ntrain unet DSC: [0.751032292842865, 0.7311290502548218, 0.08922924846410751, 0.6661666035652161]\n\ntest loss: 0.5632274456513233\ntest loss_segm: 0.552489208869445\ntest loss_shape: 0.01861746243845958\ntest loss_recon: 0.10552067109025441\ntest unet DSC: [0.819213330745697, 0.8073592185974121, 0.0863545760512352, 0.7320162653923035]\nBest val loss: 0.5615162933484102\nTime: 57.883607625961304\n\n\nEpoch 153/250\n\ntrain loss: 0.7039960034285919\ntrain loss_segm: 0.6870761970175973\ntrain loss_shape: 0.030167527638281448\ntrain loss_recon: 0.16618132223434087\ntrain unet DSC: [0.7472795248031616, 0.7269832491874695, 0.08747945725917816, 0.6636572480201721]\n\ntest loss: 0.5695017744333316\ntest loss_segm: 0.5586311427446512\ntest loss_shape: 0.01958754386466283\ntest loss_recon: 0.10674752610234114\ntest unet DSC: [0.8156468272209167, 0.8040430545806885, 0.08651507645845413, 0.7283678650856018]\nBest val loss: 0.5615162933484102\nTime: 57.51115417480469\n\n\nEpoch 154/250\n\ntrain loss: 0.6992567407933972\ntrain loss_segm: 0.6826110784011551\ntrain loss_shape: 0.029797506624762014\ntrain loss_recon: 0.16347691228118125\ntrain unet DSC: [0.7498867511749268, 0.7306941747665405, 0.08854760229587555, 0.6672942042350769]\n\ntest loss: 0.5639094060812241\ntest loss_segm: 0.5532270738711724\ntest loss_shape: 0.018688855979305048\ntest loss_recon: 0.1049544269648882\ntest unet DSC: [0.8198328018188477, 0.8054935932159424, 0.08690088242292404, 0.7312348484992981]\nBest val loss: 0.5615162933484102\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.62113165855408\n\n\nEpoch 155/250\n\ntrain loss: 0.7053771535807019\ntrain loss_segm: 0.688433274060865\ntrain loss_shape: 0.030206387768251988\ntrain loss_recon: 0.16641817656876165\ntrain unet DSC: [0.7452329993247986, 0.7276347875595093, 0.0881519466638565, 0.6608725190162659]\n\ntest loss: 0.5651241189394242\ntest loss_segm: 0.5543119311332703\ntest loss_shape: 0.018639893271028996\ntest loss_recon: 0.1062578643934849\ntest unet DSC: [0.819248616695404, 0.8047627210617065, 0.08572367578744888, 0.7307496666908264]\nBest val loss: 0.5615162933484102\nTime: 57.14288830757141\n\n\nEpoch 156/250\n\ntrain loss: 0.6997930390548103\ntrain loss_segm: 0.6831184259698361\ntrain loss_shape: 0.029637042220823372\ntrain loss_recon: 0.163782405443018\ntrain unet DSC: [0.7494859099388123, 0.7283827662467957, 0.08938463777303696, 0.6673071980476379]\n\ntest loss: 0.5626903069324982\ntest loss_segm: 0.5519437270286756\ntest loss_shape: 0.01828907750164851\ntest loss_recon: 0.10563686958108193\ntest unet DSC: [0.8211759924888611, 0.80638188123703, 0.08594465255737305, 0.7319343090057373]\nBest val loss: 0.5615162933484102\nTime: 57.66997742652893\n\n\nEpoch 157/250\n\ntrain loss: 0.7050759200053879\ntrain loss_segm: 0.6881695857530907\ntrain loss_shape: 0.029975179123067405\ntrain loss_recon: 0.16606582852104043\ntrain unet DSC: [0.7462005615234375, 0.7258161902427673, 0.08895144611597061, 0.6646467447280884]\n\ntest loss: 0.5638956404649295\ntest loss_segm: 0.5531130326099885\ntest loss_shape: 0.01836123011815242\ntest loss_recon: 0.10598995985511021\ntest unet DSC: [0.8204009532928467, 0.805359423160553, 0.0856361836194992, 0.7302583456039429]\nBest val loss: 0.5615162933484102\nTime: 57.25481057167053\n\n\nEpoch 158/250\n\ntrain loss: 0.6978474029257328\ntrain loss_segm: 0.6812854978857161\ntrain loss_shape: 0.029421282826062246\ntrain loss_recon: 0.16267693570897548\ntrain unet DSC: [0.7501175999641418, 0.7296229004859924, 0.09007309377193451, 0.6668984889984131]\n\ntest loss: 0.5621634354958167\ntest loss_segm: 0.5514340255504999\ntest loss_shape: 0.018216854319549523\ntest loss_recon: 0.10547242953609197\ntest unet DSC: [0.8217358589172363, 0.8058764338493347, 0.08618912100791931, 0.732218325138092]\nBest val loss: 0.5615162933484102\nTime: 57.46329712867737\n\n\nEpoch 159/250\n\ntrain loss: 0.7018842221815375\ntrain loss_segm: 0.6850580173202708\ntrain loss_shape: 0.030076456480199777\ntrain loss_recon: 0.16525439408761036\ntrain unet DSC: [0.7471341490745544, 0.7256727814674377, 0.08889161795377731, 0.6710090041160583]\n\ntest loss: 0.5624863833953173\ntest loss_segm: 0.551733672618866\ntest loss_shape: 0.01821317836546745\ntest loss_recon: 0.10570584591000508\ntest unet DSC: [0.821738600730896, 0.8062919974327087, 0.08612183481454849, 0.7310633659362793]\nBest val loss: 0.5615162933484102\nTime: 57.216896772384644\n\n\nEpoch 160/250\n\ntrain loss: 0.7006815136233463\ntrain loss_segm: 0.6839419324186784\ntrain loss_shape: 0.029859176247463197\ntrain loss_recon: 0.16440989190264593\ntrain unet DSC: [0.7483727931976318, 0.7284215688705444, 0.08984605222940445, 0.6683717966079712]\n\ntest loss: 0.5623516012460757\ntest loss_segm: 0.5516026386847863\ntest loss_shape: 0.0181234546769888\ntest loss_recon: 0.1056772784735912\ntest unet DSC: [0.8223051428794861, 0.8055185079574585, 0.08639074116945267, 0.7294299006462097]\nBest val loss: 0.5615162933484102\nTime: 57.63400959968567\n\n\nEpoch 161/250\n\ntrain loss: 0.7002405652517005\ntrain loss_segm: 0.6835467079772225\ntrain loss_shape: 0.02958735972171343\ntrain loss_recon: 0.16397985693397402\ntrain unet DSC: [0.7480155825614929, 0.7292441129684448, 0.08938436210155487, 0.6658225655555725]\n\ntest loss: 0.563697568881206\ntest loss_segm: 0.5529373127680558\ntest loss_shape: 0.018560910430283118\ntest loss_recon: 0.10574646723958162\ntest unet DSC: [0.820033609867096, 0.8057034611701965, 0.08612760901451111, 0.7313637137413025]\nBest val loss: 0.5615162933484102\nTime: 57.100905418395996\n\n\nEpoch 162/250\n\ntrain loss: 0.6972809377350385\ntrain loss_segm: 0.6806843495821651\ntrain loss_shape: 0.02981820324129319\ntrain loss_recon: 0.16298406363665302\ntrain unet DSC: [0.7511265277862549, 0.7305413484573364, 0.08834155648946762, 0.6703166961669922]\n\ntest loss: 0.5661788720350999\ntest loss_segm: 0.5552764343909729\ntest loss_shape: 0.01832923961755557\ntest loss_recon: 0.10719143360471114\ntest unet DSC: [0.8200932145118713, 0.8043675422668457, 0.08532775193452835, 0.7261984348297119]\nBest val loss: 0.5615162933484102\nTime: 57.264429330825806\n\n\nEpoch 163/250\n\ntrain loss: 0.7022807390629491\ntrain loss_segm: 0.6854277324827411\ntrain loss_shape: 0.029850354170591772\ntrain loss_recon: 0.16554505547767953\ntrain unet DSC: [0.7471621632575989, 0.7293043732643127, 0.08989852666854858, 0.6635336875915527]\n\ntest loss: 0.5637767903315716\ntest loss_segm: 0.5529690384864807\ntest loss_shape: 0.018302318281852283\ntest loss_recon: 0.106247262408336\ntest unet DSC: [0.8209657073020935, 0.8052942156791687, 0.08581384271383286, 0.7301121950149536]\nBest val loss: 0.5615162933484102\nTime: 57.57035207748413\n\n\nEpoch 164/250\n\ntrain loss: 0.7025232375422611\ntrain loss_segm: 0.6856715090667145\ntrain loss_shape: 0.030069876214654387\ntrain loss_recon: 0.16551027918540978\ntrain unet DSC: [0.747550368309021, 0.7283420562744141, 0.08932530134916306, 0.6657054424285889]\n\ntest loss: 0.5658055062477405\ntest loss_segm: 0.5549451212088267\ntest loss_shape: 0.01857745186545146\ntest loss_recon: 0.10674610132208237\ntest unet DSC: [0.8191487193107605, 0.804424524307251, 0.08534429967403412, 0.729764997959137]\nBest val loss: 0.5615162933484102\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 56.99903631210327\n\n\nEpoch 165/250\n\ntrain loss: 0.702265726614602\ntrain loss_segm: 0.6854536491858808\ntrain loss_shape: 0.029896511943845808\ntrain loss_recon: 0.16513111923314347\ntrain unet DSC: [0.7459080219268799, 0.7279343008995056, 0.08869470655918121, 0.6672213077545166]\n\ntest loss: 0.5643962866220719\ntest loss_segm: 0.5535869185741131\ntest loss_shape: 0.018551337317778513\ntest loss_recon: 0.10623855860187457\ntest unet DSC: [0.8198753595352173, 0.8056303262710571, 0.08574158698320389, 0.7303672432899475]\nBest val loss: 0.5615162933484102\nTime: 58.271682024002075\n\n\nEpoch 166/250\n\ntrain loss: 0.7015350845795644\ntrain loss_segm: 0.6847271760807762\ntrain loss_shape: 0.029680992488431025\ntrain loss_recon: 0.16511098484072503\ntrain unet DSC: [0.7476748824119568, 0.7272818088531494, 0.09064044058322906, 0.6695212125778198]\n\ntest loss: 0.5623569572583224\ntest loss_segm: 0.551666178000279\ntest loss_shape: 0.018510105351033885\ntest loss_recon: 0.1050567773099129\ntest unet DSC: [0.8212669491767883, 0.8065260052680969, 0.08656995743513107, 0.7333071231842041]\nBest val loss: 0.5615162933484102\nTime: 57.20311975479126\n\n\nEpoch 167/250\n\ntrain loss: 0.7047747388670716\ntrain loss_segm: 0.6878187011314344\ntrain loss_shape: 0.030040189883188358\ntrain loss_recon: 0.16655639568461647\ntrain unet DSC: [0.7443374395370483, 0.7270060777664185, 0.0897594764828682, 0.66607266664505]\n\ntest loss: 0.5627844685163254\ntest loss_segm: 0.552030052894201\ntest loss_shape: 0.018332714429841593\ntest loss_recon: 0.10571085270016621\ntest unet DSC: [0.8215523958206177, 0.8060564398765564, 0.08605260401964188, 0.7312880158424377]\nBest val loss: 0.5615162933484102\nTime: 57.79894781112671\n\n\nEpoch 168/250\n\ntrain loss: 0.7040436799767651\ntrain loss_segm: 0.6871395145035997\ntrain loss_shape: 0.030024582366871683\ntrain loss_recon: 0.16603922777915303\ntrain unet DSC: [0.7462930679321289, 0.7273778915405273, 0.08830005675554276, 0.6671618223190308]\n\ntest loss: 0.5625631855084345\ntest loss_segm: 0.5518495478691199\ntest loss_shape: 0.018508814346904937\ntest loss_recon: 0.10528547421861918\ntest unet DSC: [0.8207576274871826, 0.8064853549003601, 0.08631143718957901, 0.7321590781211853]\nBest val loss: 0.5615162933484102\nTime: 57.916136264801025\n\n\nEpoch 169/250\n\ntrain loss: 0.7007578717002386\ntrain loss_segm: 0.684027213839036\ntrain loss_shape: 0.029859149356997464\ntrain loss_recon: 0.16432067374639872\ntrain unet DSC: [0.7486433386802673, 0.7282284498214722, 0.0890546590089798, 0.667464554309845]\n\ntest loss: 0.5637442561296316\ntest loss_segm: 0.5530201769792117\ntest loss_shape: 0.018608671732437916\ntest loss_recon: 0.10537988950426762\ntest unet DSC: [0.8198274374008179, 0.8055528402328491, 0.08635136485099792, 0.7322117686271667]\nBest val loss: 0.5615162933484102\nTime: 57.26558518409729\n\n\nEpoch 170/250\n\ntrain loss: 0.7026309435126148\ntrain loss_segm: 0.6857746262339097\ntrain loss_shape: 0.030067453219826464\ntrain loss_recon: 0.16555644684954535\ntrain unet DSC: [0.7456215620040894, 0.7281220555305481, 0.08808063715696335, 0.668202817440033]\n\ntest loss: 0.5638951063156128\ntest loss_segm: 0.5531620115805895\ntest loss_shape: 0.018649106797499534\ntest loss_recon: 0.10546609921715198\ntest unet DSC: [0.819879412651062, 0.805395245552063, 0.08643722534179688, 0.7319113612174988]\nBest val loss: 0.5615162933484102\nTime: 57.812779664993286\n\n\nEpoch 171/250\n\ntrain loss: 0.7018307319170312\ntrain loss_segm: 0.685109068698521\ntrain loss_shape: 0.02981376469936929\ntrain loss_recon: 0.16423524406891835\ntrain unet DSC: [0.7476793527603149, 0.7296684384346008, 0.08853322267532349, 0.6621861457824707]\n\ntest loss: 0.5607684621444116\ntest loss_segm: 0.5501158894636692\ntest loss_shape: 0.018226738231113322\ntest loss_recon: 0.10470312155592136\ntest unet DSC: [0.8228649497032166, 0.8072841167449951, 0.08678118139505386, 0.7323253154754639]\nBest val loss: 0.5607684621444116\nTime: 57.552950620651245\n\n\nEpoch 172/250\n\ntrain loss: 0.7023078825654863\ntrain loss_segm: 0.685481930458093\ntrain loss_shape: 0.029868044483746532\ntrain loss_recon: 0.16527273072094856\ntrain unet DSC: [0.7459655404090881, 0.728352963924408, 0.0891447514295578, 0.6654229760169983]\n\ntest loss: 0.5606138568658096\ntest loss_segm: 0.549963442943035\ntest loss_shape: 0.018205842862908658\ntest loss_recon: 0.10468352691103251\ntest unet DSC: [0.8227627277374268, 0.8070905804634094, 0.08663246035575867, 0.7325204610824585]\nBest val loss: 0.5606138568658096\nTime: 57.67160081863403\n\n\nEpoch 173/250\n\ntrain loss: 0.7025541612241841\ntrain loss_segm: 0.6858135625908647\ntrain loss_shape: 0.029971139187205443\ntrain loss_recon: 0.1644088714796154\ntrain unet DSC: [0.7482126355171204, 0.7267968058586121, 0.08877384662628174, 0.6654002666473389]\n\ntest loss: 0.5625377358534397\ntest loss_segm: 0.5518177847067515\ntest loss_shape: 0.01846286625816272\ntest loss_recon: 0.10535326456794372\ntest unet DSC: [0.8211471438407898, 0.8061964511871338, 0.08621267974376678, 0.7321658134460449]\nBest val loss: 0.5606138568658096\nTime: 57.58235478401184\n\n\nEpoch 174/250\n\ntrain loss: 0.6988249473933932\ntrain loss_segm: 0.6821748195569727\ntrain loss_shape: 0.029632841319411615\ntrain loss_recon: 0.1635380072306983\ntrain unet DSC: [0.7483392357826233, 0.731824517250061, 0.08976656943559647, 0.6661139130592346]\n\ntest loss: 0.565854977338742\ntest loss_segm: 0.5550068662716792\ntest loss_shape: 0.018678395196986504\ntest loss_recon: 0.1066132471538507\ntest unet DSC: [0.8189588785171509, 0.8038875460624695, 0.08550906926393509, 0.7306285500526428]\nBest val loss: 0.5606138568658096\nTime: 57.66388154029846\n\n\nEpoch 175/250\n\ntrain loss: 0.6997025884404967\ntrain loss_segm: 0.6830200601982165\ntrain loss_shape: 0.029690309015067317\ntrain loss_recon: 0.16385630074935623\ntrain unet DSC: [0.7477708458900452, 0.7297573089599609, 0.0890861377120018, 0.6677817106246948]\n\ntest loss: 0.5637061725824307\ntest loss_segm: 0.5529334736176026\ntest loss_shape: 0.018358575562253978\ntest loss_recon: 0.10589112312747882\ntest unet DSC: [0.8209244012832642, 0.8054905533790588, 0.08594822138547897, 0.7301167249679565]\nBest val loss: 0.5606138568658096\nTime: 57.833966970443726\n\n\nEpoch 176/250\n\ntrain loss: 0.7041130795886245\ntrain loss_segm: 0.6872755679525907\ntrain loss_shape: 0.02996649665616547\ntrain loss_recon: 0.16537849286783346\ntrain unet DSC: [0.7453411221504211, 0.7274616360664368, 0.08922065049409866, 0.6641829013824463]\n\ntest loss: 0.5625225244424282\ntest loss_segm: 0.5517877859947009\ntest loss_shape: 0.018375749078889687\ntest loss_recon: 0.1055098047050146\ntest unet DSC: [0.8211701512336731, 0.8061314821243286, 0.08624981343746185, 0.7321574091911316]\nBest val loss: 0.5606138568658096\nTime: 57.458293199539185\n\n\nEpoch 177/250\n\ntrain loss: 0.7073358727406852\ntrain loss_segm: 0.6903682860392558\ntrain loss_shape: 0.030014244214737716\ntrain loss_recon: 0.16667446890209295\ntrain unet DSC: [0.7420816421508789, 0.7254877090454102, 0.08926623314619064, 0.6636743545532227]\n\ntest loss: 0.5620230826047751\ntest loss_segm: 0.5513572692871094\ntest loss_shape: 0.018430491455663472\ntest loss_recon: 0.10481512785339966\ntest unet DSC: [0.8214877247810364, 0.8062323331832886, 0.08661388605833054, 0.7318045496940613]\nBest val loss: 0.5606138568658096\nTime: 58.230432987213135\n\n\nEpoch 178/250\n\ntrain loss: 0.7008869183968894\ntrain loss_segm: 0.6841747587240194\ntrain loss_shape: 0.029820355657440952\ntrain loss_recon: 0.16413960954811\ntrain unet DSC: [0.748232364654541, 0.7308005690574646, 0.0899612158536911, 0.6631125807762146]\n\ntest loss: 0.5614959796269735\ntest loss_segm: 0.55078148154112\ntest loss_shape: 0.018206134414634645\ntest loss_recon: 0.1053243715984699\ntest unet DSC: [0.822182297706604, 0.8062574863433838, 0.08643835037946701, 0.7323078513145447]\nBest val loss: 0.5606138568658096\nTime: 57.375096559524536\n\n\nEpoch 179/250\n\ntrain loss: 0.7004933551519732\ntrain loss_segm: 0.6837905132317845\ntrain loss_shape: 0.02985115395269439\ntrain loss_recon: 0.1640433080824493\ntrain unet DSC: [0.7476711273193359, 0.7285023927688599, 0.089750275015831, 0.6684548854827881]\n\ntest loss: 0.5635790121860993\ntest loss_segm: 0.5528589678116334\ntest loss_shape: 0.018589649301690932\ntest loss_recon: 0.10534143381011792\ntest unet DSC: [0.8203036785125732, 0.8056660890579224, 0.08632000535726547, 0.7315031886100769]\nBest val loss: 0.5606138568658096\nTime: 57.42109417915344\n\n\nEpoch 180/250\n\ntrain loss: 0.6984972399246844\ntrain loss_segm: 0.6819061235159258\ntrain loss_shape: 0.029913012657456006\ntrain loss_recon: 0.1629198806572564\ntrain unet DSC: [0.747717022895813, 0.73005610704422, 0.08924844861030579, 0.6703184247016907]\n\ntest loss: 0.5635935557194245\ntest loss_segm: 0.5527990804268763\ntest loss_shape: 0.01836110792385462\ntest loss_recon: 0.10610862353291267\ntest unet DSC: [0.8208671808242798, 0.8052824139595032, 0.08584930002689362, 0.73135906457901]\nBest val loss: 0.5606138568658096\nTime: 57.43390917778015\n\n\nEpoch 181/250\n\ntrain loss: 0.700519733225243\ntrain loss_segm: 0.6837982131710535\ntrain loss_shape: 0.029942254901309556\ntrain loss_recon: 0.164220999336884\ntrain unet DSC: [0.7488275766372681, 0.7276803255081177, 0.08909740298986435, 0.6669785976409912]\n\ntest loss: 0.5738398501506219\ntest loss_segm: 0.5628246703209021\ntest loss_shape: 0.020093627656117465\ntest loss_recon: 0.10814249181212524\ntest unet DSC: [0.8144127130508423, 0.8012094497680664, 0.08639400452375412, 0.725732684135437]\nBest val loss: 0.5606138568658096\nTime: 57.56299901008606\n\n\nEpoch 182/250\n\ntrain loss: 0.6991089167474192\ntrain loss_segm: 0.6824591546496258\ntrain loss_shape: 0.029831285010787505\ntrain loss_recon: 0.16351457062778593\ntrain unet DSC: [0.7498462796211243, 0.7308571338653564, 0.0891975611448288, 0.6663172245025635]\n\ntest loss: 0.5631208649048438\ntest loss_segm: 0.5524224944603748\ntest loss_shape: 0.018617267959201947\ntest loss_recon: 0.10512200160286365\ntest unet DSC: [0.8202720284461975, 0.80600905418396, 0.08642135560512543, 0.73246169090271]\nBest val loss: 0.5606138568658096\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.86295175552368\n\n\nEpoch 183/250\n\ntrain loss: 0.7012771469882771\ntrain loss_segm: 0.6845102947724017\ntrain loss_shape: 0.030008177473386632\ntrain loss_recon: 0.1646677525360373\ntrain unet DSC: [0.7483034729957581, 0.7284628748893738, 0.09030118584632874, 0.6658603549003601]\n\ntest loss: 0.5674495605322031\ntest loss_segm: 0.5566093761187333\ntest loss_shape: 0.019130958291964654\ntest loss_recon: 0.10648880488215348\ntest unet DSC: [0.8175768852233887, 0.8037627935409546, 0.0860343724489212, 0.7310487627983093]\nBest val loss: 0.5606138568658096\nTime: 58.292314767837524\n\n\nEpoch 184/250\n\ntrain loss: 0.6974719253521932\ntrain loss_segm: 0.6809028326710568\ntrain loss_shape: 0.029905371297198006\ntrain loss_recon: 0.16270042456026318\ntrain unet DSC: [0.7484557628631592, 0.7307119369506836, 0.08951552957296371, 0.6700359582901001]\n\ntest loss: 0.566733201344808\ntest loss_segm: 0.5559093585381141\ntest loss_shape: 0.018995560275820587\ntest loss_recon: 0.10633890225719182\ntest unet DSC: [0.8184487223625183, 0.8039994835853577, 0.08591625094413757, 0.7306717038154602]\nBest val loss: 0.5606138568658096\nTime: 57.81062412261963\n\n\nEpoch 185/250\n\ntrain loss: 0.7052564534205424\ntrain loss_segm: 0.688312399990951\ntrain loss_shape: 0.029895677997530262\ntrain loss_recon: 0.16645099366484564\ntrain unet DSC: [0.7460348606109619, 0.726561427116394, 0.08870338648557663, 0.6624022722244263]\n\ntest loss: 0.5650024230663593\ntest loss_segm: 0.5541120140980451\ntest loss_shape: 0.018277146662466038\ntest loss_recon: 0.107076344677271\ntest unet DSC: [0.8210156559944153, 0.80485999584198, 0.08549501746892929, 0.72780442237854]\nBest val loss: 0.5606138568658096\nTime: 58.29867959022522\n\n\nEpoch 186/250\n\ntrain loss: 0.7001621059224575\ntrain loss_segm: 0.6834595048729377\ntrain loss_shape: 0.029705969133426116\ntrain loss_recon: 0.16405540553829337\ntrain unet DSC: [0.7467230558395386, 0.7297462821006775, 0.0893130674958229, 0.6687958836555481]\n\ntest loss: 0.5643428992002438\ntest loss_segm: 0.5536167667462275\ntest loss_shape: 0.01871170583539284\ntest loss_recon: 0.10539009718176647\ntest unet DSC: [0.8196839690208435, 0.8053767681121826, 0.08641712367534637, 0.730786919593811]\nBest val loss: 0.5606138568658096\nTime: 57.82196617126465\n\n\nEpoch 187/250\n\ntrain loss: 0.7012639328648772\ntrain loss_segm: 0.684525565067424\ntrain loss_shape: 0.030129933788712267\ntrain loss_recon: 0.16437071335466602\ntrain unet DSC: [0.7482201457023621, 0.7287115454673767, 0.08923745900392532, 0.6675111651420593]\n\ntest loss: 0.5624760542160425\ntest loss_segm: 0.5517191932751582\ntest loss_shape: 0.018224043771624565\ntest loss_recon: 0.10574621812273295\ntest unet DSC: [0.8219863772392273, 0.806603193283081, 0.08598975092172623, 0.7305822968482971]\nBest val loss: 0.5606138568658096\nTime: 57.64735984802246\n\n\nEpoch 188/250\n\ntrain loss: 0.7017887611932392\ntrain loss_segm: 0.6850403743454173\ntrain loss_shape: 0.029860516621059256\ntrain loss_recon: 0.16449781601564795\ntrain unet DSC: [0.747879147529602, 0.7278414964675903, 0.08922816067934036, 0.6646888256072998]\n\ntest loss: 0.5724941132924496\ntest loss_segm: 0.5615223080683978\ntest loss_shape: 0.019799903130684145\ntest loss_recon: 0.10773805290078506\ntest unet DSC: [0.8153010606765747, 0.8017624020576477, 0.08614388853311539, 0.726582944393158]\nBest val loss: 0.5606138568658096\nTime: 57.92212677001953\n\n\nEpoch 189/250\n\ntrain loss: 0.6993160093132453\ntrain loss_segm: 0.6826268762727327\ntrain loss_shape: 0.029779214022940474\ntrain loss_recon: 0.16391342104990272\ntrain unet DSC: [0.7485615611076355, 0.7305505275726318, 0.0893377959728241, 0.6687700748443604]\n\ntest loss: 0.564420528900929\ntest loss_segm: 0.5536150076450446\ntest loss_shape: 0.018565805772176154\ntest loss_recon: 0.10619860362166013\ntest unet DSC: [0.8201146721839905, 0.8049397468566895, 0.0857473760843277, 0.7308262586593628]\nBest val loss: 0.5606138568658096\nTime: 57.15921711921692\n\n\nEpoch 190/250\n\ntrain loss: 0.7022665418401549\ntrain loss_segm: 0.6855143229418164\ntrain loss_shape: 0.0295811117143382\ntrain loss_recon: 0.16456410109619551\ntrain unet DSC: [0.7480640411376953, 0.7279996275901794, 0.09028458595275879, 0.6644423604011536]\n\ntest loss: 0.5623818024610862\ntest loss_segm: 0.5516413129293002\ntest loss_shape: 0.018282788017621406\ntest loss_recon: 0.10557662456845626\ntest unet DSC: [0.8221426010131836, 0.8060576319694519, 0.08614440262317657, 0.7322349548339844]\nBest val loss: 0.5606138568658096\nTime: 57.695839166641235\n\n\nEpoch 191/250\n\ntrain loss: 0.7022046209136142\ntrain loss_segm: 0.6853899933114836\ntrain loss_shape: 0.02999886464846285\ntrain loss_recon: 0.16514640263741528\ntrain unet DSC: [0.7468892335891724, 0.7289438843727112, 0.08908279240131378, 0.6654004454612732]\n\ntest loss: 0.5632230876347958\ntest loss_segm: 0.5524342304620987\ntest loss_shape: 0.018160525422829848\ntest loss_recon: 0.10607247713666695\ntest unet DSC: [0.8213812112808228, 0.8056334257125854, 0.08595030009746552, 0.7290509343147278]\nBest val loss: 0.5606138568658096\nTime: 58.67123508453369\n\n\nEpoch 192/250\n\ntrain loss: 0.701697638140449\ntrain loss_segm: 0.6849818308896656\ntrain loss_shape: 0.02988254112674843\ntrain loss_recon: 0.16416984947421884\ntrain unet DSC: [0.747825562953949, 0.7297934293746948, 0.08844359219074249, 0.6641409397125244]\n\ntest loss: 0.5628530956231631\ntest loss_segm: 0.55208697838661\ntest loss_shape: 0.018180338618082877\ntest loss_recon: 0.10584311235027435\ntest unet DSC: [0.8212310671806335, 0.8063303828239441, 0.08598314970731735, 0.7298226952552795]\nBest val loss: 0.5606138568658096\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.62881517410278\n\n\nEpoch 193/250\n\ntrain loss: 0.7021054502906678\ntrain loss_segm: 0.6852746898237663\ntrain loss_shape: 0.029874486176745048\ntrain loss_recon: 0.16532011068414285\ntrain unet DSC: [0.7474632263183594, 0.726624071598053, 0.0890817865729332, 0.6710368990898132]\n\ntest loss: 0.5642766203635778\ntest loss_segm: 0.5534997483094534\ntest loss_shape: 0.018641333501690473\ntest loss_recon: 0.10590460447546764\ntest unet DSC: [0.8198786377906799, 0.8051568269729614, 0.08600901812314987, 0.7313379049301147]\nBest val loss: 0.5606138568658096\nTime: 57.84734630584717\n\n\nEpoch 194/250\n\ntrain loss: 0.7013509099996542\ntrain loss_segm: 0.6846166381353065\ntrain loss_shape: 0.02989976402796522\ntrain loss_recon: 0.16435271152589895\ntrain unet DSC: [0.7494460344314575, 0.7271745204925537, 0.08924996107816696, 0.6638414263725281]\n\ntest loss: 0.5633860215162619\ntest loss_segm: 0.5526172266556666\ntest loss_shape: 0.018368133009435274\ntest loss_recon: 0.10585113605245566\ntest unet DSC: [0.8208382725715637, 0.8054253458976746, 0.08591648191213608, 0.7320528030395508]\nBest val loss: 0.5606138568658096\nTime: 59.03409123420715\n\n\nEpoch 195/250\n\ntrain loss: 0.6995869109147712\ntrain loss_segm: 0.6829371595684486\ntrain loss_shape: 0.029647259077033663\ntrain loss_recon: 0.1635328082579978\ntrain unet DSC: [0.7479481101036072, 0.7292924523353577, 0.0895947515964508, 0.668350875377655]\n\ntest loss: 0.5623714946783506\ntest loss_segm: 0.5516602091300182\ntest loss_shape: 0.018403663753699034\ntest loss_recon: 0.1052724345563314\ntest unet DSC: [0.8210210800170898, 0.8059707283973694, 0.08633861690759659, 0.7327342629432678]\nBest val loss: 0.5606138568658096\nTime: 57.4724280834198\n\n\nEpoch 196/250\n\ntrain loss: 0.6982717404637155\ntrain loss_segm: 0.6815829967396169\ntrain loss_shape: 0.030044410318796393\ntrain loss_recon: 0.16388301051492932\ntrain unet DSC: [0.7494184970855713, 0.7309117913246155, 0.08932992815971375, 0.6703470945358276]\n\ntest loss: 0.5639694126752707\ntest loss_segm: 0.5531822580557603\ntest loss_shape: 0.018370522616001275\ntest loss_recon: 0.10603456552594136\ntest unet DSC: [0.8204618096351624, 0.804760754108429, 0.08608748018741608, 0.7310414910316467]\nBest val loss: 0.5606138568658096\nTime: 57.36267805099487\n\n\nEpoch 197/250\n\ntrain loss: 0.7009176140344595\ntrain loss_segm: 0.6841785519183436\ntrain loss_shape: 0.029948362031409256\ntrain loss_recon: 0.1643958172843426\ntrain unet DSC: [0.7486002445220947, 0.728209376335144, 0.08860621601343155, 0.6684653759002686]\n\ntest loss: 0.5632667014232049\ntest loss_segm: 0.552546874834941\ntest loss_shape: 0.01853617235349539\ntest loss_recon: 0.10534466621585381\ntest unet DSC: [0.82023024559021, 0.8059971928596497, 0.08629520237445831, 0.7320150136947632]\nBest val loss: 0.5606138568658096\nTime: 57.607505559921265\n\n\nEpoch 198/250\n\ntrain loss: 0.7021107039874113\ntrain loss_segm: 0.685330186086365\ntrain loss_shape: 0.029996103135562394\ntrain loss_recon: 0.1648056150614461\ntrain unet DSC: [0.7484017014503479, 0.7276505827903748, 0.0895281657576561, 0.6654947996139526]\n\ntest loss: 0.5611650095536158\ntest loss_segm: 0.550461124151181\ntest loss_shape: 0.018151808482332107\ntest loss_recon: 0.10522367795690513\ntest unet DSC: [0.8229025602340698, 0.8062518835067749, 0.08648424595594406, 0.7327932715415955]\nBest val loss: 0.5606138568658096\nTime: 57.68785762786865\n\n\nEpoch 199/250\n\ntrain loss: 0.7035582791778106\ntrain loss_segm: 0.6866420650783973\ntrain loss_shape: 0.02992691605387232\ntrain loss_recon: 0.1661694812105049\ntrain unet DSC: [0.7445958256721497, 0.7291271090507507, 0.08968882262706757, 0.6658620834350586]\n\ntest loss: 0.5629971952010424\ntest loss_segm: 0.5522189224377657\ntest loss_shape: 0.018247540610340927\ntest loss_recon: 0.10595795980248696\ntest unet DSC: [0.821618914604187, 0.8054078817367554, 0.08593352884054184, 0.7306435704231262]\nBest val loss: 0.5606138568658096\nTime: 58.1603844165802\n\n\nEpoch 200/250\n\ntrain loss: 0.6992813039429581\ntrain loss_segm: 0.682583491636228\ntrain loss_shape: 0.029976530554645425\ntrain loss_recon: 0.16398049674079387\ntrain unet DSC: [0.7489580512046814, 0.7294876575469971, 0.08929140120744705, 0.6680829524993896]\n\ntest loss: 0.5642449213908269\ntest loss_segm: 0.5534630807546469\ntest loss_shape: 0.018602850775306042\ntest loss_recon: 0.10595810652161256\ntest unet DSC: [0.8198791146278381, 0.8050557374954224, 0.08593688160181046, 0.7309904098510742]\nBest val loss: 0.5606138568658096\nTime: 57.48398733139038\n\n\nEpoch 201/250\n\ntrain loss: 0.7039398724519754\ntrain loss_segm: 0.6870695797702934\ntrain loss_shape: 0.030380874895785427\ntrain loss_recon: 0.16566488439146476\ntrain unet DSC: [0.7478030920028687, 0.7266533374786377, 0.08817031979560852, 0.6658313870429993]\n\ntest loss: 0.5622683992752662\ntest loss_segm: 0.551532527575126\ntest loss_shape: 0.01829878844989416\ntest loss_recon: 0.1055288705497216\ntest unet DSC: [0.8218472003936768, 0.8061399459838867, 0.08616531640291214, 0.7316487431526184]\nBest val loss: 0.5606138568658096\nTime: 57.47086429595947\n\n\nEpoch 202/250\n\ntrain loss: 0.7042049659958368\ntrain loss_segm: 0.6872517296785041\ntrain loss_shape: 0.030038427467210384\ntrain loss_recon: 0.16652852683504926\ntrain unet DSC: [0.7450242042541504, 0.7290226221084595, 0.08797509968280792, 0.6664083003997803]\n\ntest loss: 0.5641406606405209\ntest loss_segm: 0.5534064295964364\ntest loss_shape: 0.018720950811910324\ntest loss_recon: 0.1054702486174229\ntest unet DSC: [0.8201101422309875, 0.8050675988197327, 0.08633901178836823, 0.7317953705787659]\nBest val loss: 0.5606138568658096\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.899897813797\n\n\nEpoch 203/250\n\ntrain loss: 0.6989300948909566\ntrain loss_segm: 0.6823502043380013\ntrain loss_shape: 0.029458034477090535\ntrain loss_recon: 0.1628530977081649\ntrain unet DSC: [0.7502843141555786, 0.731144905090332, 0.08933345973491669, 0.662808895111084]\n\ntest loss: 0.5639240076908698\ntest loss_segm: 0.5531534178134723\ntest loss_shape: 0.018597838015128404\ntest loss_recon: 0.10584615065883367\ntest unet DSC: [0.8202764987945557, 0.8053765892982483, 0.08598534017801285, 0.7315762639045715]\nBest val loss: 0.5606138568658096\nTime: 57.5674843788147\n\n\nEpoch 204/250\n\ntrain loss: 0.7031754821161681\ntrain loss_segm: 0.6863223642488069\ntrain loss_shape: 0.029878440147996704\ntrain loss_recon: 0.16554334665401071\ntrain unet DSC: [0.7462894916534424, 0.7275570631027222, 0.09044055640697479, 0.6662241220474243]\n\ntest loss: 0.5634863368975811\ntest loss_segm: 0.5527069538067548\ntest loss_shape: 0.01830120198428631\ntest loss_recon: 0.10596372311313947\ntest unet DSC: [0.8209773302078247, 0.8051611185073853, 0.08589743077754974, 0.7300124168395996]\nBest val loss: 0.5606138568658096\nTime: 57.31343746185303\n\n\nEpoch 205/250\n\ntrain loss: 0.7010661171206946\ntrain loss_segm: 0.6843364587690257\ntrain loss_shape: 0.029911363037609603\ntrain loss_recon: 0.16430541948427127\ntrain unet DSC: [0.7463864684104919, 0.728580117225647, 0.09046167135238647, 0.6667049527168274]\n\ntest loss: 0.5632940851725065\ntest loss_segm: 0.5525444073554797\ntest loss_shape: 0.018367501763770215\ntest loss_recon: 0.10565999006995788\ntest unet DSC: [0.8211663961410522, 0.8056361675262451, 0.08598434925079346, 0.7309565544128418]\nBest val loss: 0.5606138568658096\nTime: 57.48083209991455\n\n\nEpoch 206/250\n\ntrain loss: 0.6994603947748111\ntrain loss_segm: 0.6827375756788857\ntrain loss_shape: 0.02960443114743957\ntrain loss_recon: 0.16426778794561006\ntrain unet DSC: [0.7489103078842163, 0.7283738255500793, 0.08917980641126633, 0.6717903017997742]\n\ntest loss: 0.5635011356610519\ntest loss_segm: 0.5527643011166499\ntest loss_shape: 0.018448292277753353\ntest loss_recon: 0.10552351224498871\ntest unet DSC: [0.8207070827484131, 0.8051600456237793, 0.08609616756439209, 0.7309132218360901]\nBest val loss: 0.5606138568658096\nTime: 57.40273332595825\n\n\nEpoch 207/250\n\ntrain loss: 0.7015548069265825\ntrain loss_segm: 0.6847977638244629\ntrain loss_shape: 0.030000837354720394\ntrain loss_recon: 0.16457035420816155\ntrain unet DSC: [0.7473453283309937, 0.7291768193244934, 0.08939725905656815, 0.6671210527420044]\n\ntest loss: 0.5629326043984829\ntest loss_segm: 0.5521741517079182\ntest loss_shape: 0.01834508829200879\ntest loss_recon: 0.10574997474367802\ntest unet DSC: [0.8210472464561462, 0.8058153390884399, 0.08596497029066086, 0.7319329380989075]\nBest val loss: 0.5606138568658096\nTime: 58.231918811798096\n\n\nEpoch 208/250\n\ntrain loss: 0.7030253919619548\ntrain loss_segm: 0.6861947627761696\ntrain loss_shape: 0.02997547721702464\ntrain loss_recon: 0.16530875588142419\ntrain unet DSC: [0.7480348944664001, 0.7264239192008972, 0.0893961489200592, 0.6665630340576172]\n\ntest loss: 0.5620427536658752\ntest loss_segm: 0.5512793033550947\ntest loss_shape: 0.01817749275897558\ntest loss_recon: 0.10581674722906871\ntest unet DSC: [0.8223240971565247, 0.8056933283805847, 0.08619651943445206, 0.7311131954193115]\nBest val loss: 0.5606138568658096\nTime: 58.80600690841675\n\n\nEpoch 209/250\n\ntrain loss: 0.6992072606388526\ntrain loss_segm: 0.68252526016175\ntrain loss_shape: 0.029781757453102853\ntrain loss_recon: 0.16384185039544408\ntrain unet DSC: [0.7481940984725952, 0.7293689250946045, 0.08890581876039505, 0.670996367931366]\n\ntest loss: 0.5647819538911184\ntest loss_segm: 0.5539679206334628\ntest loss_shape: 0.018572942616465766\ntest loss_recon: 0.1062830771582249\ntest unet DSC: [0.8196611404418945, 0.8046452403068542, 0.08571036159992218, 0.7308980822563171]\nBest val loss: 0.5606138568658096\nTime: 57.68347382545471\n\n\nEpoch 210/250\n\ntrain loss: 0.6989565472814101\ntrain loss_segm: 0.6823115533665766\ntrain loss_shape: 0.03004848502010484\ntrain loss_recon: 0.16344508490984952\ntrain unet DSC: [0.7481642365455627, 0.7304688692092896, 0.08901475369930267, 0.6691107153892517]\n\ntest loss: 0.5668557217487922\ntest loss_segm: 0.5560273237717457\ntest loss_shape: 0.018984666619545374\ntest loss_recon: 0.10638545042811295\ntest unet DSC: [0.8181574940681458, 0.8039308190345764, 0.08582699298858643, 0.7304267287254333]\nBest val loss: 0.5606138568658096\nTime: 57.31223726272583\n\n\nEpoch 211/250\n\ntrain loss: 0.7016853927811489\ntrain loss_segm: 0.684931400456006\ntrain loss_shape: 0.029877048811014696\ntrain loss_recon: 0.1645522722149197\ntrain unet DSC: [0.7497513294219971, 0.7276505827903748, 0.08915399014949799, 0.6630551218986511]\n\ntest loss: 0.5650944083164899\ntest loss_segm: 0.5542194636968466\ntest loss_shape: 0.01827794463875202\ntest loss_recon: 0.10692160796278562\ntest unet DSC: [0.820118248462677, 0.8049233555793762, 0.08545253425836563, 0.727423369884491]\nBest val loss: 0.5606138568658096\nTime: 58.28973650932312\n\n\nEpoch 212/250\n\ntrain loss: 0.7024425807632978\ntrain loss_segm: 0.6856202792517746\ntrain loss_shape: 0.029688443552372577\ntrain loss_recon: 0.16525419124791138\ntrain unet DSC: [0.7482531666755676, 0.7286413908004761, 0.08969325572252274, 0.6643013954162598]\n\ntest loss: 0.566943727242641\ntest loss_segm: 0.5561386392666743\ntest loss_shape: 0.01905900378448841\ntest loss_recon: 0.10614494110147159\ntest unet DSC: [0.8180418014526367, 0.8040053248405457, 0.08606649935245514, 0.7315031886100769]\nBest val loss: 0.5606138568658096\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 60.39352893829346\n\n\nEpoch 213/250\n\ntrain loss: 0.7024064686479448\ntrain loss_segm: 0.6856239396560041\ntrain loss_shape: 0.03001541492258069\ntrain loss_recon: 0.16482376922629302\ntrain unet DSC: [0.7471553683280945, 0.7271696329116821, 0.08860980719327927, 0.6664502024650574]\n\ntest loss: 0.5626446306705475\ntest loss_segm: 0.5519593870028471\ntest loss_shape: 0.018523055916795365\ntest loss_recon: 0.10500013608580981\ntest unet DSC: [0.8205784559249878, 0.8062391877174377, 0.0866207629442215, 0.7318718433380127]\nBest val loss: 0.5606138568658096\nTime: 58.98107647895813\n\n\nEpoch 214/250\n\ntrain loss: 0.6988694324523588\ntrain loss_segm: 0.6821627186823495\ntrain loss_shape: 0.02973609992974921\ntrain loss_recon: 0.1640935401561894\ntrain unet DSC: [0.7502648830413818, 0.7283578515052795, 0.09062449634075165, 0.6701561808586121]\n\ntest loss: 0.5632371008396149\ntest loss_segm: 0.5525009540411142\ntest loss_shape: 0.01852924684779002\ntest loss_recon: 0.10550858357395881\ntest unet DSC: [0.820676863193512, 0.8057652711868286, 0.08614259958267212, 0.7312595248222351]\nBest val loss: 0.5606138568658096\nTime: 59.70427346229553\n\n\nEpoch 215/250\n\ntrain loss: 0.7034931933578057\ntrain loss_segm: 0.6866314516791815\ntrain loss_shape: 0.03001853453490553\ntrain loss_recon: 0.16561557015380526\ntrain unet DSC: [0.7447170615196228, 0.7299183011054993, 0.08996352553367615, 0.662845253944397]\n\ntest loss: 0.5624154515755482\ntest loss_segm: 0.5516657408995506\ntest loss_shape: 0.018222031470101614\ntest loss_recon: 0.10567486677796413\ntest unet DSC: [0.8220611810684204, 0.8060857057571411, 0.08607076853513718, 0.7311747670173645]\nBest val loss: 0.5606138568658096\nTime: 57.73342251777649\n\n\nEpoch 216/250\n\ntrain loss: 0.6988220761848402\ntrain loss_segm: 0.6821537949616396\ntrain loss_shape: 0.02959501910718936\ntrain loss_recon: 0.16372330430187756\ntrain unet DSC: [0.7502005696296692, 0.7292749881744385, 0.08981408178806305, 0.6679683923721313]\n\ntest loss: 0.564701571678504\ntest loss_segm: 0.5539628557669811\ntest loss_shape: 0.018742821513651274\ntest loss_recon: 0.10551285065519504\ntest unet DSC: [0.8195525407791138, 0.8049132227897644, 0.08632340282201767, 0.7311503887176514]\nBest val loss: 0.5606138568658096\nTime: 58.56755614280701\n\n\nEpoch 217/250\n\ntrain loss: 0.7020021461987798\ntrain loss_segm: 0.6851798212980922\ntrain loss_shape: 0.030236730817705393\ntrain loss_recon: 0.16519959650556498\ntrain unet DSC: [0.7470701336860657, 0.7266681790351868, 0.08926469087600708, 0.6694583296775818]\n\ntest loss: 0.5656131758139684\ntest loss_segm: 0.5548465465887998\ntest loss_shape: 0.018778372436570816\ntest loss_recon: 0.10578838917307365\ntest unet DSC: [0.8188475370407104, 0.8041568398475647, 0.08626268804073334, 0.7315652370452881]\nBest val loss: 0.5606138568658096\nTime: 58.649367809295654\n\n\nEpoch 218/250\n\ntrain loss: 0.7044325578439085\ntrain loss_segm: 0.6875161468228207\ntrain loss_shape: 0.030404069413772866\ntrain loss_recon: 0.16612372368196898\ntrain unet DSC: [0.745700478553772, 0.7261838912963867, 0.08912594616413116, 0.6653454899787903]\n\ntest loss: 0.5635207173151847\ntest loss_segm: 0.5527705565477029\ntest loss_shape: 0.018429092943477325\ntest loss_recon: 0.10565867571112438\ntest unet DSC: [0.8208154439926147, 0.8056374788284302, 0.0860772654414177, 0.730797290802002]\nBest val loss: 0.5606138568658096\nTime: 65.63740158081055\n\n\nEpoch 219/250\n\ntrain loss: 0.6980903703200666\ntrain loss_segm: 0.6814485131939755\ntrain loss_shape: 0.029665431219942962\ntrain loss_recon: 0.16345206091675576\ntrain unet DSC: [0.7509734034538269, 0.7296035885810852, 0.08852318674325943, 0.6682990789413452]\n\ntest loss: 0.5622964363831741\ntest loss_segm: 0.5515347382961175\ntest loss_shape: 0.01812034671027691\ntest loss_recon: 0.10580494856605163\ntest unet DSC: [0.822138786315918, 0.8060660362243652, 0.08612843602895737, 0.730125904083252]\nBest val loss: 0.5606138568658096\nTime: 67.20577597618103\n\n\nEpoch 220/250\n\ntrain loss: 0.702906383743769\ntrain loss_segm: 0.6860174080239066\ntrain loss_shape: 0.02992125024099516\ntrain loss_recon: 0.16589763175837602\ntrain unet DSC: [0.7457430958747864, 0.7286388278007507, 0.08935678750276566, 0.6666579842567444]\n\ntest loss: 0.5647043853234022\ntest loss_segm: 0.5539023249577253\ntest loss_shape: 0.018463010971362773\ntest loss_recon: 0.10617429715318558\ntest unet DSC: [0.8197624087333679, 0.804171621799469, 0.0857083648443222, 0.7305826544761658]\nBest val loss: 0.5606138568658096\nTime: 57.24927616119385\n\n\nEpoch 221/250\n\ntrain loss: 0.702586022736151\ntrain loss_segm: 0.6858076196682604\ntrain loss_shape: 0.029920182709546782\ntrain loss_recon: 0.16479200850936432\ntrain unet DSC: [0.7466341257095337, 0.7301933169364929, 0.09013204276561737, 0.6624957919120789]\n\ntest loss: 0.5663284880992694\ntest loss_segm: 0.5555345599467938\ntest loss_shape: 0.019154689895610016\ntest loss_recon: 0.106023858468502\ntest unet DSC: [0.8183375000953674, 0.8043918013572693, 0.08641103655099869, 0.7315958738327026]\nBest val loss: 0.5606138568658096\nTime: 58.056116580963135\n\n\nEpoch 222/250\n\ntrain loss: 0.7007818874679034\ntrain loss_segm: 0.6840808817857429\ntrain loss_shape: 0.029650028771426105\ntrain loss_recon: 0.1640450298031674\ntrain unet DSC: [0.7478916049003601, 0.7282422780990601, 0.08903040736913681, 0.6669278740882874]\n\ntest loss: 0.5624951536838825\ntest loss_segm: 0.5518053770065308\ntest loss_shape: 0.01841037505521224\ntest loss_recon: 0.1050566978370532\ntest unet DSC: [0.8212324380874634, 0.8061084151268005, 0.08642227947711945, 0.7311771512031555]\nBest val loss: 0.5606138568658096\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.49965858459473\n\nValidation loss stopped to decrease for 50 epochs. Training terminated.\nBest epoch: 172\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678189134472
        }
      },
      "id": "079d26b5-2f36-4b51-b754-e20b153a800a"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.1, 0.01)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/250\n\ntrain loss: 2.02580789221993\ntrain loss_segm: 1.9961237688607807\ntrain loss_shape: 0.20212556308583368\ntrain loss_recon: 0.9471587489677381\ntrain unet DSC: [0.002621248597279191, 0.0036599356681108475, 0.0003427632909733802, 0.00010464298975421116]\n\ntest loss: 2.017950143569555\ntest loss_segm: 1.9958823827596812\ntest loss_shape: 0.1261158463282463\ntest loss_recon: 0.9456169070341648\ntest unet DSC: [0.0025205116253346205, 0.003937106113880873, 0.00029245304176583886, 0.00016121016233228147]\nBest val loss: 2.017950143569555\nTime: 58.691466093063354\n\n\nEpoch 2/250\n\ntrain loss: 2.0180559580839135\ntrain loss_segm: 1.9958300190635874\ntrain loss_shape: 0.12788370044171055\ntrain loss_recon: 0.9437563585329659\ntrain unet DSC: [0.002219975693151355, 0.003512290772050619, 0.00021634955191984773, 0.00026937341317534447]\n\ntest loss: 2.0172505256457205\ntest loss_segm: 1.995748153099647\ntest loss_shape: 0.12021416234664428\ntest loss_recon: 0.9480951061615577\ntest unet DSC: [0.0026979923713952303, 0.004832167644053698, 0.00024723203387111425, 0.0002503732102923095]\nBest val loss: 2.0172505256457205\nTime: 57.94921565055847\n\n\nEpoch 3/250\n\ntrain loss: 2.0159784751602365\ntrain loss_segm: 1.9953024741969532\ntrain loss_shape: 0.11303135832842393\ntrain loss_recon: 0.9372864652283585\ntrain unet DSC: [0.0021997003350406885, 0.004793573636561632, 0.0001295706897508353, 0.0005217954749241471]\n\ntest loss: 2.01333640783261\ntest loss_segm: 1.9949379914846175\ntest loss_shape: 0.09371316184600194\ntest loss_recon: 0.9027066215490683\ntest unet DSC: [0.002292680786922574, 0.006547405384480953, 9.985706128645688e-05, 0.0007006048108451068]\nBest val loss: 2.01333640783261\nTime: 58.29121732711792\n\n\nEpoch 4/250\n\ntrain loss: 2.013132056857966\ntrain loss_segm: 1.9943720200393773\ntrain loss_shape: 0.11193905006858366\ntrain loss_recon: 0.7566133281098136\ntrain unet DSC: [0.0020684644114226103, 0.012012326158583164, 0.00010011214180849493, 0.0009627527324482799]\n\ntest loss: 2.007745718344664\ntest loss_segm: 1.9911374954076915\ntest loss_shape: 0.09516226557584909\ntest loss_recon: 0.7091999481885861\ntest unet DSC: [0.0022973557934165, 0.056068725883960724, 6.515363929793239e-05, 0.0014793529408052564]\nBest val loss: 2.007745718344664\nTime: 57.413124561309814\n\n\nEpoch 5/250\n\ntrain loss: 1.9375394937358326\ntrain loss_segm: 1.9160983343667621\ntrain loss_shape: 0.1260203492792347\ntrain loss_recon: 0.8839123697975014\ntrain unet DSC: [0.002428411040455103, 0.35926756262779236, 0.00014638117863796651, 0.0010539870709180832]\n\ntest loss: 1.9211955009362636\ntest loss_segm: 1.901661487726065\ntest loss_shape: 0.10364287766890648\ntest loss_recon: 0.9169730100876246\ntest unet DSC: [0.0024179916363209486, 0.3810434937477112, 0.00016764803149271756, 0.0012380961561575532]\nBest val loss: 1.9211955009362636\nTime: 58.030463457107544\n\n\nEpoch 6/250\n\ntrain loss: 1.8846390956564794\ntrain loss_segm: 1.8635820074926448\ntrain loss_shape: 0.11889368286238441\ntrain loss_recon: 0.9167723048336899\ntrain unet DSC: [0.0023134653456509113, 0.5380107760429382, 0.00017701485194265842, 0.0010427671950310469]\n\ntest loss: 1.8569105099409053\ntest loss_segm: 1.837891269952823\ntest loss_shape: 0.10006289050365105\ntest loss_recon: 0.901293641481644\ntest unet DSC: [0.0020063999108970165, 0.6366629600524902, 0.0001579910604050383, 0.001032005064189434]\nBest val loss: 1.8569105099409053\nTime: 57.7268283367157\n\n\nEpoch 7/250\n\ntrain loss: 1.8587602301488948\ntrain loss_segm: 1.8398325631890116\ntrain loss_shape: 0.11574697513369066\ntrain loss_recon: 0.7352972094771228\ntrain unet DSC: [0.0016874580178409815, 0.6276238560676575, 0.00015342741971835494, 0.0020149466581642628]\n\ntest loss: 1.8399740946598542\ntest loss_segm: 1.825266190064259\ntest loss_shape: 0.08991028139224419\ntest loss_recon: 0.571686938787118\ntest unet DSC: [0.001289459876716137, 0.6783906817436218, 0.00016274687368422747, 0.0032707261852920055]\nBest val loss: 1.8399740946598542\nTime: 57.7937388420105\n\n\nEpoch 8/250\n\ntrain loss: 1.8435191972346245\ntrain loss_segm: 1.8270459522174884\ntrain loss_shape: 0.10807897424018836\ntrain loss_recon: 0.5665346387820908\ntrain unet DSC: [0.0011090643238276243, 0.6433578133583069, 0.0001298412389587611, 0.03423643112182617]\n\ntest loss: 1.791071341587947\ntest loss_segm: 1.7744961243409376\ntest loss_shape: 0.09411914799457942\ntest loss_recon: 0.7163303677852337\ntest unet DSC: [0.001214522635564208, 0.6588855385780334, 0.00014362341607920825, 0.24411338567733765]\nBest val loss: 1.791071341587947\nTime: 57.36054015159607\n\n\nEpoch 9/250\n\ntrain loss: 1.7352814206594154\ntrain loss_segm: 1.7175535069236272\ntrain loss_shape: 0.11375260164466085\ntrain loss_recon: 0.6352652296235289\ntrain unet DSC: [0.0012133704731240869, 0.6505019664764404, 0.00015619609621353447, 0.47907567024230957]\n\ntest loss: 1.8140872288972905\ntest loss_segm: 1.79619174125867\ntest loss_shape: 0.09326147498228611\ntest loss_recon: 0.8569347048417116\ntest unet DSC: [0.0012518877629190683, 0.5559055209159851, 0.00016754395619500428, 0.23742331564426422]\nBest val loss: 1.791071341587947\nTime: 57.78788948059082\n\n\nEpoch 10/250\n\ntrain loss: 1.699631487266927\ntrain loss_segm: 1.6838875601563272\ntrain loss_shape: 0.10459420485775682\ntrain loss_recon: 0.5284511658209788\ntrain unet DSC: [0.0011080573312938213, 0.6674729585647583, 0.00014515001385007054, 0.5785109996795654]\n\ntest loss: 1.6957510984860933\ntest loss_segm: 1.6815745127506745\ntest loss_shape: 0.08385018908824676\ntest loss_recon: 0.5791568878369454\ntest unet DSC: [0.0010866111842915416, 0.6965922713279724, 0.00012384839646983892, 0.5487121939659119]\nBest val loss: 1.6957510984860933\nTime: 57.986762285232544\n\n\nEpoch 11/250\n\ntrain loss: 1.6906480691100978\ntrain loss_segm: 1.6759289910521689\ntrain loss_shape: 0.09991793254319625\ntrain loss_recon: 0.4727279856046544\ntrain unet DSC: [0.000982433557510376, 0.6710202097892761, 0.00012290431186556816, 0.5941402912139893]\n\ntest loss: 1.695527856166546\ntest loss_segm: 1.6811094131225195\ntest loss_shape: 0.08453114521809113\ntest loss_recon: 0.5965342964881506\ntest unet DSC: [0.0008934867801144719, 0.6714271306991577, 0.00013063829101156443, 0.5577963590621948]\nBest val loss: 1.695527856166546\nTime: 58.261898040771484\n\n\nEpoch 12/250\n\ntrain loss: 1.6790966421742983\ntrain loss_segm: 1.664190381388121\ntrain loss_shape: 0.10309923044111155\ntrain loss_recon: 0.4596332907676697\ntrain unet DSC: [0.000811527599580586, 0.677714467048645, 9.655826579546556e-05, 0.6087843775749207]\n\ntest loss: 1.6335485042669835\ntest loss_segm: 1.621436290251903\ntest loss_shape: 0.08455009777576496\ntest loss_recon: 0.3657211137887759\ntest unet DSC: [0.0006711386376991868, 0.7538310885429382, 6.953994306968525e-05, 0.6623081564903259]\nBest val loss: 1.6335485042669835\nTime: 57.97654056549072\n\n\nEpoch 13/250\n\ntrain loss: 1.6300148828120171\ntrain loss_segm: 1.6138571749759625\ntrain loss_shape: 0.11060982046625283\ntrain loss_recon: 0.5096723399584806\ntrain unet DSC: [0.0005237794248387218, 0.6760493516921997, 7.290406938409433e-05, 0.617962121963501]\n\ntest loss: 1.532702271754925\ntest loss_segm: 1.5243399785115168\ntest loss_shape: 0.0577981673563138\ntest loss_recon: 0.2582469903505765\ntest unet DSC: [0.0004822204355150461, 0.7321474552154541, 8.354446617886424e-05, 0.6100652813911438]\nBest val loss: 1.532702271754925\nTime: 58.109394550323486\n\n\nEpoch 14/250\n\ntrain loss: 1.48041587539866\ntrain loss_segm: 1.4711281959014604\ntrain loss_shape: 0.06362796438079846\ntrain loss_recon: 0.2924880361821078\ntrain unet DSC: [0.0003825599851552397, 0.608618438243866, 6.338432285701856e-05, 0.6181395053863525]\n\ntest loss: 1.3734801732576811\ntest loss_segm: 1.3661162914373937\ntest loss_shape: 0.05039845660137825\ntest loss_recon: 0.23240416936385325\ntest unet DSC: [0.0003770316834561527, 0.5475452542304993, 5.2537743613356724e-05, 0.6525450348854065]\nBest val loss: 1.3734801732576811\nTime: 58.614612102508545\n\n\nEpoch 15/250\n\ntrain loss: 1.3560210157044326\ntrain loss_segm: 1.3484531414659717\ntrain loss_shape: 0.05221030211571274\ntrain loss_recon: 0.23468554133101355\ntrain unet DSC: [0.00037128073745407164, 0.5087026953697205, 7.009576074779034e-05, 0.6152773499488831]\n\ntest loss: 1.2582839421736889\ntest loss_segm: 1.2518169176884186\ntest loss_shape: 0.04477626567658705\ntest loss_recon: 0.1989396978647281\ntest unet DSC: [0.00037280924152582884, 0.5279431343078613, 5.3573250625049695e-05, 0.6731641292572021]\nBest val loss: 1.2582839421736889\nTime: 58.090484619140625\n\n\nEpoch 16/250\n\ntrain loss: 1.2542828804329982\ntrain loss_segm: 1.2472133364858506\ntrain loss_shape: 0.049293229662919345\ntrain loss_recon: 0.21402187852919857\ntrain unet DSC: [0.00028998410562053323, 0.49445071816444397, 7.174629718065262e-05, 0.6364861130714417]\n\ntest loss: 1.1927701983696375\ntest loss_segm: 1.1860038393583052\ntest loss_shape: 0.04691911187882607\ntest loss_recon: 0.20744513089840227\ntest unet DSC: [0.00022881542099639773, 0.481121301651001, 3.0031780624995008e-05, 0.6405505537986755]\nBest val loss: 1.1927701983696375\nTime: 58.154500007629395\n\n\nEpoch 17/250\n\ntrain loss: 1.178172072655038\ntrain loss_segm: 1.1715778772589527\ntrain loss_shape: 0.04593618207057066\ntrain loss_recon: 0.2000569402228428\ntrain unet DSC: [0.00025189563166350126, 0.499329149723053, 7.247873872984201e-05, 0.6730997562408447]\n\ntest loss: 1.0907815190461965\ntest loss_segm: 1.0854812264442444\ntest loss_shape: 0.03771711737872698\ntest loss_recon: 0.15285831928635255\ntest unet DSC: [0.00020606898760888726, 0.5387991666793823, 4.509964128374122e-05, 0.711020827293396]\nBest val loss: 1.0907815190461965\nTime: 58.06877684593201\n\n\nEpoch 18/250\n\ntrain loss: 1.1437642574310303\ntrain loss_segm: 1.137325483786909\ntrain loss_shape: 0.04464339184327216\ntrain loss_recon: 0.19744364992727206\ntrain unet DSC: [0.00025544935488142073, 0.5027445554733276, 7.059252675389871e-05, 0.6714405417442322]\n\ntest loss: 1.062652804912665\ntest loss_segm: 1.0573783364051428\ntest loss_shape: 0.037633517470497355\ntest loss_recon: 0.15111184502259278\ntest unet DSC: [0.00024402592680417, 0.5483282804489136, 4.244730735081248e-05, 0.7030371427536011]\nBest val loss: 1.062652804912665\nTime: 58.020190715789795\n\n\nEpoch 19/250\n\ntrain loss: 1.1166060804566251\ntrain loss_segm: 1.1102842413171936\ntrain loss_shape: 0.043693532306653786\ntrain loss_recon: 0.1952489771043198\ntrain unet DSC: [0.00026701734168455005, 0.5085650086402893, 6.318372470559552e-05, 0.6773229837417603]\n\ntest loss: 1.0734877189000447\ntest loss_segm: 1.0676694558216975\ntest loss_shape: 0.04125256516421453\ntest loss_recon: 0.16929995641112328\ntest unet DSC: [0.0002549595956224948, 0.5276831388473511, 3.5023393138544634e-05, 0.6625406742095947]\nBest val loss: 1.062652804912665\nTime: 77.57614231109619\n\n\nEpoch 20/250\n\ntrain loss: 1.0944759298728992\ntrain loss_segm: 1.0883060078832167\ntrain loss_shape: 0.04250024171852613\ntrain loss_recon: 0.19198970317463332\ntrain unet DSC: [0.00026858641649596393, 0.516457736492157, 6.470296648330986e-05, 0.6792047023773193]\n\ntest loss: 1.0086143536445422\ntest loss_segm: 1.003753764507098\ntest loss_shape: 0.03438151723299271\ntest loss_recon: 0.14224358036732063\ntest unet DSC: [0.00022267493477556854, 0.5645542740821838, 4.492505468078889e-05, 0.7355369329452515]\nBest val loss: 1.0086143536445422\nTime: 57.26924157142639\n\n\nEpoch 21/250\n\ntrain loss: 1.0760453260397609\ntrain loss_segm: 1.0699350120900553\ntrain loss_shape: 0.042021431264620795\ntrain loss_recon: 0.1908165240966821\ntrain unet DSC: [0.0002701823250390589, 0.5195052027702332, 6.260187365114689e-05, 0.6860057711601257]\n\ntest loss: 1.017471535083575\ntest loss_segm: 1.0122321889950678\ntest loss_shape: 0.0368228025543384\ntest loss_recon: 0.15570611296555933\ntest unet DSC: [0.00023003120440989733, 0.5488623380661011, 4.420199547894299e-05, 0.6988393068313599]\nBest val loss: 1.0086143536445422\nTime: 57.43258833885193\n\n\nEpoch 22/250\n\ntrain loss: 1.058464159693899\ntrain loss_segm: 1.0525478184977664\ntrain loss_shape: 0.04052886405747525\ntrain loss_recon: 0.1863452442273309\ntrain unet DSC: [0.0002733462315518409, 0.5305065512657166, 6.282160029513761e-05, 0.6828234791755676]\n\ntest loss: 0.987643573528681\ntest loss_segm: 0.9828655994855441\ntest loss_shape: 0.033648583464897595\ntest loss_recon: 0.14131200867585647\ntest unet DSC: [0.00022275450464803725, 0.564816415309906, 2.7473945010569878e-05, 0.717227578163147]\nBest val loss: 0.987643573528681\nTime: 57.57804274559021\n\n\nEpoch 23/250\n\ntrain loss: 1.0502373674247838\ntrain loss_segm: 1.0442738461343548\ntrain loss_shape: 0.04087722435876538\ntrain loss_recon: 0.18758078466487837\ntrain unet DSC: [0.0002738587209023535, 0.5307937860488892, 5.857249198015779e-05, 0.6878243088722229]\n\ntest loss: 0.9647928720865494\ntest loss_segm: 0.960245376978165\ntest loss_shape: 0.03127378134582287\ntest loss_recon: 0.14201182996233305\ntest unet DSC: [0.00015129950770642608, 0.5645846724510193, 3.222124360036105e-05, 0.7363815307617188]\nBest val loss: 0.9647928720865494\nTime: 57.427807569503784\n\n\nEpoch 24/250\n\ntrain loss: 1.0441597402095795\ntrain loss_segm: 1.0382862736152698\ntrain loss_shape: 0.039941104034645646\ntrain loss_recon: 0.18793568165996408\ntrain unet DSC: [0.0002638354490045458, 0.5274443626403809, 5.2208204579073936e-05, 0.685922384262085]\n\ntest loss: 0.972701256091778\ntest loss_segm: 0.9679130468613062\ntest loss_shape: 0.0327309659944895\ntest loss_recon: 0.15151165320705146\ntest unet DSC: [0.00016952089208643883, 0.5640084743499756, 2.8890319299534895e-05, 0.7053751945495605]\nBest val loss: 0.9647928720865494\nTime: 58.25101327896118\n\n\nEpoch 25/250\n\ntrain loss: 1.0349811772002449\ntrain loss_segm: 1.0290902045708668\ntrain loss_shape: 0.04012905120189431\ntrain loss_recon: 0.18780681028773513\ntrain unet DSC: [0.0002684022474568337, 0.5341170430183411, 5.6518259953008965e-05, 0.6862583756446838]\n\ntest loss: 0.9665570778724475\ntest loss_segm: 0.9620353090457427\ntest loss_shape: 0.030682088759465095\ntest loss_recon: 0.1453561089359797\ntest unet DSC: [0.00015251639706548303, 0.5628756284713745, 3.1760941055836156e-05, 0.6899142861366272]\nBest val loss: 0.9647928720865494\nTime: 58.604170083999634\n\n\nEpoch 26/250\n\ntrain loss: 1.027766712104218\ntrain loss_segm: 1.021924664325352\ntrain loss_shape: 0.03979108389467001\ntrain loss_recon: 0.18629435100887395\ntrain unet DSC: [0.00026934596826322377, 0.5342893004417419, 5.5608892580494285e-05, 0.6850947141647339]\n\ntest loss: 0.945607935770964\ntest loss_segm: 0.9409489050889627\ntest loss_shape: 0.032708034970057316\ntest loss_recon: 0.13882292186220488\ntest unet DSC: [0.00019382033497095108, 0.574221670627594, 3.087516961386427e-05, 0.725273609161377]\nBest val loss: 0.945607935770964\nTime: 58.49166798591614\n\n\nEpoch 27/250\n\ntrain loss: 1.0214046805719785\ntrain loss_segm: 1.0156907190250446\ntrain loss_shape: 0.038782953709081004\ntrain loss_recon: 0.18356647663101366\ntrain unet DSC: [0.0002524221199564636, 0.5358389616012573, 5.0284746976103634e-05, 0.6839618682861328]\n\ntest loss: 0.9235836863517761\ntest loss_segm: 0.9194241899710435\ntest loss_shape: 0.02837723799240895\ntest loss_recon: 0.13217743954215294\ntest unet DSC: [0.00015516490384470671, 0.5754901170730591, 2.489932558091823e-05, 0.7471774220466614]\nBest val loss: 0.9235836863517761\nTime: 57.970656633377075\n\n\nEpoch 28/250\n\ntrain loss: 1.0114665725563146\ntrain loss_segm: 1.005801567548438\ntrain loss_shape: 0.03843356235117852\ntrain loss_recon: 0.18216475429414195\ntrain unet DSC: [0.00027339899679645896, 0.536954939365387, 5.07461627421435e-05, 0.6919825077056885]\n\ntest loss: 0.9287444230837699\ntest loss_segm: 0.9242246441352062\ntest loss_shape: 0.031714864600545324\ntest loss_recon: 0.13482860055489418\ntest unet DSC: [0.0001946006523212418, 0.5871475338935852, 2.8327498512226157e-05, 0.7328086495399475]\nBest val loss: 0.9235836863517761\nTime: 57.63894605636597\n\n\nEpoch 29/250\n\ntrain loss: 1.0090530593938465\ntrain loss_segm: 1.0033336730697486\ntrain loss_shape: 0.038736261392035815\ntrain loss_recon: 0.18457585837267623\ntrain unet DSC: [0.00027501521981321275, 0.5394876003265381, 5.3417301387526095e-05, 0.689998984336853]\n\ntest loss: 0.949561704427768\ntest loss_segm: 0.9447268706101638\ntest loss_shape: 0.033341359156064496\ntest loss_recon: 0.15006972046998832\ntest unet DSC: [0.00023396227334160358, 0.5829820036888123, 3.72136892110575e-05, 0.7020106911659241]\nBest val loss: 0.9235836863517761\nTime: 58.01116991043091\n\n\nEpoch 30/250\n\ntrain loss: 1.0073145250730877\ntrain loss_segm: 1.0016876564750188\ntrain loss_shape: 0.03781840609551608\ntrain loss_recon: 0.18450278883116156\ntrain unet DSC: [0.0002589808136690408, 0.5408557057380676, 5.369996506487951e-05, 0.6902544498443604]\n\ntest loss: 0.9199245266425304\ntest loss_segm: 0.915774785555326\ntest loss_shape: 0.028342295008210037\ntest loss_recon: 0.13155112606592667\ntest unet DSC: [0.00015916545817162842, 0.5761204957962036, 2.4766113710938953e-05, 0.727111279964447]\nBest val loss: 0.9199245266425304\nTime: 57.198665618896484\n\n\nEpoch 31/250\n\ntrain loss: 0.9994667474227615\ntrain loss_segm: 0.9938373576991165\ntrain loss_shape: 0.03819413628146241\ntrain loss_recon: 0.18099745202668105\ntrain unet DSC: [0.0002754098386503756, 0.5447985529899597, 5.112379221827723e-05, 0.6904597282409668]\n\ntest loss: 0.9277880650300246\ntest loss_segm: 0.9231663376857073\ntest loss_shape: 0.031260407028289944\ntest loss_recon: 0.14956863654347566\ntest unet DSC: [0.00014913656923454255, 0.5782647728919983, 1.562289435241837e-05, 0.7254225611686707]\nBest val loss: 0.9199245266425304\nTime: 58.12145686149597\n\n\nEpoch 32/250\n\ntrain loss: 0.9997560638415662\ntrain loss_segm: 0.9941064892690393\ntrain loss_shape: 0.03819506298279083\ntrain loss_recon: 0.18300669112160237\ntrain unet DSC: [0.00026635051472112536, 0.5445078611373901, 5.04915660712868e-05, 0.6890542507171631]\n\ntest loss: 0.9060947757500869\ntest loss_segm: 0.9018757419708447\ntest loss_shape: 0.029159322906381045\ntest loss_recon: 0.13031033483835366\ntest unet DSC: [0.00018633354920893908, 0.5884733200073242, 2.626671084726695e-05, 0.7377380728721619]\nBest val loss: 0.9060947757500869\nTime: 57.91024351119995\n\n\nEpoch 33/250\n\ntrain loss: 0.9916306617139261\ntrain loss_segm: 0.9861056921602804\ntrain loss_shape: 0.03725794848950603\ntrain loss_recon: 0.17991744679740712\ntrain unet DSC: [0.00027055409736931324, 0.5494266748428345, 4.795541462954134e-05, 0.6941483616828918]\n\ntest loss: 0.908510990631886\ntest loss_segm: 0.9043000294612005\ntest loss_shape: 0.02851181171643428\ntest loss_recon: 0.135978296208076\ntest unet DSC: [0.00013992107415106148, 0.5774989724159241, 1.5193852959782816e-05, 0.7393561005592346]\nBest val loss: 0.9060947757500869\nTime: 57.75830626487732\n\n\nEpoch 34/250\n\ntrain loss: 0.9925409861003296\ntrain loss_segm: 0.9869095724594744\ntrain loss_shape: 0.03805046618173394\ntrain loss_recon: 0.18263640547100501\ntrain unet DSC: [0.0002810659643728286, 0.5468062162399292, 5.3742085583508015e-05, 0.6960657238960266]\n\ntest loss: 0.9068506344770774\ntest loss_segm: 0.9025218532635615\ntest loss_shape: 0.029485238524965752\ntest loss_recon: 0.1380259450047444\ntest unet DSC: [0.0001507002889411524, 0.5851531028747559, 2.2319747586152516e-05, 0.7433164715766907]\nBest val loss: 0.9060947757500869\nTime: 57.643789529800415\n\n\nEpoch 35/250\n\ntrain loss: 0.9910331247728082\ntrain loss_segm: 0.9854219072981726\ntrain loss_shape: 0.037916961739051944\ntrain loss_recon: 0.18195185980087594\ntrain unet DSC: [0.00026426755357533693, 0.5474314093589783, 5.2636063628597185e-05, 0.6913692355155945]\n\ntest loss: 0.8734578658372928\ntest loss_segm: 0.869905004134545\ntest loss_shape: 0.023983790706365537\ntest loss_recon: 0.11544844823387954\ntest unet DSC: [0.00011758351320168003, 0.6041401624679565, 2.0257695723557845e-05, 0.7620661854743958]\nBest val loss: 0.8734578658372928\nTime: 57.53818988800049\n\n\nEpoch 36/250\n\ntrain loss: 0.9896902076051205\ntrain loss_segm: 0.984138507254516\ntrain loss_shape: 0.037221761903713776\ntrain loss_recon: 0.18295222803761688\ntrain unet DSC: [0.00025709709734655917, 0.5476016402244568, 4.7910511057125404e-05, 0.6954771876335144]\n\ntest loss: 0.8754140245608795\ntest loss_segm: 0.8718057641616235\ntest loss_shape: 0.024311752727207463\ntest loss_recon: 0.11770891293119161\ntest unet DSC: [0.00012035988038405776, 0.6049339771270752, 1.7906617358676158e-05, 0.7568084597587585]\nBest val loss: 0.8734578658372928\nTime: 58.282708168029785\n\n\nEpoch 37/250\n\ntrain loss: 0.9822514008117628\ntrain loss_segm: 0.9767741621295108\ntrain loss_shape: 0.036881849287620075\ntrain loss_recon: 0.17890584148183653\ntrain unet DSC: [0.00026929908199235797, 0.5515626072883606, 5.4128668125486e-05, 0.6938015222549438]\n\ntest loss: 0.894816782229986\ntest loss_segm: 0.8906688109422342\ntest loss_shape: 0.028665265163932092\ntest loss_recon: 0.12814358211098573\ntest unet DSC: [0.00017673851107247174, 0.5991215109825134, 2.6300769604858942e-05, 0.7320753931999207]\nBest val loss: 0.8734578658372928\nTime: 57.46635961532593\n\n\nEpoch 38/250\n\ntrain loss: 0.9811988144735747\ntrain loss_segm: 0.9757123016103914\ntrain loss_shape: 0.036848642769022076\ntrain loss_recon: 0.1801646734717526\ntrain unet DSC: [0.0002645901986397803, 0.5553297996520996, 4.800440729013644e-05, 0.693112850189209]\n\ntest loss: 0.9065096378326416\ntest loss_segm: 0.9021930526464413\ntest loss_shape: 0.02956339745567395\ntest loss_recon: 0.13602472364138335\ntest unet DSC: [0.00016441289335489273, 0.581796407699585, 2.5778081180760637e-05, 0.731238603591919]\nBest val loss: 0.8734578658372928\nTime: 58.33390831947327\n\n\nEpoch 39/250\n\ntrain loss: 0.9781893291805364\ntrain loss_segm: 0.9727718584899661\ntrain loss_shape: 0.036349351930467386\ntrain loss_recon: 0.1782532578593568\ntrain unet DSC: [0.0002544921881053597, 0.556330144405365, 4.785640339832753e-05, 0.6968005895614624]\n\ntest loss: 0.8850383758544922\ntest loss_segm: 0.8811769913404416\ntest loss_shape: 0.02623550221323967\ntest loss_recon: 0.1237839746933717\ntest unet DSC: [0.00015367167361546308, 0.5992689728736877, 2.8751916033797897e-05, 0.7468120455741882]\nBest val loss: 0.8734578658372928\nTime: 57.97123742103577\n\n\nEpoch 40/250\n\ntrain loss: 0.9800066227399851\ntrain loss_segm: 0.9745280244682408\ntrain loss_shape: 0.03688072926022961\ntrain loss_recon: 0.17905276221565053\ntrain unet DSC: [0.0002565785252954811, 0.5558547377586365, 4.6136319724610075e-05, 0.6916313767433167]\n\ntest loss: 0.886208523542453\ntest loss_segm: 0.8822191846676362\ntest loss_shape: 0.027178948267530173\ntest loss_recon: 0.12714447520482233\ntest unet DSC: [0.00015185109805315733, 0.6005375385284424, 1.94405311049195e-05, 0.7431877255439758]\nBest val loss: 0.8734578658372928\nTime: 57.582008600234985\n\n\nEpoch 41/250\n\ntrain loss: 0.9712328507175928\ntrain loss_segm: 0.9659061137633987\ntrain loss_shape: 0.035798738663426685\ntrain loss_recon: 0.17468636117498332\ntrain unet DSC: [0.00025386339984834194, 0.563719630241394, 4.778350194101222e-05, 0.6956950426101685]\n\ntest loss: 0.8778581267748123\ntest loss_segm: 0.8741193658266312\ntest loss_shape: 0.025319283589338645\ntest loss_recon: 0.1206834465265274\ntest unet DSC: [0.00012938751024194062, 0.603735089302063, 1.5421384887304157e-05, 0.7447463870048523]\nBest val loss: 0.8734578658372928\nTime: 58.561140298843384\n\n\nEpoch 42/250\n\ntrain loss: 0.9748665328267254\ntrain loss_segm: 0.969474899240687\ntrain loss_shape: 0.03624759460033118\ntrain loss_recon: 0.1766874854559008\ntrain unet DSC: [0.0002520544803701341, 0.5621835589408875, 4.39370924141258e-05, 0.6914364695549011]\n\ntest loss: 0.879238927975679\ntest loss_segm: 0.8753773401945065\ntest loss_shape: 0.02604535322349805\ntest loss_recon: 0.12570638648974589\ntest unet DSC: [0.00013005256187170744, 0.6124768257141113, 2.3623726519872434e-05, 0.7445796728134155]\nBest val loss: 0.8734578658372928\nTime: 58.21428942680359\n\n\nEpoch 43/250\n\ntrain loss: 0.9712122591990459\ntrain loss_segm: 0.9658612175078332\ntrain loss_shape: 0.03580537075294724\ntrain loss_recon: 0.17705061890279192\ntrain unet DSC: [0.00025984004605561495, 0.5635429620742798, 4.5795561163686216e-05, 0.699019730091095]\n\ntest loss: 0.8714949075992291\ntest loss_segm: 0.8676585784325233\ntest loss_shape: 0.025786133960653573\ntest loss_recon: 0.12577145336530146\ntest unet DSC: [0.00012698779755737633, 0.6203811764717102, 1.9771759980358183e-05, 0.7583017349243164]\nBest val loss: 0.8714949075992291\nTime: 57.42867684364319\n\n\nEpoch 44/250\n\ntrain loss: 0.9734982817987853\ntrain loss_segm: 0.96806947490837\ntrain loss_shape: 0.03633599788469227\ntrain loss_recon: 0.1795211527732354\ntrain unet DSC: [0.00025368263595737517, 0.5650528073310852, 5.059285103925504e-05, 0.6979452967643738]\n\ntest loss: 0.8610373903543521\ntest loss_segm: 0.8573883481514759\ntest loss_shape: 0.02469711607465377\ntest loss_recon: 0.11793243808624072\ntest unet DSC: [0.00010794545960379764, 0.6183019876480103, 2.222144576080609e-05, 0.7694302201271057]\nBest val loss: 0.8610373903543521\nTime: 58.03003406524658\n\n\nEpoch 45/250\n\ntrain loss: 0.9697379066219812\ntrain loss_segm: 0.9643933003461813\ntrain loss_shape: 0.03572152051603115\ntrain loss_recon: 0.17724523014282878\ntrain unet DSC: [0.00024933292297646403, 0.5685993432998657, 4.982253449270502e-05, 0.695083737373352]\n\ntest loss: 0.8720392905748807\ntest loss_segm: 0.8683137924243243\ntest loss_shape: 0.02553297655704694\ntest loss_recon: 0.1172195302370267\ntest unet DSC: [0.0001542521786177531, 0.6309890747070312, 3.093839404755272e-05, 0.7362895607948303]\nBest val loss: 0.8610373903543521\nTime: 57.874423027038574\n\n\nEpoch 46/250\n\ntrain loss: 0.9685676169546344\ntrain loss_segm: 0.9632351519186285\ntrain loss_shape: 0.03574681515463545\ntrain loss_recon: 0.17577870972926105\ntrain unet DSC: [0.00025888276286423206, 0.5709931254386902, 4.6247623686213046e-05, 0.6936684846878052]\n\ntest loss: 0.9259832272162805\ntest loss_segm: 0.921514277274792\ntest loss_shape: 0.031216633959840506\ntest loss_recon: 0.1347286147184861\ntest unet DSC: [0.00021901115542277694, 0.6142639517784119, 3.543965794960968e-05, 0.6303355693817139]\nBest val loss: 0.8610373903543521\nTime: 58.73456525802612\n\n\nEpoch 47/250\n\ntrain loss: 0.9689296016210243\ntrain loss_segm: 0.9635716439047947\ntrain loss_shape: 0.035776116366533536\ntrain loss_recon: 0.1780345962394642\ntrain unet DSC: [0.00025340457796119153, 0.5774425268173218, 5.005023558624089e-05, 0.6918298602104187]\n\ntest loss: 0.8685454053756518\ntest loss_segm: 0.8647920749126337\ntest loss_shape: 0.025125805264673173\ntest loss_recon: 0.12407575624111371\ntest unet DSC: [0.00014182765153236687, 0.6284542679786682, 2.1466956241056323e-05, 0.7489604949951172]\nBest val loss: 0.8610373903543521\nTime: 58.14899301528931\n\n\nEpoch 48/250\n\ntrain loss: 0.9669489189039303\ntrain loss_segm: 0.9615797724904893\ntrain loss_shape: 0.03588186309354592\ntrain loss_recon: 0.17809564903189865\ntrain unet DSC: [0.0002559381537139416, 0.5821666717529297, 4.778927905135788e-05, 0.6968023180961609]\n\ntest loss: 0.8601042995086083\ntest loss_segm: 0.8564712649736649\ntest loss_shape: 0.024128705263137817\ntest loss_recon: 0.12201544078878868\ntest unet DSC: [0.00012083822366548702, 0.6420150995254517, 2.6762190827867016e-05, 0.7612802982330322]\nBest val loss: 0.8601042995086083\nTime: 58.68179178237915\n\n\nEpoch 49/250\n\ntrain loss: 0.963036977792088\ntrain loss_segm: 0.9577034979681426\ntrain loss_shape: 0.03575041867745451\ntrain loss_recon: 0.1758437722544127\ntrain unet DSC: [0.0002507531607989222, 0.5914936065673828, 4.906616231892258e-05, 0.6958072781562805]\n\ntest loss: 0.8611878569309528\ntest loss_segm: 0.8573610293559539\ntest loss_shape: 0.02619468031498866\ntest loss_recon: 0.12073590931219932\ntest unet DSC: [0.00015168022946454585, 0.6690334677696228, 2.267357376695145e-05, 0.753592848777771]\nBest val loss: 0.8601042995086083\nTime: 57.8559947013855\n\n\nEpoch 50/250\n\ntrain loss: 0.9644235598890087\ntrain loss_segm: 0.9590876000591472\ntrain loss_shape: 0.03566052653838562\ntrain loss_recon: 0.17699078876006452\ntrain unet DSC: [0.00024620251497253776, 0.6071311831474304, 4.638623431674205e-05, 0.6908261179924011]\n\ntest loss: 0.8711253358767583\ntest loss_segm: 0.8671308679458423\ntest loss_shape: 0.027357953576705396\ntest loss_recon: 0.12586629505340868\ntest unet DSC: [0.00016887017409317195, 0.7086537480354309, 2.1464686142280698e-05, 0.7397880554199219]\nBest val loss: 0.8601042995086083\nTime: 58.204978942871094\n\n\nEpoch 51/250\n\ntrain loss: 0.9553670860544036\ntrain loss_segm: 0.9501059870176678\ntrain loss_shape: 0.035043132232054124\ntrain loss_recon: 0.17567890212882922\ntrain unet DSC: [0.00024484266759827733, 0.6479831337928772, 4.582014662446454e-05, 0.6958222985267639]\n\ntest loss: 0.8524201879134545\ntest loss_segm: 0.8485876887272565\ntest loss_shape: 0.025776383037177417\ntest loss_recon: 0.12548636578214475\ntest unet DSC: [0.00016221195983234793, 0.7896997332572937, 2.4535920601920225e-05, 0.7516536116600037]\nBest val loss: 0.8524201879134545\nTime: 58.23613381385803\n\n\nEpoch 52/250\n\ntrain loss: 0.9491325488573388\ntrain loss_segm: 0.9438138905959793\ntrain loss_shape: 0.03539738395027345\ntrain loss_recon: 0.17789202541867388\ntrain unet DSC: [0.00025107149849645793, 0.6990761756896973, 4.801915565622039e-05, 0.6947068572044373]\n\ntest loss: 0.828108338209299\ntest loss_segm: 0.8244406703190926\ntest loss_shape: 0.024570815551739473\ntest loss_recon: 0.12105834235747655\ntest unet DSC: [0.00012997881276533008, 0.8176022171974182, 1.8818398530129343e-05, 0.767613410949707]\nBest val loss: 0.828108338209299\nTime: 58.46583819389343\n\n\nEpoch 53/250\n\ntrain loss: 0.9400108132181288\ntrain loss_segm: 0.9347592550742475\ntrain loss_shape: 0.034571928738416\ntrain loss_recon: 0.17943705571225926\ntrain unet DSC: [0.00024485643371008337, 0.7149583101272583, 4.6932007535360754e-05, 0.7034534215927124]\n\ntest loss: 0.8387153866963509\ntest loss_segm: 0.8349100550015768\ntest loss_shape: 0.02517355429247404\ntest loss_recon: 0.12879807912768462\ntest unet DSC: [0.00014746781380381435, 0.8021321892738342, 2.751088686636649e-05, 0.7563124895095825]\nBest val loss: 0.828108338209299\nTime: 58.014413595199585\n\n\nEpoch 54/250\n\ntrain loss: 0.9399107958697066\ntrain loss_segm: 0.9345731923851786\ntrain loss_shape: 0.03528030865083012\ntrain loss_recon: 0.18095700746944435\ntrain unet DSC: [0.0002458938688505441, 0.7211248278617859, 4.8076915845740587e-05, 0.6938241720199585]\n\ntest loss: 0.8462528509971423\ntest loss_segm: 0.8419693326338743\ntest loss_shape: 0.028212847952277232\ntest loss_recon: 0.1462225764989853\ntest unet DSC: [9.652584412833676e-05, 0.8053818941116333, 1.9806018826784566e-05, 0.7551441192626953]\nBest val loss: 0.828108338209299\nTime: 57.630820751190186\n\n\nEpoch 55/250\n\ntrain loss: 0.9376686309711842\ntrain loss_segm: 0.9323193083835554\ntrain loss_shape: 0.03529291919467947\ntrain loss_recon: 0.18200318666198587\ntrain unet DSC: [0.0002528316399548203, 0.7203062772750854, 4.874809746979736e-05, 0.6953701376914978]\n\ntest loss: 0.8444621822772882\ntest loss_segm: 0.8404728861955496\ntest loss_shape: 0.026760543959263045\ntest loss_recon: 0.13132379385523307\ntest unet DSC: [0.00017656260752119124, 0.8005874752998352, 2.9860333597753197e-05, 0.7344117760658264]\nBest val loss: 0.828108338209299\nTime: 59.06143045425415\n\n\nEpoch 56/250\n\ntrain loss: 0.9284094990808752\ntrain loss_segm: 0.9231472388853\ntrain loss_shape: 0.03480738505155225\ntrain loss_recon: 0.1781521398998514\ntrain unet DSC: [0.00026379601331427693, 0.7290347218513489, 4.852715210290626e-05, 0.6998185515403748]\n\ntest loss: 0.8370221211360052\ntest loss_segm: 0.8330171429193937\ntest loss_shape: 0.026808814694866154\ntest loss_recon: 0.1324094207240985\ntest unet DSC: [0.00015552052354905754, 0.8081538677215576, 2.48611941060517e-05, 0.7463763356208801]\nBest val loss: 0.828108338209299\nTime: 60.66543769836426\n\n\nEpoch 57/250\n\ntrain loss: 0.9286002260220202\ntrain loss_segm: 0.9233027662657485\ntrain loss_shape: 0.034988494141946866\ntrain loss_recon: 0.17986117595735984\ntrain unet DSC: [0.0002603477332741022, 0.7286251187324524, 5.25321411259938e-05, 0.6945345997810364]\n\ntest loss: 0.8248230921916473\ntest loss_segm: 0.8209541424726828\ntest loss_shape: 0.025921431513359912\ntest loss_recon: 0.12768045077339196\ntest unet DSC: [0.00016032138955779374, 0.8162609934806824, 2.440383286739234e-05, 0.7525906562805176]\nBest val loss: 0.8248230921916473\nTime: 58.66588807106018\n\n\nEpoch 58/250\n\ntrain loss: 0.9246096678926975\ntrain loss_segm: 0.9193612842620174\ntrain loss_shape: 0.03454546630382538\ntrain loss_recon: 0.17938411292396014\ntrain unet DSC: [0.00025100246421061456, 0.7312674522399902, 4.758713475894183e-05, 0.6997448205947876]\n\ntest loss: 0.8037827962484115\ntest loss_segm: 0.8004098320618654\ntest loss_shape: 0.02187995591129248\ntest loss_recon: 0.1184963926863976\ntest unet DSC: [0.00012173625145806, 0.8230244517326355, 2.365335421927739e-05, 0.770393967628479]\nBest val loss: 0.8037827962484115\nTime: 82.69657826423645\n\n\nEpoch 59/250\n\ntrain loss: 0.9233137029635755\ntrain loss_segm: 0.9180472198920914\ntrain loss_shape: 0.03471048605404323\ntrain loss_recon: 0.17954341237303578\ntrain unet DSC: [0.0002660483296494931, 0.7309545278549194, 4.8791531298775226e-05, 0.7013301849365234]\n\ntest loss: 0.818834472925235\ntest loss_segm: 0.8151940244894761\ntest loss_shape: 0.024005466881088722\ntest loss_recon: 0.12399025710347371\ntest unet DSC: [0.00014366695540957153, 0.8116897344589233, 2.0614179447875358e-05, 0.7486381530761719]\nBest val loss: 0.8037827962484115\nTime: 59.043598651885986\n\n\nEpoch 60/250\n\ntrain loss: 0.9193201125422611\ntrain loss_segm: 0.9140712516971782\ntrain loss_shape: 0.03461525644635475\ntrain loss_recon: 0.17873373259849187\ntrain unet DSC: [0.0002618528378661722, 0.7329555749893188, 4.680135680246167e-05, 0.7048033475875854]\n\ntest loss: 0.811976765974974\ntest loss_segm: 0.808434668259743\ntest loss_shape: 0.023070496124907944\ntest loss_recon: 0.12350460457114074\ntest unet DSC: [0.0001557141513330862, 0.8207316398620605, 2.73444784397725e-05, 0.7582202553749084]\nBest val loss: 0.8037827962484115\nTime: 58.382771253585815\n\n\nEpoch 61/250\n\ntrain loss: 0.9217931062360353\ntrain loss_segm: 0.9165084071551697\ntrain loss_shape: 0.03476093000956351\ntrain loss_recon: 0.18086081865844847\ntrain unet DSC: [0.00025670224567875266, 0.7308340668678284, 4.7904559323797e-05, 0.6996350884437561]\n\ntest loss: 0.8361208744538136\ntest loss_segm: 0.8321622747641343\ntest loss_shape: 0.026612762194604445\ntest loss_recon: 0.12973196174089724\ntest unet DSC: [0.00018056471890304238, 0.8053992390632629, 2.3844675524742343e-05, 0.7253282070159912]\nBest val loss: 0.8037827962484115\nTime: 57.63225865364075\n\n\nEpoch 62/250\n\ntrain loss: 0.916843013672889\ntrain loss_segm: 0.9116329056552693\ntrain loss_shape: 0.03429857300759494\ntrain loss_recon: 0.17802499159227445\ntrain unet DSC: [0.00025818575522862375, 0.738879919052124, 4.824803181691095e-05, 0.6961149573326111]\n\ntest loss: 0.8089252236561898\ntest loss_segm: 0.805371995155628\ntest loss_shape: 0.023517681094698418\ntest loss_recon: 0.12014557411655402\ntest unet DSC: [0.0001698406704235822, 0.820592999458313, 2.966035026474856e-05, 0.7554455995559692]\nBest val loss: 0.8037827962484115\nTime: 58.56319284439087\n\n\nEpoch 63/250\n\ntrain loss: 0.9180513303491133\ntrain loss_segm: 0.9127991889851003\ntrain loss_shape: 0.0344497153676952\ntrain loss_recon: 0.18071657683275924\ntrain unet DSC: [0.0002498957619536668, 0.7336928248405457, 4.859755063080229e-05, 0.7008981108665466]\n\ntest loss: 0.807168797040597\ntest loss_segm: 0.8034070852475289\ntest loss_shape: 0.024761318802260436\ntest loss_recon: 0.1285579499716942\ntest unet DSC: [0.00011380924115655944, 0.8325774073600769, 1.650013837206643e-05, 0.7643374800682068]\nBest val loss: 0.8037827962484115\nTime: 67.03168964385986\n\n\nEpoch 64/250\n\ntrain loss: 0.9163542951964125\ntrain loss_segm: 0.9111031852191007\ntrain loss_shape: 0.03457442725289472\ntrain loss_recon: 0.17936673986760876\ntrain unet DSC: [0.00025133774033747613, 0.7344030141830444, 4.712277223006822e-05, 0.7036061882972717]\n\ntest loss: 0.8117441168198218\ntest loss_segm: 0.8080129195482303\ntest loss_shape: 0.02508697142967811\ntest loss_recon: 0.12225022635016686\ntest unet DSC: [0.00018164966604672372, 0.8187134265899658, 2.8156558983027935e-05, 0.7589299082756042]\nBest val loss: 0.8037827962484115\nTime: 91.2582836151123\n\n\nEpoch 65/250\n\ntrain loss: 0.9162698523907722\ntrain loss_segm: 0.91101265670378\ntrain loss_shape: 0.03457601236391671\ntrain loss_recon: 0.17995919324929202\ntrain unet DSC: [0.0002704234793782234, 0.7363182306289673, 5.028327723266557e-05, 0.6962519884109497]\n\ntest loss: 0.8115488046254867\ntest loss_segm: 0.8079055364315326\ntest loss_shape: 0.024093568922044374\ntest loss_recon: 0.12339118457375428\ntest unet DSC: [0.00016229339234996587, 0.8200125098228455, 2.9008620913373306e-05, 0.7502428889274597]\nBest val loss: 0.8037827962484115\nTime: 80.17696166038513\n\n\nEpoch 66/250\n\ntrain loss: 0.9106801125822188\ntrain loss_segm: 0.9054890656018559\ntrain loss_shape: 0.03409685181524558\ntrain loss_recon: 0.17813659525370296\ntrain unet DSC: [0.000266259943600744, 0.7396837472915649, 5.0341473979642615e-05, 0.7067488431930542]\n\ntest loss: 0.7972307266333164\ntest loss_segm: 0.7938037163172013\ntest loss_shape: 0.022350909857031625\ntest loss_recon: 0.11919184172382721\ntest unet DSC: [0.00013625719293486327, 0.8301029801368713, 2.8192051104269922e-05, 0.764739453792572]\nBest val loss: 0.7972307266333164\nTime: 58.48260283470154\n\n\nEpoch 67/250\n\ntrain loss: 0.9147361154797711\ntrain loss_segm: 0.9095070769515219\ntrain loss_shape: 0.03420671800599445\ntrain loss_recon: 0.18083649779422373\ntrain unet DSC: [0.000260542961768806, 0.7370145320892334, 4.447569517651573e-05, 0.6969870924949646]\n\ntest loss: 0.7974335933342959\ntest loss_segm: 0.7939083591485635\ntest loss_shape: 0.02334829754172227\ntest loss_recon: 0.11904078530959594\ntest unet DSC: [0.00016422555199824274, 0.8283535838127136, 2.9269476726767607e-05, 0.7676678895950317]\nBest val loss: 0.7972307266333164\nTime: 58.04538130760193\n\n\nEpoch 68/250\n\ntrain loss: 0.910991948239411\ntrain loss_segm: 0.9057959204987635\ntrain loss_shape: 0.03408278261983319\ntrain loss_recon: 0.17877487979735\ntrain unet DSC: [0.0002651207323651761, 0.7405903935432434, 4.831475962419063e-05, 0.6989127993583679]\n\ntest loss: 0.7962967035097953\ntest loss_segm: 0.7927542512233441\ntest loss_shape: 0.02297834435907694\ntest loss_recon: 0.12446166211977983\ntest unet DSC: [0.0001298685383517295, 0.8306332230567932, 2.5876370273181237e-05, 0.7716471552848816]\nBest val loss: 0.7962967035097953\nTime: 58.00372862815857\n\n\nEpoch 69/250\n\ntrain loss: 0.9074396336380439\ntrain loss_segm: 0.9022509331190134\ntrain loss_shape: 0.034214841968179505\ntrain loss_recon: 0.17672158457055875\ntrain unet DSC: [0.0002780916402116418, 0.7432133555412292, 5.225755739957094e-05, 0.6995322108268738]\n\ntest loss: 0.8031073350172776\ntest loss_segm: 0.7995463961210006\ntest loss_shape: 0.02330762869081436\ntest loss_recon: 0.12301775994591224\ntest unet DSC: [0.0001445791858714074, 0.8178592920303345, 3.159005427733064e-05, 0.7649419903755188]\nBest val loss: 0.7962967035097953\nTime: 58.299838066101074\n\n\nEpoch 70/250\n\ntrain loss: 0.9103718939461286\ntrain loss_segm: 0.9051643179941781\ntrain loss_shape: 0.03417457367847614\ntrain loss_recon: 0.17901201414156565\ntrain unet DSC: [0.0002714244765229523, 0.7390050292015076, 4.9693193432176486e-05, 0.7002465724945068]\n\ntest loss: 0.8303809517469162\ntest loss_segm: 0.8262403622651712\ntest loss_shape: 0.02665859494262781\ntest loss_recon: 0.14747313285867372\ntest unet DSC: [0.00011280177568551153, 0.8091061115264893, 1.83980828296626e-05, 0.7434808611869812]\nBest val loss: 0.7962967035097953\nTime: 61.305553913116455\n\n\nEpoch 71/250\n\ntrain loss: 0.9119714709776866\ntrain loss_segm: 0.9067514648165884\ntrain loss_shape: 0.03424763400107622\ntrain loss_recon: 0.17952421430168272\ntrain unet DSC: [0.00026577411335892975, 0.7376073598861694, 4.550329686026089e-05, 0.6976521015167236]\n\ntest loss: 0.7984336813290914\ntest loss_segm: 0.7949072504654909\ntest loss_shape: 0.023380788640143015\ntest loss_recon: 0.11883491392319019\ntest unet DSC: [0.0001627812598599121, 0.8279354572296143, 2.8811846277676523e-05, 0.7569838166236877]\nBest val loss: 0.7962967035097953\nTime: 71.59032773971558\n\n\nEpoch 72/250\n\ntrain loss: 0.9072461192366443\ntrain loss_segm: 0.9020410964760599\ntrain loss_shape: 0.034121234146760236\ntrain loss_recon: 0.17928984195371217\ntrain unet DSC: [0.0002818148641381413, 0.7407975792884827, 4.885111047769897e-05, 0.705808162689209]\n\ntest loss: 0.8228938350310693\ntest loss_segm: 0.8188950572258387\ntest loss_shape: 0.026457600391063936\ntest loss_recon: 0.13530125765082163\ntest unet DSC: [0.00018008617917075753, 0.8103843331336975, 2.3488790247938596e-05, 0.7442305088043213]\nBest val loss: 0.7962967035097953\nTime: 71.93120336532593\n\n\nEpoch 73/250\n\ntrain loss: 0.9113193150562576\ntrain loss_segm: 0.9060843303988252\ntrain loss_shape: 0.03429536653470389\ntrain loss_recon: 0.180545049660568\ntrain unet DSC: [0.0002713930152822286, 0.7405101656913757, 4.937595804221928e-05, 0.6947022676467896]\n\ntest loss: 0.8034065167109171\ntest loss_segm: 0.7997600329227936\ntest loss_shape: 0.024061261938932616\ntest loss_recon: 0.12403614341448514\ntest unet DSC: [0.000166432568221353, 0.8222401738166809, 2.8451857360778376e-05, 0.7601190805435181]\nBest val loss: 0.7962967035097953\nTime: 59.476698875427246\n\n\nEpoch 74/250\n\ntrain loss: 0.9062365900866592\ntrain loss_segm: 0.9010899715785738\ntrain loss_shape: 0.03375104821841173\ntrain loss_recon: 0.17715169696868222\ntrain unet DSC: [0.0002680388279259205, 0.7421492338180542, 4.627661110134795e-05, 0.6996866464614868]\n\ntest loss: 0.7913086842267941\ntest loss_segm: 0.7879233131041894\ntest loss_shape: 0.021693740756465837\ntest loss_recon: 0.12159965330591568\ntest unet DSC: [0.00012131313269492239, 0.8320204615592957, 2.3229718863149174e-05, 0.7670382261276245]\nBest val loss: 0.7913086842267941\nTime: 57.11374521255493\n\n\nEpoch 75/250\n\ntrain loss: 0.9022829419449915\ntrain loss_segm: 0.897174854444552\ntrain loss_shape: 0.03342149516308232\ntrain loss_recon: 0.17659349673533742\ntrain unet DSC: [0.0002657245786394924, 0.7426692843437195, 4.8141439037863165e-05, 0.7082321643829346]\n\ntest loss: 0.793099467570965\ntest loss_segm: 0.7896592647601397\ntest loss_shape: 0.02221877994732215\ntest loss_recon: 0.12183267747362454\ntest unet DSC: [0.00012649339623749256, 0.8281457424163818, 2.6528496164246462e-05, 0.7674376368522644]\nBest val loss: 0.7913086842267941\nTime: 64.46527600288391\n\n\nEpoch 76/250\n\ntrain loss: 0.9049429425710365\ntrain loss_segm: 0.8998187252237827\ntrain loss_shape: 0.033508909889791584\ntrain loss_recon: 0.1773325651884079\ntrain unet DSC: [0.0002693135174922645, 0.7447335124015808, 4.826749500352889e-05, 0.7011542320251465]\n\ntest loss: 0.7982050684782175\ntest loss_segm: 0.7948416975828317\ntest loss_shape: 0.022141134055952232\ntest loss_recon: 0.11492554614177117\ntest unet DSC: [0.0001641781273065135, 0.8268741965293884, 2.6915691705653444e-05, 0.737592339515686]\nBest val loss: 0.7913086842267941\nTime: 143.28566479682922\n\n\nEpoch 77/250\n\ntrain loss: 0.9064622125293635\ntrain loss_segm: 0.9012806547593467\ntrain loss_shape: 0.03403749022208437\ntrain loss_recon: 0.17778064140790625\ntrain unet DSC: [0.0002743939112406224, 0.7441476583480835, 4.8084824811667204e-05, 0.6963016390800476]\n\ntest loss: 0.7907553009497814\ntest loss_segm: 0.7873133038863157\ntest loss_shape: 0.022586058180492658\ntest loss_recon: 0.11833898178659953\ntest unet DSC: [0.00015397474635392427, 0.8310602903366089, 2.4761342501733452e-05, 0.7622120380401611]\nBest val loss: 0.7907553009497814\nTime: 138.99220848083496\n\n\nEpoch 78/250\n\ntrain loss: 0.9054197880286204\ntrain loss_segm: 0.9002640515188628\ntrain loss_shape: 0.03377353080512979\ntrain loss_recon: 0.17783857297293748\ntrain unet DSC: [0.0002771286235656589, 0.741292417049408, 5.0248207116965204e-05, 0.6993427276611328]\n\ntest loss: 0.8070928836480166\ntest loss_segm: 0.803370747810755\ntest loss_shape: 0.024191926400630902\ntest loss_recon: 0.13029482674140197\ntest unet DSC: [0.0001430209813406691, 0.8181334137916565, 2.1595611542579718e-05, 0.7555187940597534]\nBest val loss: 0.7907553009497814\nTime: 130.28269338607788\n\n\nEpoch 79/250\n\ntrain loss: 0.9033454637738723\ntrain loss_segm: 0.8982220005385483\ntrain loss_shape: 0.03354093797882147\ntrain loss_recon: 0.1769369454889358\ntrain unet DSC: [0.00027072333614341915, 0.7415149807929993, 4.658654870581813e-05, 0.7016666531562805]\n\ntest loss: 0.8202350368866553\ntest loss_segm: 0.8162999321252872\ntest loss_shape: 0.026047357429678623\ntest loss_recon: 0.13303610539207092\ntest unet DSC: [0.00017689076776150614, 0.8123760223388672, 2.2377340428647585e-05, 0.7365143895149231]\nBest val loss: 0.7907553009497814\nTime: 72.72283816337585\n\n\nEpoch 80/250\n\ntrain loss: 0.9044586974608747\ntrain loss_segm: 0.8993316338032107\ntrain loss_shape: 0.03347299135985631\ntrain loss_recon: 0.1779766663720336\ntrain unet DSC: [0.0002758217742666602, 0.7428076267242432, 4.577023719321005e-05, 0.6998844742774963]\n\ntest loss: 0.7975275424810556\ntest loss_segm: 0.7939703479791299\ntest loss_shape: 0.02325441358754268\ntest loss_recon: 0.12317538204101416\ntest unet DSC: [0.00014046675642021, 0.8296197652816772, 1.924230127769988e-05, 0.7531149983406067]\nBest val loss: 0.7907553009497814\nTime: 57.32577395439148\n\n\nEpoch 81/250\n\ntrain loss: 0.9045432432542874\ntrain loss_segm: 0.8993939455551437\ntrain loss_shape: 0.03364312752515455\ntrain loss_recon: 0.17849873317570625\ntrain unet DSC: [0.00026743701891973615, 0.7425650358200073, 5.03415176353883e-05, 0.6999255418777466]\n\ntest loss: 0.7998221317927042\ntest loss_segm: 0.7962434750336868\ntest loss_shape: 0.02347750321794779\ntest loss_recon: 0.12309121579313889\ntest unet DSC: [0.00015260503278113902, 0.8203939199447632, 2.5742258003447205e-05, 0.754368782043457]\nBest val loss: 0.7907553009497814\nTime: 57.49919891357422\n\n\nEpoch 82/250\n\ntrain loss: 0.9043432364735422\ntrain loss_segm: 0.8992032782186435\ntrain loss_shape: 0.03353099908113857\ntrain loss_recon: 0.17868603862633434\ntrain unet DSC: [0.0002605693007353693, 0.7412362694740295, 4.635560981114395e-05, 0.7003832459449768]\n\ntest loss: 0.7786175532218738\ntest loss_segm: 0.7754022020560044\ntest loss_shape: 0.02082811167033819\ntest loss_recon: 0.11325345054650918\ntest unet DSC: [0.00014864836703054607, 0.834415853023529, 2.792823943309486e-05, 0.7758973240852356]\nBest val loss: 0.7786175532218738\nTime: 57.40770173072815\n\n\nEpoch 83/250\n\ntrain loss: 0.8989788716352438\ntrain loss_segm: 0.8939257200005688\ntrain loss_shape: 0.03309742076015925\ntrain loss_recon: 0.17434119737318046\ntrain unet DSC: [0.0002591332304291427, 0.7451195120811462, 4.680710844695568e-05, 0.7042586803436279]\n\ntest loss: 0.7967215577761332\ntest loss_segm: 0.7930979132652283\ntest loss_shape: 0.023923016630877286\ntest loss_recon: 0.12313420449693997\ntest unet DSC: [0.00015450066712219268, 0.8311903476715088, 2.345444772799965e-05, 0.7548351883888245]\nBest val loss: 0.7786175532218738\nTime: 57.59632420539856\n\n\nEpoch 84/250\n\ntrain loss: 0.8997345649743382\ntrain loss_segm: 0.8946145146707946\ntrain loss_shape: 0.03363394720739202\ntrain loss_recon: 0.17566562689180615\ntrain unet DSC: [0.00027652198332361877, 0.7460430264472961, 4.826819713343866e-05, 0.702141523361206]\n\ntest loss: 0.7817864051231971\ntest loss_segm: 0.7785225052099961\ntest loss_shape: 0.02108257896720599\ntest loss_recon: 0.11556337964840424\ntest unet DSC: [0.00014385732356458902, 0.8371877670288086, 2.809370926115662e-05, 0.7656526565551758]\nBest val loss: 0.7786175532218738\nTime: 58.302101373672485\n\n\nEpoch 85/250\n\ntrain loss: 0.9047929097580004\ntrain loss_segm: 0.899649012692367\ntrain loss_shape: 0.033617730252444744\ntrain loss_recon: 0.17821254986750928\ntrain unet DSC: [0.0002585541224107146, 0.7437056303024292, 4.851921403314918e-05, 0.6937589049339294]\n\ntest loss: 0.791170766720405\ntest loss_segm: 0.7877462536860735\ntest loss_shape: 0.022004745327509366\ntest loss_recon: 0.12240346377858749\ntest unet DSC: [0.0001378196757286787, 0.8344646096229553, 2.3686527129029855e-05, 0.7584093809127808]\nBest val loss: 0.7786175532218738\nTime: 57.65952825546265\n\n\nEpoch 86/250\n\ntrain loss: 0.9033443022377884\ntrain loss_segm: 0.8982355500323863\ntrain loss_shape: 0.03325410992425831\ntrain loss_recon: 0.17833396207682695\ntrain unet DSC: [0.00026648613857105374, 0.7434197664260864, 4.581107714329846e-05, 0.6992366909980774]\n\ntest loss: 0.7750846361502622\ntest loss_segm: 0.7719548910092084\ntest loss_shape: 0.020252057398932103\ntest loss_recon: 0.11045345826408802\ntest unet DSC: [0.0001367713848594576, 0.83461594581604, 2.1691499568987638e-05, 0.7750567197799683]\nBest val loss: 0.7750846361502622\nTime: 58.030534744262695\n\n\nEpoch 87/250\n\ntrain loss: 0.9020408972909179\ntrain loss_segm: 0.8969459107405022\ntrain loss_shape: 0.03325741449253091\ntrain loss_recon: 0.17692453649979603\ntrain unet DSC: [0.00024482354638166726, 0.7446957230567932, 4.361027822596952e-05, 0.6995101571083069]\n\ntest loss: 0.7891165400162722\ntest loss_segm: 0.7857856628222343\ntest loss_shape: 0.021496815009950064\ntest loss_recon: 0.11811932797233264\ntest unet DSC: [0.00014023474068380892, 0.8350011110305786, 2.7310181394568644e-05, 0.752925455570221]\nBest val loss: 0.7750846361502622\nTime: 57.6832332611084\n\n\nEpoch 88/250\n\ntrain loss: 0.8979818590834171\ntrain loss_segm: 0.8928920664364779\ntrain loss_shape: 0.033300034572146364\ntrain loss_recon: 0.17597883968036385\ntrain unet DSC: [0.0002698565076570958, 0.7476176023483276, 5.0167684094049037e-05, 0.7050139307975769]\n\ntest loss: 0.7884109585713117\ntest loss_segm: 0.7850849368633368\ntest loss_shape: 0.022032867854413312\ntest loss_recon: 0.11227325856303558\ntest unet DSC: [0.00017002143431454897, 0.8317754864692688, 2.665453212102875e-05, 0.7481263279914856]\nBest val loss: 0.7750846361502622\nTime: 58.51609134674072\n\n\nEpoch 89/250\n\ntrain loss: 0.8947067155113703\ntrain loss_segm: 0.8896997880332077\ntrain loss_shape: 0.032730429032463815\ntrain loss_recon: 0.17338835306559938\ntrain unet DSC: [0.0002695589209906757, 0.7487263083457947, 4.581071334541775e-05, 0.7080075144767761]\n\ntest loss: 0.7876607271341177\ntest loss_segm: 0.7843896884184617\ntest loss_shape: 0.02129065228673892\ntest loss_recon: 0.11419727681921078\ntest unet DSC: [0.0001527914428152144, 0.829915463924408, 2.8191390811116435e-05, 0.7545815110206604]\nBest val loss: 0.7750846361502622\nTime: 58.48979425430298\n\n\nEpoch 90/250\n\ntrain loss: 0.9007382664499404\ntrain loss_segm: 0.8956403392779676\ntrain loss_shape: 0.03328016297773847\ntrain loss_recon: 0.1769909283405618\ntrain unet DSC: [0.0002690836554393172, 0.7443270683288574, 4.941631777910516e-05, 0.7025617957115173]\n\ntest loss: 0.7831268448096055\ntest loss_segm: 0.7799021540543972\ntest loss_shape: 0.02096070561749049\ntest loss_recon: 0.11286256997249065\ntest unet DSC: [0.00014414281758945435, 0.8353210091590881, 2.2671309125144035e-05, 0.7569854855537415]\nBest val loss: 0.7750846361502622\nTime: 58.29055714607239\n\n\nEpoch 91/250\n\ntrain loss: 0.900529922941063\ntrain loss_segm: 0.8954350423963764\ntrain loss_shape: 0.03316816880921774\ntrain loss_recon: 0.17780663780396497\ntrain unet DSC: [0.0002584249305073172, 0.7447164058685303, 4.508589699980803e-05, 0.7025424838066101]\n\ntest loss: 0.7878904801148635\ntest loss_segm: 0.7845757664778293\ntest loss_shape: 0.02162518197049697\ntest loss_recon: 0.1152191922450677\ntest unet DSC: [0.00015076839190442115, 0.8308529853820801, 2.0841192963416688e-05, 0.7535449266433716]\nBest val loss: 0.7750846361502622\nTime: 57.14549469947815\n\n\nEpoch 92/250\n\ntrain loss: 0.8972426656680771\ntrain loss_segm: 0.8922227492815331\ntrain loss_shape: 0.03263772954527713\ntrain loss_recon: 0.17561423938862886\ntrain unet DSC: [0.00025861093308776617, 0.7478636503219604, 4.476882895687595e-05, 0.7020900249481201]\n\ntest loss: 0.7761020156053396\ntest loss_segm: 0.77287275210405\ntest loss_shape: 0.02122903113754896\ntest loss_recon: 0.1106366084363216\ntest unet DSC: [0.00015974181587807834, 0.837232232093811, 2.5673680283944122e-05, 0.7737283706665039]\nBest val loss: 0.7750846361502622\nTime: 57.575867652893066\n\n\nEpoch 93/250\n\ntrain loss: 0.8992063863367974\ntrain loss_segm: 0.8941178533095347\ntrain loss_shape: 0.03314842434623574\ntrain loss_recon: 0.17736902436878108\ntrain unet DSC: [0.0002689366810955107, 0.7448975443840027, 4.567180076264776e-05, 0.7059800624847412]\n\ntest loss: 0.783815296796652\ntest loss_segm: 0.7805349658697079\ntest loss_shape: 0.021351487160875246\ntest loss_recon: 0.11451781063507764\ntest unet DSC: [0.00013510663120541722, 0.8299239873886108, 2.678672899492085e-05, 0.7598251700401306]\nBest val loss: 0.7750846361502622\nTime: 57.86617374420166\n\n\nEpoch 94/250\n\ntrain loss: 0.8956002136574516\ntrain loss_segm: 0.8905591504483283\ntrain loss_shape: 0.03280873411582618\ntrain loss_recon: 0.17601898095653026\ntrain unet DSC: [0.00026177638210356236, 0.7471470832824707, 4.6598790504503995e-05, 0.7089157104492188]\n\ntest loss: 0.8119785617559384\ntest loss_segm: 0.8084077407152225\ntest loss_shape: 0.02302803099155426\ntest loss_recon: 0.1268013740579287\ntest unet DSC: [0.00013566551206167787, 0.8172799348831177, 2.998929085151758e-05, 0.7218828201293945]\nBest val loss: 0.7750846361502622\nTime: 57.72793698310852\n\n\nEpoch 95/250\n\ntrain loss: 0.8994722751122487\ntrain loss_segm: 0.8944023360934439\ntrain loss_shape: 0.03290706245770937\ntrain loss_recon: 0.17792323488694958\ntrain unet DSC: [0.0002770234423223883, 0.743461012840271, 4.6124823711579666e-05, 0.7043282389640808]\n\ntest loss: 0.8011465240747501\ntest loss_segm: 0.7975617081691058\ntest loss_shape: 0.023463815737229127\ntest loss_recon: 0.12384316048178917\ntest unet DSC: [0.00016656580555718392, 0.8237500190734863, 2.7339965527062304e-05, 0.7427331209182739]\nBest val loss: 0.7750846361502622\nTime: 57.607606410980225\n\n\nEpoch 96/250\n\ntrain loss: 0.8964566667623157\ntrain loss_segm: 0.8914403783369668\ntrain loss_shape: 0.032571840595123885\ntrain loss_recon: 0.17591050106890593\ntrain unet DSC: [0.00025685146101750433, 0.7478005886077881, 4.716680996352807e-05, 0.7031692266464233]\n\ntest loss: 0.8063648786300268\ntest loss_segm: 0.8027695600803082\ntest loss_shape: 0.023345073852210473\ntest loss_recon: 0.12608062867552805\ntest unet DSC: [0.0001265842147404328, 0.8200206160545349, 2.544951712479815e-05, 0.7343010306358337]\nBest val loss: 0.7750846361502622\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 57.15162205696106\n\n\nEpoch 97/250\n\ntrain loss: 0.8950730958316899\ntrain loss_segm: 0.8900663841374313\ntrain loss_shape: 0.03243749029934406\ntrain loss_recon: 0.17629624818322026\ntrain unet DSC: [0.0002617115678731352, 0.7492076754570007, 4.6291770559037104e-05, 0.7068743705749512]\n\ntest loss: 0.7805531162482041\ntest loss_segm: 0.777322653012398\ntest loss_shape: 0.02097412607131096\ntest loss_recon: 0.1133052932146268\ntest unet DSC: [0.00014884390111546963, 0.8366607427597046, 2.67855230049463e-05, 0.7645495533943176]\nBest val loss: 0.7750846361502622\nTime: 57.95856213569641\n\n\nEpoch 98/250\n\ntrain loss: 0.8920437591739848\ntrain loss_segm: 0.8870944290221492\ntrain loss_shape: 0.03213942640378505\ntrain loss_recon: 0.17353907246378403\ntrain unet DSC: [0.0002567695628385991, 0.752055287361145, 4.519255890045315e-05, 0.7075293064117432]\n\ntest loss: 0.7757964944228147\ntest loss_segm: 0.7726305524508158\ntest loss_shape: 0.020282932533285558\ntest loss_recon: 0.11376471464068462\ntest unet DSC: [0.00012978879385627806, 0.8396531343460083, 2.502302777429577e-05, 0.7682018876075745]\nBest val loss: 0.7750846361502622\nTime: 57.269559144973755\n\n\nEpoch 99/250\n\ntrain loss: 0.8906988984421839\ntrain loss_segm: 0.8857800364494324\ntrain loss_shape: 0.03189547721862416\ntrain loss_recon: 0.1729313698184641\ntrain unet DSC: [0.0002588203642517328, 0.7521350979804993, 4.531661033979617e-05, 0.7089338898658752]\n\ntest loss: 0.7947200292196029\ntest loss_segm: 0.7913129650629483\ntest loss_shape: 0.02235589601481572\ntest loss_recon: 0.11714856861493526\ntest unet DSC: [0.00016211891488637775, 0.8283673524856567, 2.7306914489599876e-05, 0.7436211705207825]\nBest val loss: 0.7750846361502622\nTime: 57.29690337181091\n\n\nEpoch 100/250\n\ntrain loss: 0.8925689711600919\ntrain loss_segm: 0.887618957818309\ntrain loss_shape: 0.032148795119852205\ntrain loss_recon: 0.17351336814934695\ntrain unet DSC: [0.00025746269966475666, 0.7508774995803833, 4.4597723899642006e-05, 0.7053586840629578]\n\ntest loss: 0.7856345298962716\ntest loss_segm: 0.7823498692267981\ntest loss_shape: 0.021334887744906623\ntest loss_recon: 0.11511757091069832\ntest unet DSC: [0.00015354003699030727, 0.8343705534934998, 2.5282351998612285e-05, 0.75345778465271]\nBest val loss: 0.7750846361502622\nTime: 58.966259241104126\n\n\nEpoch 101/250\n\ntrain loss: 0.892469898432116\ntrain loss_segm: 0.8875220346299908\ntrain loss_shape: 0.03204455867975573\ntrain loss_recon: 0.17434080171434185\ntrain unet DSC: [0.0002580578438937664, 0.7506974935531616, 4.467950566322543e-05, 0.7073900103569031]\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678195386616
        }
      },
      "id": "cbcf75f5-430c-4c05-af8f-d680637e6a9d"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.01, 0.01)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678195386634
        }
      },
      "id": "57975b00-b7b9-4558-b3b6-0b04807db228"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.01, 0.001)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678195386660
        }
      },
      "id": "f6608ece-f313-4524-b599-ec1a8637abb3"
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = (1, 0.001, 0.01)\n",
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(model=model, lambdas=lambdas, n_epochs=250, dataloaders=dataloaders, learning_rate=1e-3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678195386682
        }
      },
      "id": "b255815b-05f3-4013-8e39-1271c7358e82"
    },
    {
      "cell_type": "code",
      "source": [
        "# test unet DSC: [0.7999164462089539, 0.8284406661987305, 0.8151174187660217, 0.7770190238952637]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678195386704
        }
      },
      "id": "40cf79e3-00b8-477e-b9c7-00eb154a1c76"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}