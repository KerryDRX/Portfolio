{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import scipy.ndimage as ndimage\n",
        "import nrrd\n",
        "import torchio as tio\n",
        "import monai\n",
        "import nibabel as nib\n",
        "from collections import OrderedDict, defaultdict"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, positive=False):\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n/anaconda/envs/shapeworks/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  EPS = np.finfo(np.float).eps\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1675739573707
        },
        "id": "8246ca62",
        "scrolled": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "id": "8246ca62"
    },
    {
      "cell_type": "code",
      "source": [
        "city = 'Ellipsoids'\n",
        "IMAGE_DIR = f'../dataset/{city}/images'\n",
        "IMAGE_ARTI_DIR = f'../dataset/{city}/images_arti'\n",
        "MASK_DIR = f'../dataset/{city}/segmentations'\n",
        "num_classes = 2\n",
        "TRAIN_SIZE = 8\n",
        "VAL_SIZE = 50\n",
        "TEST_SIZE = 23\n",
        "TOTAL_SIZE = TRAIN_SIZE + VAL_SIZE + TEST_SIZE\n",
        "\n",
        "output_dir = f'../results/{city}_arti_train2/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1675739638348
        }
      },
      "id": "fe9e2ac9"
    },
    {
      "cell_type": "code",
      "source": [
        "hidden1, hidden2 = 96, 48\n",
        "base_filters = 12\n",
        "\n",
        "class ConvolutionalBackbone(nn.Module):\n",
        "    def __init__(self, img_dims):\n",
        "        super(ConvolutionalBackbone, self).__init__()\n",
        "        self.img_dims = img_dims\n",
        "        self.out_fc_dim = np.copy(img_dims)\n",
        "        padvals = [4, 8, 8]\n",
        "        for i in range(3):\n",
        "            self.out_fc_dim[0] = net_utils.poolOutDim(self.out_fc_dim[0] - padvals[i], 2)\n",
        "            self.out_fc_dim[1] = net_utils.poolOutDim(self.out_fc_dim[1] - padvals[i], 2)\n",
        "            self.out_fc_dim[2] = net_utils.poolOutDim(self.out_fc_dim[2] - padvals[i], 2)\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv3d(1, base_filters, 5)),\n",
        "            # ('bn1', nn.BatchNorm3d(base_filters)),\n",
        "            ('relu1', nn.PReLU()),\n",
        "            ('mp1', nn.MaxPool3d(2)),\n",
        "\n",
        "            ('conv2', nn.Conv3d(base_filters, base_filters*2, 5)),\n",
        "            # ('bn2', nn.BatchNorm3d(base_filters*2)),\n",
        "            ('relu2', nn.PReLU()),\n",
        "            ('conv3', nn.Conv3d(base_filters*2, base_filters*4, 5)),\n",
        "            # ('bn3', nn.BatchNorm3d(base_filters*4)),\n",
        "            ('relu3', nn.PReLU()),\n",
        "            ('mp2', nn.MaxPool3d(2)),\n",
        "\n",
        "            ('conv4', nn.Conv3d(base_filters*4, base_filters*8, 5)),\n",
        "            # ('bn4', nn.BatchNorm3d(base_filters*8)),\n",
        "            ('relu4', nn.PReLU()),\n",
        "            ('conv5', nn.Conv3d(base_filters*8, base_filters*16, 5)),\n",
        "            # ('bn5', nn.BatchNorm3d(base_filters*16)),\n",
        "            ('relu5', nn.PReLU()),\n",
        "            ('mp3', nn.MaxPool3d(2)),\n",
        "\n",
        "            ('flatten', net_utils.Flatten()),\n",
        "            \n",
        "            ('fc1', nn.Linear(self.out_fc_dim[0]*self.out_fc_dim[1]*self.out_fc_dim[2]*base_filters*16, hidden1)),\n",
        "            ('relu6', nn.PReLU()),\n",
        "            ('fc2', nn.Linear(hidden1, hidden2)),\n",
        "            ('relu7', nn.PReLU()),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_features = self.features(x)\n",
        "        return x_features\n",
        "\n",
        "class DenseNormalGamma(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dense = nn.Linear(in_features, out_features * 4)\n",
        "        \n",
        "    def evidence(self, x):\n",
        "        return F.softplus(x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = self.dense(x)\n",
        "        mu, logv, logalpha, logbeta = torch.split(output, self.out_features, -1)\n",
        "        v = self.evidence(logv)\n",
        "        alpha = self.evidence(logalpha) + 1\n",
        "        beta = self.evidence(logbeta)\n",
        "        \n",
        "        return mu, v, alpha, beta\n",
        "\n",
        "class DeepSSMNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepSSMNet, self).__init__()\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        self.num_latent = 1\n",
        "        self.img_dims = (64, 64, 64)\n",
        "        print(f'MLP layers: {base_filters*16} -> {hidden1} -> {hidden2} -> {self.num_latent}')\n",
        "        self.ConvolutionalBackbone = ConvolutionalBackbone(self.img_dims)\n",
        "        self.pca_pred = nn.Sequential(OrderedDict([\n",
        "            ('linear', DenseNormalGamma(hidden2, self.num_latent))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvolutionalBackbone(x)\n",
        "        dist_params = self.pca_pred(x)\n",
        "        return dist_params\n",
        "\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n",
        "\n",
        "class StraightThroughEstimator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StraightThroughEstimator, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = STEFunction.apply(x)\n",
        "        return x\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.segmentation = monai.networks.nets.UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=num_classes,\n",
        "            channels=(32, 64, 128, 256, 512),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            norm=monai.networks.layers.Norm.BATCH,\n",
        "        )\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.ste = StraightThroughEstimator()\n",
        "\n",
        "    def forward(self, image):\n",
        "        mask = self.segmentation(image)\n",
        "        mask = self.softmax(mask)\n",
        "        binary_mask = self.ste(mask - 0.5)[:, 1:2, :, :, :]\n",
        "        dist_params = deepssm(binary_mask)\n",
        "        return mask, binary_mask, dist_params"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675739646292
        }
      },
      "id": "08c41641-3801-405d-809e-5299cd7ef1df"
    },
    {
      "cell_type": "code",
      "source": [
        "lamb = 1e-2\n",
        "deepssm = DeepSSMNet().to(DEVICE)\n",
        "deepssm.load_state_dict(torch.load(f'../dataset/Ellipsoids/segmentations/DeepSSM/best_model_{lamb}.torch'))\n",
        "for param in deepssm.parameters():\n",
        "    param.requires_grad = False\n",
        "deepssm.eval();"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "MLP layers: 192 -> 96 -> 48 -> 1\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675739731605
        }
      },
      "id": "bf88260e-02cb-47ce-8000-34dab8d6597d"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "random_state = np.random.RandomState(seed=seed)\n",
        "perm = random_state.permutation(TOTAL_SIZE)\n",
        "perm = {\n",
        "    'train': perm[:TRAIN_SIZE],\n",
        "    'validation': perm[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE],\n",
        "    'test': perm[-TEST_SIZE:],\n",
        "}\n",
        "\n",
        "def get_subjects():\n",
        "    subjects = []\n",
        "    # train_indices = random_state.permutation(TOTAL_SIZE)[:TRAIN_SIZE]\n",
        "    for i, image_path in enumerate(tqdm(sorted(glob(f'{IMAGE_DIR}/*.nrrd'))[:TOTAL_SIZE], desc='Creating Subjects')):\n",
        "        filename = image_path.split('/')[-1]\n",
        "        mask_path = f'{MASK_DIR}/{filename}'\n",
        "        image_arti_path = f'{IMAGE_ARTI_DIR}/{filename}'\n",
        "        subject = tio.Subject(\n",
        "            t1=tio.ScalarImage(\n",
        "                image_arti_path if os.path.exists(image_arti_path) else image_path\n",
        "            ),\n",
        "            label=tio.LabelMap(mask_path),\n",
        "            radius=torch.Tensor([float(filename.split('_')[-1][:5])]),\n",
        "        )\n",
        "        subjects.append(subject)\n",
        "    return subjects\n",
        "\n",
        "def get_dataloader(transform):\n",
        "    dataloader = dict()\n",
        "    for mode in ['train', 'validation', 'test']:\n",
        "        dataloader[mode] = torch.utils.data.DataLoader(\n",
        "            tio.SubjectsDataset(\n",
        "                subjects[mode], \n",
        "                transform=transform\n",
        "            ),\n",
        "            batch_size=1, \n",
        "            num_workers=os.cpu_count(),\n",
        "            shuffle=False,\n",
        "        )\n",
        "    return dataloader\n",
        "\n",
        "all_subjects = get_subjects()\n",
        "subjects = dict()\n",
        "for mode in ['train', 'validation', 'test']:\n",
        "    subjects[mode] = [all_subjects[i] for i in perm[mode]] \n",
        "    if mode == 'train':\n",
        "        subjects[mode] = subjects[mode][:2]\n",
        "\n",
        "signal = tio.Compose([ \n",
        "    tio.RescaleIntensity(percentiles=(0.1, 99.9), out_min_max=(0, 1)),\n",
        "])\n",
        "\n",
        "transform = tio.Compose([\n",
        "    signal,\n",
        "])\n",
        "\n",
        "dataloader = get_dataloader(transform)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Creating Subjects: 100%|██████████| 81/81 [00:01<00:00, 47.34it/s]\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1675740188225
        },
        "id": "iwKH3_Dd1ehr"
      },
      "id": "iwKH3_Dd1ehr"
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.names = ['loss', 'loss_unet', 'loss_ssm', 'au', 'eu']\n",
        "    def log(self, mode, epoch, values):\n",
        "        for name, value in zip(self.names, values):\n",
        "            self.metrics[(mode, epoch, name)].append(value.item())\n",
        "    def show(self, mode, epoch):\n",
        "        print()\n",
        "        for name in self.names:\n",
        "            mean = np.mean(self.metrics[(mode, epoch, name)])\n",
        "            print(f'{mode} {name}: {mean}')\n",
        "\n",
        "def test(save):\n",
        "    mode = 'test'\n",
        "    model.load_state_dict(torch.load(f'{output_dir}/best_model.torch'))\n",
        "    model.eval()\n",
        "\n",
        "    loss_dice = monai.losses.DiceLoss(squared_pred=True).to(DEVICE)\n",
        "    loss_mae = torch.nn.L1Loss().to(DEVICE)\n",
        "    metric = monai.metrics.DiceMetric(include_background=False, reduction='mean_batch')\n",
        "    metrics = Metrics()\n",
        "\n",
        "    for i, subject in enumerate(dataloader['test']):\n",
        "        image = subject['t1'][tio.DATA].to(DEVICE)\n",
        "        label = subject['label'][tio.DATA].to(DEVICE)\n",
        "        one_hot_label = monai.networks.utils.one_hot(label, num_classes=num_classes, dim=1).to(DEVICE)\n",
        "        radius = subject['radius'].to(DEVICE)\n",
        "\n",
        "        mask, binary_mask, [mu, v, alpha, beta] = model(image)\n",
        "        loss_unet = loss_dice(mask, one_hot_label)\n",
        "        loss_ssm = loss_mae(mu, radius)\n",
        "        au = torch.mean(beta / (alpha - 1))\n",
        "        eu = torch.mean(beta / (v * (alpha - 1)))\n",
        "\n",
        "        loss = loss_unet\n",
        "                \n",
        "        metric(binary_mask, one_hot_label)\n",
        "        metrics.log(mode, 1, [loss, loss_unet, loss_ssm, au, eu])\n",
        "        \n",
        "        if save:\n",
        "            dest = f'{output_dir}{mode}_sample{i}.nrrd'\n",
        "            nrrd.write(dest, binary_mask.detach().cpu().numpy()[0, 0])\n",
        "\n",
        "    metrics.show(mode, 1)\n",
        "    mean_dsc = metric.aggregate().tolist()[0]\n",
        "    metric.reset()\n",
        "    print(f'{mode} DSC: {mean_dsc}')\n",
        "\n",
        "def train(model, n_epochs, dataloader, learning_rate, save):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
        "    loss_dice = monai.losses.DiceLoss(squared_pred=True).to(DEVICE)\n",
        "    loss_mae = torch.nn.L1Loss().to(DEVICE)\n",
        "    metric = monai.metrics.DiceMetric(include_background=False, reduction='mean_batch')\n",
        "    metrics = Metrics()\n",
        "    best_val_dsc = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        print(f'\\nEpoch {epoch}/{n_epochs}')\n",
        "        for mode in ['train', 'validation']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            for subject in dataloader[mode]:\n",
        "                image = subject['t1'][tio.DATA].to(DEVICE)\n",
        "                label = subject['label'][tio.DATA].to(DEVICE)\n",
        "                one_hot_label = monai.networks.utils.one_hot(label, num_classes=num_classes, dim=1).to(DEVICE)\n",
        "                radius = subject['radius'].to(DEVICE)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                mask, binary_mask, [mu, v, alpha, beta] = model(image)\n",
        "                loss_unet = loss_dice(mask, one_hot_label)\n",
        "                loss_ssm = loss_mae(mu, radius)\n",
        "                au = torch.mean(beta / (alpha - 1))\n",
        "                eu = torch.mean(beta / (v * (alpha - 1)))\n",
        "\n",
        "                loss = loss_unet if epoch <= 20 else loss_unet + 0.2 * loss_ssm + 10 * eu\n",
        "\n",
        "                if mode == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                metric(binary_mask, one_hot_label)\n",
        "                metrics.log(mode, epoch, [loss, loss_unet, loss_ssm, au, eu])\n",
        "            \n",
        "                if save:\n",
        "                    dest = f'{output_dir}{mode}_epoch{epoch}.nrrd'\n",
        "                    if not os.path.exists(dest):\n",
        "                        nrrd.write(dest, binary_mask.detach().cpu().numpy()[0, 0])\n",
        "\n",
        "            metrics.show(mode, epoch)\n",
        "            mean_dsc = metric.aggregate().tolist()[0]\n",
        "            metric.reset()\n",
        "            print(f'{mode} DSC: {mean_dsc}')\n",
        "            \n",
        "        scheduler.step()\n",
        "        if mean_dsc > best_val_dsc:\n",
        "            best_val_dsc = mean_dsc\n",
        "            best_epoch = epoch\n",
        "            if save:\n",
        "                torch.save(model.state_dict(), f'{output_dir}/best_model.torch')\n",
        "    \n",
        "    print(f'Best model saved after epoch {best_epoch}.')"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675740276392
        }
      },
      "id": "32b9f1a8-b3e3-4f8a-9425-d55fd64735c0"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(DEVICE)\n",
        "train(model=model, n_epochs=100, dataloader=dataloader, learning_rate=3e-4, save=False)\n",
        "# del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/100\n\ntrain loss: 0.5895840227603912\ntrain loss_unet: 0.5895840227603912\ntrain loss_ssm: 16.737570762634277\ntrain au: 1.1403746604919434\ntrain eu: 6.71121621131897\ntrain DSC: 0.04935355484485626\n\nvalidation loss: 0.5385707068443298\nvalidation loss_unet: 0.5385707068443298\nvalidation loss_ssm: 18.922767543792723\nvalidation au: 0.4119389832019806\nvalidation eu: 1.3886494088172912\nvalidation DSC: 0.09845607727766037\n\nEpoch 2/100\n\ntrain loss: 0.5558838546276093\ntrain loss_unet: 0.5558838546276093\ntrain loss_ssm: 13.272321701049805\ntrain au: 0.5839272439479828\ntrain eu: 1.1228052973747253\ntrain DSC: 0.07550905644893646\n\nvalidation loss: 0.5357460451126098\nvalidation loss_unet: 0.5357460451126098\nvalidation loss_ssm: 18.15577075958252\nvalidation au: 0.22167615711688995\nvalidation eu: 0.5099799835681915\nvalidation DSC: 0.15805105865001678\n\nEpoch 3/100\n\ntrain loss: 0.535557895898819\ntrain loss_unet: 0.535557895898819\ntrain loss_ssm: 8.435646533966064\ntrain au: 0.2773507907986641\ntrain eu: 0.3399588316679001\ntrain DSC: 0.09870445728302002\n\nvalidation loss: 0.530545687675476\nvalidation loss_unet: 0.530545687675476\nvalidation loss_ssm: 16.014985971450805\nvalidation au: 0.059064550697803496\nvalidation eu: 0.06681049205362796\nvalidation DSC: 0.34544169902801514\n\nEpoch 4/100\n\ntrain loss: 0.5200713276863098\ntrain loss_unet: 0.5200713276863098\ntrain loss_ssm: 4.120370388031006\ntrain au: 0.10908488556742668\ntrain eu: 0.09241456165909767\ntrain DSC: 0.1182849258184433\n\nvalidation loss: 0.5236423176527023\nvalidation loss_unet: 0.5236423176527023\nvalidation loss_ssm: 15.49959369659424\nvalidation au: 0.04412967912852764\nvalidation eu: 0.0456408279389143\nvalidation DSC: 0.5346289277076721\n\nEpoch 5/100\n\ntrain loss: 0.507083535194397\ntrain loss_unet: 0.507083535194397\ntrain loss_ssm: 1.690760612487793\ntrain au: 0.0635621901601553\ntrain eu: 0.045451875776052475\ntrain DSC: 0.13491317629814148\n\nvalidation loss: 0.5159119874238968\nvalidation loss_unet: 0.5159119874238968\nvalidation loss_ssm: 14.911124849319458\nvalidation au: 0.035281941145658496\nvalidation eu: 0.03345176476985216\nvalidation DSC: 0.626854658126831\n\nEpoch 6/100\n\ntrain loss: 0.4954938292503357\ntrain loss_unet: 0.4954938292503357\ntrain loss_ssm: 0.6910271644592285\ntrain au: 0.049000345170497894\ntrain eu: 0.03274017572402954\ntrain DSC: 0.14973944425582886\n\nvalidation loss: 0.508328617811203\nvalidation loss_unet: 0.508328617811203\nvalidation loss_ssm: 13.412266263961792\nvalidation au: 0.0315609934926033\nvalidation eu: 0.024502927716821433\nvalidation DSC: 0.6459740400314331\n\nEpoch 7/100\n\ntrain loss: 0.48435652256011963\ntrain loss_unet: 0.48435652256011963\ntrain loss_ssm: 0.6330766677856445\ntrain au: 0.04321344383060932\ntrain eu: 0.028067948296666145\ntrain DSC: 0.16479700803756714\n\nvalidation loss: 0.5007715582847595\nvalidation loss_unet: 0.5007715582847595\nvalidation loss_ssm: 10.48733567237854\nvalidation au: 0.03749404890462756\nvalidation eu: 0.02585847470909357\nvalidation DSC: 0.6064050197601318\n\nEpoch 8/100\n\ntrain loss: 0.47358494997024536\ntrain loss_unet: 0.47358494997024536\ntrain loss_ssm: 0.8870487213134766\ntrain au: 0.041729748249053955\ntrain eu: 0.026708373799920082\ntrain DSC: 0.18093731999397278\n\nvalidation loss: 0.4938214153051376\nvalidation loss_unet: 0.4938214153051376\nvalidation loss_ssm: 6.244883480072022\nvalidation au: 0.0381796296313405\nvalidation eu: 0.024890645556151867\nvalidation DSC: 0.5312610864639282\n\nEpoch 9/100\n\ntrain loss: 0.4631350338459015\ntrain loss_unet: 0.4631350338459015\ntrain loss_ssm: 1.4726734161376953\ntrain au: 0.03556524310261011\ntrain eu: 0.02211288269609213\ntrain DSC: 0.19699300825595856\n\nvalidation loss: 0.48713231325149536\nvalidation loss_unet: 0.48713231325149536\nvalidation loss_ssm: 3.14475191116333\nvalidation au: 0.04072441212832928\nvalidation eu: 0.027036178447306155\nvalidation DSC: 0.44906020164489746\n\nEpoch 10/100\n\ntrain loss: 0.4530394524335861\ntrain loss_unet: 0.4530394524335861\ntrain loss_ssm: 1.7044744491577148\ntrain au: 0.030211842618882656\ntrain eu: 0.018097192980349064\ntrain DSC: 0.21302315592765808\n\nvalidation loss: 0.476650185585022\nvalidation loss_unet: 0.476650185585022\nvalidation loss_ssm: 2.0035088539123533\nvalidation au: 0.03095905851572752\nvalidation eu: 0.01858013765886426\nvalidation DSC: 0.4301162660121918\n\nEpoch 11/100\n\ntrain loss: 0.44325749576091766\ntrain loss_unet: 0.44325749576091766\ntrain loss_ssm: 1.9128122329711914\ntrain au: 0.027262240648269653\ntrain eu: 0.01603128155693412\ntrain DSC: 0.23101525008678436\n\nvalidation loss: 0.46395898759365084\nvalidation loss_unet: 0.46395898759365084\nvalidation loss_ssm: 1.9421368980407714\nvalidation au: 0.024881404675543308\nvalidation eu: 0.013945032404735684\nvalidation DSC: 0.4476991593837738\n\nEpoch 12/100\n\ntrain loss: 0.43386466801166534\ntrain loss_unet: 0.43386466801166534\ntrain loss_ssm: 1.897848129272461\ntrain au: 0.0258574727922678\ntrain eu: 0.015132916625589132\ntrain DSC: 0.25142377614974976\n\nvalidation loss: 0.45225382804870606\nvalidation loss_unet: 0.45225382804870606\nvalidation loss_ssm: 1.970857448577881\nvalidation au: 0.02212035859003663\nvalidation eu: 0.011968390233814716\nvalidation DSC: 0.46352508664131165\n\nEpoch 13/100\n\ntrain loss: 0.42478059232234955\ntrain loss_unet: 0.42478059232234955\ntrain loss_ssm: 1.6419601440429688\ntrain au: 0.0236039562150836\ntrain eu: 0.013714686501771212\ntrain DSC: 0.2739109396934509\n\nvalidation loss: 0.4402470809221268\nvalidation loss_unet: 0.4402470809221268\nvalidation loss_ssm: 1.9875000953674316\nvalidation au: 0.0210519528016448\nvalidation eu: 0.011213428443297743\nvalidation DSC: 0.4826383888721466\n\nEpoch 14/100\n\ntrain loss: 0.4159763753414154\ntrain loss_unet: 0.4159763753414154\ntrain loss_ssm: 1.7046833038330078\ntrain au: 0.02149346563965082\ntrain eu: 0.012229353655129671\ntrain DSC: 0.29871517419815063\n\nvalidation loss: 0.42817173779010775\nvalidation loss_unet: 0.42817173779010775\nvalidation loss_ssm: 1.917986469268799\nvalidation au: 0.019844573475420475\nvalidation eu: 0.010410928847268224\nvalidation DSC: 0.510897159576416\n\nEpoch 15/100\n\ntrain loss: 0.4074360430240631\ntrain loss_unet: 0.4074360430240631\ntrain loss_ssm: 1.626908302307129\ntrain au: 0.020023170858621597\ntrain eu: 0.011219021398574114\ntrain DSC: 0.32477957010269165\n\nvalidation loss: 0.41706200182437897\nvalidation loss_unet: 0.41706200182437897\nvalidation loss_ssm: 1.819720458984375\nvalidation au: 0.01780899116769433\nvalidation eu: 0.009038397604599594\nvalidation DSC: 0.5418059825897217\n\nEpoch 16/100\n\ntrain loss: 0.39903877675533295\ntrain loss_unet: 0.39903877675533295\ntrain loss_ssm: 1.5700225830078125\ntrain au: 0.01903965137898922\ntrain eu: 0.010517308954149485\ntrain DSC: 0.35316991806030273\n\nvalidation loss: 0.405266991853714\nvalidation loss_unet: 0.405266991853714\nvalidation loss_ssm: 1.7995283126831054\nvalidation au: 0.015573113523423672\nvalidation eu: 0.007585415728390217\nvalidation DSC: 0.5718485116958618\n\nEpoch 17/100\n\ntrain loss: 0.39070186018943787\ntrain loss_unet: 0.39070186018943787\ntrain loss_ssm: 1.3998756408691406\ntrain au: 0.018351897597312927\ntrain eu: 0.010059313848614693\ntrain DSC: 0.3822747766971588\n\nvalidation loss: 0.3941743284463882\nvalidation loss_unet: 0.3941743284463882\nvalidation loss_ssm: 1.729798355102539\nvalidation au: 0.01433390649035573\nvalidation eu: 0.006816429616883397\nvalidation DSC: 0.5930067300796509\n\nEpoch 18/100\n\ntrain loss: 0.38243143260478973\ntrain loss_unet: 0.38243143260478973\ntrain loss_ssm: 1.2852811813354492\ntrain au: 0.01699472637847066\ntrain eu: 0.00913788890466094\ntrain DSC: 0.41213542222976685\n\nvalidation loss: 0.3842885226011276\nvalidation loss_unet: 0.3842885226011276\nvalidation loss_ssm: 1.6500724983215331\nvalidation au: 0.013228297103196382\nvalidation eu: 0.006178352367132902\nvalidation DSC: 0.6112013459205627\n\nEpoch 19/100\n\ntrain loss: 0.3741929531097412\ntrain loss_unet: 0.3741929531097412\ntrain loss_ssm: 1.2452621459960938\ntrain au: 0.016281194053590298\ntrain eu: 0.008681802544742823\ntrain DSC: 0.44298291206359863\n\nvalidation loss: 0.3741240310668945\nvalidation loss_unet: 0.3741240310668945\nvalidation loss_ssm: 1.6720705032348633\nvalidation au: 0.012041398752480746\nvalidation eu: 0.005511258877813816\nvalidation DSC: 0.6349412798881531\n\nEpoch 20/100\n\ntrain loss: 0.3659093677997589\ntrain loss_unet: 0.3659093677997589\ntrain loss_ssm: 1.1633071899414062\ntrain au: 0.015005984343588352\ntrain eu: 0.007848974782973528\ntrain DSC: 0.47516927123069763\n\nvalidation loss: 0.3650125068426132\nvalidation loss_unet: 0.3650125068426132\nvalidation loss_ssm: 1.7530703163146972\nvalidation au: 0.010849427673965692\nvalidation eu: 0.004867216334678233\nvalidation DSC: 0.6624081134796143\n\nEpoch 21/100\n\ntrain loss: 0.591463178396225\ntrain loss_unet: 0.3592923432588577\ntrain loss_ssm: 0.7840147018432617\ntrain au: 0.01453002355992794\ntrain eu: 0.007536788238212466\ntrain DSC: 0.5014162659645081\n\nvalidation loss: 1.3732039761543273\nvalidation loss_unet: 0.3733214420080185\nvalidation loss_ssm: 4.293054122924804\nvalidation au: 0.025226918160915376\nvalidation eu: 0.014127170108258725\nvalidation DSC: 0.571285605430603\n\nEpoch 22/100\n\ntrain loss: 0.9631175696849823\ntrain loss_unet: 0.3653465360403061\ntrain loss_ssm: 2.008497714996338\ntrain au: 0.03150442335754633\ntrain eu: 0.019607149995863438\ntrain DSC: 0.46585190296173096\n\nvalidation loss: 1.2451755797863007\nvalidation loss_unet: 0.354473187327385\nvalidation loss_ssm: 3.424976463317871\nvalidation au: 0.034970913603901865\nvalidation eu: 0.02057070953771472\nvalidation DSC: 0.5775216817855835\n\nEpoch 23/100\n\ntrain loss: 0.9138866066932678\ntrain loss_unet: 0.3626922219991684\ntrain loss_ssm: 1.75883150100708\ntrain au: 0.03180205635726452\ntrain eu: 0.019942807033658028\ntrain DSC: 0.46234485507011414\n\nvalidation loss: 0.9765764772891998\nvalidation loss_unet: 0.3510566645860672\nvalidation loss_ssm: 2.422210750579834\nvalidation au: 0.025580032728612423\nvalidation eu: 0.01410776488482952\nvalidation DSC: 0.5522913932800293\n\nEpoch 24/100\n\ntrain loss: 0.7301658987998962\ntrain loss_unet: 0.3645191490650177\ntrain loss_ssm: 1.0354557037353516\ntrain au: 0.026052293367683887\ntrain eu: 0.01585555961355567\ntrain DSC: 0.42574387788772583\n\nvalidation loss: 0.7598774725198746\nvalidation loss_unet: 0.3556038731336594\nvalidation loss_ssm: 1.5368073844909669\nvalidation au: 0.01864692712202668\nvalidation eu: 0.009691212186589837\nvalidation DSC: 0.5183471441268921\n\nEpoch 25/100\n\ntrain loss: 0.611726701259613\ntrain loss_unet: 0.36842721700668335\ntrain loss_ssm: 0.5023374557495117\ntrain au: 0.0236218785867095\ntrain eu: 0.014283197931945324\ntrain DSC: 0.38479238748550415\n\nvalidation loss: 0.8134574979543686\nvalidation loss_unet: 0.36004424810409547\nvalidation loss_ssm: 1.8924688720703124\nvalidation au: 0.014870191495865583\nvalidation eu: 0.007491946360096336\nvalidation DSC: 0.4914991855621338\n\nEpoch 26/100\n\ntrain loss: 0.597080409526825\ntrain loss_unet: 0.37340013682842255\ntrain loss_ssm: 0.4154834747314453\ntrain au: 0.02301705814898014\ntrain eu: 0.014058358501642942\ntrain DSC: 0.3546813726425171\n\nvalidation loss: 0.7777371746301651\nvalidation loss_unet: 0.35742623507976534\nvalidation loss_ssm: 1.7211119270324706\nvalidation au: 0.014812502060085535\nvalidation eu: 0.007608854975551367\nvalidation DSC: 0.4965071976184845\n\nEpoch 27/100\n\ntrain loss: 0.5703879594802856\ntrain loss_unet: 0.37622474133968353\ntrain loss_ssm: 0.2996244430541992\ntrain au: 0.02217225171625614\ntrain eu: 0.013423834461718798\ntrain DSC: 0.3387783169746399\n\nvalidation loss: 0.7795460516214371\nvalidation loss_unet: 0.35165550947189333\nvalidation loss_ssm: 1.7898955154418945\nvalidation au: 0.013540064543485641\nvalidation eu: 0.006991143524646759\nvalidation DSC: 0.5133296847343445\n\nEpoch 28/100\n\ntrain loss: 0.5668328106403351\ntrain loss_unet: 0.37723490595817566\ntrain loss_ssm: 0.37125635147094727\ntrain au: 0.019612404517829418\ntrain eu: 0.011534664314240217\ntrain DSC: 0.333255797624588\n\nvalidation loss: 0.7325430417060852\nvalidation loss_unet: 0.34519334733486173\nvalidation loss_ssm: 1.5426273155212402\nvalidation au: 0.014801633525639773\nvalidation eu: 0.007882422050461173\nvalidation DSC: 0.537620484828949\n\nEpoch 29/100\n\ntrain loss: 0.596333384513855\ntrain loss_unet: 0.3787537068128586\ntrain loss_ssm: 0.4870572090148926\ntrain au: 0.020149684511125088\ntrain eu: 0.012016823515295982\ntrain DSC: 0.3290337920188904\n\nvalidation loss: 0.7262418633699417\nvalidation loss_unet: 0.3402712416648865\nvalidation loss_ssm: 1.5946906280517579\nvalidation au: 0.012939433623105288\nvalidation eu: 0.0067032491508871314\nvalidation DSC: 0.5526201725006104\n\nEpoch 30/100\n\ntrain loss: 0.5730752646923065\ntrain loss_unet: 0.3776951879262924\ntrain loss_ssm: 0.5062966346740723\ntrain au: 0.016499829944223166\ntrain eu: 0.009412075392901897\ntrain DSC: 0.3287859261035919\n\nvalidation loss: 0.7582798629999161\nvalidation loss_unet: 0.33693200767040254\nvalidation loss_ssm: 1.8238788795471192\nvalidation au: 0.011274930406361818\nvalidation eu: 0.005657206354662776\nvalidation DSC: 0.5592913031578064\n\nEpoch 31/100\n\ntrain loss: 0.6401042342185974\ntrain loss_unet: 0.3761424273252487\ntrain loss_ssm: 0.9119243621826172\ntrain au: 0.01471899077296257\ntrain eu: 0.008157692849636078\ntrain DSC: 0.33089011907577515\n\nvalidation loss: 0.7124999225139618\nvalidation loss_unet: 0.33149460315704343\nvalidation loss_ssm: 1.6253410148620606\nvalidation au: 0.011259594764560461\nvalidation eu: 0.005593711538240314\nvalidation DSC: 0.5838235020637512\n\nEpoch 32/100\n\ntrain loss: 0.5430472195148468\ntrain loss_unet: 0.37548570334911346\ntrain loss_ssm: 0.45567798614501953\ntrain au: 0.01397622236981988\ntrain eu: 0.007642591139301658\ntrain DSC: 0.3337392807006836\n\nvalidation loss: 0.6773118752241135\nvalidation loss_unet: 0.3258761894702911\nvalidation loss_ssm: 1.4646090126037599\nvalidation au: 0.011838193666189909\nvalidation eu: 0.0058513877913355826\nvalidation DSC: 0.6198310852050781\n\nEpoch 33/100\n\ntrain loss: 0.5183624029159546\ntrain loss_unet: 0.3755565881729126\ntrain loss_ssm: 0.34827566146850586\ntrain au: 0.013379151932895184\ntrain eu: 0.007315064547583461\ntrain DSC: 0.33636584877967834\n\nvalidation loss: 0.6790282899141311\nvalidation loss_unet: 0.32151237785816195\nvalidation loss_ssm: 1.511295566558838\nvalidation au: 0.011366832312196493\nvalidation eu: 0.005525679588317871\nvalidation DSC: 0.6388474106788635\n\nEpoch 34/100\n\ntrain loss: 0.48378264904022217\ntrain loss_unet: 0.37433041632175446\ntrain loss_ssm: 0.2350926399230957\ntrain au: 0.01176578737795353\ntrain eu: 0.006243369309231639\ntrain DSC: 0.33848702907562256\n\nvalidation loss: 0.6487216383218766\nvalidation loss_unet: 0.3191743594408035\nvalidation loss_ssm: 1.3936136436462403\nvalidation au: 0.010620061177760362\nvalidation eu: 0.005082454746589065\nvalidation DSC: 0.6354395747184753\n\nEpoch 35/100\n\ntrain loss: 0.4878137409687042\ntrain loss_unet: 0.37265877425670624\ntrain loss_ssm: 0.26082658767700195\ntrain au: 0.01191549189388752\ntrain eu: 0.006298966472968459\ntrain DSC: 0.3404858708381653\n\nvalidation loss: 0.6456438469886779\nvalidation loss_unet: 0.31785849928855897\nvalidation loss_ssm: 1.3903813934326172\nvalidation au: 0.010420413222163916\nvalidation eu: 0.004970906209200621\nvalidation DSC: 0.6343071460723877\n\nEpoch 36/100\n\ntrain loss: 0.4693310558795929\ntrain loss_unet: 0.3715571165084839\ntrain loss_ssm: 0.17667770385742188\ntrain au: 0.011797591112554073\ntrain eu: 0.006243839394301176\ntrain DSC: 0.34343209862709045\n\nvalidation loss: 0.6562235939502716\nvalidation loss_unet: 0.31584379136562346\nvalidation loss_ssm: 1.4466354179382324\nvalidation au: 0.010660205781459809\nvalidation eu: 0.005105271497741342\nvalidation DSC: 0.6434516906738281\n\nEpoch 37/100\n\ntrain loss: 0.4896458238363266\ntrain loss_unet: 0.3705061078071594\ntrain loss_ssm: 0.28937339782714844\ntrain au: 0.011584190186113119\ntrain eu: 0.006126503925770521\ntrain DSC: 0.34820425510406494\n\nvalidation loss: 0.6649207431077957\nvalidation loss_unet: 0.31441313564777373\nvalidation loss_ssm: 1.4999798393249513\nvalidation au: 0.010570886814966797\nvalidation eu: 0.005051163379102945\nvalidation DSC: 0.6482611298561096\n\nEpoch 38/100\n\ntrain loss: 0.4564032703638077\ntrain loss_unet: 0.3691209852695465\ntrain loss_ssm: 0.13704347610473633\ntrain au: 0.011352519039064646\ntrain eu: 0.00598736060783267\ntrain DSC: 0.3525731563568115\n\nvalidation loss: 0.6531434488296509\nvalidation loss_unet: 0.3145940226316452\nvalidation loss_ssm: 1.4562484550476074\nvalidation au: 0.009987546894699335\nvalidation eu: 0.004729973031207919\nvalidation DSC: 0.6436183452606201\n\nEpoch 39/100\n\ntrain loss: 0.47198644280433655\ntrain loss_unet: 0.36765924096107483\ntrain loss_ssm: 0.23415851593017578\ntrain au: 0.010961936321109533\ntrain eu: 0.00574954878538847\ntrain DSC: 0.358296662569046\n\nvalidation loss: 0.6603213465213775\nvalidation loss_unet: 0.315614093542099\nvalidation loss_ssm: 1.4929646682739257\nvalidation au: 0.009747786615043878\nvalidation eu: 0.004611431276425719\nvalidation DSC: 0.6406827569007874\n\nEpoch 40/100\n\ntrain loss: 0.4514521211385727\ntrain loss_unet: 0.36626139283180237\ntrain loss_ssm: 0.14197587966918945\ntrain au: 0.010820320341736078\ntrain eu: 0.0056795545388013124\ntrain DSC: 0.36838749051094055\n\nvalidation loss: 0.7237410062551498\nvalidation loss_unet: 0.31728356003761293\nvalidation loss_ssm: 1.7915583992004394\nvalidation au: 0.010067704636603593\nvalidation eu: 0.004814575724303723\nvalidation DSC: 0.6427972316741943\n\nEpoch 41/100\n\ntrain loss: 0.4847147911787033\ntrain loss_unet: 0.36503493785858154\ntrain loss_ssm: 0.314908504486084\ntrain au: 0.010735086165368557\ntrain eu: 0.005669815465807915\ntrain DSC: 0.3810480237007141\n\nvalidation loss: 0.7387040472030639\nvalidation loss_unet: 0.31684958696365356\nvalidation loss_ssm: 1.8728434944152832\nvalidation au: 0.009907123250886798\nvalidation eu: 0.004728575618937612\nvalidation DSC: 0.644580066204071\n\nEpoch 42/100\n\ntrain loss: 0.4390650987625122\ntrain loss_unet: 0.3632671535015106\ntrain loss_ssm: 0.1041111946105957\ntrain au: 0.010458351112902164\ntrain eu: 0.005497570615261793\ntrain DSC: 0.3878185749053955\n\nvalidation loss: 0.6836625766754151\nvalidation loss_unet: 0.31488965153694154\nvalidation loss_ssm: 1.631737594604492\nvalidation au: 0.009048033189028502\nvalidation eu: 0.004242540700361132\nvalidation DSC: 0.6432974338531494\n\nEpoch 43/100\n\ntrain loss: 0.45768970251083374\ntrain loss_unet: 0.3615601658821106\ntrain loss_ssm: 0.21470069885253906\ntrain au: 0.010171756148338318\ntrain eu: 0.005318940617144108\ntrain DSC: 0.3912014663219452\n\nvalidation loss: 0.672640939950943\nvalidation loss_unet: 0.3136519771814346\nvalidation loss_ssm: 1.5936508178710938\nvalidation au: 0.008662864957004785\nvalidation eu: 0.0040258793393149975\nvalidation DSC: 0.6449922323226929\n\nEpoch 44/100\n\ntrain loss: 0.45385831594467163\ntrain loss_unet: 0.3598588556051254\ntrain loss_ssm: 0.19915485382080078\ntrain au: 0.010306288953870535\ntrain eu: 0.0054168489295989275\ntrain DSC: 0.39899104833602905\n\nvalidation loss: 0.692023714184761\nvalidation loss_unet: 0.31237618029117586\nvalidation loss_ssm: 1.6999692153930663\nvalidation au: 0.008568491060286761\nvalidation eu: 0.003965369090437889\nvalidation DSC: 0.6559492349624634\n\nEpoch 45/100\n\ntrain loss: 0.44297343492507935\ntrain loss_unet: 0.3580428957939148\ntrain loss_ssm: 0.16578245162963867\ntrain au: 0.009913492016494274\ntrain eu: 0.00517740473151207\ntrain DSC: 0.4110390543937683\n\nvalidation loss: 0.7053612059354782\nvalidation loss_unet: 0.31030856490135195\nvalidation loss_ssm: 1.782274227142334\nvalidation au: 0.008390507139265537\nvalidation eu: 0.003859778824262321\nvalidation DSC: 0.668100893497467\n\nEpoch 46/100\n\ntrain loss: 0.4537801295518875\ntrain loss_unet: 0.35646289587020874\ntrain loss_ssm: 0.23790597915649414\ntrain au: 0.009571899194270372\ntrain eu: 0.004973604576662183\ntrain DSC: 0.41873639822006226\n\nvalidation loss: 0.7012784087657928\nvalidation loss_unet: 0.3083792370557785\nvalidation loss_ssm: 1.775659999847412\nvalidation au: 0.008244031108915806\nvalidation eu: 0.0037767165899276734\nvalidation DSC: 0.6711647510528564\n\nEpoch 47/100\n\ntrain loss: 0.42499426007270813\ntrain loss_unet: 0.3554309010505676\ntrain loss_ssm: 0.10169029235839844\ntrain au: 0.009509155061095953\ntrain eu: 0.004922529216855764\ntrain DSC: 0.41916871070861816\n\nvalidation loss: 0.6820761972665786\nvalidation loss_unet: 0.30760299861431123\nvalidation loss_ssm: 1.6826792907714845\nvalidation au: 0.00826648817397654\nvalidation eu: 0.003793733986094594\nvalidation DSC: 0.659651517868042\n\nEpoch 48/100\n\ntrain loss: 0.45007985830307007\ntrain loss_unet: 0.35512764751911163\ntrain loss_ssm: 0.22548627853393555\ntrain au: 0.00963806128129363\ntrain eu: 0.004985495703294873\ntrain DSC: 0.4145232141017914\n\nvalidation loss: 0.669171833395958\nvalidation loss_unet: 0.30664048552513123\nvalidation loss_ssm: 1.6229240226745605\nvalidation au: 0.00825702715665102\nvalidation eu: 0.003794653997756541\nvalidation DSC: 0.6532080173492432\n\nEpoch 49/100\n\ntrain loss: 0.44405311346054077\ntrain loss_unet: 0.35442782938480377\ntrain loss_ssm: 0.18762731552124023\ntrain au: 0.009993490763008595\ntrain eu: 0.0052099828608334064\ntrain DSC: 0.41309547424316406\n\nvalidation loss: 0.6884016782045365\nvalidation loss_unet: 0.3043970984220505\nvalidation loss_ssm: 1.7235931968688964\nvalidation au: 0.00850157511420548\nvalidation eu: 0.003928593262098729\nvalidation DSC: 0.658315122127533\n\nEpoch 50/100\n\ntrain loss: 0.4195443093776703\ntrain loss_unet: 0.3534315675497055\ntrain loss_ssm: 0.07645702362060547\ntrain au: 0.009745141956955194\ntrain eu: 0.005082132061943412\ntrain DSC: 0.413623183965683\n\nvalidation loss: 0.6928230768442154\nvalidation loss_unet: 0.30270758271217346\nvalidation loss_ssm: 1.752731819152832\nvalidation au: 0.008553229877725244\nvalidation eu: 0.003956912294961512\nvalidation DSC: 0.66105055809021\n\nEpoch 51/100\n\ntrain loss: 0.4188230633735657\ntrain loss_unet: 0.3528033047914505\ntrain loss_ssm: 0.09030771255493164\ntrain au: 0.00928450096398592\ntrain eu: 0.004795822547748685\ntrain DSC: 0.4137934744358063\n\nvalidation loss: 0.6912996751070023\nvalidation loss_unet: 0.3021070736646652\nvalidation loss_ssm: 1.7495532608032227\nvalidation au: 0.008491303343325853\nvalidation eu: 0.003928193803876639\nvalidation DSC: 0.660422682762146\n\nEpoch 52/100\n\ntrain loss: 0.41176076233386993\ntrain loss_unet: 0.35216405987739563\ntrain loss_ssm: 0.06597900390625\ntrain au: 0.008972764946520329\ntrain eu: 0.004640090046450496\ntrain DSC: 0.415065199136734\n\nvalidation loss: 0.6808422356843948\nvalidation loss_unet: 0.3014023888111115\nvalidation loss_ssm: 1.7081487083435059\nvalidation au: 0.00821569594554603\nvalidation eu: 0.003781009647063911\nvalidation DSC: 0.6584778428077698\n\nEpoch 53/100\n\ntrain loss: 0.42144574224948883\ntrain loss_unet: 0.35139355063438416\ntrain loss_ssm: 0.12684202194213867\ntrain au: 0.008697054348886013\ntrain eu: 0.0044683790765702724\ntrain DSC: 0.4165200889110565\n\nvalidation loss: 0.6968242257833481\nvalidation loss_unet: 0.30009249448776243\nvalidation loss_ssm: 1.7898488807678223\nvalidation au: 0.008385256482288241\nvalidation eu: 0.0038761948654428124\nvalidation DSC: 0.6643650531768799\n\nEpoch 54/100\n\ntrain loss: 0.40880170464515686\ntrain loss_unet: 0.350149005651474\ntrain loss_ssm: 0.06587648391723633\ntrain au: 0.00882924348115921\ntrain eu: 0.004547738237306476\ntrain DSC: 0.42225074768066406\n\nvalidation loss: 0.7024162554740906\nvalidation loss_unet: 0.29913309395313264\nvalidation loss_ssm: 1.8290250968933106\nvalidation au: 0.00813761556521058\nvalidation eu: 0.0037478136597201227\nvalidation DSC: 0.667530357837677\n\nEpoch 55/100\n\ntrain loss: 0.4083425849676132\ntrain loss_unet: 0.34912821650505066\ntrain loss_ssm: 0.08015918731689453\ntrain au: 0.008444065228104591\ntrain eu: 0.004318254068493843\ntrain DSC: 0.42690056562423706\n\nvalidation loss: 0.6999815928936005\nvalidation loss_unet: 0.2992193776369095\nvalidation loss_ssm: 1.823237190246582\nvalidation au: 0.007877485454082489\nvalidation eu: 0.003611477189697325\nvalidation DSC: 0.6685389876365662\n\nEpoch 56/100\n\ntrain loss: 0.400619775056839\ntrain loss_unet: 0.34794309735298157\ntrain loss_ssm: 0.05807924270629883\ntrain au: 0.008073012810200453\ntrain eu: 0.004106082953512669\ntrain DSC: 0.43450456857681274\n\nvalidation loss: 0.7239899456501007\nvalidation loss_unet: 0.30035572350025175\nvalidation loss_ssm: 1.936040153503418\nvalidation au: 0.007900422159582376\nvalidation eu: 0.0036426182882860303\nvalidation DSC: 0.6693050861358643\n\nEpoch 57/100\n\ntrain loss: 0.4246583431959152\ntrain loss_unet: 0.34666410088539124\ntrain loss_ssm: 0.18407773971557617\ntrain au: 0.008068196708336473\ntrain eu: 0.00411786965560168\ntrain DSC: 0.44631874561309814\n\nvalidation loss: 0.7339651775360108\nvalidation loss_unet: 0.30041070878505705\nvalidation loss_ssm: 1.9891325187683107\nvalidation au: 0.007759195286780596\nvalidation eu: 0.003572795279324055\nvalidation DSC: 0.6694729328155518\n\nEpoch 58/100\n\ntrain loss: 0.41455182433128357\ntrain loss_unet: 0.34549224376678467\ntrain loss_ssm: 0.14918279647827148\ntrain au: 0.007751215249300003\ntrain eu: 0.003922301111742854\ntrain DSC: 0.452358216047287\n\nvalidation loss: 0.7155424898862839\nvalidation loss_unet: 0.2993214046955109\nvalidation loss_ssm: 1.9152169036865234\nvalidation au: 0.0072944856993854045\nvalidation eu: 0.003317769356071949\nvalidation DSC: 0.670096755027771\n\nEpoch 59/100\n\ntrain loss: 0.3900274634361267\ntrain loss_unet: 0.34428921341896057\ntrain loss_ssm: 0.037494659423828125\ntrain au: 0.007587377913296223\ntrain eu: 0.003823931561782956\ntrain DSC: 0.4577680230140686\n\nvalidation loss: 0.7344297134876251\nvalidation loss_unet: 0.2985976731777191\nvalidation loss_ssm: 2.016175193786621\nvalidation au: 0.0071755931340157985\nvalidation eu: 0.0032596996799111365\nvalidation DSC: 0.6768887042999268\n\nEpoch 60/100\n\ntrain loss: 0.42209742963314056\ntrain loss_unet: 0.34276528656482697\ntrain loss_ssm: 0.2022571563720703\ntrain au: 0.007688194280490279\ntrain eu: 0.003888071747496724\ntrain DSC: 0.4682766795158386\n\nvalidation loss: 0.7268330472707748\nvalidation loss_unet: 0.2977939110994339\nvalidation loss_ssm: 1.9889245223999024\nvalidation au: 0.006923496583476662\nvalidation eu: 0.0031254226388409733\nvalidation DSC: 0.677613377571106\n\nEpoch 61/100\n\ntrain loss: 0.38658425211906433\ntrain loss_unet: 0.3419431447982788\ntrain loss_ssm: 0.03268289566040039\ntrain au: 0.0075691083911806345\ntrain eu: 0.0038104531122371554\ntrain DSC: 0.46990424394607544\n\nvalidation loss: 0.6955320596694946\nvalidation loss_unet: 0.2975209850072861\nvalidation loss_ssm: 1.8406262969970704\nvalidation au: 0.006677190102636814\nvalidation eu: 0.0029885812476277353\nvalidation DSC: 0.669894278049469\n\nEpoch 62/100\n\ntrain loss: 0.4449796825647354\ntrain loss_unet: 0.34175099432468414\ntrain loss_ssm: 0.33465147018432617\ntrain au: 0.007274184608832002\ntrain eu: 0.0036298392806202173\ntrain DSC: 0.4655555784702301\n\nvalidation loss: 0.6883177632093429\nvalidation loss_unet: 0.2971346127986908\nvalidation loss_ssm: 1.8085271644592285\nvalidation au: 0.006602486185729503\nvalidation eu: 0.002947771712206304\nvalidation DSC: 0.6665070056915283\n\nEpoch 63/100\n\ntrain loss: 0.4297884702682495\ntrain loss_unet: 0.3408404290676117\ntrain loss_ssm: 0.2655014991760254\ntrain au: 0.007209410425275564\ntrain eu: 0.00358477421104908\ntrain DSC: 0.4688073992729187\n\nvalidation loss: 0.7015807902812958\nvalidation loss_unet: 0.29581106066703794\nvalidation loss_ssm: 1.8826725006103515\nvalidation au: 0.006545654833316803\nvalidation eu: 0.002923523103818297\nvalidation DSC: 0.6722475290298462\n\nEpoch 64/100\n\ntrain loss: 0.4026957005262375\ntrain loss_unet: 0.33918942511081696\ntrain loss_ssm: 0.14440202713012695\ntrain au: 0.006994935218244791\ntrain eu: 0.003462588065303862\ntrain DSC: 0.47708314657211304\n\nvalidation loss: 0.7213152748346329\nvalidation loss_unet: 0.2947211843729019\nvalidation loss_ssm: 1.9839362907409668\nvalidation au: 0.006639642864465714\nvalidation eu: 0.0029806831385940312\nvalidation DSC: 0.6789565086364746\n\nEpoch 65/100\n\ntrain loss: 0.39090941846370697\ntrain loss_unet: 0.33766017854213715\ntrain loss_ssm: 0.09638786315917969\ntrain au: 0.006885940907523036\ntrain eu: 0.0033971667289733887\ntrain DSC: 0.4838917851448059\n\nvalidation loss: 0.7131406080722809\nvalidation loss_unet: 0.29395671367645265\nvalidation loss_ssm: 1.947635555267334\nvalidation au: 0.0066153175849467515\nvalidation eu: 0.0029656785819679498\nvalidation DSC: 0.6792933940887451\n\nEpoch 66/100\n\ntrain loss: 0.3779628276824951\ntrain loss_unet: 0.33658431470394135\ntrain loss_ssm: 0.05075645446777344\ntrain au: 0.0064240251667797565\ntrain eu: 0.003122721565887332\ntrain DSC: 0.4862534999847412\n\nvalidation loss: 0.7201944863796235\nvalidation loss_unet: 0.2935233283042908\nvalidation loss_ssm: 1.983307113647461\nvalidation au: 0.006676370389759541\nvalidation eu: 0.0030009731417521834\nvalidation DSC: 0.6826176643371582\n\nEpoch 67/100\n\ntrain loss: 0.3832378536462784\ntrain loss_unet: 0.3352288007736206\ntrain loss_ssm: 0.0793156623840332\ntrain au: 0.006576952058821917\ntrain eu: 0.003214591764844954\ntrain DSC: 0.49249374866485596\n\nvalidation loss: 0.723821342587471\nvalidation loss_unet: 0.29177255153656007\nvalidation loss_ssm: 2.011243724822998\nvalidation au: 0.006644765995442867\nvalidation eu: 0.002980005107820034\nvalidation DSC: 0.6904440522193909\n\nEpoch 68/100\n\ntrain loss: 0.3948870599269867\ntrain loss_unet: 0.3335348814725876\ntrain loss_ssm: 0.14046239852905273\ntrain au: 0.006785096367821097\ntrain eu: 0.0033259695628657937\ntrain DSC: 0.5006365180015564\n\nvalidation loss: 0.7155781888961792\nvalidation loss_unet: 0.2903415733575821\nvalidation loss_ssm: 1.9786424827575684\nvalidation au: 0.006607930408790708\nvalidation eu: 0.0029508114233613015\nvalidation DSC: 0.6942683458328247\n\nEpoch 69/100\n\ntrain loss: 0.39800912141799927\ntrain loss_unet: 0.3322245627641678\ntrain loss_ssm: 0.1605386734008789\ntrain au: 0.006882539251819253\ntrain eu: 0.003367683384567499\ntrain DSC: 0.5051175951957703\n\nvalidation loss: 0.716378361582756\nvalidation loss_unet: 0.29024197041988375\nvalidation loss_ssm: 1.981236515045166\nvalidation au: 0.006673958441242575\nvalidation eu: 0.0029889083188027145\nvalidation DSC: 0.6929014325141907\n\nEpoch 70/100\n\ntrain loss: 0.4072703868150711\ntrain loss_unet: 0.33116890490055084\ntrain loss_ssm: 0.2117624282836914\ntrain au: 0.006876544328406453\ntrain eu: 0.003374899737536907\ntrain DSC: 0.5088753700256348\n\nvalidation loss: 0.7208259242773056\nvalidation loss_unet: 0.29114017307758333\nvalidation loss_ssm: 1.9967669868469238\nvalidation au: 0.006731461966410279\nvalidation eu: 0.003033234877511859\nvalidation DSC: 0.6881213188171387\n\nEpoch 71/100\n\ntrain loss: 0.38107365369796753\ntrain loss_unet: 0.330360546708107\ntrain loss_ssm: 0.08594083786010742\ntrain au: 0.006823761155828834\ntrain eu: 0.003352494561113417\ntrain DSC: 0.5124498605728149\n\nvalidation loss: 0.7304360902309418\nvalidation loss_unet: 0.2927040022611618\nvalidation loss_ssm: 2.0324083137512208\nvalidation au: 0.006871552597731352\nvalidation eu: 0.003125041541643441\nvalidation DSC: 0.6826212406158447\n\nEpoch 72/100\n\ntrain loss: 0.3835018277168274\ntrain loss_unet: 0.3298031687736511\ntrain loss_ssm: 0.09467935562133789\ntrain au: 0.007030422566458583\ntrain eu: 0.003476278390735388\ntrain DSC: 0.5153936743736267\n\nvalidation loss: 0.7159635603427887\nvalidation loss_unet: 0.2942636168003082\nvalidation loss_ssm: 1.952547836303711\nvalidation au: 0.006843651439994574\nvalidation eu: 0.0031190371094271543\nvalidation DSC: 0.6716811656951904\n\nEpoch 73/100\n\ntrain loss: 0.37554430961608887\ntrain loss_unet: 0.329836830496788\ntrain loss_ssm: 0.056353092193603516\ntrain au: 0.006955964723601937\ntrain eu: 0.00344368617516011\ntrain DSC: 0.5131707191467285\n\nvalidation loss: 0.7348481810092926\nvalidation loss_unet: 0.2962044012546539\nvalidation loss_ssm: 2.0315567207336427\nvalidation au: 0.007032213443890214\nvalidation eu: 0.003233242426067591\nvalidation DSC: 0.6655386090278625\n\nEpoch 74/100\n\ntrain loss: 0.3876683861017227\ntrain loss_unet: 0.32952362298965454\ntrain loss_ssm: 0.11063861846923828\ntrain au: 0.0072069314774125814\ntrain eu: 0.003601705189794302\ntrain DSC: 0.517288088798523\n\nvalidation loss: 0.7774402529001236\nvalidation loss_unet: 0.2969591248035431\nvalidation loss_ssm: 2.22655969619751\nvalidation au: 0.0075248186476528645\nvalidation eu: 0.003516917759552598\nvalidation DSC: 0.6626303195953369\n\nEpoch 75/100\n\ntrain loss: 0.3872295320034027\ntrain loss_unet: 0.3288199454545975\ntrain loss_ssm: 0.10731029510498047\ntrain au: 0.007366237696260214\ntrain eu: 0.003694753162562847\ntrain DSC: 0.520409882068634\n\nvalidation loss: 0.739703220129013\nvalidation loss_unet: 0.29510404586791994\nvalidation loss_ssm: 2.060919075012207\nvalidation au: 0.007034975299611688\nvalidation eu: 0.003241535113193095\nvalidation DSC: 0.6616303324699402\n\nEpoch 76/100\n\ntrain loss: 0.4024686813354492\ntrain loss_unet: 0.3276553750038147\ntrain loss_ssm: 0.20704412460327148\ntrain au: 0.006782248383387923\ntrain eu: 0.0033404482528567314\ntrain DSC: 0.5213077068328857\n\nvalidation loss: 0.734180217385292\nvalidation loss_unet: 0.29382147789001467\nvalidation loss_ssm: 2.0403919792175294\nvalidation au: 0.007012749742716551\nvalidation eu: 0.0032280339067801834\nvalidation DSC: 0.6653720140457153\n\nEpoch 77/100\n\ntrain loss: 0.38056154549121857\ntrain loss_unet: 0.32612766325473785\ntrain loss_ssm: 0.09998416900634766\ntrain au: 0.0069622618611902\ntrain eu: 0.003443704452365637\ntrain DSC: 0.529457688331604\n\nvalidation loss: 0.8048473805189132\nvalidation loss_unet: 0.29326933443546294\nvalidation loss_ssm: 2.372957649230957\nvalidation au: 0.007839831328019499\nvalidation eu: 0.0036986508313566445\nvalidation DSC: 0.6753182411193848\n\nEpoch 78/100\n\ntrain loss: 0.41381271183490753\ntrain loss_unet: 0.3240646570920944\ntrain loss_ssm: 0.26827239990234375\ntrain au: 0.007216432131826878\ntrain eu: 0.003609356703236699\ntrain DSC: 0.5431308746337891\n\nvalidation loss: 0.7386594927310943\nvalidation loss_unet: 0.29050800681114197\nvalidation loss_ssm: 2.083032684326172\nvalidation au: 0.006876813359558582\nvalidation eu: 0.0031544945063069463\nvalidation DSC: 0.681377112865448\n\nEpoch 79/100\n\ntrain loss: 0.3599640280008316\ntrain loss_unet: 0.3222680687904358\ntrain loss_ssm: 0.023575782775878906\ntrain au: 0.006707388209179044\ntrain eu: 0.0032980797113850713\ntrain DSC: 0.5468918085098267\n\nvalidation loss: 0.7032230406999588\nvalidation loss_unet: 0.2878396683931351\nvalidation loss_ssm: 1.931849193572998\nvalidation au: 0.006423486862331629\nvalidation eu: 0.002901352923363447\nvalidation DSC: 0.6841092109680176\n\nEpoch 80/100\n\ntrain loss: 0.3753444403409958\ntrain loss_unet: 0.3207200914621353\ntrain loss_ssm: 0.11881542205810547\ntrain au: 0.006341159576550126\ntrain eu: 0.0030861259438097477\ntrain DSC: 0.54933762550354\n\nvalidation loss: 0.6912150073051453\nvalidation loss_unet: 0.2856995570659637\nvalidation loss_ssm: 1.888251953125\nvalidation au: 0.0062187464442104105\nvalidation eu: 0.0027865050919353962\nvalidation DSC: 0.6909982562065125\n\nEpoch 81/100\n\ntrain loss: 0.3603130131959915\ntrain loss_unet: 0.31914615631103516\ntrain loss_ssm: 0.0575861930847168\ntrain au: 0.006134371971711516\ntrain eu: 0.0029649623902514577\ntrain DSC: 0.5553301572799683\n\nvalidation loss: 0.6848506397008896\nvalidation loss_unet: 0.28426398158073424\nvalidation loss_ssm: 1.8664509773254394\nvalidation au: 0.006115935398265719\nvalidation eu: 0.002729645809158683\nvalidation DSC: 0.6961647272109985\n\nEpoch 82/100\n\ntrain loss: 0.37225979566574097\ntrain loss_unet: 0.31802256405353546\ntrain loss_ssm: 0.12329721450805664\ntrain au: 0.006110999034717679\ntrain eu: 0.002957777469418943\ntrain DSC: 0.558234453201294\n\nvalidation loss: 0.6744648694992066\nvalidation loss_unet: 0.2832194781303406\nvalidation loss_ssm: 1.8234624481201172\nvalidation au: 0.005981479808688164\nvalidation eu: 0.002655289825052023\nvalidation DSC: 0.6967068314552307\n\nEpoch 83/100\n\ntrain loss: 0.36264458298683167\ntrain loss_unet: 0.3172433525323868\ntrain loss_ssm: 0.08547067642211914\ntrain au: 0.0059015091974288225\ntrain eu: 0.002830709097906947\ntrain DSC: 0.560325026512146\n\nvalidation loss: 0.6808926379680633\nvalidation loss_unet: 0.28224933207035063\nvalidation loss_ssm: 1.8624005317687988\nvalidation au: 0.0059163283929228785\nvalidation eu: 0.002616319037042558\nvalidation DSC: 0.6997045278549194\n\nEpoch 84/100\n\ntrain loss: 0.3585594743490219\ntrain loss_unet: 0.31612126529216766\ntrain loss_ssm: 0.06952571868896484\ntrain au: 0.00596066121943295\ntrain eu: 0.002853306708857417\ntrain DSC: 0.5660662055015564\n\nvalidation loss: 0.6953310596942902\nvalidation loss_unet: 0.28007286667823794\nvalidation loss_ssm: 1.9480041885375976\nvalidation au: 0.005835608495399356\nvalidation eu: 0.0025657350150868298\nvalidation DSC: 0.7137734889984131\n\nEpoch 85/100\n\ntrain loss: 0.36089904606342316\ntrain loss_unet: 0.3140570968389511\ntrain loss_ssm: 0.09483671188354492\ntrain au: 0.005854679504409432\ntrain eu: 0.0027874604566022754\ntrain DSC: 0.5784773826599121\n\nvalidation loss: 0.6973985117673874\nvalidation loss_unet: 0.2787647920846939\nvalidation loss_ssm: 1.9676088905334472\nvalidation au: 0.005739003671333193\nvalidation eu: 0.002511193063110113\nvalidation DSC: 0.719835638999939\n\nEpoch 86/100\n\ntrain loss: 0.3702023774385452\ntrain loss_unet: 0.31303955614566803\ntrain loss_ssm: 0.14653968811035156\ntrain au: 0.005861995276063681\ntrain eu: 0.002785487682558596\ntrain DSC: 0.5829967260360718\n\nvalidation loss: 0.7078380632400513\nvalidation loss_unet: 0.2784636968374252\nvalidation loss_ssm: 2.020303363800049\nvalidation au: 0.005779962772503495\nvalidation eu: 0.002531368718482554\nvalidation DSC: 0.7228626012802124\n\nEpoch 87/100\n\ntrain loss: 0.36067596077919006\ntrain loss_unet: 0.3124621957540512\ntrain loss_ssm: 0.09514856338500977\ntrain au: 0.006101890001446009\ntrain eu: 0.0029184038285166025\ntrain DSC: 0.5853891372680664\n\nvalidation loss: 0.7123878544569016\nvalidation loss_unet: 0.27959197759628296\nvalidation loss_ssm: 2.0337473487854005\nvalidation au: 0.00591327884234488\nvalidation eu: 0.002604639264754951\nvalidation DSC: 0.713547945022583\n\nEpoch 88/100\n\ntrain loss: 0.3619437515735626\ntrain loss_unet: 0.313001349568367\ntrain loss_ssm: 0.09693098068237305\ntrain au: 0.0061509576626122\ntrain eu: 0.0029556213412433863\ntrain DSC: 0.5790178775787354\n\nvalidation loss: 0.7202632766962052\nvalidation loss_unet: 0.28136791169643405\nvalidation loss_ssm: 2.0588672256469724\nvalidation au: 0.006107450239360332\nvalidation eu: 0.0027121916273608803\nvalidation DSC: 0.7048105001449585\n\nEpoch 89/100\n\ntrain loss: 0.36424268782138824\ntrain loss_unet: 0.3134894520044327\ntrain loss_ssm: 0.09677791595458984\ntrain au: 0.006471569882705808\ntrain eu: 0.0031397644197568297\ntrain DSC: 0.5754594206809998\n\nvalidation loss: 0.7256494975090027\nvalidation loss_unet: 0.2832502895593643\nvalidation loss_ssm: 2.0698291778564455\nvalidation au: 0.006343427421525121\nvalidation eu: 0.0028433358296751977\nvalidation DSC: 0.6968709826469421\n\nEpoch 90/100\n\ntrain loss: 0.3470248728990555\ntrain loss_unet: 0.31401313841342926\ntrain loss_ssm: 0.004661083221435547\ntrain au: 0.006579712498933077\ntrain eu: 0.003207951900549233\ntrain DSC: 0.5715897083282471\n\nvalidation loss: 0.7141227871179581\nvalidation loss_unet: 0.28471924364566803\nvalidation loss_ssm: 2.004476909637451\nvalidation au: 0.0063477569073438645\nvalidation eu: 0.002850815593264997\nvalidation DSC: 0.6832732558250427\n\nEpoch 91/100\n\ntrain loss: 0.3933244049549103\ntrain loss_unet: 0.3150101602077484\ntrain loss_ssm: 0.23940372467041016\ntrain au: 0.006283456459641457\ntrain eu: 0.0030433497158810496\ntrain DSC: 0.5616878271102905\n\nvalidation loss: 0.7090798777341842\nvalidation loss_unet: 0.28553655862808225\nvalidation loss_ssm: 1.9745198059082032\nvalidation au: 0.006362088555470109\nvalidation eu: 0.002863935511559248\nvalidation DSC: 0.6757118701934814\n\nEpoch 92/100\n\ntrain loss: 0.3854771554470062\ntrain loss_unet: 0.3150519132614136\ntrain loss_ssm: 0.19908523559570312\ntrain au: 0.006312573794275522\ntrain eu: 0.003060820745304227\ntrain DSC: 0.5593497157096863\n\nvalidation loss: 0.7312856411933899\nvalidation loss_unet: 0.28601815342903136\nvalidation loss_ssm: 2.0766989517211916\nvalidation au: 0.00658776007592678\nvalidation eu: 0.0029927690094336866\nvalidation DSC: 0.6753891110420227\n\nEpoch 93/100\n\ntrain loss: 0.3668474853038788\ntrain loss_unet: 0.31421108543872833\ntrain loss_ssm: 0.10041618347167969\ntrain au: 0.006637410493567586\ntrain eu: 0.003255316521972418\ntrain DSC: 0.5644060969352722\n\nvalidation loss: 0.7404009681940079\nvalidation loss_unet: 0.2854830056428909\nvalidation loss_ssm: 2.122858180999756\nvalidation au: 0.006649127267301083\nvalidation eu: 0.0030346316145732997\nvalidation DSC: 0.6738997101783752\n\nEpoch 94/100\n\ntrain loss: 0.3645961731672287\ntrain loss_unet: 0.31307290494441986\ntrain loss_ssm: 0.09313201904296875\ntrain au: 0.006708013359457254\ntrain eu: 0.0032896861666813493\ntrain DSC: 0.5690547227859497\n\nvalidation loss: 0.7382373732328414\nvalidation loss_unet: 0.283948450088501\nvalidation loss_ssm: 2.1221635055541994\nvalidation au: 0.006550041884183884\nvalidation eu: 0.0029856218118220566\nvalidation DSC: 0.6724640130996704\n\nEpoch 95/100\n\ntrain loss: 0.3610049933195114\ntrain loss_unet: 0.3116883784532547\ntrain loss_ssm: 0.09029388427734375\ntrain au: 0.0064283027313649654\ntrain eu: 0.003125784336589277\ntrain DSC: 0.5729796886444092\n\nvalidation loss: 0.7156974339485168\nvalidation loss_unet: 0.2819642323255539\nvalidation loss_ssm: 2.027986755371094\nvalidation au: 0.006239712843671441\nvalidation eu: 0.0028135848324745893\nvalidation DSC: 0.6705910563468933\n\nEpoch 96/100\n\ntrain loss: 0.34339092671871185\ntrain loss_unet: 0.31042030453681946\ntrain loss_ssm: 0.017026424407958984\ntrain au: 0.006126617779955268\ntrain eu: 0.0029565348522737622\ntrain DSC: 0.5735364556312561\n\nvalidation loss: 0.7077670502662659\nvalidation loss_unet: 0.2799375456571579\nvalidation loss_ssm: 2.0014475059509276\nvalidation au: 0.00613477217964828\nvalidation eu: 0.0027539999969303608\nvalidation DSC: 0.6743446588516235\n\nEpoch 97/100\n\ntrain loss: 0.3375738859176636\ntrain loss_unet: 0.3089195787906647\ntrain loss_ssm: 0.0017995834350585938\ntrain au: 0.005915940506383777\ntrain eu: 0.00282943865749985\ntrain DSC: 0.5775259733200073\n\nvalidation loss: 0.6878037023544311\nvalidation loss_unet: 0.27794135808944703\nvalidation loss_ssm: 1.917825584411621\nvalidation au: 0.0059138967283070085\nvalidation eu: 0.0026297224313020706\nvalidation DSC: 0.6770551800727844\n\nEpoch 98/100\n\ntrain loss: 0.3614271432161331\ntrain loss_unet: 0.3077673316001892\ntrain loss_ssm: 0.13476991653442383\ntrain au: 0.005636593792587519\ntrain eu: 0.002670581452548504\ntrain DSC: 0.5786871314048767\n\nvalidation loss: 0.6783952915668487\nvalidation loss_unet: 0.27580818474292756\nvalidation loss_ssm: 1.885080909729004\nvalidation au: 0.005784365190193057\nvalidation eu: 0.0025570917199365796\nvalidation DSC: 0.6836988925933838\n\nEpoch 99/100\n\ntrain loss: 0.34867964684963226\ntrain loss_unet: 0.30610010027885437\ntrain loss_ssm: 0.08284282684326172\ntrain au: 0.005509510636329651\ntrain eu: 0.0026010986184701324\ntrain DSC: 0.5856784582138062\n\nvalidation loss: 0.6825184446573257\nvalidation loss_unet: 0.27312524378299713\nvalidation loss_ssm: 1.9193096542358399\nvalidation au: 0.0057754274737089875\nvalidation eu: 0.002553127398714423\nvalidation DSC: 0.6953991055488586\n\nEpoch 100/100\n\ntrain loss: 0.3615511506795883\ntrain loss_unet: 0.3040858805179596\ntrain loss_ssm: 0.15503692626953125\ntrain au: 0.005579496035352349\ntrain eu: 0.002645790344104171\ntrain DSC: 0.5965946912765503\n\nvalidation loss: 0.6833262234926224\nvalidation loss_unet: 0.27122504711151124\nvalidation loss_ssm: 1.9369314765930177\nvalidation au: 0.005622863750904799\nvalidation eu: 0.0024714879295788706\nvalidation DSC: 0.7039792537689209\nBest model saved after epoch 86.\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675740519368
        }
      },
      "id": "22747d54-19fe-4441-bd5e-adb5a4ee829b"
    },
    {
      "cell_type": "code",
      "source": [
        "test(save=True)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675740519634
        }
      },
      "id": "f70f3763-6d8c-40d3-8557-c21f3662346c"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "91a3f331-6eec-4656-819d-e683ee658d3f"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "shapeworks"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "shapeworks",
      "language": "python",
      "display_name": "Python (shapeworks)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}