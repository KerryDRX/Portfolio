{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nrrd\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import monai"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1677686134764
        },
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "id": "cb163df8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "gather": {
          "logged": 1675606236724
        }
      },
      "id": "2a01678e"
    },
    {
      "cell_type": "code",
      "source": [
        "class Ellipsoids(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, radii):\n",
        "        self.images = torch.FloatTensor(np.array(images))\n",
        "        self.radii = torch.FloatTensor(np.array(radii))\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        radius = self.radii[index]\n",
        "        return image, radius"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1677686134945
        }
      },
      "id": "caed0757"
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = sorted(glob('../dataset/Ellipsoids/segmentations/*.nrrd'))\n",
        "images = np.array([nrrd.read(path)[0][None, :] for path in image_paths])\n",
        "radii = np.array([float(path.split('_')[-1][:5]) for path in image_paths])[:, None]\n",
        "images.shape, radii.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "((81, 1, 64, 64, 64), (81, 1))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677686140160
        }
      },
      "id": "b587ea94-5293-42dc-9af3-94ed877fe8a3"
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 61\n",
        "perm = np.random.RandomState(seed=0).permutation(81)\n",
        "perm = {\n",
        "    'train': perm[:train_size],\n",
        "    'val': perm[train_size:],\n",
        "}\n",
        "modes = list(perm.keys())\n",
        "dataloaders = dict()\n",
        "for mode in modes:\n",
        "    dataloaders[mode] = torch.utils.data.DataLoader(\n",
        "        Ellipsoids(images[perm[mode]], radii[perm[mode]]),\n",
        "        batch_size=1,\n",
        "        shuffle=(mode == 'train'),\n",
        "        num_workers=os.cpu_count(),\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1677686140752
        }
      },
      "id": "dcdd91a2"
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_dir = '../dataset/Ellipsoids/models/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677686141004
        }
      },
      "id": "7990e17a-8ba5-4c9b-8191-304631db51b8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Training"
      ],
      "metadata": {},
      "id": "75d9f92e"
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc = nn.Linear(64 ** 3, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def train_encoder(model, dataloaders, num_epochs, learning_rate):\n",
        "    opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.99)\n",
        "    loss_mae = torch.nn.L1Loss().to(device)\n",
        "    \n",
        "    t0 = time.time()\n",
        "    best_val_loss = np.Inf\n",
        "    \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for mode in ['train', 'val']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            losses = []\n",
        "            for image, radius in dataloaders[mode]:\n",
        "                image = image.to(device)\n",
        "                radius = radius.to(device)\n",
        "                \n",
        "                pred_radius = model(image)\n",
        "                loss = loss_mae(pred_radius, radius)\n",
        "                \n",
        "                if mode == 'train':\n",
        "                    opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            print(f'{mode} loss: {np.mean(losses)}')\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        val_loss = np.mean(losses)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{model_dir}/best_encoder.torch')\n",
        "        print(f'Best val loss: {best_val_loss}')\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "        t0 = time.time()\n",
        "        \n",
        "    print(f\"Training complete, model saved. Best model after epoch {best_epoch}\")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1677686141810
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "3b7c100a-5118-4009-807e-4098b504934a"
    },
    {
      "cell_type": "code",
      "source": [
        "del encoder\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677686340153
        }
      },
      "id": "a7aa4f97-e90f-4f72-842a-f8bb148d7949"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().to(device)\n",
        "train_encoder(model=encoder, dataloaders=dataloaders, num_epochs=500, learning_rate=1e-6)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/500\ntrain loss: 19.348507474680417\nval loss: 20.410970878601074\nBest val loss: 20.410970878601074\nTime: 0.6211552619934082\n\nEpoch 2/500\ntrain loss: 18.909148231881563\nval loss: 19.95078296661377\nBest val loss: 19.95078296661377\nTime: 0.5598747730255127\n\nEpoch 3/500\ntrain loss: 18.477684083532115\nval loss: 19.49126214981079\nBest val loss: 19.49126214981079\nTime: 0.5211882591247559\n\nEpoch 4/500\ntrain loss: 18.048988561161227\nval loss: 19.036054134368896\nBest val loss: 19.036054134368896\nTime: 0.5424017906188965\n\nEpoch 5/500\ntrain loss: 17.623235999560748\nval loss: 18.593648862838744\nBest val loss: 18.593648862838744\nTime: 0.5795338153839111\n\nEpoch 6/500\ntrain loss: 17.20410074953173\nval loss: 18.152721977233888\nBest val loss: 18.152721977233888\nTime: 0.5948479175567627\n\nEpoch 7/500\ntrain loss: 16.789176518799827\nval loss: 17.7118718624115\nBest val loss: 17.7118718624115\nTime: 0.5738470554351807\n\nEpoch 8/500\ntrain loss: 16.375852850616955\nval loss: 17.28484377861023\nBest val loss: 17.28484377861023\nTime: 0.6159801483154297\n\nEpoch 9/500\ntrain loss: 15.968486574829601\nval loss: 16.856013774871826\nBest val loss: 16.856013774871826\nTime: 0.546527624130249\n\nEpoch 10/500\ntrain loss: 15.565083527174153\nval loss: 16.429091691970825\nBest val loss: 16.429091691970825\nTime: 0.5841383934020996\n\nEpoch 11/500\ntrain loss: 15.165317253988297\nval loss: 16.00801749229431\nBest val loss: 16.00801749229431\nTime: 0.5722465515136719\n\nEpoch 12/500\ntrain loss: 14.768798703052958\nval loss: 15.598713397979736\nBest val loss: 15.598713397979736\nTime: 0.547656774520874\n\nEpoch 13/500\ntrain loss: 14.378693432104392\nval loss: 15.180759382247924\nBest val loss: 15.180759382247924\nTime: 0.5578155517578125\n\nEpoch 14/500\ntrain loss: 13.989887151561799\nval loss: 14.772635412216186\nBest val loss: 14.772635412216186\nTime: 0.5139961242675781\n\nEpoch 15/500\ntrain loss: 13.605497860517659\nval loss: 14.370920848846435\nBest val loss: 14.370920848846435\nTime: 0.5296478271484375\n\nEpoch 16/500\ntrain loss: 13.226357131707864\nval loss: 13.968895888328552\nBest val loss: 13.968895888328552\nTime: 0.5144288539886475\n\nEpoch 17/500\ntrain loss: 12.84951756430454\nval loss: 13.5740718126297\nBest val loss: 13.5740718126297\nTime: 0.5167968273162842\n\nEpoch 18/500\ntrain loss: 12.477251560961614\nval loss: 13.1820974111557\nBest val loss: 13.1820974111557\nTime: 0.5089888572692871\n\nEpoch 19/500\ntrain loss: 12.108744230426726\nval loss: 12.79321722984314\nBest val loss: 12.79321722984314\nTime: 0.5064265727996826\n\nEpoch 20/500\ntrain loss: 11.743662732546447\nval loss: 12.408987236022949\nBest val loss: 12.408987236022949\nTime: 0.5459508895874023\n\nEpoch 21/500\ntrain loss: 11.38203351224055\nval loss: 12.029859948158265\nBest val loss: 12.029859948158265\nTime: 0.5633313655853271\n\nEpoch 22/500\ntrain loss: 11.023789648149835\nval loss: 11.656128215789796\nBest val loss: 11.656128215789796\nTime: 0.623664140701294\n\nEpoch 23/500\ntrain loss: 10.67076654121524\nval loss: 11.280259251594543\nBest val loss: 11.280259251594543\nTime: 0.5247693061828613\n\nEpoch 24/500\ntrain loss: 10.319045512402644\nval loss: 10.914318490028382\nBest val loss: 10.914318490028382\nTime: 0.5767257213592529\n\nEpoch 25/500\ntrain loss: 9.971978078123\nval loss: 10.550422668457031\nBest val loss: 10.550422668457031\nTime: 0.5388562679290771\n\nEpoch 26/500\ntrain loss: 9.629026788179992\nval loss: 10.18913791179657\nBest val loss: 10.18913791179657\nTime: 0.5282058715820312\n\nEpoch 27/500\ntrain loss: 9.288921997195384\nval loss: 9.827540063858033\nBest val loss: 9.827540063858033\nTime: 0.5435950756072998\n\nEpoch 28/500\ntrain loss: 8.951964863011094\nval loss: 9.471302342414855\nBest val loss: 9.471302342414855\nTime: 0.5139906406402588\n\nEpoch 29/500\ntrain loss: 8.61771146586684\nval loss: 9.123493266105651\nBest val loss: 9.123493266105651\nTime: 0.6174046993255615\n\nEpoch 30/500\ntrain loss: 8.287554084277543\nval loss: 8.78008222579956\nBest val loss: 8.78008222579956\nTime: 0.5706064701080322\n\nEpoch 31/500\ntrain loss: 7.961364636655714\nval loss: 8.432805013656616\nBest val loss: 8.432805013656616\nTime: 0.5565149784088135\n\nEpoch 32/500\ntrain loss: 7.637619573561872\nval loss: 8.093179941177368\nBest val loss: 8.093179941177368\nTime: 0.5620532035827637\n\nEpoch 33/500\ntrain loss: 7.316858948254194\nval loss: 7.757069063186646\nBest val loss: 7.757069063186646\nTime: 0.5752432346343994\n\nEpoch 34/500\ntrain loss: 6.999718486285601\nval loss: 7.423327016830444\nBest val loss: 7.423327016830444\nTime: 0.5623645782470703\n\nEpoch 35/500\ntrain loss: 6.687006285933197\nval loss: 7.0879651546478275\nBest val loss: 7.0879651546478275\nTime: 0.5235097408294678\n\nEpoch 36/500\ntrain loss: 6.373799144244585\nval loss: 6.769353818893433\nBest val loss: 6.769353818893433\nTime: 0.5276565551757812\n\nEpoch 37/500\ntrain loss: 6.067905386940378\nval loss: 6.440791368484497\nBest val loss: 6.440791368484497\nTime: 0.5306146144866943\n\nEpoch 38/500\ntrain loss: 5.762739955401812\nval loss: 6.120351839065552\nBest val loss: 6.120351839065552\nTime: 0.5205442905426025\n\nEpoch 39/500\ntrain loss: 5.461381302505243\nval loss: 5.804393672943116\nBest val loss: 5.804393672943116\nTime: 0.574974536895752\n\nEpoch 40/500\ntrain loss: 5.162333019444199\nval loss: 5.491800212860108\nBest val loss: 5.491800212860108\nTime: 0.5109786987304688\n\nEpoch 41/500\ntrain loss: 4.8676257055313865\nval loss: 5.178649282455444\nBest val loss: 5.178649282455444\nTime: 0.5390722751617432\n\nEpoch 42/500\ntrain loss: 4.575112295932457\nval loss: 4.86856861114502\nBest val loss: 4.86856861114502\nTime: 0.5487737655639648\n\nEpoch 43/500\ntrain loss: 4.284473247215396\nval loss: 4.5651576042175295\nBest val loss: 4.5651576042175295\nTime: 0.564765453338623\n\nEpoch 44/500\ntrain loss: 3.997212284901103\nval loss: 4.2691779136657715\nBest val loss: 4.2691779136657715\nTime: 0.5077328681945801\n\nEpoch 45/500\ntrain loss: 3.713935179788558\nval loss: 3.969477653503418\nBest val loss: 3.969477653503418\nTime: 0.5450742244720459\n\nEpoch 46/500\ntrain loss: 3.4331920811387358\nval loss: 3.6704862117767334\nBest val loss: 3.6704862117767334\nTime: 0.5506665706634521\n\nEpoch 47/500\ntrain loss: 3.154220205838563\nval loss: 3.3791664123535154\nBest val loss: 3.3791664123535154\nTime: 0.5518286228179932\n\nEpoch 48/500\ntrain loss: 2.8785000316432265\nval loss: 3.0928375720977783\nBest val loss: 3.0928375720977783\nTime: 0.5302138328552246\n\nEpoch 49/500\ntrain loss: 2.607364466932953\nval loss: 2.8000820159912108\nBest val loss: 2.8000820159912108\nTime: 0.5538570880889893\n\nEpoch 50/500\ntrain loss: 2.335387745841605\nval loss: 2.5233998775482176\nBest val loss: 2.5233998775482176\nTime: 0.5165932178497314\n\nEpoch 51/500\ntrain loss: 2.0774423724315207\nval loss: 2.254013109207153\nBest val loss: 2.254013109207153\nTime: 0.541881799697876\n\nEpoch 52/500\ntrain loss: 1.88009768626729\nval loss: 2.0547458648681642\nBest val loss: 2.0547458648681642\nTime: 0.5362238883972168\n\nEpoch 53/500\ntrain loss: 1.7325196813364498\nval loss: 1.894463062286377\nBest val loss: 1.894463062286377\nTime: 0.5155482292175293\n\nEpoch 54/500\ntrain loss: 1.6088252145735944\nval loss: 1.7516984462738037\nBest val loss: 1.7516984462738037\nTime: 0.5461149215698242\n\nEpoch 55/500\ntrain loss: 1.5110544142175892\nval loss: 1.6378723621368407\nBest val loss: 1.6378723621368407\nTime: 0.49947547912597656\n\nEpoch 56/500\ntrain loss: 1.4255085616815286\nval loss: 1.5466026306152343\nBest val loss: 1.5466026306152343\nTime: 0.5838377475738525\n\nEpoch 57/500\ntrain loss: 1.3580346811013144\nval loss: 1.4747843742370605\nBest val loss: 1.4747843742370605\nTime: 0.5387389659881592\n\nEpoch 58/500\ntrain loss: 1.3074303767720208\nval loss: 1.4173936367034912\nBest val loss: 1.4173936367034912\nTime: 0.5554831027984619\n\nEpoch 59/500\ntrain loss: 1.2678309112298685\nval loss: 1.3625945568084716\nBest val loss: 1.3625945568084716\nTime: 0.5892946720123291\n\nEpoch 60/500\ntrain loss: 1.2279130279040726\nval loss: 1.3016634464263916\nBest val loss: 1.3016634464263916\nTime: 0.511483907699585\n\nEpoch 61/500\ntrain loss: 1.190009726852667\nval loss: 1.2516094207763673\nBest val loss: 1.2516094207763673\nTime: 0.5463700294494629\n\nEpoch 62/500\ntrain loss: 1.1530967149578157\nval loss: 1.2076261043548584\nBest val loss: 1.2076261043548584\nTime: 0.5338046550750732\n\nEpoch 63/500\ntrain loss: 1.1162309490266393\nval loss: 1.1638875484466553\nBest val loss: 1.1638875484466553\nTime: 0.512197732925415\n\nEpoch 64/500\ntrain loss: 1.0822223600794056\nval loss: 1.1280060291290284\nBest val loss: 1.1280060291290284\nTime: 0.5588321685791016\n\nEpoch 65/500\ntrain loss: 1.052535291577949\nval loss: 1.0969502449035644\nBest val loss: 1.0969502449035644\nTime: 0.540433406829834\n\nEpoch 66/500\ntrain loss: 1.0232372127595495\nval loss: 1.0649312019348145\nBest val loss: 1.0649312019348145\nTime: 0.5705752372741699\n\nEpoch 67/500\ntrain loss: 0.9962542486972497\nval loss: 1.0285735130310059\nBest val loss: 1.0285735130310059\nTime: 0.524075984954834\n\nEpoch 68/500\ntrain loss: 0.9658857720797179\nval loss: 1.0052898406982422\nBest val loss: 1.0052898406982422\nTime: 0.5573110580444336\n\nEpoch 69/500\ntrain loss: 0.9401245429867604\nval loss: 0.9735921382904053\nBest val loss: 0.9735921382904053\nTime: 0.5838453769683838\n\nEpoch 70/500\ntrain loss: 0.9133945840304015\nval loss: 0.9456421375274658\nBest val loss: 0.9456421375274658\nTime: 0.5340075492858887\n\nEpoch 71/500\ntrain loss: 0.888388024001825\nval loss: 0.9187780380249023\nBest val loss: 0.9187780380249023\nTime: 0.552156925201416\n\nEpoch 72/500\ntrain loss: 0.8628299431722672\nval loss: 0.8908980846405029\nBest val loss: 0.8908980846405029\nTime: 0.5282013416290283\n\nEpoch 73/500\ntrain loss: 0.8396379126877082\nval loss: 0.8639174938201905\nBest val loss: 0.8639174938201905\nTime: 0.5155889987945557\n\nEpoch 74/500\ntrain loss: 0.8148532773627609\nval loss: 0.8397780418395996\nBest val loss: 0.8397780418395996\nTime: 0.5283932685852051\n\nEpoch 75/500\ntrain loss: 0.7919013539298636\nval loss: 0.8174824714660645\nBest val loss: 0.8174824714660645\nTime: 0.5516703128814697\n\nEpoch 76/500\ntrain loss: 0.767375867874896\nval loss: 0.7898488521575928\nBest val loss: 0.7898488521575928\nTime: 0.5528690814971924\n\nEpoch 77/500\ntrain loss: 0.7440441788220015\nval loss: 0.7685957908630371\nBest val loss: 0.7685957908630371\nTime: 0.5306246280670166\n\nEpoch 78/500\ntrain loss: 0.7212817395319704\nval loss: 0.7454301357269287\nBest val loss: 0.7454301357269287\nTime: 0.5568296909332275\n\nEpoch 79/500\ntrain loss: 0.6982485036380955\nval loss: 0.7213250160217285\nBest val loss: 0.7213250160217285\nTime: 0.5862877368927002\n\nEpoch 80/500\ntrain loss: 0.6766661190595783\nval loss: 0.6983924865722656\nBest val loss: 0.6983924865722656\nTime: 0.5389232635498047\n\nEpoch 81/500\ntrain loss: 0.6543613965394067\nval loss: 0.6796361446380615\nBest val loss: 0.6796361446380615\nTime: 0.5980048179626465\n\nEpoch 82/500\ntrain loss: 0.632058081079702\nval loss: 0.6565359592437744\nBest val loss: 0.6565359592437744\nTime: 0.5105507373809814\n\nEpoch 83/500\ntrain loss: 0.6108146573676437\nval loss: 0.635661506652832\nBest val loss: 0.635661506652832\nTime: 0.528698205947876\n\nEpoch 84/500\ntrain loss: 0.5893606436057169\nval loss: 0.6128642559051514\nBest val loss: 0.6128642559051514\nTime: 0.507326602935791\n\nEpoch 85/500\ntrain loss: 0.5691857416121686\nval loss: 0.5934154033660889\nBest val loss: 0.5934154033660889\nTime: 0.5116627216339111\n\nEpoch 86/500\ntrain loss: 0.5490168743446225\nval loss: 0.5737066268920898\nBest val loss: 0.5737066268920898\nTime: 0.5379724502563477\n\nEpoch 87/500\ntrain loss: 0.5301836983102267\nval loss: 0.5542885303497315\nBest val loss: 0.5542885303497315\nTime: 0.5294530391693115\n\nEpoch 88/500\ntrain loss: 0.5099080820552638\nval loss: 0.5349977970123291\nBest val loss: 0.5349977970123291\nTime: 0.49835968017578125\n\nEpoch 89/500\ntrain loss: 0.49089316071056927\nval loss: 0.5182657241821289\nBest val loss: 0.5182657241821289\nTime: 0.606421947479248\n\nEpoch 90/500\ntrain loss: 0.4742569845230853\nval loss: 0.5031921863555908\nBest val loss: 0.5031921863555908\nTime: 0.5075938701629639\n\nEpoch 91/500\ntrain loss: 0.4576334406117924\nval loss: 0.48908061981201173\nBest val loss: 0.48908061981201173\nTime: 0.5439643859863281\n\nEpoch 92/500\ntrain loss: 0.4424713635053791\nval loss: 0.47641754150390625\nBest val loss: 0.47641754150390625\nTime: 0.5675983428955078\n\nEpoch 93/500\ntrain loss: 0.42745701211397763\nval loss: 0.4628137111663818\nBest val loss: 0.4628137111663818\nTime: 0.6739397048950195\n\nEpoch 94/500\ntrain loss: 0.414249701578109\nval loss: 0.45032992362976076\nBest val loss: 0.45032992362976076\nTime: 0.5698871612548828\n\nEpoch 95/500\ntrain loss: 0.4014702937642082\nval loss: 0.4392889976501465\nBest val loss: 0.4392889976501465\nTime: 0.5153789520263672\n\nEpoch 96/500\ntrain loss: 0.38994548359855274\nval loss: 0.43056321144104004\nBest val loss: 0.43056321144104004\nTime: 0.5270266532897949\n\nEpoch 97/500\ntrain loss: 0.37859175635165854\nval loss: 0.42014517784118655\nBest val loss: 0.42014517784118655\nTime: 0.5059776306152344\n\nEpoch 98/500\ntrain loss: 0.36851768806332447\nval loss: 0.41084928512573243\nBest val loss: 0.41084928512573243\nTime: 0.5793650150299072\n\nEpoch 99/500\ntrain loss: 0.3609124637040936\nval loss: 0.4020946979522705\nBest val loss: 0.4020946979522705\nTime: 0.5374391078948975\n\nEpoch 100/500\ntrain loss: 0.3525704086803999\nval loss: 0.39458017349243163\nBest val loss: 0.39458017349243163\nTime: 0.5423338413238525\n\nEpoch 101/500\ntrain loss: 0.3459770953068968\nval loss: 0.3871911525726318\nBest val loss: 0.3871911525726318\nTime: 0.5455837249755859\n\nEpoch 102/500\ntrain loss: 0.33910215096395524\nval loss: 0.378419303894043\nBest val loss: 0.378419303894043\nTime: 0.5266673564910889\n\nEpoch 103/500\ntrain loss: 0.3325435607159724\nval loss: 0.3712772369384766\nBest val loss: 0.3712772369384766\nTime: 0.507453441619873\n\nEpoch 104/500\ntrain loss: 0.32713983879714714\nval loss: 0.3651281833648682\nBest val loss: 0.3651281833648682\nTime: 0.5292525291442871\n\nEpoch 105/500\ntrain loss: 0.3213701873529153\nval loss: 0.3566762924194336\nBest val loss: 0.3566762924194336\nTime: 0.5310196876525879\n\nEpoch 106/500\ntrain loss: 0.31677527505843367\nval loss: 0.34995236396789553\nBest val loss: 0.34995236396789553\nTime: 0.6006972789764404\n\nEpoch 107/500\ntrain loss: 0.31070960935999137\nval loss: 0.3446235179901123\nBest val loss: 0.3446235179901123\nTime: 0.5758762359619141\n\nEpoch 108/500\ntrain loss: 0.3063385291177718\nval loss: 0.33899874687194825\nBest val loss: 0.33899874687194825\nTime: 0.5572528839111328\n\nEpoch 109/500\ntrain loss: 0.30089601923207765\nval loss: 0.3335890293121338\nBest val loss: 0.3335890293121338\nTime: 0.5206732749938965\n\nEpoch 110/500\ntrain loss: 0.2962653832357438\nval loss: 0.328125524520874\nBest val loss: 0.328125524520874\nTime: 0.5391309261322021\n\nEpoch 111/500\ntrain loss: 0.2925461159377802\nval loss: 0.3235318183898926\nBest val loss: 0.3235318183898926\nTime: 0.5484697818756104\n\nEpoch 112/500\ntrain loss: 0.28749658240646614\nval loss: 0.3180387496948242\nBest val loss: 0.3180387496948242\nTime: 0.5286669731140137\n\nEpoch 113/500\ntrain loss: 0.28284298005651254\nval loss: 0.3131280422210693\nBest val loss: 0.3131280422210693\nTime: 0.5345144271850586\n\nEpoch 114/500\ntrain loss: 0.2789768938158379\nval loss: 0.30849084854125974\nBest val loss: 0.30849084854125974\nTime: 0.5227160453796387\n\nEpoch 115/500\ntrain loss: 0.27618691178618887\nval loss: 0.3037156105041504\nBest val loss: 0.3037156105041504\nTime: 0.5769600868225098\n\nEpoch 116/500\ntrain loss: 0.27278130953429175\nval loss: 0.29906749725341797\nBest val loss: 0.29906749725341797\nTime: 0.5623025894165039\n\nEpoch 117/500\ntrain loss: 0.26801395416259766\nval loss: 0.29431972503662107\nBest val loss: 0.29431972503662107\nTime: 0.5416383743286133\n\nEpoch 118/500\ntrain loss: 0.2651635154348905\nval loss: 0.29022626876831054\nBest val loss: 0.29022626876831054\nTime: 0.6679325103759766\n\nEpoch 119/500\ntrain loss: 0.26141374619280705\nval loss: 0.28543810844421386\nBest val loss: 0.28543810844421386\nTime: 0.5324544906616211\n\nEpoch 120/500\ntrain loss: 0.258153039900983\nval loss: 0.2808354377746582\nBest val loss: 0.2808354377746582\nTime: 0.526935338973999\n\nEpoch 121/500\ntrain loss: 0.25405299077268506\nval loss: 0.2770855903625488\nBest val loss: 0.2770855903625488\nTime: 0.5751504898071289\n\nEpoch 122/500\ntrain loss: 0.2506537359268939\nval loss: 0.2724775314331055\nBest val loss: 0.2724775314331055\nTime: 0.5214798450469971\n\nEpoch 123/500\ntrain loss: 0.2474421829473777\nval loss: 0.2681403636932373\nBest val loss: 0.2681403636932373\nTime: 0.5545625686645508\n\nEpoch 124/500\ntrain loss: 0.24429182146416337\nval loss: 0.2637928485870361\nBest val loss: 0.2637928485870361\nTime: 0.5585410594940186\n\nEpoch 125/500\ntrain loss: 0.2416157018942911\nval loss: 0.2594092845916748\nBest val loss: 0.2594092845916748\nTime: 0.572913408279419\n\nEpoch 126/500\ntrain loss: 0.23832164826940316\nval loss: 0.25591697692871096\nBest val loss: 0.25591697692871096\nTime: 0.53542160987854\n\nEpoch 127/500\ntrain loss: 0.23638587701516073\nval loss: 0.2528064727783203\nBest val loss: 0.2528064727783203\nTime: 0.6293013095855713\n\nEpoch 128/500\ntrain loss: 0.23287258773553568\nval loss: 0.24847803115844727\nBest val loss: 0.24847803115844727\nTime: 0.5758261680603027\n\nEpoch 129/500\ntrain loss: 0.23045258443863664\nval loss: 0.2451070785522461\nBest val loss: 0.2451070785522461\nTime: 0.5912542343139648\n\nEpoch 130/500\ntrain loss: 0.22725041186223266\nval loss: 0.24153599739074708\nBest val loss: 0.24153599739074708\nTime: 0.539527177810669\n\nEpoch 131/500\ntrain loss: 0.22468419934882491\nval loss: 0.2375875473022461\nBest val loss: 0.2375875473022461\nTime: 0.5576434135437012\n\nEpoch 132/500\ntrain loss: 0.22178678043553088\nval loss: 0.23397383689880372\nBest val loss: 0.23397383689880372\nTime: 0.542823076248169\n\nEpoch 133/500\ntrain loss: 0.21930766496501986\nval loss: 0.2305164337158203\nBest val loss: 0.2305164337158203\nTime: 0.5870668888092041\n\nEpoch 134/500\ntrain loss: 0.21749605897997246\nval loss: 0.22665295600891114\nBest val loss: 0.22665295600891114\nTime: 0.5374970436096191\n\nEpoch 135/500\ntrain loss: 0.21431941673403881\nval loss: 0.22414159774780273\nBest val loss: 0.22414159774780273\nTime: 0.564056396484375\n\nEpoch 136/500\ntrain loss: 0.21213398605096537\nval loss: 0.2207857608795166\nBest val loss: 0.2207857608795166\nTime: 0.5980238914489746\n\nEpoch 137/500\ntrain loss: 0.20962110112925045\nval loss: 0.21778063774108886\nBest val loss: 0.21778063774108886\nTime: 0.5532822608947754\n\nEpoch 138/500\ntrain loss: 0.20761194385466028\nval loss: 0.21488418579101562\nBest val loss: 0.21488418579101562\nTime: 0.5599243640899658\n\nEpoch 139/500\ntrain loss: 0.20593696344094198\nval loss: 0.21178202629089354\nBest val loss: 0.21178202629089354\nTime: 0.5301640033721924\n\nEpoch 140/500\ntrain loss: 0.2033836802498239\nval loss: 0.2090625286102295\nBest val loss: 0.2090625286102295\nTime: 0.5550892353057861\n\nEpoch 141/500\ntrain loss: 0.2008438579371718\nval loss: 0.206388521194458\nBest val loss: 0.206388521194458\nTime: 0.6235308647155762\n\nEpoch 142/500\ntrain loss: 0.19925933587746542\nval loss: 0.2033228874206543\nBest val loss: 0.2033228874206543\nTime: 0.5356674194335938\n\nEpoch 143/500\ntrain loss: 0.19756928428274687\nval loss: 0.200669002532959\nBest val loss: 0.200669002532959\nTime: 0.541865348815918\n\nEpoch 144/500\ntrain loss: 0.19502986845422965\nval loss: 0.19831347465515137\nBest val loss: 0.19831347465515137\nTime: 0.5595672130584717\n\nEpoch 145/500\ntrain loss: 0.1928118408703413\nval loss: 0.1955014705657959\nBest val loss: 0.1955014705657959\nTime: 0.5686204433441162\n\nEpoch 146/500\ntrain loss: 0.19093164850453861\nval loss: 0.19305858612060547\nBest val loss: 0.19305858612060547\nTime: 0.5315148830413818\n\nEpoch 147/500\ntrain loss: 0.1891069724911549\nval loss: 0.19054079055786133\nBest val loss: 0.19054079055786133\nTime: 0.5390374660491943\n\nEpoch 148/500\ntrain loss: 0.18704994389268217\nval loss: 0.18756861686706544\nBest val loss: 0.18756861686706544\nTime: 0.5544300079345703\n\nEpoch 149/500\ntrain loss: 0.18545368069508036\nval loss: 0.185294246673584\nBest val loss: 0.185294246673584\nTime: 0.6699583530426025\n\nEpoch 150/500\ntrain loss: 0.18359768976930713\nval loss: 0.1824793338775635\nBest val loss: 0.1824793338775635\nTime: 0.6019034385681152\n\nEpoch 151/500\ntrain loss: 0.18208413045914446\nval loss: 0.18036203384399413\nBest val loss: 0.18036203384399413\nTime: 0.5739150047302246\n\nEpoch 152/500\ntrain loss: 0.1802106138135566\nval loss: 0.1779273509979248\nBest val loss: 0.1779273509979248\nTime: 0.5808260440826416\n\nEpoch 153/500\ntrain loss: 0.17851005616735238\nval loss: 0.175504732131958\nBest val loss: 0.175504732131958\nTime: 0.5714879035949707\n\nEpoch 154/500\ntrain loss: 0.176366852932289\nval loss: 0.1730832576751709\nBest val loss: 0.1730832576751709\nTime: 0.5942389965057373\n\nEpoch 155/500\ntrain loss: 0.1754658495793577\nval loss: 0.1708378314971924\nBest val loss: 0.1708378314971924\nTime: 0.5439696311950684\n\nEpoch 156/500\ntrain loss: 0.17337548928182633\nval loss: 0.1688563346862793\nBest val loss: 0.1688563346862793\nTime: 0.5330038070678711\n\nEpoch 157/500\ntrain loss: 0.17146452919381563\nval loss: 0.1665039539337158\nBest val loss: 0.1665039539337158\nTime: 0.6042098999023438\n\nEpoch 158/500\ntrain loss: 0.17002765467909517\nval loss: 0.16437215805053712\nBest val loss: 0.16437215805053712\nTime: 0.5269074440002441\n\nEpoch 159/500\ntrain loss: 0.16859804997678662\nval loss: 0.16324419975280763\nBest val loss: 0.16324419975280763\nTime: 0.5652444362640381\n\nEpoch 160/500\ntrain loss: 0.16678545904941247\nval loss: 0.16064200401306153\nBest val loss: 0.16064200401306153\nTime: 0.613262414932251\n\nEpoch 161/500\ntrain loss: 0.16526752221779745\nval loss: 0.1595468044281006\nBest val loss: 0.1595468044281006\nTime: 0.5826418399810791\n\nEpoch 162/500\ntrain loss: 0.16373406081903177\nval loss: 0.15752921104431153\nBest val loss: 0.15752921104431153\nTime: 0.5287351608276367\n\nEpoch 163/500\ntrain loss: 0.16243601627037174\nval loss: 0.15536041259765626\nBest val loss: 0.15536041259765626\nTime: 0.5685403347015381\n\nEpoch 164/500\ntrain loss: 0.1605061390360848\nval loss: 0.15351223945617676\nBest val loss: 0.15351223945617676\nTime: 0.5761845111846924\n\nEpoch 165/500\ntrain loss: 0.15967292472964428\nval loss: 0.15255074501037597\nBest val loss: 0.15255074501037597\nTime: 0.5275709629058838\n\nEpoch 166/500\ntrain loss: 0.15857902902071594\nval loss: 0.15046191215515137\nBest val loss: 0.15046191215515137\nTime: 0.5506510734558105\n\nEpoch 167/500\ntrain loss: 0.15611501599921554\nval loss: 0.1497560977935791\nBest val loss: 0.1497560977935791\nTime: 0.6278989315032959\n\nEpoch 168/500\ntrain loss: 0.15493231914082511\nval loss: 0.14824128150939941\nBest val loss: 0.14824128150939941\nTime: 0.5888776779174805\n\nEpoch 169/500\ntrain loss: 0.15352260089311442\nval loss: 0.1462862968444824\nBest val loss: 0.1462862968444824\nTime: 0.568209171295166\n\nEpoch 170/500\ntrain loss: 0.1520337745791576\nval loss: 0.14531612396240234\nBest val loss: 0.14531612396240234\nTime: 0.5207967758178711\n\nEpoch 171/500\ntrain loss: 0.15115351755110945\nval loss: 0.14476971626281737\nBest val loss: 0.14476971626281737\nTime: 0.517930269241333\n\nEpoch 172/500\ntrain loss: 0.15013838596031315\nval loss: 0.14128122329711915\nBest val loss: 0.14128122329711915\nTime: 0.5811159610748291\n\nEpoch 173/500\ntrain loss: 0.14846113861584273\nval loss: 0.1410961627960205\nBest val loss: 0.1410961627960205\nTime: 0.5211222171783447\n\nEpoch 174/500\ntrain loss: 0.1475375910274318\nval loss: 0.13816771507263184\nBest val loss: 0.13816771507263184\nTime: 0.5443024635314941\n\nEpoch 175/500\ntrain loss: 0.1469151074769067\nval loss: 0.13757004737854003\nBest val loss: 0.13757004737854003\nTime: 0.5736331939697266\n\nEpoch 176/500\ntrain loss: 0.14516928938568616\nval loss: 0.13639869689941406\nBest val loss: 0.13639869689941406\nTime: 0.495086669921875\n\nEpoch 177/500\ntrain loss: 0.14385432884341381\nval loss: 0.13530364036560058\nBest val loss: 0.13530364036560058\nTime: 0.6185789108276367\n\nEpoch 178/500\ntrain loss: 0.1425980739906186\nval loss: 0.13466806411743165\nBest val loss: 0.13466806411743165\nTime: 0.507530689239502\n\nEpoch 179/500\ntrain loss: 0.14169700810166655\nval loss: 0.13266587257385254\nBest val loss: 0.13266587257385254\nTime: 0.5738284587860107\n\nEpoch 180/500\ntrain loss: 0.1411655613633453\nval loss: 0.1312565326690674\nBest val loss: 0.1312565326690674\nTime: 0.5092618465423584\n\nEpoch 181/500\ntrain loss: 0.13982596162889824\nval loss: 0.13007063865661622\nBest val loss: 0.13007063865661622\nTime: 0.5233757495880127\n\nEpoch 182/500\ntrain loss: 0.13889075107261784\nval loss: 0.12874903678894042\nBest val loss: 0.12874903678894042\nTime: 0.5923736095428467\n\nEpoch 183/500\ntrain loss: 0.13808899238461353\nval loss: 0.1275106906890869\nBest val loss: 0.1275106906890869\nTime: 0.614173173904419\n\nEpoch 184/500\ntrain loss: 0.13710161115302413\nval loss: 0.12639055252075196\nBest val loss: 0.12639055252075196\nTime: 0.5043802261352539\n\nEpoch 185/500\ntrain loss: 0.13626644259593526\nval loss: 0.1246518611907959\nBest val loss: 0.1246518611907959\nTime: 0.5269210338592529\n\nEpoch 186/500\ntrain loss: 0.13501811418376986\nval loss: 0.12522478103637696\nBest val loss: 0.1246518611907959\nTime: 0.44374918937683105\n\nEpoch 187/500\ntrain loss: 0.13430404663085938\nval loss: 0.12320642471313477\nBest val loss: 0.12320642471313477\nTime: 0.601825475692749\n\nEpoch 188/500\ntrain loss: 0.13352422245213244\nval loss: 0.12179145812988282\nBest val loss: 0.12179145812988282\nTime: 0.5175652503967285\n\nEpoch 189/500\ntrain loss: 0.1322961088086738\nval loss: 0.121116304397583\nBest val loss: 0.121116304397583\nTime: 0.5710811614990234\n\nEpoch 190/500\ntrain loss: 0.1317357391607566\nval loss: 0.12060742378234864\nBest val loss: 0.12060742378234864\nTime: 0.512324333190918\n\nEpoch 191/500\ntrain loss: 0.13083284409319768\nval loss: 0.11904869079589844\nBest val loss: 0.11904869079589844\nTime: 0.5807862281799316\n\nEpoch 192/500\ntrain loss: 0.12966940832919763\nval loss: 0.11881880760192871\nBest val loss: 0.11881880760192871\nTime: 0.57427978515625\n\nEpoch 193/500\ntrain loss: 0.12897202225982166\nval loss: 0.11865572929382324\nBest val loss: 0.11865572929382324\nTime: 0.5581107139587402\n\nEpoch 194/500\ntrain loss: 0.12808712193223296\nval loss: 0.11801624298095703\nBest val loss: 0.11801624298095703\nTime: 0.6006460189819336\n\nEpoch 195/500\ntrain loss: 0.12739181518554688\nval loss: 0.11589736938476562\nBest val loss: 0.11589736938476562\nTime: 0.5503685474395752\n\nEpoch 196/500\ntrain loss: 0.12653638495773564\nval loss: 0.11667709350585938\nBest val loss: 0.11589736938476562\nTime: 0.4523770809173584\n\nEpoch 197/500\ntrain loss: 0.12596699448882556\nval loss: 0.11509904861450196\nBest val loss: 0.11509904861450196\nTime: 0.5534632205963135\n\nEpoch 198/500\ntrain loss: 0.12494151318659548\nval loss: 0.11465797424316407\nBest val loss: 0.11465797424316407\nTime: 0.877187967300415\n\nEpoch 199/500\ntrain loss: 0.12419589621121765\nval loss: 0.11265907287597657\nBest val loss: 0.11265907287597657\nTime: 0.5760438442230225\n\nEpoch 200/500\ntrain loss: 0.12372143542180296\nval loss: 0.11275272369384766\nBest val loss: 0.11265907287597657\nTime: 0.4448573589324951\n\nEpoch 201/500\ntrain loss: 0.12277757926065413\nval loss: 0.1118812084197998\nBest val loss: 0.1118812084197998\nTime: 0.5729331970214844\n\nEpoch 202/500\ntrain loss: 0.12177536135814229\nval loss: 0.11114335060119629\nBest val loss: 0.11114335060119629\nTime: 0.5538790225982666\n\nEpoch 203/500\ntrain loss: 0.12097702651727395\nval loss: 0.11057710647583008\nBest val loss: 0.11057710647583008\nTime: 0.5589063167572021\n\nEpoch 204/500\ntrain loss: 0.1202794997418513\nval loss: 0.11015534400939941\nBest val loss: 0.11015534400939941\nTime: 0.5407059192657471\n\nEpoch 205/500\ntrain loss: 0.11959277606401288\nval loss: 0.10948095321655274\nBest val loss: 0.10948095321655274\nTime: 0.5552453994750977\n\nEpoch 206/500\ntrain loss: 0.11867876521876601\nval loss: 0.10951004028320313\nBest val loss: 0.10948095321655274\nTime: 0.44879698753356934\n\nEpoch 207/500\ntrain loss: 0.11806613109150871\nval loss: 0.1084317684173584\nBest val loss: 0.1084317684173584\nTime: 0.5166947841644287\n\nEpoch 208/500\ntrain loss: 0.11757261244977107\nval loss: 0.10786104202270508\nBest val loss: 0.10786104202270508\nTime: 0.5083105564117432\n\nEpoch 209/500\ntrain loss: 0.11692958581643026\nval loss: 0.10791687965393067\nBest val loss: 0.10786104202270508\nTime: 0.4394710063934326\n\nEpoch 210/500\ntrain loss: 0.11649012956462923\nval loss: 0.10728402137756347\nBest val loss: 0.10728402137756347\nTime: 0.4970858097076416\n\nEpoch 211/500\ntrain loss: 0.1161076123597192\nval loss: 0.10684080123901367\nBest val loss: 0.10684080123901367\nTime: 0.5371544361114502\n\nEpoch 212/500\ntrain loss: 0.1153956241294986\nval loss: 0.10600414276123046\nBest val loss: 0.10600414276123046\nTime: 0.5506386756896973\n\nEpoch 213/500\ntrain loss: 0.11489936953685323\nval loss: 0.1058434009552002\nBest val loss: 0.1058434009552002\nTime: 0.5889511108398438\n\nEpoch 214/500\ntrain loss: 0.11487649698726467\nval loss: 0.10622944831848144\nBest val loss: 0.1058434009552002\nTime: 0.46372532844543457\n\nEpoch 215/500\ntrain loss: 0.11386977649125897\nval loss: 0.10484576225280762\nBest val loss: 0.10484576225280762\nTime: 0.541893482208252\n\nEpoch 216/500\ntrain loss: 0.11316310382280194\nval loss: 0.10466609001159669\nBest val loss: 0.10466609001159669\nTime: 0.5286366939544678\n\nEpoch 217/500\ntrain loss: 0.11250131638323674\nval loss: 0.1047978401184082\nBest val loss: 0.10466609001159669\nTime: 0.4439516067504883\n\nEpoch 218/500\ntrain loss: 0.1121551169723761\nval loss: 0.10381507873535156\nBest val loss: 0.10381507873535156\nTime: 0.5784389972686768\n\nEpoch 219/500\ntrain loss: 0.11151323162141394\nval loss: 0.10311803817749024\nBest val loss: 0.10311803817749024\nTime: 0.59562087059021\n\nEpoch 220/500\ntrain loss: 0.11121188617143475\nval loss: 0.10268659591674804\nBest val loss: 0.10268659591674804\nTime: 0.5250599384307861\n\nEpoch 221/500\ntrain loss: 0.11051485968417808\nval loss: 0.10247621536254883\nBest val loss: 0.10247621536254883\nTime: 0.5662922859191895\n\nEpoch 222/500\ntrain loss: 0.10991825041223745\nval loss: 0.1023106575012207\nBest val loss: 0.1023106575012207\nTime: 0.5482897758483887\n\nEpoch 223/500\ntrain loss: 0.10960755582715644\nval loss: 0.1021838665008545\nBest val loss: 0.1021838665008545\nTime: 0.501727819442749\n\nEpoch 224/500\ntrain loss: 0.10902856607906154\nval loss: 0.10148611068725585\nBest val loss: 0.10148611068725585\nTime: 0.529181718826294\n\nEpoch 225/500\ntrain loss: 0.1085748203465196\nval loss: 0.10069513320922852\nBest val loss: 0.10069513320922852\nTime: 0.5064456462860107\n\nEpoch 226/500\ntrain loss: 0.10795912195424565\nval loss: 0.10121259689331055\nBest val loss: 0.10069513320922852\nTime: 0.45041370391845703\n\nEpoch 227/500\ntrain loss: 0.10770428766969775\nval loss: 0.10028691291809082\nBest val loss: 0.10028691291809082\nTime: 0.5622456073760986\n\nEpoch 228/500\ntrain loss: 0.1073095290387263\nval loss: 0.09991068840026855\nBest val loss: 0.09991068840026855\nTime: 0.5578591823577881\n\nEpoch 229/500\ntrain loss: 0.10663825175801261\nval loss: 0.09915833473205567\nBest val loss: 0.09915833473205567\nTime: 0.5361526012420654\n\nEpoch 230/500\ntrain loss: 0.10631691041539927\nval loss: 0.09908027648925781\nBest val loss: 0.09908027648925781\nTime: 0.5294220447540283\n\nEpoch 231/500\ntrain loss: 0.10588084674272381\nval loss: 0.09890880584716796\nBest val loss: 0.09890880584716796\nTime: 0.5635368824005127\n\nEpoch 232/500\ntrain loss: 0.10570365092793449\nval loss: 0.09825806617736817\nBest val loss: 0.09825806617736817\nTime: 0.5121500492095947\n\nEpoch 233/500\ntrain loss: 0.10510571276555296\nval loss: 0.09779090881347656\nBest val loss: 0.09779090881347656\nTime: 0.5642979145050049\n\nEpoch 234/500\ntrain loss: 0.10442518015376857\nval loss: 0.0972355842590332\nBest val loss: 0.0972355842590332\nTime: 0.5996584892272949\n\nEpoch 235/500\ntrain loss: 0.10392540790995614\nval loss: 0.0969010353088379\nBest val loss: 0.0969010353088379\nTime: 0.5589215755462646\n\nEpoch 236/500\ntrain loss: 0.10377735388083537\nval loss: 0.09655065536499023\nBest val loss: 0.09655065536499023\nTime: 0.6034195423126221\n\nEpoch 237/500\ntrain loss: 0.10333952356557377\nval loss: 0.09627828598022461\nBest val loss: 0.09627828598022461\nTime: 0.5352449417114258\n\nEpoch 238/500\ntrain loss: 0.10297001385297931\nval loss: 0.09627938270568848\nBest val loss: 0.09627828598022461\nTime: 0.43755125999450684\n\nEpoch 239/500\ntrain loss: 0.10232118700371413\nval loss: 0.09596185684204102\nBest val loss: 0.09596185684204102\nTime: 0.5715529918670654\n\nEpoch 240/500\ntrain loss: 0.1020820179923636\nval loss: 0.09567995071411133\nBest val loss: 0.09567995071411133\nTime: 0.543220043182373\n\nEpoch 241/500\ntrain loss: 0.10157147391897733\nval loss: 0.0955855369567871\nBest val loss: 0.0955855369567871\nTime: 0.5897481441497803\n\nEpoch 242/500\ntrain loss: 0.10132405015288806\nval loss: 0.0950998306274414\nBest val loss: 0.0950998306274414\nTime: 0.5259466171264648\n\nEpoch 243/500\ntrain loss: 0.10080142099349225\nval loss: 0.09506230354309082\nBest val loss: 0.09506230354309082\nTime: 0.5655810832977295\n\nEpoch 244/500\ntrain loss: 0.10032597526175077\nval loss: 0.09487066268920899\nBest val loss: 0.09487066268920899\nTime: 0.5135519504547119\n\nEpoch 245/500\ntrain loss: 0.10005291172715484\nval loss: 0.0945547103881836\nBest val loss: 0.0945547103881836\nTime: 0.5067565441131592\n\nEpoch 246/500\ntrain loss: 0.09955548458412046\nval loss: 0.09430584907531739\nBest val loss: 0.09430584907531739\nTime: 0.5382745265960693\n\nEpoch 247/500\ntrain loss: 0.09912625297171171\nval loss: 0.09414620399475097\nBest val loss: 0.09414620399475097\nTime: 0.5293209552764893\n\nEpoch 248/500\ntrain loss: 0.09885927106513352\nval loss: 0.09384841918945312\nBest val loss: 0.09384841918945312\nTime: 0.5486125946044922\n\nEpoch 249/500\ntrain loss: 0.09858736444692143\nval loss: 0.09378342628479004\nBest val loss: 0.09378342628479004\nTime: 0.5149645805358887\n\nEpoch 250/500\ntrain loss: 0.09848790090592181\nval loss: 0.09358553886413574\nBest val loss: 0.09358553886413574\nTime: 0.529212474822998\n\nEpoch 251/500\ntrain loss: 0.09778410489441919\nval loss: 0.09328889846801758\nBest val loss: 0.09328889846801758\nTime: 0.6906459331512451\n\nEpoch 252/500\ntrain loss: 0.09744465937379931\nval loss: 0.09316983222961425\nBest val loss: 0.09316983222961425\nTime: 0.5617892742156982\n\nEpoch 253/500\ntrain loss: 0.09721127494436796\nval loss: 0.0928865909576416\nBest val loss: 0.0928865909576416\nTime: 0.5267164707183838\n\nEpoch 254/500\ntrain loss: 0.09677144347644243\nval loss: 0.09258956909179687\nBest val loss: 0.09258956909179687\nTime: 0.5231060981750488\n\nEpoch 255/500\ntrain loss: 0.09692509447942015\nval loss: 0.09251346588134765\nBest val loss: 0.09251346588134765\nTime: 0.5256478786468506\n\nEpoch 256/500\ntrain loss: 0.09622722375588338\nval loss: 0.09249143600463867\nBest val loss: 0.09249143600463867\nTime: 0.5406222343444824\n\nEpoch 257/500\ntrain loss: 0.09597484401014984\nval loss: 0.09219121932983398\nBest val loss: 0.09219121932983398\nTime: 0.517585277557373\n\nEpoch 258/500\ntrain loss: 0.0957279987022525\nval loss: 0.09196391105651855\nBest val loss: 0.09196391105651855\nTime: 0.5116913318634033\n\nEpoch 259/500\ntrain loss: 0.09575968883076652\nval loss: 0.09143247604370117\nBest val loss: 0.09143247604370117\nTime: 0.5134003162384033\n\nEpoch 260/500\ntrain loss: 0.09524525189008869\nval loss: 0.09146819114685059\nBest val loss: 0.09143247604370117\nTime: 0.45099616050720215\n\nEpoch 261/500\ntrain loss: 0.09495391220342918\nval loss: 0.09135899543762208\nBest val loss: 0.09135899543762208\nTime: 0.5577094554901123\n\nEpoch 262/500\ntrain loss: 0.09487749318607518\nval loss: 0.09120683670043946\nBest val loss: 0.09120683670043946\nTime: 0.6004486083984375\n\nEpoch 263/500\ntrain loss: 0.09461864096219422\nval loss: 0.09102659225463867\nBest val loss: 0.09102659225463867\nTime: 0.5188901424407959\n\nEpoch 264/500\ntrain loss: 0.09413444018754803\nval loss: 0.09067773818969727\nBest val loss: 0.09067773818969727\nTime: 0.5721402168273926\n\nEpoch 265/500\ntrain loss: 0.0939550868800429\nval loss: 0.0906446933746338\nBest val loss: 0.0906446933746338\nTime: 0.5855679512023926\n\nEpoch 266/500\ntrain loss: 0.09375318933705815\nval loss: 0.0905992031097412\nBest val loss: 0.0905992031097412\nTime: 0.5280053615570068\n\nEpoch 267/500\ntrain loss: 0.09351996124767867\nval loss: 0.09038281440734863\nBest val loss: 0.09038281440734863\nTime: 0.5269150733947754\n\nEpoch 268/500\ntrain loss: 0.09328762429659485\nval loss: 0.08993759155273437\nBest val loss: 0.08993759155273437\nTime: 0.5499172210693359\n\nEpoch 269/500\ntrain loss: 0.09287859181888768\nval loss: 0.08996548652648925\nBest val loss: 0.08993759155273437\nTime: 0.4439218044281006\n\nEpoch 270/500\ntrain loss: 0.09282651494760982\nval loss: 0.08981161117553711\nBest val loss: 0.08981161117553711\nTime: 0.5270259380340576\n\nEpoch 271/500\ntrain loss: 0.09247764212186219\nval loss: 0.0896371841430664\nBest val loss: 0.0896371841430664\nTime: 0.5653340816497803\n\nEpoch 272/500\ntrain loss: 0.09220326533083056\nval loss: 0.08933405876159668\nBest val loss: 0.08933405876159668\nTime: 0.5440685749053955\n\nEpoch 273/500\ntrain loss: 0.09209012203529233\nval loss: 0.08919634819030761\nBest val loss: 0.08919634819030761\nTime: 0.5236091613769531\n\nEpoch 274/500\ntrain loss: 0.09185024949370837\nval loss: 0.08895950317382813\nBest val loss: 0.08895950317382813\nTime: 0.5842785835266113\n\nEpoch 275/500\ntrain loss: 0.09170707327420594\nval loss: 0.08888487815856934\nBest val loss: 0.08888487815856934\nTime: 0.5523085594177246\n\nEpoch 276/500\ntrain loss: 0.09161115865238377\nval loss: 0.0887343406677246\nBest val loss: 0.0887343406677246\nTime: 0.5156311988830566\n\nEpoch 277/500\ntrain loss: 0.09113302387175012\nval loss: 0.08857841491699218\nBest val loss: 0.08857841491699218\nTime: 0.5851011276245117\n\nEpoch 278/500\ntrain loss: 0.09102455514376281\nval loss: 0.0883756160736084\nBest val loss: 0.0883756160736084\nTime: 0.5357444286346436\n\nEpoch 279/500\ntrain loss: 0.09078554247246413\nval loss: 0.0883206844329834\nBest val loss: 0.0883206844329834\nTime: 0.5513577461242676\n\nEpoch 280/500\ntrain loss: 0.09054418470038743\nval loss: 0.08815979957580566\nBest val loss: 0.08815979957580566\nTime: 0.5277135372161865\n\nEpoch 281/500\ntrain loss: 0.09030614133741034\nval loss: 0.08798136711120605\nBest val loss: 0.08798136711120605\nTime: 0.5414729118347168\n\nEpoch 282/500\ntrain loss: 0.0902183720322906\nval loss: 0.08787379264831544\nBest val loss: 0.08787379264831544\nTime: 0.5604526996612549\n\nEpoch 283/500\ntrain loss: 0.08982797528876633\nval loss: 0.08767786026000976\nBest val loss: 0.08767786026000976\nTime: 0.5003736019134521\n\nEpoch 284/500\ntrain loss: 0.08974767903812596\nval loss: 0.08755550384521485\nBest val loss: 0.08755550384521485\nTime: 0.5547661781311035\n\nEpoch 285/500\ntrain loss: 0.08966084777331743\nval loss: 0.08737382888793946\nBest val loss: 0.08737382888793946\nTime: 0.5189361572265625\n\nEpoch 286/500\ntrain loss: 0.08950405433529714\nval loss: 0.08727812767028809\nBest val loss: 0.08727812767028809\nTime: 0.5282304286956787\n\nEpoch 287/500\ntrain loss: 0.0890785279821177\nval loss: 0.08704071044921875\nBest val loss: 0.08704071044921875\nTime: 0.5259590148925781\n\nEpoch 288/500\ntrain loss: 0.08892819138823962\nval loss: 0.08689451217651367\nBest val loss: 0.08689451217651367\nTime: 0.6009273529052734\n\nEpoch 289/500\ntrain loss: 0.08875390349841508\nval loss: 0.08673982620239258\nBest val loss: 0.08673982620239258\nTime: 0.5576274394989014\n\nEpoch 290/500\ntrain loss: 0.08859990854732326\nval loss: 0.08660283088684081\nBest val loss: 0.08660283088684081\nTime: 0.5714943408966064\n\nEpoch 291/500\ntrain loss: 0.08842751237212634\nval loss: 0.08631176948547363\nBest val loss: 0.08631176948547363\nTime: 0.5802562236785889\n\nEpoch 292/500\ntrain loss: 0.08842247822245614\nval loss: 0.08618173599243165\nBest val loss: 0.08618173599243165\nTime: 0.5027728080749512\n\nEpoch 293/500\ntrain loss: 0.08799155813748719\nval loss: 0.08608431816101074\nBest val loss: 0.08608431816101074\nTime: 0.5351712703704834\n\nEpoch 294/500\ntrain loss: 0.08786276520275679\nval loss: 0.08612899780273438\nBest val loss: 0.08608431816101074\nTime: 0.45688486099243164\n\nEpoch 295/500\ntrain loss: 0.08765430137759349\nval loss: 0.08590116500854492\nBest val loss: 0.08590116500854492\nTime: 0.5954060554504395\n\nEpoch 296/500\ntrain loss: 0.08742097948418288\nval loss: 0.08578672409057617\nBest val loss: 0.08578672409057617\nTime: 0.5824837684631348\n\nEpoch 297/500\ntrain loss: 0.08721794065881948\nval loss: 0.08568601608276367\nBest val loss: 0.08568601608276367\nTime: 0.531336784362793\n\nEpoch 298/500\ntrain loss: 0.08714097445128394\nval loss: 0.0854306697845459\nBest val loss: 0.0854306697845459\nTime: 0.5349459648132324\n\nEpoch 299/500\ntrain loss: 0.08700841372130347\nval loss: 0.08534679412841797\nBest val loss: 0.08534679412841797\nTime: 0.5435135364532471\n\nEpoch 300/500\ntrain loss: 0.08673652273709656\nval loss: 0.08529829978942871\nBest val loss: 0.08529829978942871\nTime: 0.5286121368408203\n\nEpoch 301/500\ntrain loss: 0.08654650703805392\nval loss: 0.08517236709594726\nBest val loss: 0.08517236709594726\nTime: 0.5692083835601807\n\nEpoch 302/500\ntrain loss: 0.08640237714423508\nval loss: 0.08502674102783203\nBest val loss: 0.08502674102783203\nTime: 0.5820333957672119\n\nEpoch 303/500\ntrain loss: 0.08627718003069768\nval loss: 0.08483262062072754\nBest val loss: 0.08483262062072754\nTime: 0.8299710750579834\n\nEpoch 304/500\ntrain loss: 0.0861120849359231\nval loss: 0.08474245071411132\nBest val loss: 0.08474245071411132\nTime: 0.5319392681121826\n\nEpoch 305/500\ntrain loss: 0.08600267816762455\nval loss: 0.08468594551086425\nBest val loss: 0.08468594551086425\nTime: 0.6256954669952393\n\nEpoch 306/500\ntrain loss: 0.08578364575495485\nval loss: 0.08449764251708984\nBest val loss: 0.08449764251708984\nTime: 0.5635507106781006\n\nEpoch 307/500\ntrain loss: 0.08561445455082127\nval loss: 0.08445844650268555\nBest val loss: 0.08445844650268555\nTime: 0.5350494384765625\n\nEpoch 308/500\ntrain loss: 0.08545175145884029\nval loss: 0.08427720069885254\nBest val loss: 0.08427720069885254\nTime: 0.5346343517303467\n\nEpoch 309/500\ntrain loss: 0.0852954270409756\nval loss: 0.08417282104492188\nBest val loss: 0.08417282104492188\nTime: 0.5203609466552734\n\nEpoch 310/500\ntrain loss: 0.08516174066262167\nval loss: 0.0842498779296875\nBest val loss: 0.08417282104492188\nTime: 0.45491552352905273\n\nEpoch 311/500\ntrain loss: 0.08498948519347144\nval loss: 0.08404159545898438\nBest val loss: 0.08404159545898438\nTime: 0.5899274349212646\n\nEpoch 312/500\ntrain loss: 0.08489360183965965\nval loss: 0.08396892547607422\nBest val loss: 0.08396892547607422\nTime: 0.5035572052001953\n\nEpoch 313/500\ntrain loss: 0.08487057295001921\nval loss: 0.08385248184204101\nBest val loss: 0.08385248184204101\nTime: 0.5383956432342529\n\nEpoch 314/500\ntrain loss: 0.08461903744056577\nval loss: 0.08373546600341797\nBest val loss: 0.08373546600341797\nTime: 0.5619010925292969\n\nEpoch 315/500\ntrain loss: 0.08453983557028849\nval loss: 0.0836918830871582\nBest val loss: 0.0836918830871582\nTime: 0.5280864238739014\n\nEpoch 316/500\ntrain loss: 0.08443952779300877\nval loss: 0.0835184097290039\nBest val loss: 0.0835184097290039\nTime: 0.6168532371520996\n\nEpoch 317/500\ntrain loss: 0.08425923644519243\nval loss: 0.08341875076293945\nBest val loss: 0.08341875076293945\nTime: 0.5160524845123291\n\nEpoch 318/500\ntrain loss: 0.08416341562740139\nval loss: 0.08334059715270996\nBest val loss: 0.08334059715270996\nTime: 0.5389659404754639\n\nEpoch 319/500\ntrain loss: 0.08403745244760982\nval loss: 0.08317370414733886\nBest val loss: 0.08317370414733886\nTime: 0.610187292098999\n\nEpoch 320/500\ntrain loss: 0.08396556729176005\nval loss: 0.08313512802124023\nBest val loss: 0.08313512802124023\nTime: 0.6570031642913818\n\nEpoch 321/500\ntrain loss: 0.08374128185334753\nval loss: 0.08297386169433593\nBest val loss: 0.08297386169433593\nTime: 0.5510568618774414\n\nEpoch 322/500\ntrain loss: 0.08371446953445184\nval loss: 0.08287553787231446\nBest val loss: 0.08287553787231446\nTime: 0.5684192180633545\n\nEpoch 323/500\ntrain loss: 0.08353259915211161\nval loss: 0.08286552429199219\nBest val loss: 0.08286552429199219\nTime: 0.5073142051696777\n\nEpoch 324/500\ntrain loss: 0.08351662119881051\nval loss: 0.08272805213928222\nBest val loss: 0.08272805213928222\nTime: 0.6477649211883545\n\nEpoch 325/500\ntrain loss: 0.08333725225730021\nval loss: 0.08258886337280273\nBest val loss: 0.08258886337280273\nTime: 0.559218168258667\n\nEpoch 326/500\ntrain loss: 0.08323302034471856\nval loss: 0.08260869979858398\nBest val loss: 0.08258886337280273\nTime: 0.4481022357940674\n\nEpoch 327/500\ntrain loss: 0.08315295860415599\nval loss: 0.08245906829833985\nBest val loss: 0.08245906829833985\nTime: 0.5233674049377441\n\nEpoch 328/500\ntrain loss: 0.08302863699490906\nval loss: 0.08236279487609863\nBest val loss: 0.08236279487609863\nTime: 0.5044004917144775\n\nEpoch 329/500\ntrain loss: 0.08299478937367924\nval loss: 0.08220496177673339\nBest val loss: 0.08220496177673339\nTime: 0.6225745677947998\n\nEpoch 330/500\ntrain loss: 0.0827791651741403\nval loss: 0.08220343589782715\nBest val loss: 0.08220343589782715\nTime: 0.6047341823577881\n\nEpoch 331/500\ntrain loss: 0.08265110703765369\nval loss: 0.08209972381591797\nBest val loss: 0.08209972381591797\nTime: 0.5778424739837646\n\nEpoch 332/500\ntrain loss: 0.08259441813484567\nval loss: 0.0820892333984375\nBest val loss: 0.0820892333984375\nTime: 0.5358638763427734\n\nEpoch 333/500\ntrain loss: 0.08249192159683978\nval loss: 0.08188080787658691\nBest val loss: 0.08188080787658691\nTime: 0.5135226249694824\n\nEpoch 334/500\ntrain loss: 0.08249797195684715\nval loss: 0.08183059692382813\nBest val loss: 0.08183059692382813\nTime: 0.5739645957946777\n\nEpoch 335/500\ntrain loss: 0.08252515949186731\nval loss: 0.08184332847595215\nBest val loss: 0.08183059692382813\nTime: 0.4537777900695801\n\nEpoch 336/500\ntrain loss: 0.08215847953421171\nval loss: 0.08173279762268067\nBest val loss: 0.08173279762268067\nTime: 0.5078439712524414\n\nEpoch 337/500\ntrain loss: 0.08215397694071785\nval loss: 0.0815579891204834\nBest val loss: 0.0815579891204834\nTime: 0.5746793746948242\n\nEpoch 338/500\ntrain loss: 0.08198366008821081\nval loss: 0.08148102760314942\nBest val loss: 0.08148102760314942\nTime: 0.5275590419769287\n\nEpoch 339/500\ntrain loss: 0.08189267017802254\nval loss: 0.08143939971923828\nBest val loss: 0.08143939971923828\nTime: 0.5736346244812012\n\nEpoch 340/500\ntrain loss: 0.0817853114644035\nval loss: 0.0813410758972168\nBest val loss: 0.0813410758972168\nTime: 0.5299866199493408\n\nEpoch 341/500\ntrain loss: 0.08171217558813877\nval loss: 0.08130836486816406\nBest val loss: 0.08130836486816406\nTime: 0.5142159461975098\n\nEpoch 342/500\ntrain loss: 0.08158019331635022\nval loss: 0.08119821548461914\nBest val loss: 0.08119821548461914\nTime: 0.5228924751281738\n\nEpoch 343/500\ntrain loss: 0.08149897465940381\nval loss: 0.08114681243896485\nBest val loss: 0.08114681243896485\nTime: 0.5315513610839844\n\nEpoch 344/500\ntrain loss: 0.08145130657758869\nval loss: 0.08113646507263184\nBest val loss: 0.08113646507263184\nTime: 0.5106191635131836\n\nEpoch 345/500\ntrain loss: 0.08133658424752657\nval loss: 0.08109097480773926\nBest val loss: 0.08109097480773926\nTime: 0.5304250717163086\n\nEpoch 346/500\ntrain loss: 0.08119075024714235\nval loss: 0.0809201717376709\nBest val loss: 0.0809201717376709\nTime: 0.5375478267669678\n\nEpoch 347/500\ntrain loss: 0.08114084650258549\nval loss: 0.08075432777404785\nBest val loss: 0.08075432777404785\nTime: 0.548980712890625\n\nEpoch 348/500\ntrain loss: 0.08123172697473745\nval loss: 0.08088059425354004\nBest val loss: 0.08075432777404785\nTime: 0.44754576683044434\n\nEpoch 349/500\ntrain loss: 0.08104047618928503\nval loss: 0.08064866065979004\nBest val loss: 0.08064866065979004\nTime: 0.6020350456237793\n\nEpoch 350/500\ntrain loss: 0.08085891848704854\nval loss: 0.08056416511535644\nBest val loss: 0.08056416511535644\nTime: 0.5442168712615967\n\nEpoch 351/500\ntrain loss: 0.08077194651619332\nval loss: 0.08052992820739746\nBest val loss: 0.08052992820739746\nTime: 0.5172591209411621\n\nEpoch 352/500\ntrain loss: 0.08066712051141457\nval loss: 0.08041815757751465\nBest val loss: 0.08041815757751465\nTime: 0.5461950302124023\n\nEpoch 353/500\ntrain loss: 0.0806232358588547\nval loss: 0.0802992820739746\nBest val loss: 0.0802992820739746\nTime: 0.5156927108764648\n\nEpoch 354/500\ntrain loss: 0.08055433679799565\nval loss: 0.08021306991577148\nBest val loss: 0.08021306991577148\nTime: 0.5071859359741211\n\nEpoch 355/500\ntrain loss: 0.08045751540387264\nval loss: 0.08018341064453124\nBest val loss: 0.08018341064453124\nTime: 0.5874302387237549\n\nEpoch 356/500\ntrain loss: 0.08042232325819672\nval loss: 0.08007249832153321\nBest val loss: 0.08007249832153321\nTime: 0.5617215633392334\n\nEpoch 357/500\ntrain loss: 0.08028855870981685\nval loss: 0.07996034622192383\nBest val loss: 0.07996034622192383\nTime: 0.5185651779174805\n\nEpoch 358/500\ntrain loss: 0.0802492235527664\nval loss: 0.07998676300048828\nBest val loss: 0.07996034622192383\nTime: 0.44898462295532227\n\nEpoch 359/500\ntrain loss: 0.08022495957671619\nval loss: 0.07982254028320312\nBest val loss: 0.07982254028320312\nTime: 0.5770583152770996\n\nEpoch 360/500\ntrain loss: 0.08012036808201524\nval loss: 0.07983922958374023\nBest val loss: 0.07982254028320312\nTime: 0.44446706771850586\n\nEpoch 361/500\ntrain loss: 0.08002063876292745\nval loss: 0.07964816093444824\nBest val loss: 0.07964816093444824\nTime: 0.5085587501525879\n\nEpoch 362/500\ntrain loss: 0.07994926952924884\nval loss: 0.07965645790100098\nBest val loss: 0.07964816093444824\nTime: 0.4466729164123535\n\nEpoch 363/500\ntrain loss: 0.07987275670786373\nval loss: 0.07958521842956542\nBest val loss: 0.07958521842956542\nTime: 0.518507719039917\n\nEpoch 364/500\ntrain loss: 0.07978798913173989\nval loss: 0.07960338592529297\nBest val loss: 0.07958521842956542\nTime: 0.44837117195129395\n\nEpoch 365/500\ntrain loss: 0.07971158574839107\nval loss: 0.07948112487792969\nBest val loss: 0.07948112487792969\nTime: 0.5342967510223389\n\nEpoch 366/500\ntrain loss: 0.07966693502957703\nval loss: 0.0794517993927002\nBest val loss: 0.0794517993927002\nTime: 0.5199816226959229\n\nEpoch 367/500\ntrain loss: 0.07958135448518347\nval loss: 0.07930536270141601\nBest val loss: 0.07930536270141601\nTime: 0.5483863353729248\n\nEpoch 368/500\ntrain loss: 0.07961660916688013\nval loss: 0.07919583320617676\nBest val loss: 0.07919583320617676\nTime: 0.5630254745483398\n\nEpoch 369/500\ntrain loss: 0.07949084922915599\nval loss: 0.07923407554626465\nBest val loss: 0.07919583320617676\nTime: 0.4514021873474121\n\nEpoch 370/500\ntrain loss: 0.07939226119244686\nval loss: 0.0791628360748291\nBest val loss: 0.0791628360748291\nTime: 0.6618082523345947\n\nEpoch 371/500\ntrain loss: 0.07932017279452964\nval loss: 0.07902383804321289\nBest val loss: 0.07902383804321289\nTime: 0.6027865409851074\n\nEpoch 372/500\ntrain loss: 0.07928974902043577\nval loss: 0.07905168533325195\nBest val loss: 0.07902383804321289\nTime: 0.45580053329467773\n\nEpoch 373/500\ntrain loss: 0.07917832546546812\nval loss: 0.0789872169494629\nBest val loss: 0.0789872169494629\nTime: 0.5485489368438721\n\nEpoch 374/500\ntrain loss: 0.07913786466004419\nval loss: 0.07892770767211914\nBest val loss: 0.07892770767211914\nTime: 0.5475320816040039\n\nEpoch 375/500\ntrain loss: 0.07906413469158236\nval loss: 0.07883973121643066\nBest val loss: 0.07883973121643066\nTime: 0.5314102172851562\n\nEpoch 376/500\ntrain loss: 0.07903166286280898\nval loss: 0.07879586219787597\nBest val loss: 0.07879586219787597\nTime: 0.6083731651306152\n\nEpoch 377/500\ntrain loss: 0.07892027057585169\nval loss: 0.0787440299987793\nBest val loss: 0.0787440299987793\nTime: 0.581235408782959\n\nEpoch 378/500\ntrain loss: 0.07886602057785284\nval loss: 0.07869400978088378\nBest val loss: 0.07869400978088378\nTime: 0.5628492832183838\n\nEpoch 379/500\ntrain loss: 0.07886733383428855\nval loss: 0.07860164642333985\nBest val loss: 0.07860164642333985\nTime: 0.5915083885192871\n\nEpoch 380/500\ntrain loss: 0.07878675617155481\nval loss: 0.07859940528869629\nBest val loss: 0.07859940528869629\nTime: 0.5185220241546631\n\nEpoch 381/500\ntrain loss: 0.07870323931584593\nval loss: 0.07856101989746093\nBest val loss: 0.07856101989746093\nTime: 0.5300955772399902\n\nEpoch 382/500\ntrain loss: 0.07864603449086674\nval loss: 0.07849788665771484\nBest val loss: 0.07849788665771484\nTime: 0.5780961513519287\n\nEpoch 383/500\ntrain loss: 0.07860707454994077\nval loss: 0.07842350006103516\nBest val loss: 0.07842350006103516\nTime: 0.6462247371673584\n\nEpoch 384/500\ntrain loss: 0.07855305906201972\nval loss: 0.07835497856140136\nBest val loss: 0.07835497856140136\nTime: 0.5643675327301025\n\nEpoch 385/500\ntrain loss: 0.07846117801353579\nval loss: 0.07833585739135743\nBest val loss: 0.07833585739135743\nTime: 0.5633642673492432\n\nEpoch 386/500\ntrain loss: 0.07842437556532562\nval loss: 0.0782468318939209\nBest val loss: 0.0782468318939209\nTime: 0.5877058506011963\n\nEpoch 387/500\ntrain loss: 0.078380428376745\nval loss: 0.07825145721435547\nBest val loss: 0.0782468318939209\nTime: 0.44875478744506836\n\nEpoch 388/500\ntrain loss: 0.07835804048131724\nval loss: 0.07817249298095703\nBest val loss: 0.07817249298095703\nTime: 0.5538747310638428\n\nEpoch 389/500\ntrain loss: 0.07826784790539351\nval loss: 0.07812991142272949\nBest val loss: 0.07812991142272949\nTime: 0.5798828601837158\n\nEpoch 390/500\ntrain loss: 0.07820556202872855\nval loss: 0.07808127403259277\nBest val loss: 0.07808127403259277\nTime: 0.5955338478088379\n\nEpoch 391/500\ntrain loss: 0.07813530280941823\nval loss: 0.07802400588989258\nBest val loss: 0.07802400588989258\nTime: 0.5331432819366455\n\nEpoch 392/500\ntrain loss: 0.07811677651327165\nval loss: 0.07795476913452148\nBest val loss: 0.07795476913452148\nTime: 0.6418437957763672\n\nEpoch 393/500\ntrain loss: 0.07805066030533588\nval loss: 0.07792277336120605\nBest val loss: 0.07792277336120605\nTime: 0.5900416374206543\n\nEpoch 394/500\ntrain loss: 0.07801986131511751\nval loss: 0.07789511680603027\nBest val loss: 0.07789511680603027\nTime: 0.6849086284637451\n\nEpoch 395/500\ntrain loss: 0.07799064135942303\nval loss: 0.07784714698791503\nBest val loss: 0.07784714698791503\nTime: 0.5532536506652832\n\nEpoch 396/500\ntrain loss: 0.07791165836521836\nval loss: 0.07782869338989258\nBest val loss: 0.07782869338989258\nTime: 0.6025235652923584\n\nEpoch 397/500\ntrain loss: 0.07787957738657467\nval loss: 0.07780838012695312\nBest val loss: 0.07780838012695312\nTime: 0.5871481895446777\n\nEpoch 398/500\ntrain loss: 0.07780281442110656\nval loss: 0.07776885032653809\nBest val loss: 0.07776885032653809\nTime: 0.5819835662841797\n\nEpoch 399/500\ntrain loss: 0.07777078034447842\nval loss: 0.0777367115020752\nBest val loss: 0.0777367115020752\nTime: 0.5134029388427734\n\nEpoch 400/500\ntrain loss: 0.07771460736384157\nval loss: 0.0777367115020752\nBest val loss: 0.0777367115020752\nTime: 0.45764803886413574\n\nEpoch 401/500\ntrain loss: 0.07763696889408299\nval loss: 0.07770857810974122\nBest val loss: 0.07770857810974122\nTime: 0.5216219425201416\n\nEpoch 402/500\ntrain loss: 0.07762058445664703\nval loss: 0.07768440246582031\nBest val loss: 0.07768440246582031\nTime: 0.5127685070037842\n\nEpoch 403/500\ntrain loss: 0.07759205239718078\nval loss: 0.07765851020812989\nBest val loss: 0.07765851020812989\nTime: 0.5285699367523193\n\nEpoch 404/500\ntrain loss: 0.0775313455550397\nval loss: 0.07762026786804199\nBest val loss: 0.07762026786804199\nTime: 0.5160431861877441\n\nEpoch 405/500\ntrain loss: 0.07746608921738922\nval loss: 0.07760257720947265\nBest val loss: 0.07760257720947265\nTime: 0.5667746067047119\n\nEpoch 406/500\ntrain loss: 0.07742522192783043\nval loss: 0.07757911682128907\nBest val loss: 0.07757911682128907\nTime: 0.560021162033081\n\nEpoch 407/500\ntrain loss: 0.07744318539979028\nval loss: 0.07753639221191407\nBest val loss: 0.07753639221191407\nTime: 0.5470468997955322\n\nEpoch 408/500\ntrain loss: 0.07733854700307377\nval loss: 0.07751965522766113\nBest val loss: 0.07751965522766113\nTime: 0.5733563899993896\n\nEpoch 409/500\ntrain loss: 0.07729719505935419\nval loss: 0.07749695777893066\nBest val loss: 0.07749695777893066\nTime: 0.5337164402008057\n\nEpoch 410/500\ntrain loss: 0.07724868274125897\nval loss: 0.07747693061828613\nBest val loss: 0.07747693061828613\nTime: 0.5562288761138916\n\nEpoch 411/500\ntrain loss: 0.07726333180411918\nval loss: 0.07747035026550293\nBest val loss: 0.07747035026550293\nTime: 0.513847827911377\n\nEpoch 412/500\ntrain loss: 0.07716854283067047\nval loss: 0.07742600440979004\nBest val loss: 0.07742600440979004\nTime: 0.5605447292327881\n\nEpoch 413/500\ntrain loss: 0.07714512309090035\nval loss: 0.07741622924804688\nBest val loss: 0.07741622924804688\nTime: 0.5738694667816162\n\nEpoch 414/500\ntrain loss: 0.0770919831072698\nval loss: 0.0773777961730957\nBest val loss: 0.0773777961730957\nTime: 0.5638332366943359\n\nEpoch 415/500\ntrain loss: 0.07704562828189036\nval loss: 0.07736234664916992\nBest val loss: 0.07736234664916992\nTime: 0.5782434940338135\n\nEpoch 416/500\ntrain loss: 0.07699237886022349\nval loss: 0.07733321189880371\nBest val loss: 0.07733321189880371\nTime: 0.5999259948730469\n\nEpoch 417/500\ntrain loss: 0.07699699089175364\nval loss: 0.07732219696044922\nBest val loss: 0.07732219696044922\nTime: 0.5303213596343994\n\nEpoch 418/500\ntrain loss: 0.07691253599573354\nval loss: 0.07730088233947754\nBest val loss: 0.07730088233947754\nTime: 0.5122675895690918\n\nEpoch 419/500\ntrain loss: 0.07688520775466669\nval loss: 0.07727460861206055\nBest val loss: 0.07727460861206055\nTime: 0.5108506679534912\n\nEpoch 420/500\ntrain loss: 0.0768694643114434\nval loss: 0.07726483345031739\nBest val loss: 0.07726483345031739\nTime: 0.50693678855896\n\nEpoch 421/500\ntrain loss: 0.07681083679199219\nval loss: 0.07723636627197265\nBest val loss: 0.07723636627197265\nTime: 0.5534322261810303\n\nEpoch 422/500\ntrain loss: 0.07677981892570121\nval loss: 0.07721595764160157\nBest val loss: 0.07721595764160157\nTime: 0.5354886054992676\n\nEpoch 423/500\ntrain loss: 0.07671778319311924\nval loss: 0.07718849182128906\nBest val loss: 0.07718849182128906\nTime: 0.7082910537719727\n\nEpoch 424/500\ntrain loss: 0.07668642137871413\nval loss: 0.07717385292053222\nBest val loss: 0.07717385292053222\nTime: 0.5857558250427246\n\nEpoch 425/500\ntrain loss: 0.07666161021248238\nval loss: 0.0771566390991211\nBest val loss: 0.0771566390991211\nTime: 0.5564765930175781\n\nEpoch 426/500\ntrain loss: 0.07659694796702901\nval loss: 0.0771240234375\nBest val loss: 0.0771240234375\nTime: 0.5437667369842529\n\nEpoch 427/500\ntrain loss: 0.07657443499955975\nval loss: 0.0771111011505127\nBest val loss: 0.0771111011505127\nTime: 0.5384409427642822\n\nEpoch 428/500\ntrain loss: 0.07652732974193135\nval loss: 0.07708640098571777\nBest val loss: 0.07708640098571777\nTime: 0.5606932640075684\n\nEpoch 429/500\ntrain loss: 0.07651984105344678\nval loss: 0.07706937789916993\nBest val loss: 0.07706937789916993\nTime: 0.552091121673584\n\nEpoch 430/500\ntrain loss: 0.07645489739590004\nval loss: 0.07704753875732422\nBest val loss: 0.07704753875732422\nTime: 0.5354769229888916\n\nEpoch 431/500\ntrain loss: 0.0764238169935883\nval loss: 0.07702980041503907\nBest val loss: 0.07702980041503907\nTime: 0.5603947639465332\n\nEpoch 432/500\ntrain loss: 0.07646621641565542\nval loss: 0.07699861526489257\nBest val loss: 0.07699861526489257\nTime: 0.5210044384002686\n\nEpoch 433/500\ntrain loss: 0.07637383507900551\nval loss: 0.07699270248413086\nBest val loss: 0.07699270248413086\nTime: 0.6025569438934326\n\nEpoch 434/500\ntrain loss: 0.07632347794829822\nval loss: 0.07697272300720215\nBest val loss: 0.07697272300720215\nTime: 0.6282045841217041\n\nEpoch 435/500\ntrain loss: 0.07630904776151062\nval loss: 0.07695822715759278\nBest val loss: 0.07695822715759278\nTime: 0.5260829925537109\n\nEpoch 436/500\ntrain loss: 0.07625195237456775\nval loss: 0.07693910598754883\nBest val loss: 0.07693910598754883\nTime: 0.5377705097198486\n\nEpoch 437/500\ntrain loss: 0.07625775259049213\nval loss: 0.07690539360046386\nBest val loss: 0.07690539360046386\nTime: 0.5219392776489258\n\nEpoch 438/500\ntrain loss: 0.07622063746217822\nval loss: 0.07690362930297852\nBest val loss: 0.07690362930297852\nTime: 0.5651109218597412\n\nEpoch 439/500\ntrain loss: 0.07619810886070376\nval loss: 0.07688140869140625\nBest val loss: 0.07688140869140625\nTime: 0.5709278583526611\n\nEpoch 440/500\ntrain loss: 0.07612517622650647\nval loss: 0.07686343193054199\nBest val loss: 0.07686343193054199\nTime: 0.5470559597015381\n\nEpoch 441/500\ntrain loss: 0.0760864507956583\nval loss: 0.07684326171875\nBest val loss: 0.07684326171875\nTime: 0.5677599906921387\n\nEpoch 442/500\ntrain loss: 0.0761164837196225\nval loss: 0.07683930397033692\nBest val loss: 0.07683930397033692\nTime: 0.5851597785949707\n\nEpoch 443/500\ntrain loss: 0.07602511859330975\nval loss: 0.07680292129516601\nBest val loss: 0.07680292129516601\nTime: 0.5283033847808838\n\nEpoch 444/500\ntrain loss: 0.0759973056980821\nval loss: 0.07679152488708496\nBest val loss: 0.07679152488708496\nTime: 0.5761775970458984\n\nEpoch 445/500\ntrain loss: 0.07597244762983478\nval loss: 0.07677321434020996\nBest val loss: 0.07677321434020996\nTime: 0.5413639545440674\n\nEpoch 446/500\ntrain loss: 0.07593597349573354\nval loss: 0.07675666809082031\nBest val loss: 0.07675666809082031\nTime: 0.5929505825042725\n\nEpoch 447/500\ntrain loss: 0.07591056823730469\nval loss: 0.0767444133758545\nBest val loss: 0.0767444133758545\nTime: 0.5228710174560547\n\nEpoch 448/500\ntrain loss: 0.07588514734487065\nval loss: 0.07672181129455566\nBest val loss: 0.07672181129455566\nTime: 0.5447697639465332\n\nEpoch 449/500\ntrain loss: 0.07586250930535988\nval loss: 0.07671823501586914\nBest val loss: 0.07671823501586914\nTime: 0.539008378982544\n\nEpoch 450/500\ntrain loss: 0.07584703164022477\nval loss: 0.07669367790222167\nBest val loss: 0.07669367790222167\nTime: 0.5490410327911377\n\nEpoch 451/500\ntrain loss: 0.07580258416347817\nval loss: 0.0766829490661621\nBest val loss: 0.0766829490661621\nTime: 0.5401349067687988\n\nEpoch 452/500\ntrain loss: 0.07577861723352651\nval loss: 0.07666711807250977\nBest val loss: 0.07666711807250977\nTime: 0.5025310516357422\n\nEpoch 453/500\ntrain loss: 0.07576592242131468\nval loss: 0.07666029930114746\nBest val loss: 0.07666029930114746\nTime: 0.5130631923675537\n\nEpoch 454/500\ntrain loss: 0.07572036493019979\nval loss: 0.07663283348083497\nBest val loss: 0.07663283348083497\nTime: 0.5586838722229004\n\nEpoch 455/500\ntrain loss: 0.0757002908675397\nval loss: 0.07661380767822265\nBest val loss: 0.07661380767822265\nTime: 0.5537643432617188\n\nEpoch 456/500\ntrain loss: 0.0756612371225826\nval loss: 0.07660474777221679\nBest val loss: 0.07660474777221679\nTime: 0.5167660713195801\n\nEpoch 457/500\ntrain loss: 0.07562921867995966\nval loss: 0.0765869140625\nBest val loss: 0.0765869140625\nTime: 0.5592629909515381\n\nEpoch 458/500\ntrain loss: 0.0756011712746542\nval loss: 0.07657232284545898\nBest val loss: 0.07657232284545898\nTime: 0.5401976108551025\n\nEpoch 459/500\ntrain loss: 0.07558080016589555\nval loss: 0.07656044960021972\nBest val loss: 0.07656044960021972\nTime: 0.5307693481445312\n\nEpoch 460/500\ntrain loss: 0.07557901788930424\nval loss: 0.07654266357421875\nBest val loss: 0.07654266357421875\nTime: 0.5482015609741211\n\nEpoch 461/500\ntrain loss: 0.0755333978621686\nval loss: 0.07652902603149414\nBest val loss: 0.07652902603149414\nTime: 0.6654329299926758\n\nEpoch 462/500\ntrain loss: 0.07548995096175397\nval loss: 0.07651777267456054\nBest val loss: 0.07651777267456054\nTime: 0.6068487167358398\n\nEpoch 463/500\ntrain loss: 0.0754690013948034\nval loss: 0.07649946212768555\nBest val loss: 0.07649946212768555\nTime: 0.6070666313171387\n\nEpoch 464/500\ntrain loss: 0.07544290824014632\nval loss: 0.07649059295654297\nBest val loss: 0.07649059295654297\nTime: 0.5997061729431152\n\nEpoch 465/500\ntrain loss: 0.07541637733334401\nval loss: 0.07646942138671875\nBest val loss: 0.07646942138671875\nTime: 0.5478973388671875\n\nEpoch 466/500\ntrain loss: 0.07539699116691215\nval loss: 0.07645888328552246\nBest val loss: 0.07645888328552246\nTime: 0.546698808670044\n\nEpoch 467/500\ntrain loss: 0.07538055982746061\nval loss: 0.0764399528503418\nBest val loss: 0.0764399528503418\nTime: 0.5901048183441162\n\nEpoch 468/500\ntrain loss: 0.0753749315855933\nval loss: 0.07643442153930664\nBest val loss: 0.07643442153930664\nTime: 0.6176998615264893\n\nEpoch 469/500\ntrain loss: 0.07533972380591221\nval loss: 0.0764164924621582\nBest val loss: 0.0764164924621582\nTime: 0.5277204513549805\n\nEpoch 470/500\ntrain loss: 0.07529812171810964\nval loss: 0.07641134262084961\nBest val loss: 0.07641134262084961\nTime: 0.5497803688049316\n\nEpoch 471/500\ntrain loss: 0.07533928605376697\nval loss: 0.07639336585998535\nBest val loss: 0.07639336585998535\nTime: 0.5423591136932373\n\nEpoch 472/500\ntrain loss: 0.07526596256944\nval loss: 0.07638759613037109\nBest val loss: 0.07638759613037109\nTime: 0.5609686374664307\n\nEpoch 473/500\ntrain loss: 0.07527213800148885\nval loss: 0.07636442184448242\nBest val loss: 0.07636442184448242\nTime: 0.5444352626800537\n\nEpoch 474/500\ntrain loss: 0.07523464765705046\nval loss: 0.07634968757629394\nBest val loss: 0.07634968757629394\nTime: 0.5737137794494629\n\nEpoch 475/500\ntrain loss: 0.07518511912861808\nval loss: 0.07634477615356446\nBest val loss: 0.07634477615356446\nTime: 0.5343010425567627\n\nEpoch 476/500\ntrain loss: 0.0751539761902856\nval loss: 0.07633066177368164\nBest val loss: 0.07633066177368164\nTime: 0.6308398246765137\n\nEpoch 477/500\ntrain loss: 0.0751541168963323\nval loss: 0.0763211727142334\nBest val loss: 0.0763211727142334\nTime: 0.519904375076294\n\nEpoch 478/500\ntrain loss: 0.07511778346827773\nval loss: 0.0763066291809082\nBest val loss: 0.0763066291809082\nTime: 0.5996034145355225\n\nEpoch 479/500\ntrain loss: 0.07509986689833344\nval loss: 0.07629671096801757\nBest val loss: 0.07629671096801757\nTime: 0.619356632232666\n\nEpoch 480/500\ntrain loss: 0.0750859839017274\nval loss: 0.07627811431884765\nBest val loss: 0.07627811431884765\nTime: 0.5779635906219482\n\nEpoch 481/500\ntrain loss: 0.07507135047287238\nval loss: 0.07627549171447753\nBest val loss: 0.07627549171447753\nTime: 0.6056146621704102\n\nEpoch 482/500\ntrain loss: 0.07503446985463627\nval loss: 0.0762636661529541\nBest val loss: 0.0762636661529541\nTime: 0.5461254119873047\n\nEpoch 483/500\ntrain loss: 0.07500886135413999\nval loss: 0.07625188827514648\nBest val loss: 0.07625188827514648\nTime: 0.5578057765960693\n\nEpoch 484/500\ntrain loss: 0.07501450523001249\nval loss: 0.07623257637023925\nBest val loss: 0.07623257637023925\nTime: 0.5856139659881592\n\nEpoch 485/500\ntrain loss: 0.07498129860299532\nval loss: 0.07622804641723632\nBest val loss: 0.07622804641723632\nTime: 0.5927188396453857\n\nEpoch 486/500\ntrain loss: 0.07495182850321785\nval loss: 0.07621731758117675\nBest val loss: 0.07621731758117675\nTime: 0.5405426025390625\n\nEpoch 487/500\ntrain loss: 0.0749633038630251\nval loss: 0.07620978355407715\nBest val loss: 0.07620978355407715\nTime: 0.6035301685333252\n\nEpoch 488/500\ntrain loss: 0.07490891315897957\nval loss: 0.0761939525604248\nBest val loss: 0.0761939525604248\nTime: 0.6242668628692627\n\nEpoch 489/500\ntrain loss: 0.0748917001192687\nval loss: 0.07618188858032227\nBest val loss: 0.07618188858032227\nTime: 0.6577115058898926\n\nEpoch 490/500\ntrain loss: 0.07487662893826844\nval loss: 0.07617383003234864\nBest val loss: 0.07617383003234864\nTime: 0.5751807689666748\n\nEpoch 491/500\ntrain loss: 0.0748650285064197\nval loss: 0.07616629600524902\nBest val loss: 0.07616629600524902\nTime: 0.6114926338195801\n\nEpoch 492/500\ntrain loss: 0.07484608009213307\nval loss: 0.07615489959716797\nBest val loss: 0.07615489959716797\nTime: 0.5106451511383057\n\nEpoch 493/500\ntrain loss: 0.07482372346471568\nval loss: 0.07614006996154785\nBest val loss: 0.07614006996154785\nTime: 0.5892605781555176\n\nEpoch 494/500\ntrain loss: 0.07478978203945473\nval loss: 0.07613368034362793\nBest val loss: 0.07613368034362793\nTime: 0.5181975364685059\n\nEpoch 495/500\ntrain loss: 0.07478337209732806\nval loss: 0.07612371444702148\nBest val loss: 0.07612371444702148\nTime: 0.5231513977050781\n\nEpoch 496/500\ntrain loss: 0.07475676302049981\nval loss: 0.07611150741577148\nBest val loss: 0.07611150741577148\nTime: 0.5385105609893799\n\nEpoch 497/500\ntrain loss: 0.0747418638135566\nval loss: 0.07610564231872559\nBest val loss: 0.07610564231872559\nTime: 0.5678281784057617\n\nEpoch 498/500\ntrain loss: 0.0747182095637087\nval loss: 0.07609491348266602\nBest val loss: 0.07609491348266602\nTime: 0.552955150604248\n\nEpoch 499/500\ntrain loss: 0.07472707404464972\nval loss: 0.07608513832092285\nBest val loss: 0.07608513832092285\nTime: 0.6157867908477783\n\nEpoch 500/500\ntrain loss: 0.07468243895984086\nval loss: 0.07607407569885254\nBest val loss: 0.07607407569885254\nTime: 0.5311124324798584\n\nTraining complete, model saved. Best model after epoch 500\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1677686619184
        }
      },
      "id": "54c8cd4b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "38e3035c-fa8e-4f2f-a6dd-f1dcacace76f"
    },
    {
      "cell_type": "code",
      "source": [
        "model = monai.networks.nets.AutoEncoder(\n",
        "    spatial_dims=3, in_channels=1, out_channels=1,\n",
        "    kernel_size=(3, 3, 3),\n",
        "    channels=[channel*1 for channel in (1, 2, 4, 8, 16)],\n",
        "    strides=(2, 2, 2, 2, 2),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677687706372
        }
      },
      "id": "38ddf5c8-f776-449a-88b0-c9ce5013e2fd"
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(1, 16*2*2*2)\n",
        "        self.decode = monai.networks.nets.AutoEncoder(\n",
        "            spatial_dims=3, in_channels=1, out_channels=1,\n",
        "            kernel_size=(3, 3, 3),\n",
        "            channels=[channel*1 for channel in (1, 2, 4, 8, 16)],\n",
        "            strides=(2, 2, 2, 2, 2),\n",
        "        ).decode\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = torch.reshape(x, (1, 16, 2, 2, 2))\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "def train_decoder(model, dataloaders, num_epochs, learning_rate):\n",
        "    opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    opt.zero_grad()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.99)\n",
        "    loss_dice = monai.losses.DiceLoss(sigmoid=True, squared_pred=True).to(device)\n",
        "\n",
        "    metric = monai.metrics.DiceMetric(include_background=False, reduction='mean_batch')\n",
        "\n",
        "    t0 = time.time()\n",
        "    best_val_dsc = 0\n",
        "    \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for mode in ['train', 'val']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            losses = []\n",
        "            for image, radius in dataloaders[mode]:\n",
        "                image = image.to(device)\n",
        "                radius = radius.to(device)\n",
        "                \n",
        "                pred_segm = model(radius)\n",
        "                loss = loss_dice(pred_segm, image)\n",
        "                \n",
        "                if mode == 'train':\n",
        "                    opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                metric((pred_segm > 0).float(), image)\n",
        "\n",
        "            print(f'{mode} loss: {np.mean(losses)}')\n",
        "            mean_dsc = metric.aggregate().tolist()[0]\n",
        "            metric.reset()\n",
        "            print(f'{mode} DSC: {mean_dsc}')\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        if mean_dsc > best_val_dsc:\n",
        "            best_val_dsc = mean_dsc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{model_dir}/best_decoder.torch')\n",
        "        print(f'Best val DSC: {best_val_dsc}')\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "        t0 = time.time()\n",
        "        \n",
        "    print(f\"Training complete, model saved. Best model after epoch {best_epoch}\")"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677688450748
        }
      },
      "id": "172e904d-9601-4d97-ba4f-aa8031cbea28"
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder().to(device)\n",
        "train_decoder(model=decoder, dataloaders=dataloaders, num_epochs=800, learning_rate=3e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/800\ntrain loss: 0.8572288956798491\ntrain DSC: 0.20258405804634094\nval loss: 0.8015431195497513\nval DSC: 0.6159278154373169\nBest val DSC: 0.6159278154373169\nTime: 1.1787209510803223\n\nEpoch 2/800\ntrain loss: 0.7791847879769372\ntrain DSC: 0.6285985708236694\nval loss: 0.7265393555164337\nval DSC: 0.6727702021598816\nBest val DSC: 0.6727702021598816\nTime: 1.170889139175415\n\nEpoch 3/800\ntrain loss: 0.6966711057991278\ntrain DSC: 0.682629406452179\nval loss: 0.6229707896709442\nval DSC: 0.7676151990890503\nBest val DSC: 0.7676151990890503\nTime: 1.1702656745910645\n\nEpoch 4/800\ntrain loss: 0.5806515656533788\ntrain DSC: 0.7535243630409241\nval loss: 0.48680145740509034\nval DSC: 0.8066158294677734\nBest val DSC: 0.8066158294677734\nTime: 1.1696629524230957\n\nEpoch 5/800\ntrain loss: 0.4505528377704933\ntrain DSC: 0.7923573851585388\nval loss: 0.3618992030620575\nval DSC: 0.8406642079353333\nBest val DSC: 0.8406642079353333\nTime: 1.1872260570526123\n\nEpoch 6/800\ntrain loss: 0.344969383028687\ntrain DSC: 0.8195464611053467\nval loss: 0.2755544513463974\nval DSC: 0.8424575924873352\nBest val DSC: 0.8424575924873352\nTime: 1.2096364498138428\n\nEpoch 7/800\ntrain loss: 0.2770494212869738\ntrain DSC: 0.8281855583190918\nval loss: 0.2182960629463196\nval DSC: 0.8614552617073059\nBest val DSC: 0.8614552617073059\nTime: 1.1475415229797363\n\nEpoch 8/800\ntrain loss: 0.23088014907524235\ntrain DSC: 0.8386569619178772\nval loss: 0.18716436326503755\nval DSC: 0.8697201013565063\nBest val DSC: 0.8697201013565063\nTime: 1.2025938034057617\n\nEpoch 9/800\ntrain loss: 0.20194609145649145\ntrain DSC: 0.845051109790802\nval loss: 0.16642957031726838\nval DSC: 0.8758171796798706\nBest val DSC: 0.8758171796798706\nTime: 1.1689198017120361\n\nEpoch 10/800\ntrain loss: 0.18583752483618063\ntrain DSC: 0.844471275806427\nval loss: 0.14632124304771424\nval DSC: 0.8783348202705383\nBest val DSC: 0.8783348202705383\nTime: 1.1888015270233154\n\nEpoch 11/800\ntrain loss: 0.16838251371852686\ntrain DSC: 0.8521745800971985\nval loss: 0.13318879008293152\nval DSC: 0.8788465261459351\nBest val DSC: 0.8788465261459351\nTime: 1.2280082702636719\n\nEpoch 12/800\ntrain loss: 0.15292012886922868\ntrain DSC: 0.8600380420684814\nval loss: 0.12620960175991058\nval DSC: 0.8817197680473328\nBest val DSC: 0.8817197680473328\nTime: 1.1910004615783691\n\nEpoch 13/800\ntrain loss: 0.1415619869701198\ntrain DSC: 0.8635531067848206\nval loss: 0.11478234529495239\nval DSC: 0.8877622485160828\nBest val DSC: 0.8877622485160828\nTime: 1.1892638206481934\n\nEpoch 14/800\ntrain loss: 0.1270839943260443\ntrain DSC: 0.8769062757492065\nval loss: 0.10154730379581452\nval DSC: 0.900198757648468\nBest val DSC: 0.900198757648468\nTime: 1.145829200744629\n\nEpoch 15/800\ntrain loss: 0.11735234690494224\ntrain DSC: 0.883774995803833\nval loss: 0.08949168622493744\nval DSC: 0.9113763570785522\nBest val DSC: 0.9113763570785522\nTime: 1.1508827209472656\n\nEpoch 16/800\ntrain loss: 0.10242507399105635\ntrain DSC: 0.8997395038604736\nval loss: 0.0783510833978653\nval DSC: 0.9243024587631226\nBest val DSC: 0.9243024587631226\nTime: 1.2076566219329834\n\nEpoch 17/800\ntrain loss: 0.08737404913198753\ntrain DSC: 0.915244460105896\nval loss: 0.06982805132865906\nval DSC: 0.9308229684829712\nBest val DSC: 0.9308229684829712\nTime: 1.1422295570373535\n\nEpoch 18/800\ntrain loss: 0.07738882596375513\ntrain DSC: 0.9261261820793152\nval loss: 0.05990528464317322\nval DSC: 0.9432266354560852\nBest val DSC: 0.9432266354560852\nTime: 1.1740317344665527\n\nEpoch 19/800\ntrain loss: 0.07677396105938271\ntrain DSC: 0.9254646897315979\nval loss: 0.06035109162330628\nval DSC: 0.9370056390762329\nBest val DSC: 0.9432266354560852\nTime: 1.116020679473877\n\nEpoch 20/800\ntrain loss: 0.06535074456793363\ntrain DSC: 0.9355530142784119\nval loss: 0.053418558835983274\nval DSC: 0.9459151029586792\nBest val DSC: 0.9459151029586792\nTime: 1.1640129089355469\n\nEpoch 21/800\ntrain loss: 0.05753757719133721\ntrain DSC: 0.9432250261306763\nval loss: 0.050805401802062986\nval DSC: 0.9444230794906616\nBest val DSC: 0.9459151029586792\nTime: 1.1176962852478027\n\nEpoch 22/800\ntrain loss: 0.05214575470470991\ntrain DSC: 0.9488335847854614\nval loss: 0.04849698543548584\nval DSC: 0.947657585144043\nBest val DSC: 0.947657585144043\nTime: 1.2608263492584229\n\nEpoch 23/800\ntrain loss: 0.053365665381072\ntrain DSC: 0.9480296969413757\nval loss: 0.044131490588188174\nval DSC: 0.9551278352737427\nBest val DSC: 0.9551278352737427\nTime: 1.19096040725708\n\nEpoch 24/800\ntrain loss: 0.048724103169363056\ntrain DSC: 0.951010525226593\nval loss: 0.041035419702529906\nval DSC: 0.9567470550537109\nBest val DSC: 0.9567470550537109\nTime: 1.1778488159179688\n\nEpoch 25/800\ntrain loss: 0.04577046144204062\ntrain DSC: 0.9524750113487244\nval loss: 0.04123219549655914\nval DSC: 0.9538345336914062\nBest val DSC: 0.9567470550537109\nTime: 1.139441728591919\n\nEpoch 26/800\ntrain loss: 0.04385958050118118\ntrain DSC: 0.953731119632721\nval loss: 0.03601724207401276\nval DSC: 0.9611776471138\nBest val DSC: 0.9611776471138\nTime: 1.2191147804260254\n\nEpoch 27/800\ntrain loss: 0.03860044381657585\ntrain DSC: 0.960503876209259\nval loss: 0.033579963445663455\nval DSC: 0.9645450711250305\nBest val DSC: 0.9645450711250305\nTime: 1.1815412044525146\n\nEpoch 28/800\ntrain loss: 0.03843134348509741\ntrain DSC: 0.959625244140625\nval loss: 0.03317524492740631\nval DSC: 0.96478271484375\nBest val DSC: 0.96478271484375\nTime: 1.1480815410614014\n\nEpoch 29/800\ntrain loss: 0.037127358991591655\ntrain DSC: 0.9602643251419067\nval loss: 0.03134357929229736\nval DSC: 0.9668148159980774\nBest val DSC: 0.9668148159980774\nTime: 1.176199197769165\n\nEpoch 30/800\ntrain loss: 0.034762956079889516\ntrain DSC: 0.9636375904083252\nval loss: 0.0315879762172699\nval DSC: 0.9646562337875366\nBest val DSC: 0.9668148159980774\nTime: 1.111579418182373\n\nEpoch 31/800\ntrain loss: 0.03414394523276657\ntrain DSC: 0.9640491604804993\nval loss: 0.02961862087249756\nval DSC: 0.9680377244949341\nBest val DSC: 0.9680377244949341\nTime: 1.176609992980957\n\nEpoch 32/800\ntrain loss: 0.03226442708343756\ntrain DSC: 0.965798556804657\nval loss: 0.03027295768260956\nval DSC: 0.9660224914550781\nBest val DSC: 0.9680377244949341\nTime: 1.1182265281677246\n\nEpoch 33/800\ntrain loss: 0.03270909043609119\ntrain DSC: 0.9649703502655029\nval loss: 0.029808494448661804\nval DSC: 0.9665988683700562\nBest val DSC: 0.9680377244949341\nTime: 1.129197359085083\n\nEpoch 34/800\ntrain loss: 0.03232301649500112\ntrain DSC: 0.964608907699585\nval loss: 0.02768646478652954\nval DSC: 0.96941739320755\nBest val DSC: 0.96941739320755\nTime: 1.1824896335601807\n\nEpoch 35/800\ntrain loss: 0.031093361924906245\ntrain DSC: 0.9663786292076111\nval loss: 0.02805926501750946\nval DSC: 0.9695237874984741\nBest val DSC: 0.9695237874984741\nTime: 1.1745667457580566\n\nEpoch 36/800\ntrain loss: 0.030337490996376414\ntrain DSC: 0.9670880436897278\nval loss: 0.02711494266986847\nval DSC: 0.96924889087677\nBest val DSC: 0.9695237874984741\nTime: 1.1170248985290527\n\nEpoch 37/800\ntrain loss: 0.02840407070566396\ntrain DSC: 0.969236433506012\nval loss: 0.025685426592826844\nval DSC: 0.9714825749397278\nBest val DSC: 0.9714825749397278\nTime: 1.1669349670410156\n\nEpoch 38/800\ntrain loss: 0.027623053456916184\ntrain DSC: 0.9698045253753662\nval loss: 0.02596413791179657\nval DSC: 0.9708927273750305\nBest val DSC: 0.9714825749397278\nTime: 1.1308202743530273\n\nEpoch 39/800\ntrain loss: 0.027660481265333832\ntrain DSC: 0.9694722294807434\nval loss: 0.02936641275882721\nval DSC: 0.9655230641365051\nBest val DSC: 0.9714825749397278\nTime: 1.1190392971038818\n\nEpoch 40/800\ntrain loss: 0.0275361674730895\ntrain DSC: 0.9695141911506653\nval loss: 0.024186518788337708\nval DSC: 0.9724414944648743\nBest val DSC: 0.9724414944648743\nTime: 1.2222402095794678\n\nEpoch 41/800\ntrain loss: 0.026454864955339275\ntrain DSC: 0.970740020275116\nval loss: 0.022893652319908142\nval DSC: 0.9751843214035034\nBest val DSC: 0.9751843214035034\nTime: 1.1703100204467773\n\nEpoch 42/800\ntrain loss: 0.026667868504758743\ntrain DSC: 0.9702824354171753\nval loss: 0.025144171714782716\nval DSC: 0.9703391790390015\nBest val DSC: 0.9751843214035034\nTime: 1.105889081954956\n\nEpoch 43/800\ntrain loss: 0.025737227963619543\ntrain DSC: 0.9711866974830627\nval loss: 0.022980937361717226\nval DSC: 0.9738042950630188\nBest val DSC: 0.9751843214035034\nTime: 1.1179118156433105\n\nEpoch 44/800\ntrain loss: 0.024715677636568664\ntrain DSC: 0.972565770149231\nval loss: 0.022007474303245546\nval DSC: 0.9756029844284058\nBest val DSC: 0.9756029844284058\nTime: 1.1396512985229492\n\nEpoch 45/800\ntrain loss: 0.024846638812393438\ntrain DSC: 0.9724447131156921\nval loss: 0.02659100592136383\nval DSC: 0.9673200845718384\nBest val DSC: 0.9756029844284058\nTime: 1.1149847507476807\n\nEpoch 46/800\ntrain loss: 0.024289379354383123\ntrain DSC: 0.9728571176528931\nval loss: 0.02626650929450989\nval DSC: 0.9680854678153992\nBest val DSC: 0.9756029844284058\nTime: 1.1109619140625\n\nEpoch 47/800\ntrain loss: 0.025075924201089828\ntrain DSC: 0.9709755182266235\nval loss: 0.021274569630622863\nval DSC: 0.9760903120040894\nBest val DSC: 0.9760903120040894\nTime: 1.1926538944244385\n\nEpoch 48/800\ntrain loss: 0.02296302357658011\ntrain DSC: 0.9746083617210388\nval loss: 0.020907515287399293\nval DSC: 0.9768036007881165\nBest val DSC: 0.9768036007881165\nTime: 1.1793107986450195\n\nEpoch 49/800\ntrain loss: 0.0233752326887162\ntrain DSC: 0.9732730388641357\nval loss: 0.025062528252601624\nval DSC: 0.9690736532211304\nBest val DSC: 0.9768036007881165\nTime: 1.1217267513275146\n\nEpoch 50/800\ntrain loss: 0.026167786512218537\ntrain DSC: 0.9689491391181946\nval loss: 0.020815059542655945\nval DSC: 0.9759550094604492\nBest val DSC: 0.9768036007881165\nTime: 1.1259617805480957\n\nEpoch 51/800\ntrain loss: 0.022830080790597885\ntrain DSC: 0.9742280840873718\nval loss: 0.02028217017650604\nval DSC: 0.9770362973213196\nBest val DSC: 0.9770362973213196\nTime: 1.1713957786560059\n\nEpoch 52/800\ntrain loss: 0.022731254335309637\ntrain DSC: 0.9737563729286194\nval loss: 0.02288139462471008\nval DSC: 0.9722875356674194\nBest val DSC: 0.9770362973213196\nTime: 1.117699146270752\n\nEpoch 53/800\ntrain loss: 0.021788600038309566\ntrain DSC: 0.975863516330719\nval loss: 0.02121756076812744\nval DSC: 0.9745772480964661\nBest val DSC: 0.9770362973213196\nTime: 1.1160297393798828\n\nEpoch 54/800\ntrain loss: 0.02346399377604\ntrain DSC: 0.9725680947303772\nval loss: 0.021007245779037474\nval DSC: 0.9756147265434265\nBest val DSC: 0.9770362973213196\nTime: 1.1165080070495605\n\nEpoch 55/800\ntrain loss: 0.022804917859249426\ntrain DSC: 0.9733576774597168\nval loss: 0.019645658135414124\nval DSC: 0.9771590232849121\nBest val DSC: 0.9771590232849121\nTime: 1.1918084621429443\n\nEpoch 56/800\ntrain loss: 0.02159432798135476\ntrain DSC: 0.9751124382019043\nval loss: 0.021389716863632204\nval DSC: 0.9742051959037781\nBest val DSC: 0.9771590232849121\nTime: 1.1172876358032227\n\nEpoch 57/800\ntrain loss: 0.021553444080665462\ntrain DSC: 0.9748587012290955\nval loss: 0.019061324000358582\nval DSC: 0.9782440066337585\nBest val DSC: 0.9782440066337585\nTime: 1.1912026405334473\n\nEpoch 58/800\ntrain loss: 0.020810205428326716\ntrain DSC: 0.976028561592102\nval loss: 0.019029095768928528\nval DSC: 0.9789084196090698\nBest val DSC: 0.9789084196090698\nTime: 1.2322900295257568\n\nEpoch 59/800\ntrain loss: 0.02001796296385468\ntrain DSC: 0.97758948802948\nval loss: 0.021126973628997802\nval DSC: 0.9743671417236328\nBest val DSC: 0.9789084196090698\nTime: 1.1353809833526611\n\nEpoch 60/800\ntrain loss: 0.020205253460368174\ntrain DSC: 0.9768298268318176\nval loss: 0.01848909854888916\nval DSC: 0.9789250493049622\nBest val DSC: 0.9789250493049622\nTime: 1.2007927894592285\n\nEpoch 61/800\ntrain loss: 0.020075499034318767\ntrain DSC: 0.976601779460907\nval loss: 0.01841968297958374\nval DSC: 0.9794021844863892\nBest val DSC: 0.9794021844863892\nTime: 1.168642282485962\n\nEpoch 62/800\ntrain loss: 0.02022025721972106\ntrain DSC: 0.9764434099197388\nval loss: 0.019270113110542296\nval DSC: 0.9769704937934875\nBest val DSC: 0.9794021844863892\nTime: 1.119933843612671\n\nEpoch 63/800\ntrain loss: 0.019554634563258438\ntrain DSC: 0.9775874614715576\nval loss: 0.02147068977355957\nval DSC: 0.9738584756851196\nBest val DSC: 0.9794021844863892\nTime: 1.113097906112671\n\nEpoch 64/800\ntrain loss: 0.019928394771013103\ntrain DSC: 0.976922869682312\nval loss: 0.01787220239639282\nval DSC: 0.9794443249702454\nBest val DSC: 0.9794443249702454\nTime: 1.1633524894714355\n\nEpoch 65/800\ntrain loss: 0.019306045086657415\ntrain DSC: 0.9775974154472351\nval loss: 0.018400794267654418\nval DSC: 0.9785078167915344\nBest val DSC: 0.9794443249702454\nTime: 1.1044697761535645\n\nEpoch 66/800\ntrain loss: 0.019604027271270752\ntrain DSC: 0.9769953489303589\nval loss: 0.019609054923057555\nval DSC: 0.9753576517105103\nBest val DSC: 0.9794443249702454\nTime: 1.1266775131225586\n\nEpoch 67/800\ntrain loss: 0.01926360267107604\ntrain DSC: 0.9774014353752136\nval loss: 0.02384568750858307\nval DSC: 0.9684236645698547\nBest val DSC: 0.9794443249702454\nTime: 1.1336102485656738\n\nEpoch 68/800\ntrain loss: 0.018852603240091293\ntrain DSC: 0.9783680438995361\nval loss: 0.018517309427261354\nval DSC: 0.9784172177314758\nBest val DSC: 0.9794443249702454\nTime: 1.106994390487671\n\nEpoch 69/800\ntrain loss: 0.01854863811711796\ntrain DSC: 0.9785939455032349\nval loss: 0.017168721556663512\nval DSC: 0.9806810617446899\nBest val DSC: 0.9806810617446899\nTime: 1.2178211212158203\n\nEpoch 70/800\ntrain loss: 0.017894298326773722\ntrain DSC: 0.9797981381416321\nval loss: 0.017631205916404723\nval DSC: 0.9793136715888977\nBest val DSC: 0.9806810617446899\nTime: 1.1319153308868408\n\nEpoch 71/800\ntrain loss: 0.01835900056557577\ntrain DSC: 0.9784845113754272\nval loss: 0.017589622735977174\nval DSC: 0.9789217114448547\nBest val DSC: 0.9806810617446899\nTime: 1.1412725448608398\n\nEpoch 72/800\ntrain loss: 0.01832990372767214\ntrain DSC: 0.9787695407867432\nval loss: 0.017795994877815247\nval DSC: 0.97833651304245\nBest val DSC: 0.9806810617446899\nTime: 1.1309442520141602\n\nEpoch 73/800\ntrain loss: 0.0187073359723951\ntrain DSC: 0.9775825142860413\nval loss: 0.018232098221778868\nval DSC: 0.9777027368545532\nBest val DSC: 0.9806810617446899\nTime: 1.1222169399261475\n\nEpoch 74/800\ntrain loss: 0.017640812475173198\ntrain DSC: 0.9793736934661865\nval loss: 0.01699185073375702\nval DSC: 0.9799030423164368\nBest val DSC: 0.9806810617446899\nTime: 1.11094069480896\n\nEpoch 75/800\ntrain loss: 0.018057219317701996\ntrain DSC: 0.978688657283783\nval loss: 0.017874205112457277\nval DSC: 0.9778100848197937\nBest val DSC: 0.9806810617446899\nTime: 1.1153149604797363\n\nEpoch 76/800\ntrain loss: 0.017684249604334595\ntrain DSC: 0.9793489575386047\nval loss: 0.019041308760643007\nval DSC: 0.9754844903945923\nBest val DSC: 0.9806810617446899\nTime: 1.1278870105743408\n\nEpoch 77/800\ntrain loss: 0.018013799776796433\ntrain DSC: 0.9785542488098145\nval loss: 0.01733481287956238\nval DSC: 0.9789983034133911\nBest val DSC: 0.9806810617446899\nTime: 1.1105163097381592\n\nEpoch 78/800\ntrain loss: 0.017591903444196356\ntrain DSC: 0.9792447686195374\nval loss: 0.016264885663986206\nval DSC: 0.980993390083313\nBest val DSC: 0.980993390083313\nTime: 1.1960735321044922\n\nEpoch 79/800\ntrain loss: 0.016850601454250148\ntrain DSC: 0.9803715944290161\nval loss: 0.016918641328811646\nval DSC: 0.9800403714179993\nBest val DSC: 0.980993390083313\nTime: 1.1285014152526855\n\nEpoch 80/800\ntrain loss: 0.018109611800459564\ntrain DSC: 0.9779872298240662\nval loss: 0.017517203092575075\nval DSC: 0.9776545763015747\nBest val DSC: 0.980993390083313\nTime: 1.1023287773132324\n\nEpoch 81/800\ntrain loss: 0.01703697052158293\ntrain DSC: 0.9798129200935364\nval loss: 0.016313189268112184\nval DSC: 0.9804903268814087\nBest val DSC: 0.980993390083313\nTime: 1.1235828399658203\n\nEpoch 82/800\ntrain loss: 0.016904281788184993\ntrain DSC: 0.9803469181060791\nval loss: 0.016270548105239868\nval DSC: 0.9809499979019165\nBest val DSC: 0.980993390083313\nTime: 1.1147592067718506\n\nEpoch 83/800\ntrain loss: 0.017056366459268037\ntrain DSC: 0.9800253510475159\nval loss: 0.016587597131729127\nval DSC: 0.9797319173812866\nBest val DSC: 0.980993390083313\nTime: 1.1124162673950195\n\nEpoch 84/800\ntrain loss: 0.016886185427181056\ntrain DSC: 0.9805140495300293\nval loss: 0.015847814083099366\nval DSC: 0.9815391302108765\nBest val DSC: 0.9815391302108765\nTime: 1.2127938270568848\n\nEpoch 85/800\ntrain loss: 0.01695663616305492\ntrain DSC: 0.9799467325210571\nval loss: 0.015332427620887757\nval DSC: 0.9821876287460327\nBest val DSC: 0.9821876287460327\nTime: 1.1585078239440918\n\nEpoch 86/800\ntrain loss: 0.016531853402247193\ntrain DSC: 0.9804458618164062\nval loss: 0.01503712236881256\nval DSC: 0.9828243255615234\nBest val DSC: 0.9828243255615234\nTime: 1.177947759628296\n\nEpoch 87/800\ntrain loss: 0.01659062846762235\ntrain DSC: 0.9802437424659729\nval loss: 0.015158927440643311\nval DSC: 0.9823940396308899\nBest val DSC: 0.9828243255615234\nTime: 1.1338846683502197\n\nEpoch 88/800\ntrain loss: 0.016527988871590037\ntrain DSC: 0.9803405404090881\nval loss: 0.015529707074165344\nval DSC: 0.9811391830444336\nBest val DSC: 0.9828243255615234\nTime: 1.1121807098388672\n\nEpoch 89/800\ntrain loss: 0.01619207761326774\ntrain DSC: 0.980950117111206\nval loss: 0.015570646524429322\nval DSC: 0.9813671112060547\nBest val DSC: 0.9828243255615234\nTime: 1.105889081954956\n\nEpoch 90/800\ntrain loss: 0.01641950450959753\ntrain DSC: 0.9806894063949585\nval loss: 0.015167537331581115\nval DSC: 0.9820032119750977\nBest val DSC: 0.9828243255615234\nTime: 1.128218412399292\n\nEpoch 91/800\ntrain loss: 0.015771188696876902\ntrain DSC: 0.9814618229866028\nval loss: 0.014962702989578247\nval DSC: 0.9827276468276978\nBest val DSC: 0.9828243255615234\nTime: 1.1195118427276611\n\nEpoch 92/800\ntrain loss: 0.017531623605821955\ntrain DSC: 0.9782558679580688\nval loss: 0.021012741327285766\nval DSC: 0.9722919464111328\nBest val DSC: 0.9828243255615234\nTime: 1.137096881866455\n\nEpoch 93/800\ntrain loss: 0.01695771197803685\ntrain DSC: 0.9795548319816589\nval loss: 0.01583434045314789\nval DSC: 0.9802621603012085\nBest val DSC: 0.9828243255615234\nTime: 1.1168596744537354\n\nEpoch 94/800\ntrain loss: 0.01540781533131834\ntrain DSC: 0.9819296002388\nval loss: 0.015580946207046508\nval DSC: 0.9811757802963257\nBest val DSC: 0.9828243255615234\nTime: 1.1130588054656982\n\nEpoch 95/800\ntrain loss: 0.015730456250612854\ntrain DSC: 0.9811844229698181\nval loss: 0.014633676409721375\nval DSC: 0.9825588464736938\nBest val DSC: 0.9828243255615234\nTime: 1.1282808780670166\n\nEpoch 96/800\ntrain loss: 0.015143270375298673\ntrain DSC: 0.9820849895477295\nval loss: 0.015127933025360108\nval DSC: 0.9814704656600952\nBest val DSC: 0.9828243255615234\nTime: 1.1535944938659668\n\nEpoch 97/800\ntrain loss: 0.015986351693262818\ntrain DSC: 0.9807186722755432\nval loss: 0.014618879556655884\nval DSC: 0.9826833605766296\nBest val DSC: 0.9828243255615234\nTime: 1.157668113708496\n\nEpoch 98/800\ntrain loss: 0.015251365841412153\ntrain DSC: 0.9819757342338562\nval loss: 0.014527997374534607\nval DSC: 0.9828678965568542\nBest val DSC: 0.9828678965568542\nTime: 1.1889138221740723\n\nEpoch 99/800\ntrain loss: 0.015247701621446454\ntrain DSC: 0.9817732572555542\nval loss: 0.014647933840751647\nval DSC: 0.9823627471923828\nBest val DSC: 0.9828678965568542\nTime: 1.148552656173706\n\nEpoch 100/800\ntrain loss: 0.01480594717088293\ntrain DSC: 0.9828083515167236\nval loss: 0.014385101199150086\nval DSC: 0.9830495715141296\nBest val DSC: 0.9830495715141296\nTime: 1.1697914600372314\n\nEpoch 101/800\ntrain loss: 0.015507700990458003\ntrain DSC: 0.9815192222595215\nval loss: 0.015932312607765196\nval DSC: 0.9796246290206909\nBest val DSC: 0.9830495715141296\nTime: 1.1492955684661865\n\nEpoch 102/800\ntrain loss: 0.014980239946334089\ntrain DSC: 0.9821789860725403\nval loss: 0.014881879091262817\nval DSC: 0.9818848371505737\nBest val DSC: 0.9830495715141296\nTime: 1.157796859741211\n\nEpoch 103/800\ntrain loss: 0.015157994676808843\ntrain DSC: 0.9818043112754822\nval loss: 0.014374172687530518\nval DSC: 0.9824588894844055\nBest val DSC: 0.9830495715141296\nTime: 1.1254396438598633\n\nEpoch 104/800\ntrain loss: 0.014421589061862132\ntrain DSC: 0.9831162691116333\nval loss: 0.013713213801383971\nval DSC: 0.9840580224990845\nBest val DSC: 0.9840580224990845\nTime: 1.1737051010131836\n\nEpoch 105/800\ntrain loss: 0.015197449043148854\ntrain DSC: 0.9816573262214661\nval loss: 0.014131245017051697\nval DSC: 0.983531653881073\nBest val DSC: 0.9840580224990845\nTime: 1.1567318439483643\n\nEpoch 106/800\ntrain loss: 0.014303068645664902\ntrain DSC: 0.9830505847930908\nval loss: 0.014215585589408875\nval DSC: 0.9831783175468445\nBest val DSC: 0.9840580224990845\nTime: 1.1031789779663086\n\nEpoch 107/800\ntrain loss: 0.014459021755906402\ntrain DSC: 0.9829386472702026\nval loss: 0.013686999678611755\nval DSC: 0.9835503697395325\nBest val DSC: 0.9840580224990845\nTime: 1.1311030387878418\n\nEpoch 108/800\ntrain loss: 0.014180758937460477\ntrain DSC: 0.983008086681366\nval loss: 0.013856416940689087\nval DSC: 0.9837090373039246\nBest val DSC: 0.9840580224990845\nTime: 1.1273808479309082\n\nEpoch 109/800\ntrain loss: 0.014262370398787201\ntrain DSC: 0.9829757213592529\nval loss: 0.014779576659202575\nval DSC: 0.9814750552177429\nBest val DSC: 0.9840580224990845\nTime: 1.1655452251434326\n\nEpoch 110/800\ntrain loss: 0.014495149987642883\ntrain DSC: 0.982611894607544\nval loss: 0.014683270454406738\nval DSC: 0.9814068078994751\nBest val DSC: 0.9840580224990845\nTime: 1.1298279762268066\n\nEpoch 111/800\ntrain loss: 0.01422937873934136\ntrain DSC: 0.9828528761863708\nval loss: 0.014474272727966309\nval DSC: 0.9819900393486023\nBest val DSC: 0.9840580224990845\nTime: 1.1724786758422852\n\nEpoch 112/800\ntrain loss: 0.014309161022061208\ntrain DSC: 0.9829812049865723\nval loss: 0.013488891720771789\nval DSC: 0.9839113354682922\nBest val DSC: 0.9840580224990845\nTime: 1.1549808979034424\n\nEpoch 113/800\ntrain loss: 0.014339889659256231\ntrain DSC: 0.9826968908309937\nval loss: 0.014052033424377441\nval DSC: 0.9830207824707031\nBest val DSC: 0.9840580224990845\nTime: 1.1427056789398193\n\nEpoch 114/800\ntrain loss: 0.013708980357060667\ntrain DSC: 0.9840507507324219\nval loss: 0.013966429233551025\nval DSC: 0.9825067520141602\nBest val DSC: 0.9840580224990845\nTime: 1.1256422996520996\n\nEpoch 115/800\ntrain loss: 0.014031713126135653\ntrain DSC: 0.9831122159957886\nval loss: 0.013443410396575928\nval DSC: 0.9838935732841492\nBest val DSC: 0.9840580224990845\nTime: 1.1106183528900146\n\nEpoch 116/800\ntrain loss: 0.013400341643661749\ntrain DSC: 0.9840993285179138\nval loss: 0.013416364789009094\nval DSC: 0.9838094711303711\nBest val DSC: 0.9840580224990845\nTime: 1.1066007614135742\n\nEpoch 117/800\ntrain loss: 0.01513213505510424\ntrain DSC: 0.9815547466278076\nval loss: 0.014117464423179626\nval DSC: 0.9818388819694519\nBest val DSC: 0.9840580224990845\nTime: 1.1096220016479492\n\nEpoch 118/800\ntrain loss: 0.013988605288208509\ntrain DSC: 0.982968807220459\nval loss: 0.01379413902759552\nval DSC: 0.9832159876823425\nBest val DSC: 0.9840580224990845\nTime: 1.1278064250946045\n\nEpoch 119/800\ntrain loss: 0.01402008728902848\ntrain DSC: 0.9827319383621216\nval loss: 0.01458905041217804\nval DSC: 0.9816787838935852\nBest val DSC: 0.9840580224990845\nTime: 1.1421856880187988\n\nEpoch 120/800\ntrain loss: 0.01395938826389\ntrain DSC: 0.9829814434051514\nval loss: 0.01340634524822235\nval DSC: 0.9840375781059265\nBest val DSC: 0.9840580224990845\nTime: 1.1244022846221924\n\nEpoch 121/800\ntrain loss: 0.014051448126308253\ntrain DSC: 0.9829444289207458\nval loss: 0.013725748658180237\nval DSC: 0.9833164215087891\nBest val DSC: 0.9840580224990845\nTime: 1.1667654514312744\n\nEpoch 122/800\ntrain loss: 0.013163957439485143\ntrain DSC: 0.9844083786010742\nval loss: 0.015133354067802429\nval DSC: 0.9802795648574829\nBest val DSC: 0.9840580224990845\nTime: 1.1641688346862793\n\nEpoch 123/800\ntrain loss: 0.013865730801566702\ntrain DSC: 0.9831576347351074\nval loss: 0.012986516952514649\nval DSC: 0.9845070838928223\nBest val DSC: 0.9845070838928223\nTime: 1.2345027923583984\n\nEpoch 124/800\ntrain loss: 0.014337847467328682\ntrain DSC: 0.9824165105819702\nval loss: 0.013405752182006837\nval DSC: 0.9838530421257019\nBest val DSC: 0.9845070838928223\nTime: 1.1380939483642578\n\nEpoch 125/800\ntrain loss: 0.013826706370369334\ntrain DSC: 0.9830898642539978\nval loss: 0.012987351417541504\nval DSC: 0.984268069267273\nBest val DSC: 0.9845070838928223\nTime: 1.1237099170684814\n\nEpoch 126/800\ntrain loss: 0.013489458404603551\ntrain DSC: 0.9839817881584167\nval loss: 0.012874719500541688\nval DSC: 0.9846708178520203\nBest val DSC: 0.9846708178520203\nTime: 1.1770648956298828\n\nEpoch 127/800\ntrain loss: 0.012937659122904793\ntrain DSC: 0.9844856858253479\nval loss: 0.013596335053443908\nval DSC: 0.9828157424926758\nBest val DSC: 0.9846708178520203\nTime: 1.1207902431488037\n\nEpoch 128/800\ntrain loss: 0.013065181794713755\ntrain DSC: 0.984398365020752\nval loss: 0.013720202445983886\nval DSC: 0.9831945300102234\nBest val DSC: 0.9846708178520203\nTime: 1.1093692779541016\n\nEpoch 129/800\ntrain loss: 0.013562840516449974\ntrain DSC: 0.9834480881690979\nval loss: 0.01304071843624115\nval DSC: 0.9840339422225952\nBest val DSC: 0.9846708178520203\nTime: 1.1036341190338135\n\nEpoch 130/800\ntrain loss: 0.013981288573780998\ntrain DSC: 0.9824938178062439\nval loss: 0.017248892784118654\nval DSC: 0.9760502576828003\nBest val DSC: 0.9846708178520203\nTime: 1.1235244274139404\n\nEpoch 131/800\ntrain loss: 0.014418702633654485\ntrain DSC: 0.9820958375930786\nval loss: 0.012859642505645752\nval DSC: 0.9846517443656921\nBest val DSC: 0.9846708178520203\nTime: 1.1214733123779297\n\nEpoch 132/800\ntrain loss: 0.01285764916998441\ntrain DSC: 0.9844871163368225\nval loss: 0.013386270403862\nval DSC: 0.9832776784896851\nBest val DSC: 0.9846708178520203\nTime: 1.134126901626587\n\nEpoch 133/800\ntrain loss: 0.012739251871578029\ntrain DSC: 0.9848011136054993\nval loss: 0.012949153780937195\nval DSC: 0.9840094447135925\nBest val DSC: 0.9846708178520203\nTime: 1.1417021751403809\n\nEpoch 134/800\ntrain loss: 0.012787425127185758\ntrain DSC: 0.9847232103347778\nval loss: 0.013552793860435485\nval DSC: 0.9834932088851929\nBest val DSC: 0.9846708178520203\nTime: 1.1514160633087158\n\nEpoch 135/800\ntrain loss: 0.013057346226739102\ntrain DSC: 0.9841076135635376\nval loss: 0.0137076735496521\nval DSC: 0.9825904965400696\nBest val DSC: 0.9846708178520203\nTime: 1.1251604557037354\n\nEpoch 136/800\ntrain loss: 0.012829774715861336\ntrain DSC: 0.9846023917198181\nval loss: 0.012571105360984802\nval DSC: 0.9845994710922241\nBest val DSC: 0.9846708178520203\nTime: 1.1240959167480469\n\nEpoch 137/800\ntrain loss: 0.012352125566513812\ntrain DSC: 0.9852285385131836\nval loss: 0.0133830726146698\nval DSC: 0.9825763702392578\nBest val DSC: 0.9846708178520203\nTime: 1.1361522674560547\n\nEpoch 138/800\ntrain loss: 0.012915707025371615\ntrain DSC: 0.9841800928115845\nval loss: 0.014316663146018982\nval DSC: 0.9808858633041382\nBest val DSC: 0.9846708178520203\nTime: 1.1163623332977295\n\nEpoch 139/800\ntrain loss: 0.013068812792418434\ntrain DSC: 0.9839612245559692\nval loss: 0.012763485312461853\nval DSC: 0.9843772649765015\nBest val DSC: 0.9846708178520203\nTime: 1.1248297691345215\n\nEpoch 140/800\ntrain loss: 0.012958248130610733\ntrain DSC: 0.9840034246444702\nval loss: 0.013057389855384826\nval DSC: 0.9844682812690735\nBest val DSC: 0.9846708178520203\nTime: 1.1339330673217773\n\nEpoch 141/800\ntrain loss: 0.012667851369889056\ntrain DSC: 0.9845806360244751\nval loss: 0.012496042251586913\nval DSC: 0.9849217534065247\nBest val DSC: 0.9849217534065247\nTime: 1.2086760997772217\n\nEpoch 142/800\ntrain loss: 0.012433797609610636\ntrain DSC: 0.9851052165031433\nval loss: 0.012143659591674804\nval DSC: 0.9853037595748901\nBest val DSC: 0.9853037595748901\nTime: 1.1933059692382812\n\nEpoch 143/800\ntrain loss: 0.012531815005130455\ntrain DSC: 0.9846317768096924\nval loss: 0.012424245476722717\nval DSC: 0.9847263097763062\nBest val DSC: 0.9853037595748901\nTime: 1.1322190761566162\n\nEpoch 144/800\ntrain loss: 0.012417866558325096\ntrain DSC: 0.984943687915802\nval loss: 0.013054746389389037\nval DSC: 0.9833022356033325\nBest val DSC: 0.9853037595748901\nTime: 1.1225383281707764\n\nEpoch 145/800\ntrain loss: 0.012103792096747727\ntrain DSC: 0.9855039715766907\nval loss: 0.012134471535682678\nval DSC: 0.9853371381759644\nBest val DSC: 0.9853371381759644\nTime: 1.1834776401519775\n\nEpoch 146/800\ntrain loss: 0.01231085765557211\ntrain DSC: 0.985064685344696\nval loss: 0.013058227300643922\nval DSC: 0.9834012985229492\nBest val DSC: 0.9853371381759644\nTime: 1.129157304763794\n\nEpoch 147/800\ntrain loss: 0.012019232648317932\ntrain DSC: 0.9856318235397339\nval loss: 0.012656813859939576\nval DSC: 0.9839885830879211\nBest val DSC: 0.9853371381759644\nTime: 1.1594452857971191\n\nEpoch 148/800\ntrain loss: 0.012583845951518074\ntrain DSC: 0.9843652844429016\nval loss: 0.012407597899436951\nval DSC: 0.9847120046615601\nBest val DSC: 0.9853371381759644\nTime: 1.1748204231262207\n\nEpoch 149/800\ntrain loss: 0.012486078700081248\ntrain DSC: 0.9848418831825256\nval loss: 0.012100571393966674\nval DSC: 0.9852121472358704\nBest val DSC: 0.9853371381759644\nTime: 1.1383957862854004\n\nEpoch 150/800\ntrain loss: 0.011861283271039118\ntrain DSC: 0.9856475591659546\nval loss: 0.011970201134681701\nval DSC: 0.9855720400810242\nBest val DSC: 0.9855720400810242\nTime: 1.1933836936950684\n\nEpoch 151/800\ntrain loss: 0.012688221501522377\ntrain DSC: 0.9842169284820557\nval loss: 0.013616305589675904\nval DSC: 0.9823635816574097\nBest val DSC: 0.9855720400810242\nTime: 1.1394858360290527\n\nEpoch 152/800\ntrain loss: 0.012114716357872134\ntrain DSC: 0.9852803349494934\nval loss: 0.011762657761573791\nval DSC: 0.9856279492378235\nBest val DSC: 0.9856279492378235\nTime: 1.2196261882781982\n\nEpoch 153/800\ntrain loss: 0.012552899415375756\ntrain DSC: 0.9844180941581726\nval loss: 0.012814751267433167\nval DSC: 0.9839338064193726\nBest val DSC: 0.9856279492378235\nTime: 1.1624350547790527\n\nEpoch 154/800\ntrain loss: 0.011926016846641165\ntrain DSC: 0.9856388568878174\nval loss: 0.011922329664230347\nval DSC: 0.9854649305343628\nBest val DSC: 0.9856279492378235\nTime: 1.1356217861175537\n\nEpoch 155/800\ntrain loss: 0.011664678815935478\ntrain DSC: 0.9860400557518005\nval loss: 0.012343490123748779\nval DSC: 0.9847205281257629\nBest val DSC: 0.9856279492378235\nTime: 1.1201648712158203\n\nEpoch 156/800\ntrain loss: 0.012178781579752437\ntrain DSC: 0.9849557876586914\nval loss: 0.015237432718276978\nval DSC: 0.9788426160812378\nBest val DSC: 0.9856279492378235\nTime: 1.1277337074279785\n\nEpoch 157/800\ntrain loss: 0.013232131473353653\ntrain DSC: 0.9829211831092834\nval loss: 0.014080438017845153\nval DSC: 0.9815042614936829\nBest val DSC: 0.9856279492378235\nTime: 1.113501787185669\n\nEpoch 158/800\ntrain loss: 0.011738000346011803\ntrain DSC: 0.9859139919281006\nval loss: 0.01314728856086731\nval DSC: 0.9831887483596802\nBest val DSC: 0.9856279492378235\nTime: 1.1146492958068848\n\nEpoch 159/800\ntrain loss: 0.012539890945934858\ntrain DSC: 0.9843506217002869\nval loss: 0.011877059936523438\nval DSC: 0.9852458834648132\nBest val DSC: 0.9856279492378235\nTime: 1.1324000358581543\n\nEpoch 160/800\ntrain loss: 0.012017236381280617\ntrain DSC: 0.985338568687439\nval loss: 0.012607231736183167\nval DSC: 0.9839218258857727\nBest val DSC: 0.9856279492378235\nTime: 1.1429872512817383\n\nEpoch 161/800\ntrain loss: 0.011759074985003863\ntrain DSC: 0.9857447147369385\nval loss: 0.012449306249618531\nval DSC: 0.9839943051338196\nBest val DSC: 0.9856279492378235\nTime: 1.1398744583129883\n\nEpoch 162/800\ntrain loss: 0.012007711363620445\ntrain DSC: 0.9851788282394409\nval loss: 0.012146654725074767\nval DSC: 0.9848533868789673\nBest val DSC: 0.9856279492378235\nTime: 1.1332154273986816\n\nEpoch 163/800\ntrain loss: 0.012140873025675288\ntrain DSC: 0.9848887920379639\nval loss: 0.01357468068599701\nval DSC: 0.9815805554389954\nBest val DSC: 0.9856279492378235\nTime: 1.1358115673065186\n\nEpoch 164/800\ntrain loss: 0.01157682547803785\ntrain DSC: 0.9858236908912659\nval loss: 0.013013532757759095\nval DSC: 0.9832102060317993\nBest val DSC: 0.9856279492378235\nTime: 1.1244070529937744\n\nEpoch 165/800\ntrain loss: 0.011570813225918129\ntrain DSC: 0.9859659075737\nval loss: 0.012717604637145996\nval DSC: 0.9839035868644714\nBest val DSC: 0.9856279492378235\nTime: 1.125091552734375\n\nEpoch 166/800\ntrain loss: 0.01186644835550277\ntrain DSC: 0.985405683517456\nval loss: 0.01172260046005249\nval DSC: 0.9852834939956665\nBest val DSC: 0.9856279492378235\nTime: 1.1326978206634521\n\nEpoch 167/800\ntrain loss: 0.01216054744407779\ntrain DSC: 0.9850708842277527\nval loss: 0.012489280104637146\nval DSC: 0.9835673570632935\nBest val DSC: 0.9856279492378235\nTime: 1.1388838291168213\n\nEpoch 168/800\ntrain loss: 0.011475908951681168\ntrain DSC: 0.9860071539878845\nval loss: 0.011462527513504028\nval DSC: 0.9860051870346069\nBest val DSC: 0.9860051870346069\nTime: 1.188535451889038\n\nEpoch 169/800\ntrain loss: 0.011348277818961222\ntrain DSC: 0.9863806962966919\nval loss: 0.011862614750862121\nval DSC: 0.9848865270614624\nBest val DSC: 0.9860051870346069\nTime: 1.128053903579712\n\nEpoch 170/800\ntrain loss: 0.011439052761578168\ntrain DSC: 0.986019492149353\nval loss: 0.012179040908813476\nval DSC: 0.9848149418830872\nBest val DSC: 0.9860051870346069\nTime: 1.119415283203125\n\nEpoch 171/800\ntrain loss: 0.011667065933102468\ntrain DSC: 0.9856094717979431\nval loss: 0.011708307266235351\nval DSC: 0.9853013157844543\nBest val DSC: 0.9860051870346069\nTime: 1.1245746612548828\n\nEpoch 172/800\ntrain loss: 0.011384694302668337\ntrain DSC: 0.9862909913063049\nval loss: 0.011554577946662902\nval DSC: 0.9855596423149109\nBest val DSC: 0.9860051870346069\nTime: 1.1353273391723633\n\nEpoch 173/800\ntrain loss: 0.011422632170505211\ntrain DSC: 0.9861630201339722\nval loss: 0.011359381675720214\nval DSC: 0.9861854314804077\nBest val DSC: 0.9861854314804077\nTime: 1.1912238597869873\n\nEpoch 174/800\ntrain loss: 0.011527443518404101\ntrain DSC: 0.9858159422874451\nval loss: 0.011648944020271302\nval DSC: 0.9855619668960571\nBest val DSC: 0.9861854314804077\nTime: 1.1073322296142578\n\nEpoch 175/800\ntrain loss: 0.011224589386924368\ntrain DSC: 0.9863073825836182\nval loss: 0.012450730800628662\nval DSC: 0.9833751916885376\nBest val DSC: 0.9861854314804077\nTime: 1.1236679553985596\n\nEpoch 176/800\ntrain loss: 0.011480677323263199\ntrain DSC: 0.9859018325805664\nval loss: 0.011526867747306824\nval DSC: 0.9854799509048462\nBest val DSC: 0.9861854314804077\nTime: 1.120359182357788\n\nEpoch 177/800\ntrain loss: 0.011157410066635882\ntrain DSC: 0.9863887429237366\nval loss: 0.01256721019744873\nval DSC: 0.9832895398139954\nBest val DSC: 0.9861854314804077\nTime: 1.119422197341919\n\nEpoch 178/800\ntrain loss: 0.01144970929036375\ntrain DSC: 0.9858134984970093\nval loss: 0.011628007888793946\nval DSC: 0.9853441119194031\nBest val DSC: 0.9861854314804077\nTime: 1.1439118385314941\n\nEpoch 179/800\ntrain loss: 0.01145592673880155\ntrain DSC: 0.98607337474823\nval loss: 0.01130906045436859\nval DSC: 0.9862812161445618\nBest val DSC: 0.9862812161445618\nTime: 1.2514674663543701\n\nEpoch 180/800\ntrain loss: 0.011047936853815297\ntrain DSC: 0.9865608215332031\nval loss: 0.011223629117012024\nval DSC: 0.9863298535346985\nBest val DSC: 0.9863298535346985\nTime: 1.1880643367767334\n\nEpoch 181/800\ntrain loss: 0.01147283100690998\ntrain DSC: 0.9857896566390991\nval loss: 0.011449143290519714\nval DSC: 0.9859097599983215\nBest val DSC: 0.9863298535346985\nTime: 1.1166069507598877\n\nEpoch 182/800\ntrain loss: 0.011049869607706538\ntrain DSC: 0.9865729808807373\nval loss: 0.012484505772590637\nval DSC: 0.9832803606987\nBest val DSC: 0.9863298535346985\nTime: 1.1255443096160889\n\nEpoch 183/800\ntrain loss: 0.011408713020262171\ntrain DSC: 0.9858713746070862\nval loss: 0.011074921488761902\nval DSC: 0.9863897562026978\nBest val DSC: 0.9863897562026978\nTime: 1.1604886054992676\n\nEpoch 184/800\ntrain loss: 0.011070271984475558\ntrain DSC: 0.9865288734436035\nval loss: 0.012381967902183533\nval DSC: 0.9838680028915405\nBest val DSC: 0.9863897562026978\nTime: 1.138857126235962\n\nEpoch 185/800\ntrain loss: 0.011439799285325849\ntrain DSC: 0.9858258366584778\nval loss: 0.011571913957595825\nval DSC: 0.9852789044380188\nBest val DSC: 0.9863897562026978\nTime: 1.122767448425293\n\nEpoch 186/800\ntrain loss: 0.011642118946450656\ntrain DSC: 0.9855058789253235\nval loss: 0.01156083345413208\nval DSC: 0.9856491088867188\nBest val DSC: 0.9863897562026978\nTime: 1.1209123134613037\n\nEpoch 187/800\ntrain loss: 0.011102133109921315\ntrain DSC: 0.98635333776474\nval loss: 0.011737659573554993\nval DSC: 0.9852221608161926\nBest val DSC: 0.9863897562026978\nTime: 1.1327905654907227\n\nEpoch 188/800\ntrain loss: 0.011017811102945297\ntrain DSC: 0.9865034222602844\nval loss: 0.011235177516937256\nval DSC: 0.9861696362495422\nBest val DSC: 0.9863897562026978\nTime: 1.1389203071594238\n\nEpoch 189/800\ntrain loss: 0.010969548928933065\ntrain DSC: 0.9865837693214417\nval loss: 0.01113034188747406\nval DSC: 0.9862431287765503\nBest val DSC: 0.9863897562026978\nTime: 1.1171727180480957\n\nEpoch 190/800\ntrain loss: 0.010813156112295682\ntrain DSC: 0.9868571162223816\nval loss: 0.013491421937942505\nval DSC: 0.9819682240486145\nBest val DSC: 0.9863897562026978\nTime: 1.1339316368103027\n\nEpoch 191/800\ntrain loss: 0.011319170232679024\ntrain DSC: 0.9861230254173279\nval loss: 0.013441690802574157\nval DSC: 0.9817034602165222\nBest val DSC: 0.9863897562026978\nTime: 1.1571152210235596\n\nEpoch 192/800\ntrain loss: 0.011105657600965655\ntrain DSC: 0.986049234867096\nval loss: 0.01220608949661255\nval DSC: 0.9838680028915405\nBest val DSC: 0.9863897562026978\nTime: 1.1294503211975098\n\nEpoch 193/800\ntrain loss: 0.010999350274195437\ntrain DSC: 0.9863847494125366\nval loss: 0.011588013172149659\nval DSC: 0.9854094386100769\nBest val DSC: 0.9863897562026978\nTime: 1.1247711181640625\n\nEpoch 194/800\ntrain loss: 0.010994422631185562\ntrain DSC: 0.9865188002586365\nval loss: 0.011792153120040894\nval DSC: 0.984722912311554\nBest val DSC: 0.9863897562026978\nTime: 1.1194207668304443\n\nEpoch 195/800\ntrain loss: 0.01070257972498409\ntrain DSC: 0.9871278405189514\nval loss: 0.011033067107200622\nval DSC: 0.9864374995231628\nBest val DSC: 0.9864374995231628\nTime: 1.224931001663208\n\nEpoch 196/800\ntrain loss: 0.011166372260109324\ntrain DSC: 0.9860342144966125\nval loss: 0.011385858058929443\nval DSC: 0.9859573245048523\nBest val DSC: 0.9864374995231628\nTime: 1.1196627616882324\n\nEpoch 197/800\ntrain loss: 0.010766272662115878\ntrain DSC: 0.9867639541625977\nval loss: 0.011315515637397766\nval DSC: 0.9857414364814758\nBest val DSC: 0.9864374995231628\nTime: 1.1351487636566162\n\nEpoch 198/800\ntrain loss: 0.010809635529752637\ntrain DSC: 0.9867860078811646\nval loss: 0.011086967587471009\nval DSC: 0.9863815307617188\nBest val DSC: 0.9864374995231628\nTime: 1.135512351989746\n\nEpoch 199/800\ntrain loss: 0.011119770222022885\ntrain DSC: 0.9861318469047546\nval loss: 0.011095157265663147\nval DSC: 0.9860744476318359\nBest val DSC: 0.9864374995231628\nTime: 1.115840196609497\n\nEpoch 200/800\ntrain loss: 0.010704805616472588\ntrain DSC: 0.9869552850723267\nval loss: 0.01097819209098816\nval DSC: 0.9863292574882507\nBest val DSC: 0.9864374995231628\nTime: 1.138735055923462\n\nEpoch 201/800\ntrain loss: 0.011163620675196414\ntrain DSC: 0.9859929084777832\nval loss: 0.01088826060295105\nval DSC: 0.9866584539413452\nBest val DSC: 0.9866584539413452\nTime: 1.2495434284210205\n\nEpoch 202/800\ntrain loss: 0.010870109816066554\ntrain DSC: 0.9866999983787537\nval loss: 0.011267447471618652\nval DSC: 0.9858380556106567\nBest val DSC: 0.9866584539413452\nTime: 1.1423616409301758\n\nEpoch 203/800\ntrain loss: 0.010827382079890518\ntrain DSC: 0.9867298603057861\nval loss: 0.010887223482131957\nval DSC: 0.9866533279418945\nBest val DSC: 0.9866584539413452\nTime: 1.1170246601104736\n\nEpoch 204/800\ntrain loss: 0.010438358197446729\ntrain DSC: 0.987334132194519\nval loss: 0.010812276601791381\nval DSC: 0.9865459203720093\nBest val DSC: 0.9866584539413452\nTime: 1.127912998199463\n\nEpoch 205/800\ntrain loss: 0.010554461205591921\ntrain DSC: 0.9869291186332703\nval loss: 0.010946354269981385\nval DSC: 0.9865223169326782\nBest val DSC: 0.9866584539413452\nTime: 1.1433608531951904\n\nEpoch 206/800\ntrain loss: 0.010562560597404104\ntrain DSC: 0.9870564341545105\nval loss: 0.010905665159225465\nval DSC: 0.9863713383674622\nBest val DSC: 0.9866584539413452\nTime: 1.1150612831115723\n\nEpoch 207/800\ntrain loss: 0.010484300675939341\ntrain DSC: 0.9871758818626404\nval loss: 0.010773071646690368\nval DSC: 0.9865609407424927\nBest val DSC: 0.9866584539413452\nTime: 1.1610770225524902\n\nEpoch 208/800\ntrain loss: 0.010510270712805575\ntrain DSC: 0.9869748950004578\nval loss: 0.011284446716308594\nval DSC: 0.9858134984970093\nBest val DSC: 0.9866584539413452\nTime: 1.1002788543701172\n\nEpoch 209/800\ntrain loss: 0.010718317305455442\ntrain DSC: 0.9867526292800903\nval loss: 0.010926952958106995\nval DSC: 0.9864028096199036\nBest val DSC: 0.9866584539413452\nTime: 1.1357536315917969\n\nEpoch 210/800\ntrain loss: 0.01052002242354096\ntrain DSC: 0.9870175719261169\nval loss: 0.011008375883102417\nval DSC: 0.9862136840820312\nBest val DSC: 0.9866584539413452\nTime: 1.1733851432800293\n\nEpoch 211/800\ntrain loss: 0.010742518745484899\ntrain DSC: 0.9868472814559937\nval loss: 0.01097698211669922\nval DSC: 0.9863756895065308\nBest val DSC: 0.9866584539413452\nTime: 1.1548042297363281\n\nEpoch 212/800\ntrain loss: 0.010816153932790288\ntrain DSC: 0.9867612719535828\nval loss: 0.011482447385787964\nval DSC: 0.9851821660995483\nBest val DSC: 0.9866584539413452\nTime: 1.1397922039031982\n\nEpoch 213/800\ntrain loss: 0.010304551632677923\ntrain DSC: 0.9874462485313416\nval loss: 0.010980013012886047\nval DSC: 0.9860041737556458\nBest val DSC: 0.9866584539413452\nTime: 1.122727870941162\n\nEpoch 214/800\ntrain loss: 0.010547728812108274\ntrain DSC: 0.9869436621665955\nval loss: 0.011668607592582703\nval DSC: 0.9844242930412292\nBest val DSC: 0.9866584539413452\nTime: 1.123288869857788\n\nEpoch 215/800\ntrain loss: 0.010858825972822846\ntrain DSC: 0.9867562651634216\nval loss: 0.010982048511505128\nval DSC: 0.9860321283340454\nBest val DSC: 0.9866584539413452\nTime: 1.144852638244629\n\nEpoch 216/800\ntrain loss: 0.010398275539523265\ntrain DSC: 0.9873912930488586\nval loss: 0.01080896556377411\nval DSC: 0.9865821599960327\nBest val DSC: 0.9866584539413452\nTime: 1.1494431495666504\n\nEpoch 217/800\ntrain loss: 0.010380199698151136\ntrain DSC: 0.9872575402259827\nval loss: 0.010803252458572388\nval DSC: 0.9865620732307434\nBest val DSC: 0.9866584539413452\nTime: 1.1384212970733643\n\nEpoch 218/800\ntrain loss: 0.011076046795141502\ntrain DSC: 0.9862391948699951\nval loss: 0.010955238342285156\nval DSC: 0.9862607717514038\nBest val DSC: 0.9866584539413452\nTime: 1.1289889812469482\n\nEpoch 219/800\ntrain loss: 0.010423084751504367\ntrain DSC: 0.987302839756012\nval loss: 0.01081726849079132\nval DSC: 0.9864705801010132\nBest val DSC: 0.9866584539413452\nTime: 1.1124529838562012\n\nEpoch 220/800\ntrain loss: 0.010589116909464851\ntrain DSC: 0.986709713935852\nval loss: 0.010907083749771118\nval DSC: 0.9861139059066772\nBest val DSC: 0.9866584539413452\nTime: 1.1378333568572998\n\nEpoch 221/800\ntrain loss: 0.010397223175549116\ntrain DSC: 0.9871983528137207\nval loss: 0.011149775981903077\nval DSC: 0.985687255859375\nBest val DSC: 0.9866584539413452\nTime: 1.1263561248779297\n\nEpoch 222/800\ntrain loss: 0.010253097190231573\ntrain DSC: 0.9874523878097534\nval loss: 0.010687026381492614\nval DSC: 0.9865476489067078\nBest val DSC: 0.9866584539413452\nTime: 1.1241567134857178\n\nEpoch 223/800\ntrain loss: 0.010252126904784655\ntrain DSC: 0.9873563647270203\nval loss: 0.011013948917388916\nval DSC: 0.9863039255142212\nBest val DSC: 0.9866584539413452\nTime: 1.1139230728149414\n\nEpoch 224/800\ntrain loss: 0.01058898597467141\ntrain DSC: 0.9867643713951111\nval loss: 0.011376681923866271\nval DSC: 0.9852313995361328\nBest val DSC: 0.9866584539413452\nTime: 1.1257421970367432\n\nEpoch 225/800\ntrain loss: 0.010368292448950595\ntrain DSC: 0.9871468544006348\nval loss: 0.011005768179893493\nval DSC: 0.9860498309135437\nBest val DSC: 0.9866584539413452\nTime: 1.131723165512085\n\nEpoch 226/800\ntrain loss: 0.01020992486203303\ntrain DSC: 0.9875714778900146\nval loss: 0.01081724464893341\nval DSC: 0.9864741563796997\nBest val DSC: 0.9866584539413452\nTime: 1.135500192642212\n\nEpoch 227/800\ntrain loss: 0.01036856408979072\ntrain DSC: 0.9871661067008972\nval loss: 0.011094221472740173\nval DSC: 0.9862077832221985\nBest val DSC: 0.9866584539413452\nTime: 1.1300859451293945\n\nEpoch 228/800\ntrain loss: 0.010346554341863413\ntrain DSC: 0.9872225522994995\nval loss: 0.010885706543922425\nval DSC: 0.9861627817153931\nBest val DSC: 0.9866584539413452\nTime: 1.1072843074798584\n\nEpoch 229/800\ntrain loss: 0.010369975058758845\ntrain DSC: 0.9873928427696228\nval loss: 0.01140395998954773\nval DSC: 0.9851091504096985\nBest val DSC: 0.9866584539413452\nTime: 1.130589246749878\n\nEpoch 230/800\ntrain loss: 0.010462159993218595\ntrain DSC: 0.9870125651359558\nval loss: 0.010928398370742798\nval DSC: 0.9862359762191772\nBest val DSC: 0.9866584539413452\nTime: 1.1231043338775635\n\nEpoch 231/800\ntrain loss: 0.01021712920704826\ntrain DSC: 0.9876943230628967\nval loss: 0.010741677880287171\nval DSC: 0.9865750074386597\nBest val DSC: 0.9866584539413452\nTime: 1.1384429931640625\n\nEpoch 232/800\ntrain loss: 0.010543709895649895\ntrain DSC: 0.986748218536377\nval loss: 0.010499587655067444\nval DSC: 0.9868021011352539\nBest val DSC: 0.9868021011352539\nTime: 1.2246758937835693\n\nEpoch 233/800\ntrain loss: 0.010264556916033635\ntrain DSC: 0.987246036529541\nval loss: 0.010927534103393555\nval DSC: 0.9859685897827148\nBest val DSC: 0.9868021011352539\nTime: 1.138122797012329\n\nEpoch 234/800\ntrain loss: 0.01025141164904735\ntrain DSC: 0.9873853325843811\nval loss: 0.010765105485916138\nval DSC: 0.9860738515853882\nBest val DSC: 0.9868021011352539\nTime: 1.1223399639129639\n\nEpoch 235/800\ntrain loss: 0.010207611029265358\ntrain DSC: 0.9874476194381714\nval loss: 0.010494604706764221\nval DSC: 0.9869682192802429\nBest val DSC: 0.9869682192802429\nTime: 1.179431676864624\n\nEpoch 236/800\ntrain loss: 0.010151736071852387\ntrain DSC: 0.9876113533973694\nval loss: 0.010778972506523132\nval DSC: 0.9861222505569458\nBest val DSC: 0.9869682192802429\nTime: 1.137660264968872\n\nEpoch 237/800\ntrain loss: 0.010222131111582771\ntrain DSC: 0.9874511361122131\nval loss: 0.010540381073951721\nval DSC: 0.9866932034492493\nBest val DSC: 0.9869682192802429\nTime: 1.1419546604156494\n\nEpoch 238/800\ntrain loss: 0.010256823946218022\ntrain DSC: 0.9872084856033325\nval loss: 0.010657042264938354\nval DSC: 0.9864553213119507\nBest val DSC: 0.9869682192802429\nTime: 1.1057384014129639\n\nEpoch 239/800\ntrain loss: 0.010131190057660713\ntrain DSC: 0.9874283075332642\nval loss: 0.010507893562316895\nval DSC: 0.9867662191390991\nBest val DSC: 0.9869682192802429\nTime: 1.1192355155944824\n\nEpoch 240/800\ntrain loss: 0.010212256283056541\ntrain DSC: 0.9873104691505432\nval loss: 0.010566827654838563\nval DSC: 0.9867502450942993\nBest val DSC: 0.9869682192802429\nTime: 1.1132426261901855\n\nEpoch 241/800\ntrain loss: 0.010239499514220192\ntrain DSC: 0.987381100654602\nval loss: 0.010713598132133484\nval DSC: 0.986241340637207\nBest val DSC: 0.9869682192802429\nTime: 1.155048131942749\n\nEpoch 242/800\ntrain loss: 0.010105998789677854\ntrain DSC: 0.9876271486282349\nval loss: 0.01048121154308319\nval DSC: 0.9868158102035522\nBest val DSC: 0.9869682192802429\nTime: 1.1282954216003418\n\nEpoch 243/800\ntrain loss: 0.010100120403727547\ntrain DSC: 0.987502932548523\nval loss: 0.010624819993972778\nval DSC: 0.9864624738693237\nBest val DSC: 0.9869682192802429\nTime: 1.1234443187713623\n\nEpoch 244/800\ntrain loss: 0.010011342705273237\ntrain DSC: 0.9877370595932007\nval loss: 0.01049310564994812\nval DSC: 0.9867708086967468\nBest val DSC: 0.9869682192802429\nTime: 1.1411969661712646\n\nEpoch 245/800\ntrain loss: 0.010137701620821093\ntrain DSC: 0.987594485282898\nval loss: 0.010697326064109803\nval DSC: 0.9864896535873413\nBest val DSC: 0.9869682192802429\nTime: 1.115297555923462\n\nEpoch 246/800\ntrain loss: 0.010063671674884733\ntrain DSC: 0.9876954555511475\nval loss: 0.010457652807235717\nval DSC: 0.9870953559875488\nBest val DSC: 0.9870953559875488\nTime: 1.201808214187622\n\nEpoch 247/800\ntrain loss: 0.009925589209697286\ntrain DSC: 0.987705409526825\nval loss: 0.01052381694316864\nval DSC: 0.9866420030593872\nBest val DSC: 0.9870953559875488\nTime: 1.1597449779510498\n\nEpoch 248/800\ntrain loss: 0.009887308370871623\ntrain DSC: 0.9879395365715027\nval loss: 0.01053190529346466\nval DSC: 0.9865617752075195\nBest val DSC: 0.9870953559875488\nTime: 1.160585880279541\n\nEpoch 249/800\ntrain loss: 0.009851939365512034\ntrain DSC: 0.9878722429275513\nval loss: 0.010528603196144104\nval DSC: 0.9867078065872192\nBest val DSC: 0.9870953559875488\nTime: 1.1402819156646729\n\nEpoch 250/800\ntrain loss: 0.009902825121019707\ntrain DSC: 0.9878134727478027\nval loss: 0.010673272609710693\nval DSC: 0.9865486025810242\nBest val DSC: 0.9870953559875488\nTime: 1.1341633796691895\n\nEpoch 251/800\ntrain loss: 0.010178228870767062\ntrain DSC: 0.9876542091369629\nval loss: 0.010467043519020081\nval DSC: 0.9867876172065735\nBest val DSC: 0.9870953559875488\nTime: 1.1502652168273926\n\nEpoch 252/800\ntrain loss: 0.010051724363545903\ntrain DSC: 0.9876164793968201\nval loss: 0.010449293255805969\nval DSC: 0.9867551922798157\nBest val DSC: 0.9870953559875488\nTime: 1.157792091369629\n\nEpoch 253/800\ntrain loss: 0.00997925977237889\ntrain DSC: 0.9876793622970581\nval loss: 0.010671180486679078\nval DSC: 0.9864094853401184\nBest val DSC: 0.9870953559875488\nTime: 1.1408343315124512\n\nEpoch 254/800\ntrain loss: 0.009896080024906845\ntrain DSC: 0.9877378344535828\nval loss: 0.010409250855445862\nval DSC: 0.9868724942207336\nBest val DSC: 0.9870953559875488\nTime: 1.1552553176879883\n\nEpoch 255/800\ntrain loss: 0.009893706587494397\ntrain DSC: 0.987939715385437\nval loss: 0.010519611835479736\nval DSC: 0.9868051409721375\nBest val DSC: 0.9870953559875488\nTime: 1.124302864074707\n\nEpoch 256/800\ntrain loss: 0.010071111507103091\ntrain DSC: 0.9876899123191833\nval loss: 0.010402864217758179\nval DSC: 0.9870145916938782\nBest val DSC: 0.9870953559875488\nTime: 1.13950777053833\n\nEpoch 257/800\ntrain loss: 0.010116043638010493\ntrain DSC: 0.9873124361038208\nval loss: 0.010479912161827087\nval DSC: 0.986598789691925\nBest val DSC: 0.9870953559875488\nTime: 1.12363862991333\n\nEpoch 258/800\ntrain loss: 0.010195021746588534\ntrain DSC: 0.987271249294281\nval loss: 0.010445016622543334\nval DSC: 0.9866628646850586\nBest val DSC: 0.9870953559875488\nTime: 1.1618351936340332\n\nEpoch 259/800\ntrain loss: 0.009994347564509658\ntrain DSC: 0.9874310493469238\nval loss: 0.01127944588661194\nval DSC: 0.9853050112724304\nBest val DSC: 0.9870953559875488\nTime: 1.1347315311431885\n\nEpoch 260/800\ntrain loss: 0.01011730999243064\ntrain DSC: 0.9872984290122986\nval loss: 0.010602360963821411\nval DSC: 0.9864684343338013\nBest val DSC: 0.9870953559875488\nTime: 1.1189162731170654\n\nEpoch 261/800\ntrain loss: 0.009795463476024691\ntrain DSC: 0.9880270957946777\nval loss: 0.01062479317188263\nval DSC: 0.9867172241210938\nBest val DSC: 0.9870953559875488\nTime: 1.12483549118042\n\nEpoch 262/800\ntrain loss: 0.00975088897298594\ntrain DSC: 0.9880910515785217\nval loss: 0.010356396436691284\nval DSC: 0.9867960214614868\nBest val DSC: 0.9870953559875488\nTime: 1.120990514755249\n\nEpoch 263/800\ntrain loss: 0.009837415374693324\ntrain DSC: 0.9879099130630493\nval loss: 0.010660004615783692\nval DSC: 0.986118495464325\nBest val DSC: 0.9870953559875488\nTime: 1.120370626449585\n\nEpoch 264/800\ntrain loss: 0.009991139662070353\ntrain DSC: 0.9877325296401978\nval loss: 0.010373476147651672\nval DSC: 0.9867986440658569\nBest val DSC: 0.9870953559875488\nTime: 1.1366734504699707\n\nEpoch 265/800\ntrain loss: 0.009960422750379219\ntrain DSC: 0.9877566695213318\nval loss: 0.010602453351020813\nval DSC: 0.986648678779602\nBest val DSC: 0.9870953559875488\nTime: 1.136683702468872\n\nEpoch 266/800\ntrain loss: 0.01001551600753284\ntrain DSC: 0.9875355958938599\nval loss: 0.010509032011032104\nval DSC: 0.9864434003829956\nBest val DSC: 0.9870953559875488\nTime: 1.1354830265045166\n\nEpoch 267/800\ntrain loss: 0.009862564626287242\ntrain DSC: 0.9878450036048889\nval loss: 0.010558155179023743\nval DSC: 0.9863189458847046\nBest val DSC: 0.9870953559875488\nTime: 1.1402075290679932\n\nEpoch 268/800\ntrain loss: 0.009846722493406202\ntrain DSC: 0.9877789616584778\nval loss: 0.010528838634490967\nval DSC: 0.9864400029182434\nBest val DSC: 0.9870953559875488\nTime: 1.1448345184326172\n\nEpoch 269/800\ntrain loss: 0.009775325900218526\ntrain DSC: 0.9879953265190125\nval loss: 0.01054561734199524\nval DSC: 0.9863179922103882\nBest val DSC: 0.9870953559875488\nTime: 1.132718801498413\n\nEpoch 270/800\ntrain loss: 0.009854646002660032\ntrain DSC: 0.9877622723579407\nval loss: 0.010319143533706665\nval DSC: 0.9868561625480652\nBest val DSC: 0.9870953559875488\nTime: 1.1603960990905762\n\nEpoch 271/800\ntrain loss: 0.009778479083639676\ntrain DSC: 0.987992525100708\nval loss: 0.010369136929512024\nval DSC: 0.9867054224014282\nBest val DSC: 0.9870953559875488\nTime: 1.1168899536132812\n\nEpoch 272/800\ntrain loss: 0.009713623367372106\ntrain DSC: 0.9881557822227478\nval loss: 0.010392507910728455\nval DSC: 0.9868024587631226\nBest val DSC: 0.9870953559875488\nTime: 1.1382098197937012\n\nEpoch 273/800\ntrain loss: 0.009660193177520251\ntrain DSC: 0.9880925416946411\nval loss: 0.011627197265625\nval DSC: 0.9845157861709595\nBest val DSC: 0.9870953559875488\nTime: 1.0942535400390625\n\nEpoch 274/800\ntrain loss: 0.009942887259311363\ntrain DSC: 0.9876566529273987\nval loss: 0.010442376136779785\nval DSC: 0.9866191744804382\nBest val DSC: 0.9870953559875488\nTime: 1.1231412887573242\n\nEpoch 275/800\ntrain loss: 0.009691624367823367\ntrain DSC: 0.9881706237792969\nval loss: 0.01033320426940918\nval DSC: 0.9867070913314819\nBest val DSC: 0.9870953559875488\nTime: 1.1081516742706299\n\nEpoch 276/800\ntrain loss: 0.009895224063122859\ntrain DSC: 0.987710177898407\nval loss: 0.010463416576385498\nval DSC: 0.9867004156112671\nBest val DSC: 0.9870953559875488\nTime: 1.1159803867340088\n\nEpoch 277/800\ntrain loss: 0.009828051582711642\ntrain DSC: 0.9877656698226929\nval loss: 0.010447162389755248\nval DSC: 0.9866769909858704\nBest val DSC: 0.9870953559875488\nTime: 1.131697416305542\n\nEpoch 278/800\ntrain loss: 0.009904230227235888\ntrain DSC: 0.9877570867538452\nval loss: 0.01029321551322937\nval DSC: 0.9870023727416992\nBest val DSC: 0.9870953559875488\nTime: 1.116307258605957\n\nEpoch 279/800\ntrain loss: 0.009750391616195928\ntrain DSC: 0.9879076480865479\nval loss: 0.010370546579360962\nval DSC: 0.9868764877319336\nBest val DSC: 0.9870953559875488\nTime: 1.1204018592834473\n\nEpoch 280/800\ntrain loss: 0.009595437128035749\ntrain DSC: 0.9882861971855164\nval loss: 0.010397186875343323\nval DSC: 0.986699104309082\nBest val DSC: 0.9870953559875488\nTime: 1.1177043914794922\n\nEpoch 281/800\ntrain loss: 0.00979450882458296\ntrain DSC: 0.987862229347229\nval loss: 0.010300910472869873\nval DSC: 0.9869129061698914\nBest val DSC: 0.9870953559875488\nTime: 1.1173040866851807\n\nEpoch 282/800\ntrain loss: 0.00976234576741203\ntrain DSC: 0.9879147410392761\nval loss: 0.010390529036521911\nval DSC: 0.9866941571235657\nBest val DSC: 0.9870953559875488\nTime: 1.1244580745697021\n\nEpoch 283/800\ntrain loss: 0.009884539197702876\ntrain DSC: 0.9877955317497253\nval loss: 0.01083667278289795\nval DSC: 0.9856972694396973\nBest val DSC: 0.9870953559875488\nTime: 1.1301984786987305\n\nEpoch 284/800\ntrain loss: 0.009601542207061267\ntrain DSC: 0.9883126616477966\nval loss: 0.010428082942962647\nval DSC: 0.9866102337837219\nBest val DSC: 0.9870953559875488\nTime: 1.1210336685180664\n\nEpoch 285/800\ntrain loss: 0.009733726743791924\ntrain DSC: 0.9879372119903564\nval loss: 0.011152279376983643\nval DSC: 0.9851964712142944\nBest val DSC: 0.9870953559875488\nTime: 1.0956659317016602\n\nEpoch 286/800\ntrain loss: 0.00988187946257044\ntrain DSC: 0.9876154661178589\nval loss: 0.01027427613735199\nval DSC: 0.9869877099990845\nBest val DSC: 0.9870953559875488\nTime: 1.1001250743865967\n\nEpoch 287/800\ntrain loss: 0.009709079734614639\ntrain DSC: 0.9880566596984863\nval loss: 0.010288265347480775\nval DSC: 0.9869382977485657\nBest val DSC: 0.9870953559875488\nTime: 1.1076157093048096\n\nEpoch 288/800\ntrain loss: 0.009502853526443731\ntrain DSC: 0.988261342048645\nval loss: 0.010566002130508423\nval DSC: 0.9863704442977905\nBest val DSC: 0.9870953559875488\nTime: 1.10884690284729\n\nEpoch 289/800\ntrain loss: 0.009899064165646912\ntrain DSC: 0.987586498260498\nval loss: 0.010216915607452392\nval DSC: 0.9869195818901062\nBest val DSC: 0.9870953559875488\nTime: 1.1246612071990967\n\nEpoch 290/800\ntrain loss: 0.009711716018739294\ntrain DSC: 0.9879330396652222\nval loss: 0.010199910402297974\nval DSC: 0.9870254397392273\nBest val DSC: 0.9870953559875488\nTime: 1.1184942722320557\n\nEpoch 291/800\ntrain loss: 0.00960239719171993\ntrain DSC: 0.988161027431488\nval loss: 0.010268646478652953\nval DSC: 0.9868561029434204\nBest val DSC: 0.9870953559875488\nTime: 1.1303088665008545\n\nEpoch 292/800\ntrain loss: 0.00962696798512193\ntrain DSC: 0.988165020942688\nval loss: 0.010248765349388123\nval DSC: 0.9869852066040039\nBest val DSC: 0.9870953559875488\nTime: 1.1180710792541504\n\nEpoch 293/800\ntrain loss: 0.009727757485186467\ntrain DSC: 0.9877774715423584\nval loss: 0.010283711552619933\nval DSC: 0.9869223833084106\nBest val DSC: 0.9870953559875488\nTime: 1.1235966682434082\n\nEpoch 294/800\ntrain loss: 0.009855049555418922\ntrain DSC: 0.9876754283905029\nval loss: 0.0102887362241745\nval DSC: 0.9868944883346558\nBest val DSC: 0.9870953559875488\nTime: 1.1244244575500488\n\nEpoch 295/800\ntrain loss: 0.009531191137970471\ntrain DSC: 0.988191545009613\nval loss: 0.010411950945854186\nval DSC: 0.9864808917045593\nBest val DSC: 0.9870953559875488\nTime: 1.1413593292236328\n\nEpoch 296/800\ntrain loss: 0.009595170372822245\ntrain DSC: 0.9880949258804321\nval loss: 0.01044079065322876\nval DSC: 0.9864362478256226\nBest val DSC: 0.9870953559875488\nTime: 1.1248507499694824\n\nEpoch 297/800\ntrain loss: 0.009717835754644676\ntrain DSC: 0.9878027439117432\nval loss: 0.010278412699699402\nval DSC: 0.986763596534729\nBest val DSC: 0.9870953559875488\nTime: 1.1293513774871826\n\nEpoch 298/800\ntrain loss: 0.009552607770825996\ntrain DSC: 0.9883521795272827\nval loss: 0.010326948761940003\nval DSC: 0.9868675470352173\nBest val DSC: 0.9870953559875488\nTime: 1.1238772869110107\n\nEpoch 299/800\ntrain loss: 0.009701669216156006\ntrain DSC: 0.9879913926124573\nval loss: 0.010173243284225465\nval DSC: 0.9870972633361816\nBest val DSC: 0.9870972633361816\nTime: 1.2155919075012207\n\nEpoch 300/800\ntrain loss: 0.009760385654011711\ntrain DSC: 0.9878010749816895\nval loss: 0.010346871614456177\nval DSC: 0.9865293502807617\nBest val DSC: 0.9870972633361816\nTime: 1.167940378189087\n\nEpoch 301/800\ntrain loss: 0.009543164831693055\ntrain DSC: 0.9881086349487305\nval loss: 0.010395500063896179\nval DSC: 0.9865352511405945\nBest val DSC: 0.9870972633361816\nTime: 1.1198749542236328\n\nEpoch 302/800\ntrain loss: 0.009555295842592834\ntrain DSC: 0.9882368445396423\nval loss: 0.010399079322814942\nval DSC: 0.9866458177566528\nBest val DSC: 0.9870972633361816\nTime: 1.113680362701416\n\nEpoch 303/800\ntrain loss: 0.009671290389827041\ntrain DSC: 0.9880354404449463\nval loss: 0.010166892409324646\nval DSC: 0.9870479702949524\nBest val DSC: 0.9870972633361816\nTime: 1.1001172065734863\n\nEpoch 304/800\ntrain loss: 0.0095541350177077\ntrain DSC: 0.9881078004837036\nval loss: 0.010370129346847534\nval DSC: 0.9867456555366516\nBest val DSC: 0.9870972633361816\nTime: 1.1192197799682617\n\nEpoch 305/800\ntrain loss: 0.009507713747806236\ntrain DSC: 0.9882804751396179\nval loss: 0.010224705934524536\nval DSC: 0.9869263768196106\nBest val DSC: 0.9870972633361816\nTime: 1.1049444675445557\n\nEpoch 306/800\ntrain loss: 0.009674248148183354\ntrain DSC: 0.9879637360572815\nval loss: 0.010219728946685791\nval DSC: 0.987021267414093\nBest val DSC: 0.9870972633361816\nTime: 1.1250288486480713\n\nEpoch 307/800\ntrain loss: 0.009508095803807994\ntrain DSC: 0.9881905317306519\nval loss: 0.010174423456192017\nval DSC: 0.9870306849479675\nBest val DSC: 0.9870972633361816\nTime: 1.103513479232788\n\nEpoch 308/800\ntrain loss: 0.009481529720494004\ntrain DSC: 0.9882540702819824\nval loss: 0.010168197751045226\nval DSC: 0.9870953559875488\nBest val DSC: 0.9870972633361816\nTime: 1.1385552883148193\n\nEpoch 309/800\ntrain loss: 0.009557521734081332\ntrain DSC: 0.9882085919380188\nval loss: 0.010180473327636719\nval DSC: 0.9869011640548706\nBest val DSC: 0.9870972633361816\nTime: 1.1314032077789307\n\nEpoch 310/800\ntrain loss: 0.009433655465235476\ntrain DSC: 0.9884206056594849\nval loss: 0.010768106579780579\nval DSC: 0.9859094619750977\nBest val DSC: 0.9870972633361816\nTime: 1.1225316524505615\n\nEpoch 311/800\ntrain loss: 0.009552377169249488\ntrain DSC: 0.9882866144180298\nval loss: 0.010377031564712525\nval DSC: 0.9865501523017883\nBest val DSC: 0.9870972633361816\nTime: 1.1163055896759033\n\nEpoch 312/800\ntrain loss: 0.009505235757984098\ntrain DSC: 0.988227128982544\nval loss: 0.010165238380432129\nval DSC: 0.9869575500488281\nBest val DSC: 0.9870972633361816\nTime: 1.1068322658538818\n\nEpoch 313/800\ntrain loss: 0.009481914707871734\ntrain DSC: 0.9883266687393188\nval loss: 0.010143989324569702\nval DSC: 0.9869929552078247\nBest val DSC: 0.9870972633361816\nTime: 1.1340210437774658\n\nEpoch 314/800\ntrain loss: 0.009573470373622706\ntrain DSC: 0.9880517721176147\nval loss: 0.01011565923690796\nval DSC: 0.9870641827583313\nBest val DSC: 0.9870972633361816\nTime: 1.1042869091033936\n\nEpoch 315/800\ntrain loss: 0.009551824116315997\ntrain DSC: 0.9882184863090515\nval loss: 0.010173791646957397\nval DSC: 0.9871122241020203\nBest val DSC: 0.9871122241020203\nTime: 1.1920857429504395\n\nEpoch 316/800\ntrain loss: 0.009426696378676618\ntrain DSC: 0.9883961081504822\nval loss: 0.010136505961418152\nval DSC: 0.9870243072509766\nBest val DSC: 0.9871122241020203\nTime: 1.1195249557495117\n\nEpoch 317/800\ntrain loss: 0.009557178763092542\ntrain DSC: 0.98807293176651\nval loss: 0.010443797707557679\nval DSC: 0.9863256216049194\nBest val DSC: 0.9871122241020203\nTime: 1.1126012802124023\n\nEpoch 318/800\ntrain loss: 0.009472832327983419\ntrain DSC: 0.9882789850234985\nval loss: 0.010268831253051757\nval DSC: 0.9869283437728882\nBest val DSC: 0.9871122241020203\nTime: 1.1263396739959717\n\nEpoch 319/800\ntrain loss: 0.009505752657280594\ntrain DSC: 0.9882402420043945\nval loss: 0.010129556059837341\nval DSC: 0.9870806932449341\nBest val DSC: 0.9871122241020203\nTime: 1.121727705001831\n\nEpoch 320/800\ntrain loss: 0.009427567974465792\ntrain DSC: 0.9884143471717834\nval loss: 0.010289394855499267\nval DSC: 0.9866694211959839\nBest val DSC: 0.9871122241020203\nTime: 1.1233887672424316\n\nEpoch 321/800\ntrain loss: 0.00946097882067571\ntrain DSC: 0.9883038997650146\nval loss: 0.010141909122467041\nval DSC: 0.98700350522995\nBest val DSC: 0.9871122241020203\nTime: 1.1159141063690186\n\nEpoch 322/800\ntrain loss: 0.009481209223387672\ntrain DSC: 0.9882394075393677\nval loss: 0.01020674705505371\nval DSC: 0.987099826335907\nBest val DSC: 0.9871122241020203\nTime: 1.1228830814361572\n\nEpoch 323/800\ntrain loss: 0.009430001993648341\ntrain DSC: 0.9883478879928589\nval loss: 0.01013471782207489\nval DSC: 0.9870856404304504\nBest val DSC: 0.9871122241020203\nTime: 1.1190485954284668\n\nEpoch 324/800\ntrain loss: 0.009467610570250964\ntrain DSC: 0.9881889224052429\nval loss: 0.010122805833816528\nval DSC: 0.9870088696479797\nBest val DSC: 0.9871122241020203\nTime: 1.168482780456543\n\nEpoch 325/800\ntrain loss: 0.00939594135909784\ntrain DSC: 0.9885247945785522\nval loss: 0.010121989250183105\nval DSC: 0.9871177673339844\nBest val DSC: 0.9871177673339844\nTime: 1.21817946434021\n\nEpoch 326/800\ntrain loss: 0.009489887073391774\ntrain DSC: 0.9881859421730042\nval loss: 0.010135382413864136\nval DSC: 0.9870203137397766\nBest val DSC: 0.9871177673339844\nTime: 1.1116480827331543\n\nEpoch 327/800\ntrain loss: 0.009381133024809791\ntrain DSC: 0.9884805083274841\nval loss: 0.010221004486083984\nval DSC: 0.9866930246353149\nBest val DSC: 0.9871177673339844\nTime: 1.1167469024658203\n\nEpoch 328/800\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 144588) exited unexpectedly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3973/1427734215.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3973/4174725950.py\u001b[0m in \u001b[0;36mtrain_decoder\u001b[0;34m(model, dataloaders, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 144588) exited unexpectedly"
          ]
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677800435472
        }
      },
      "id": "1953e3b1-46e1-4a87-9a3d-b7def234dfa9"
    },
    {
      "cell_type": "code",
      "source": [
        "del decoder\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677688996436
        }
      },
      "id": "7c0442a1-2b0e-4652-ac09-76fdee163e92"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "6f15e4b8-b1f7-47a0-837e-d91fd0acf2b9"
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.autoencoder = monai.networks.nets.AutoEncoder(\n",
        "            spatial_dims=3, in_channels=1, out_channels=1,\n",
        "            kernel_size=(3, 3, 3),\n",
        "            channels=[channel*1 for channel in (1, 2, 4, 8, 16)],\n",
        "            strides=(2, 2, 2, 2, 2),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16*2*2*2, 1)\n",
        "        self.fc2 = nn.Linear(1, 16*2*2*2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.autoencoder.encode(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.reshape(x, (1, 16, 2, 2, 2))\n",
        "        x = self.autoencoder.decode(x)\n",
        "        return x\n",
        "\n",
        "def train_autoencoder(model, dataloaders, num_epochs, learning_rate):\n",
        "    opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    opt.zero_grad()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.99)\n",
        "    loss_dice = monai.losses.DiceLoss(sigmoid=True, squared_pred=True).to(device)\n",
        "\n",
        "    metric = monai.metrics.DiceMetric(include_background=False, reduction='mean_batch')\n",
        "\n",
        "    t0 = time.time()\n",
        "    best_val_dsc = 0\n",
        "    \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for mode in ['train', 'val']:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            losses = []\n",
        "            for image, _ in dataloaders[mode]:\n",
        "                image = image.to(device)\n",
        "                \n",
        "                pred_segm = model(image)\n",
        "                loss = loss_dice(pred_segm, image)\n",
        "                \n",
        "                if mode == 'train':\n",
        "                    opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                metric((pred_segm > 0).float(), image)\n",
        "\n",
        "            print(f'{mode} loss: {np.mean(losses)}')\n",
        "            mean_dsc = metric.aggregate().tolist()[0]\n",
        "            metric.reset()\n",
        "            print(f'{mode} DSC: {mean_dsc}')\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        if mean_dsc > best_val_dsc:\n",
        "            best_val_dsc = mean_dsc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{model_dir}/best_antocoder.torch')\n",
        "        print(f'Best val DSC: {best_val_dsc}')\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "        t0 = time.time()\n",
        "        \n",
        "    print(f\"Training complete, model saved. Best model after epoch {best_epoch}\")"
      ],
      "outputs": [],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677689035259
        }
      },
      "id": "acdf90e7-4753-4787-be2b-28caecaefcae"
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Autoencoder().to(device)\n",
        "train_autoencoder(model=autoencoder, dataloaders=dataloaders, num_epochs=800, learning_rate=3e-3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/800\ntrain loss: 0.856694152120684\ntrain DSC: 0.22146211564540863\nval loss: 0.7879289716482163\nval DSC: 0.470258891582489\nBest val DSC: 0.470258891582489\nTime: 1.6074154376983643\n\nEpoch 2/800\ntrain loss: 0.742305799585874\ntrain DSC: 0.5333993434906006\nval loss: 0.6696064352989197\nval DSC: 0.6439070105552673\nBest val DSC: 0.6439070105552673\nTime: 1.5894250869750977\n\nEpoch 3/800\ntrain loss: 0.6389368225316532\ntrain DSC: 0.6664378046989441\nval loss: 0.568173748254776\nval DSC: 0.718650221824646\nBest val DSC: 0.718650221824646\nTime: 1.5681190490722656\n\nEpoch 4/800\ntrain loss: 0.541881118641525\ntrain DSC: 0.7319675087928772\nval loss: 0.46698082983493805\nval DSC: 0.7975710034370422\nBest val DSC: 0.7975710034370422\nTime: 1.5989782810211182\n\nEpoch 5/800\ntrain loss: 0.44564765496332137\ntrain DSC: 0.8009586930274963\nval loss: 0.3700594544410706\nval DSC: 0.873343288898468\nBest val DSC: 0.873343288898468\nTime: 1.5778107643127441\n\nEpoch 6/800\ntrain loss: 0.35559920991053345\ntrain DSC: 0.8623420000076294\nval loss: 0.28873425126075747\nval DSC: 0.9033038020133972\nBest val DSC: 0.9033038020133972\nTime: 1.5791468620300293\n\nEpoch 7/800\ntrain loss: 0.2808173562659592\ntrain DSC: 0.8953466415405273\nval loss: 0.22775628864765168\nval DSC: 0.9221700429916382\nBest val DSC: 0.9221700429916382\nTime: 1.585871696472168\n\nEpoch 8/800\ntrain loss: 0.22772879971832524\ntrain DSC: 0.9115074276924133\nval loss: 0.1859934687614441\nval DSC: 0.9353286027908325\nBest val DSC: 0.9353286027908325\nTime: 1.6134154796600342\n\nEpoch 9/800\ntrain loss: 0.19054283274978887\ntrain DSC: 0.9191752076148987\nval loss: 0.15268006324768066\nval DSC: 0.9424301981925964\nBest val DSC: 0.9424301981925964\nTime: 1.6172723770141602\n\nEpoch 10/800\ntrain loss: 0.15804856820184676\ntrain DSC: 0.9334653615951538\nval loss: 0.131110343337059\nval DSC: 0.9491893649101257\nBest val DSC: 0.9491893649101257\nTime: 1.5717670917510986\n\nEpoch 11/800\ntrain loss: 0.13430198880492664\ntrain DSC: 0.9403062462806702\nval loss: 0.11240670382976532\nval DSC: 0.950873076915741\nBest val DSC: 0.950873076915741\nTime: 1.8379852771759033\n\nEpoch 12/800\ntrain loss: 0.11650978346339992\ntrain DSC: 0.9461856484413147\nval loss: 0.0962142676115036\nval DSC: 0.9538270831108093\nBest val DSC: 0.9538270831108093\nTime: 1.5924782752990723\n\nEpoch 13/800\ntrain loss: 0.10724800238843823\ntrain DSC: 0.9422221183776855\nval loss: 0.09221735894680023\nval DSC: 0.9433580636978149\nBest val DSC: 0.9538270831108093\nTime: 1.5377707481384277\n\nEpoch 14/800\ntrain loss: 0.09456857599195886\ntrain DSC: 0.9478014707565308\nval loss: 0.07615170180797577\nval DSC: 0.9590529203414917\nBest val DSC: 0.9590529203414917\nTime: 1.6042718887329102\n\nEpoch 15/800\ntrain loss: 0.08063324064504905\ntrain DSC: 0.959123432636261\nval loss: 0.06840847134590149\nval DSC: 0.962558388710022\nBest val DSC: 0.962558388710022\nTime: 1.579185962677002\n\nEpoch 16/800\ntrain loss: 0.07241849527984369\ntrain DSC: 0.962953507900238\nval loss: 0.06193572580814362\nval DSC: 0.9640653729438782\nBest val DSC: 0.9640653729438782\nTime: 1.612501859664917\n\nEpoch 17/800\ntrain loss: 0.06732982592504533\ntrain DSC: 0.9629167914390564\nval loss: 0.056773635745048526\nval DSC: 0.966598391532898\nBest val DSC: 0.966598391532898\nTime: 1.5898239612579346\n\nEpoch 18/800\ntrain loss: 0.062340591774612174\ntrain DSC: 0.9639843702316284\nval loss: 0.054333421587944034\nval DSC: 0.9655172228813171\nBest val DSC: 0.966598391532898\nTime: 1.552595853805542\n\nEpoch 19/800\ntrain loss: 0.06123434911008741\ntrain DSC: 0.9609177112579346\nval loss: 0.0514477401971817\nval DSC: 0.9649219512939453\nBest val DSC: 0.966598391532898\nTime: 1.5136017799377441\n\nEpoch 20/800\ntrain loss: 0.05493759913522689\ntrain DSC: 0.9660326838493347\nval loss: 0.04695563316345215\nval DSC: 0.9691787958145142\nBest val DSC: 0.9691787958145142\nTime: 1.6009535789489746\n\nEpoch 21/800\ntrain loss: 0.052296209530752216\ntrain DSC: 0.9653698205947876\nval loss: 0.04445032775402069\nval DSC: 0.9700048565864563\nBest val DSC: 0.9700048565864563\nTime: 1.610933780670166\n\nEpoch 22/800\ntrain loss: 0.04988289367957193\ntrain DSC: 0.9661920666694641\nval loss: 0.04122336208820343\nval DSC: 0.9720745086669922\nBest val DSC: 0.9720745086669922\nTime: 1.5972092151641846\n\nEpoch 23/800\ntrain loss: 0.04544170664959266\ntrain DSC: 0.9696584939956665\nval loss: 0.03945983946323395\nval DSC: 0.972425103187561\nBest val DSC: 0.972425103187561\nTime: 1.5928537845611572\n\nEpoch 24/800\ntrain loss: 0.04280258299874478\ntrain DSC: 0.9711551070213318\nval loss: 0.03792494535446167\nval DSC: 0.9726846814155579\nBest val DSC: 0.9726846814155579\nTime: 1.7312047481536865\n\nEpoch 25/800\ntrain loss: 0.04130071206171004\ntrain DSC: 0.9712942242622375\nval loss: 0.040550267696380614\nval DSC: 0.9669845700263977\nBest val DSC: 0.9726846814155579\nTime: 1.5896646976470947\n\nEpoch 26/800\ntrain loss: 0.03960619207288398\ntrain DSC: 0.9720634818077087\nval loss: 0.035148686170578\nval DSC: 0.9729589223861694\nBest val DSC: 0.9729589223861694\nTime: 1.6086137294769287\n\nEpoch 27/800\ntrain loss: 0.03870639546972807\ntrain DSC: 0.9712734818458557\nval loss: 0.03698164522647858\nval DSC: 0.9681892395019531\nBest val DSC: 0.9729589223861694\nTime: 1.5412631034851074\n\nEpoch 28/800\ntrain loss: 0.03612585732194244\ntrain DSC: 0.9735152721405029\nval loss: 0.03401345312595368\nval DSC: 0.971463680267334\nBest val DSC: 0.9729589223861694\nTime: 1.5283844470977783\n\nEpoch 29/800\ntrain loss: 0.03481819492871644\ntrain DSC: 0.9741008281707764\nval loss: 0.030529659986495972\nval DSC: 0.9761108160018921\nBest val DSC: 0.9761108160018921\nTime: 1.5915758609771729\n\nEpoch 30/800\ntrain loss: 0.032314702135617615\ntrain DSC: 0.9768297672271729\nval loss: 0.02893838584423065\nval DSC: 0.9777666330337524\nBest val DSC: 0.9777666330337524\nTime: 1.603987216949463\n\nEpoch 31/800\ntrain loss: 0.031439967819901764\ntrain DSC: 0.9767439961433411\nval loss: 0.030619317293167116\nval DSC: 0.9736148715019226\nBest val DSC: 0.9777666330337524\nTime: 1.5531322956085205\n\nEpoch 32/800\ntrain loss: 0.030231059574690022\ntrain DSC: 0.9773704409599304\nval loss: 0.028370898962020875\nval DSC: 0.9767814874649048\nBest val DSC: 0.9777666330337524\nTime: 1.528778314590454\n\nEpoch 33/800\ntrain loss: 0.03056940582932019\ntrain DSC: 0.9751989841461182\nval loss: 0.02856091260910034\nval DSC: 0.974834144115448\nBest val DSC: 0.9777666330337524\nTime: 1.5152149200439453\n\nEpoch 34/800\ntrain loss: 0.02871062032511977\ntrain DSC: 0.9779666066169739\nval loss: 0.0253878653049469\nval DSC: 0.9794946908950806\nBest val DSC: 0.9794946908950806\nTime: 1.628779411315918\n\nEpoch 35/800\ntrain loss: 0.026700907066220143\ntrain DSC: 0.9798657894134521\nval loss: 0.024721330404281615\nval DSC: 0.9796391725540161\nBest val DSC: 0.9796391725540161\nTime: 1.5761065483093262\n\nEpoch 36/800\ntrain loss: 0.02605922788870139\ntrain DSC: 0.9799869060516357\nval loss: 0.023980280756950377\nval DSC: 0.9799810647964478\nBest val DSC: 0.9799810647964478\nTime: 1.5763249397277832\n\nEpoch 37/800\ntrain loss: 0.026007106069658625\ntrain DSC: 0.9793356657028198\nval loss: 0.022953063249588013\nval DSC: 0.9812122583389282\nBest val DSC: 0.9812122583389282\nTime: 1.5581016540527344\n\nEpoch 38/800\ntrain loss: 0.025531721896812565\ntrain DSC: 0.979112982749939\nval loss: 0.02274012267589569\nval DSC: 0.9809839129447937\nBest val DSC: 0.9812122583389282\nTime: 1.527113914489746\n\nEpoch 39/800\ntrain loss: 0.02425967083602655\ntrain DSC: 0.9805421829223633\nval loss: 0.022735610604286194\nval DSC: 0.9802402257919312\nBest val DSC: 0.9812122583389282\nTime: 1.5479187965393066\n\nEpoch 40/800\ntrain loss: 0.024736835331213278\ntrain DSC: 0.9790879487991333\nval loss: 0.02286357879638672\nval DSC: 0.9794219732284546\nBest val DSC: 0.9812122583389282\nTime: 1.5378522872924805\n\nEpoch 41/800\ntrain loss: 0.0244358983196196\ntrain DSC: 0.978812575340271\nval loss: 0.022709569334983824\nval DSC: 0.9790859222412109\nBest val DSC: 0.9812122583389282\nTime: 1.522458553314209\n\nEpoch 42/800\ntrain loss: 0.023616062813117854\ntrain DSC: 0.9797289371490479\nval loss: 0.021701526641845704\nval DSC: 0.9799370765686035\nBest val DSC: 0.9812122583389282\nTime: 1.55059814453125\n\nEpoch 43/800\ntrain loss: 0.02308505675831779\ntrain DSC: 0.9799668192863464\nval loss: 0.020515766739845277\nval DSC: 0.9819854497909546\nBest val DSC: 0.9819854497909546\nTime: 1.6050755977630615\n\nEpoch 44/800\ntrain loss: 0.02207606249168271\ntrain DSC: 0.9810360670089722\nval loss: 0.020708364248275758\nval DSC: 0.9809037446975708\nBest val DSC: 0.9819854497909546\nTime: 1.5558714866638184\n\nEpoch 45/800\ntrain loss: 0.021530854897420914\ntrain DSC: 0.9812179207801819\nval loss: 0.019916409254074098\nval DSC: 0.9818681478500366\nBest val DSC: 0.9819854497909546\nTime: 1.5232877731323242\n\nEpoch 46/800\ntrain loss: 0.021603038076494562\ntrain DSC: 0.9803786873817444\nval loss: 0.021829235553741454\nval DSC: 0.9775546789169312\nBest val DSC: 0.9819854497909546\nTime: 1.5444622039794922\n\nEpoch 47/800\ntrain loss: 0.02080156666333558\ntrain DSC: 0.9815706014633179\nval loss: 0.020838305354118347\nval DSC: 0.9793767929077148\nBest val DSC: 0.9819854497909546\nTime: 1.5141949653625488\n\nEpoch 48/800\ntrain loss: 0.02138325816295186\ntrain DSC: 0.9803194403648376\nval loss: 0.018924543261528017\nval DSC: 0.982264518737793\nBest val DSC: 0.982264518737793\nTime: 1.6148357391357422\n\nEpoch 49/800\ntrain loss: 0.020502825252345352\ntrain DSC: 0.9813326001167297\nval loss: 0.018964701890945436\nval DSC: 0.9820494651794434\nBest val DSC: 0.982264518737793\nTime: 1.5565407276153564\n\nEpoch 50/800\ntrain loss: 0.01986543272362381\ntrain DSC: 0.9817700386047363\nval loss: 0.02004910409450531\nval DSC: 0.9799531698226929\nBest val DSC: 0.982264518737793\nTime: 1.5643906593322754\n\nEpoch 51/800\ntrain loss: 0.019805283820042846\ntrain DSC: 0.9814515709877014\nval loss: 0.018467238545417784\nval DSC: 0.9818373918533325\nBest val DSC: 0.982264518737793\nTime: 1.517225980758667\n\nEpoch 52/800\ntrain loss: 0.018416824888010493\ntrain DSC: 0.9835842847824097\nval loss: 0.01823473870754242\nval DSC: 0.981967568397522\nBest val DSC: 0.982264518737793\nTime: 1.527933120727539\n\nEpoch 53/800\ntrain loss: 0.018395829396169693\ntrain DSC: 0.9832158088684082\nval loss: 0.01776675283908844\nval DSC: 0.9823377728462219\nBest val DSC: 0.9823377728462219\nTime: 1.587662935256958\n\nEpoch 54/800\ntrain loss: 0.017895587155076324\ntrain DSC: 0.9837659001350403\nval loss: 0.01732201874256134\nval DSC: 0.9833410382270813\nBest val DSC: 0.9833410382270813\nTime: 1.609435796737671\n\nEpoch 55/800\ntrain loss: 0.018211128281765295\ntrain DSC: 0.9830077290534973\nval loss: 0.017655056715011597\nval DSC: 0.9821513891220093\nBest val DSC: 0.9833410382270813\nTime: 1.5143122673034668\n\nEpoch 56/800\ntrain loss: 0.018332027998126922\ntrain DSC: 0.9820449352264404\nval loss: 0.017709442973136903\nval DSC: 0.9815079569816589\nBest val DSC: 0.9833410382270813\nTime: 1.5390489101409912\n\nEpoch 57/800\ntrain loss: 0.017770604031984924\ntrain DSC: 0.9830204844474792\nval loss: 0.016731271147727968\nval DSC: 0.9832477569580078\nBest val DSC: 0.9833410382270813\nTime: 1.5362346172332764\n\nEpoch 58/800\ntrain loss: 0.017130564470760157\ntrain DSC: 0.9836528897285461\nval loss: 0.016562023758888246\nval DSC: 0.9836642146110535\nBest val DSC: 0.9836642146110535\nTime: 1.61177659034729\n\nEpoch 59/800\ntrain loss: 0.017137959355213603\ntrain DSC: 0.9834105968475342\nval loss: 0.01614900827407837\nval DSC: 0.9836427569389343\nBest val DSC: 0.9836642146110535\nTime: 1.522308588027954\n\nEpoch 60/800\ntrain loss: 0.017885105531723774\ntrain DSC: 0.9819632172584534\nval loss: 0.016879057884216307\nval DSC: 0.9823839068412781\nBest val DSC: 0.9836642146110535\nTime: 1.5572340488433838\n\nEpoch 61/800\ntrain loss: 0.016763977340010346\ntrain DSC: 0.9835876226425171\nval loss: 0.017115482687950136\nval DSC: 0.9816845059394836\nBest val DSC: 0.9836642146110535\nTime: 1.5126984119415283\n\nEpoch 62/800\ntrain loss: 0.01698665247588861\ntrain DSC: 0.9828546643257141\nval loss: 0.01617870628833771\nval DSC: 0.9832538366317749\nBest val DSC: 0.9836642146110535\nTime: 1.5472440719604492\n\nEpoch 63/800\ntrain loss: 0.016218560640929174\ntrain DSC: 0.9840725660324097\nval loss: 0.015787619352340698\nval DSC: 0.9837285876274109\nBest val DSC: 0.9837285876274109\nTime: 1.6853206157684326\n\nEpoch 64/800\ntrain loss: 0.023491516465046366\ntrain DSC: 0.975145697593689\nval loss: 0.018666696548461915\nval DSC: 0.9798452258110046\nBest val DSC: 0.9837285876274109\nTime: 1.573784351348877\n\nEpoch 65/800\ntrain loss: 0.01914647758984175\ntrain DSC: 0.9798284769058228\nval loss: 0.01622871160507202\nval DSC: 0.9830654859542847\nBest val DSC: 0.9837285876274109\nTime: 1.533296823501587\n\nEpoch 66/800\ntrain loss: 0.016829189707021245\ntrain DSC: 0.9830130934715271\nval loss: 0.015576893091201782\nval DSC: 0.9841395616531372\nBest val DSC: 0.9841395616531372\nTime: 1.6172492504119873\n\nEpoch 67/800\ntrain loss: 0.015735393664875967\ntrain DSC: 0.9844034910202026\nval loss: 0.015768098831176757\nval DSC: 0.9831298589706421\nBest val DSC: 0.9841395616531372\nTime: 1.5538749694824219\n\nEpoch 68/800\ntrain loss: 0.016498100562173813\ntrain DSC: 0.9828778505325317\nval loss: 0.015322065353393555\nval DSC: 0.983410656452179\nBest val DSC: 0.9841395616531372\nTime: 1.5317609310150146\n\nEpoch 69/800\ntrain loss: 0.0160255422357653\ntrain DSC: 0.9830551147460938\nval loss: 0.015233629941940307\nval DSC: 0.9835233688354492\nBest val DSC: 0.9841395616531372\nTime: 1.547058343887329\n\nEpoch 70/800\ntrain loss: 0.015127380363276748\ntrain DSC: 0.9845643639564514\nval loss: 0.014665770530700683\nval DSC: 0.9848111867904663\nBest val DSC: 0.9848111867904663\nTime: 1.5641374588012695\n\nEpoch 71/800\ntrain loss: 0.015404028970687116\ntrain DSC: 0.9840360283851624\nval loss: 0.014743378758430481\nval DSC: 0.9842008352279663\nBest val DSC: 0.9848111867904663\nTime: 1.537294864654541\n\nEpoch 72/800\ntrain loss: 0.015217011092139071\ntrain DSC: 0.9842730760574341\nval loss: 0.01587171256542206\nval DSC: 0.9827054738998413\nBest val DSC: 0.9848111867904663\nTime: 1.5213432312011719\n\nEpoch 73/800\ntrain loss: 0.014736996322381691\ntrain DSC: 0.9845873713493347\nval loss: 0.014214339852333068\nval DSC: 0.9849034547805786\nBest val DSC: 0.9849034547805786\nTime: 1.5826447010040283\n\nEpoch 74/800\ntrain loss: 0.014508164319835726\ntrain DSC: 0.985138475894928\nval loss: 0.01420394778251648\nval DSC: 0.9847658276557922\nBest val DSC: 0.9849034547805786\nTime: 1.527022361755371\n\nEpoch 75/800\ntrain loss: 0.01468517741218942\ntrain DSC: 0.9845268726348877\nval loss: 0.014325448870658874\nval DSC: 0.984298050403595\nBest val DSC: 0.9849034547805786\nTime: 1.559324026107788\n\nEpoch 76/800\ntrain loss: 0.015048220509388407\ntrain DSC: 0.9838680624961853\nval loss: 0.015188217163085938\nval DSC: 0.9826425313949585\nBest val DSC: 0.9849034547805786\nTime: 1.5672521591186523\n\nEpoch 77/800\ntrain loss: 0.015281794501132652\ntrain DSC: 0.9833449721336365\nval loss: 0.014118310809135438\nval DSC: 0.9844173192977905\nBest val DSC: 0.9849034547805786\nTime: 1.5306415557861328\n\nEpoch 78/800\ntrain loss: 0.014597557607244273\ntrain DSC: 0.9842654466629028\nval loss: 0.014214611053466797\nval DSC: 0.9841829538345337\nBest val DSC: 0.9849034547805786\nTime: 1.5343153476715088\n\nEpoch 79/800\ntrain loss: 0.014194762120481397\ntrain DSC: 0.9845234155654907\nval loss: 0.013911625742912293\nval DSC: 0.9846929311752319\nBest val DSC: 0.9849034547805786\nTime: 1.5381722450256348\n\nEpoch 80/800\ntrain loss: 0.01408095437972272\ntrain DSC: 0.9847158193588257\nval loss: 0.013850542902946471\nval DSC: 0.9843835830688477\nBest val DSC: 0.9849034547805786\nTime: 1.536088228225708\n\nEpoch 81/800\ntrain loss: 0.013857348043410505\ntrain DSC: 0.9851753115653992\nval loss: 0.01336180865764618\nval DSC: 0.9853240847587585\nBest val DSC: 0.9853240847587585\nTime: 1.6178314685821533\n\nEpoch 82/800\ntrain loss: 0.013383514568453929\ntrain DSC: 0.9856793880462646\nval loss: 0.014490047097206115\nval DSC: 0.9832631945610046\nBest val DSC: 0.9853240847587585\nTime: 1.5137972831726074\n\nEpoch 83/800\ntrain loss: 0.01310265064239502\ntrain DSC: 0.9862416386604309\nval loss: 0.013652405142784119\nval DSC: 0.9843700528144836\nBest val DSC: 0.9853240847587585\nTime: 1.5371193885803223\n\nEpoch 84/800\ntrain loss: 0.013788912139955114\ntrain DSC: 0.9848498702049255\nval loss: 0.013501346111297607\nval DSC: 0.9848541021347046\nBest val DSC: 0.9853240847587585\nTime: 1.5275375843048096\n\nEpoch 85/800\ntrain loss: 0.013350348980700383\ntrain DSC: 0.9853379130363464\nval loss: 0.013183715939521789\nval DSC: 0.9850085377693176\nBest val DSC: 0.9853240847587585\nTime: 1.5696134567260742\n\nEpoch 86/800\ntrain loss: 0.013087558941762955\ntrain DSC: 0.985744833946228\nval loss: 0.013251423835754395\nval DSC: 0.9849403500556946\nBest val DSC: 0.9853240847587585\nTime: 1.5326409339904785\n\nEpoch 87/800\ntrain loss: 0.013002364361872439\ntrain DSC: 0.9858335852622986\nval loss: 0.013682377338409425\nval DSC: 0.9841971397399902\nBest val DSC: 0.9853240847587585\nTime: 1.5419080257415771\n\nEpoch 88/800\ntrain loss: 0.013433427107138712\ntrain DSC: 0.984925389289856\nval loss: 0.013325861096382141\nval DSC: 0.9850113987922668\nBest val DSC: 0.9853240847587585\nTime: 1.5351622104644775\n\nEpoch 89/800\ntrain loss: 0.012806024707731654\ntrain DSC: 0.9858163595199585\nval loss: 0.013054394721984863\nval DSC: 0.985235869884491\nBest val DSC: 0.9853240847587585\nTime: 1.5673410892486572\n\nEpoch 90/800\ntrain loss: 0.013118475187020223\ntrain DSC: 0.9851425886154175\nval loss: 0.012856945395469666\nval DSC: 0.9854879379272461\nBest val DSC: 0.9854879379272461\nTime: 1.6556689739227295\n\nEpoch 91/800\ntrain loss: 0.013015035723076492\ntrain DSC: 0.9854201078414917\nval loss: 0.012779626250267028\nval DSC: 0.9851058125495911\nBest val DSC: 0.9854879379272461\nTime: 1.545624017715454\n\nEpoch 92/800\ntrain loss: 0.01379902147855915\ntrain DSC: 0.9842102527618408\nval loss: 0.015384340286254882\nval DSC: 0.9808982014656067\nBest val DSC: 0.9854879379272461\nTime: 1.5530033111572266\n\nEpoch 93/800\ntrain loss: 0.013791306096999372\ntrain DSC: 0.9842430353164673\nval loss: 0.013281014561653138\nval DSC: 0.9848659634590149\nBest val DSC: 0.9854879379272461\nTime: 1.5718297958374023\n\nEpoch 94/800\ntrain loss: 0.012679308164315145\ntrain DSC: 0.9856945872306824\nval loss: 0.013124647736549377\nval DSC: 0.984379768371582\nBest val DSC: 0.9854879379272461\nTime: 1.552600622177124\n\nEpoch 95/800\ntrain loss: 0.012448409541708524\ntrain DSC: 0.9858143925666809\nval loss: 0.01335969865322113\nval DSC: 0.9843652844429016\nBest val DSC: 0.9854879379272461\nTime: 1.5643284320831299\n\nEpoch 96/800\ntrain loss: 0.01265043215673478\ntrain DSC: 0.9855954647064209\nval loss: 0.012542009353637695\nval DSC: 0.9848562479019165\nBest val DSC: 0.9854879379272461\nTime: 1.5304501056671143\n\nEpoch 97/800\ntrain loss: 0.011993620239320348\ntrain DSC: 0.9866741895675659\nval loss: 0.012400665879249572\nval DSC: 0.9852958917617798\nBest val DSC: 0.9854879379272461\nTime: 1.5922057628631592\n\nEpoch 98/800\ntrain loss: 0.01224887957338427\ntrain DSC: 0.9860485196113586\nval loss: 0.014137834310531616\nval DSC: 0.9834468960762024\nBest val DSC: 0.9854879379272461\nTime: 1.5244169235229492\n\nEpoch 99/800\ntrain loss: 0.012808097190544253\ntrain DSC: 0.985126256942749\nval loss: 0.012468749284744262\nval DSC: 0.9853113889694214\nBest val DSC: 0.9854879379272461\nTime: 1.5339908599853516\n\nEpoch 100/800\ntrain loss: 0.012350756613934626\ntrain DSC: 0.9857784509658813\nval loss: 0.012344714999198914\nval DSC: 0.9856670498847961\nBest val DSC: 0.9856670498847961\nTime: 1.624549388885498\n\nEpoch 101/800\ntrain loss: 0.011913292720669606\ntrain DSC: 0.9866553544998169\nval loss: 0.012518686056137086\nval DSC: 0.9852007031440735\nBest val DSC: 0.9856670498847961\nTime: 1.5352251529693604\n\nEpoch 102/800\ntrain loss: 0.011882243586368248\ntrain DSC: 0.9863405227661133\nval loss: 0.01194370985031128\nval DSC: 0.98590087890625\nBest val DSC: 0.98590087890625\nTime: 1.564046859741211\n\nEpoch 103/800\ntrain loss: 0.011782335453346128\ntrain DSC: 0.9865460395812988\nval loss: 0.013201454281806945\nval DSC: 0.9836339950561523\nBest val DSC: 0.98590087890625\nTime: 1.5343797206878662\n\nEpoch 104/800\ntrain loss: 0.011661363429710513\ntrain DSC: 0.9867274761199951\nval loss: 0.012237927317619324\nval DSC: 0.9854696393013\nBest val DSC: 0.98590087890625\nTime: 1.5288684368133545\n\nEpoch 105/800\ntrain loss: 0.011539392783993581\ntrain DSC: 0.9868335127830505\nval loss: 0.012046340107917785\nval DSC: 0.9855899810791016\nBest val DSC: 0.98590087890625\nTime: 1.548013687133789\n\nEpoch 106/800\ntrain loss: 0.011775161399215948\ntrain DSC: 0.986247718334198\nval loss: 0.012021732330322266\nval DSC: 0.9855784177780151\nBest val DSC: 0.98590087890625\nTime: 1.5516858100891113\n\nEpoch 107/800\ntrain loss: 0.011679938582123304\ntrain DSC: 0.9864224195480347\nval loss: 0.011916178464889526\nval DSC: 0.9855497479438782\nBest val DSC: 0.98590087890625\nTime: 1.535310983657837\n\nEpoch 108/800\ntrain loss: 0.011200109466177518\ntrain DSC: 0.9872526526451111\nval loss: 0.011893686652183533\nval DSC: 0.9858845472335815\nBest val DSC: 0.98590087890625\nTime: 1.5236625671386719\n\nEpoch 109/800\ntrain loss: 0.011999575818171267\ntrain DSC: 0.985673725605011\nval loss: 0.012074297666549683\nval DSC: 0.9857763051986694\nBest val DSC: 0.98590087890625\nTime: 1.540640115737915\n\nEpoch 110/800\ntrain loss: 0.011685776905935318\ntrain DSC: 0.9861116409301758\nval loss: 0.012365257740020752\nval DSC: 0.9846901893615723\nBest val DSC: 0.98590087890625\nTime: 1.5490167140960693\n\nEpoch 111/800\ntrain loss: 0.01147405143643989\ntrain DSC: 0.9867487549781799\nval loss: 0.01163254976272583\nval DSC: 0.9859197735786438\nBest val DSC: 0.9859197735786438\nTime: 1.738353967666626\n\nEpoch 112/800\ntrain loss: 0.011000315674015732\ntrain DSC: 0.9871865510940552\nval loss: 0.011521580815315246\nval DSC: 0.9861639142036438\nBest val DSC: 0.9861639142036438\nTime: 1.6429357528686523\n\nEpoch 113/800\ntrain loss: 0.011312660623769292\ntrain DSC: 0.9866662621498108\nval loss: 0.012010639905929566\nval DSC: 0.9847685098648071\nBest val DSC: 0.9861639142036438\nTime: 1.5238323211669922\n\nEpoch 114/800\ntrain loss: 0.011189019093747999\ntrain DSC: 0.9868904948234558\nval loss: 0.01242121160030365\nval DSC: 0.9843684434890747\nBest val DSC: 0.9861639142036438\nTime: 1.5454294681549072\n\nEpoch 115/800\ntrain loss: 0.011071582309535293\ntrain DSC: 0.9869027137756348\nval loss: 0.011458110809326173\nval DSC: 0.9862188100814819\nBest val DSC: 0.9862188100814819\nTime: 1.636214256286621\n\nEpoch 116/800\ntrain loss: 0.011550832967289159\ntrain DSC: 0.9861311316490173\nval loss: 0.012054210901260376\nval DSC: 0.9851258993148804\nBest val DSC: 0.9862188100814819\nTime: 1.5597033500671387\n\nEpoch 117/800\ntrain loss: 0.010700859007288198\ntrain DSC: 0.9874328374862671\nval loss: 0.011355373263359069\nval DSC: 0.9863212704658508\nBest val DSC: 0.9863212704658508\nTime: 1.5549399852752686\n\nEpoch 118/800\ntrain loss: 0.010683384097990443\ntrain DSC: 0.9875833988189697\nval loss: 0.011249145865440369\nval DSC: 0.9864824414253235\nBest val DSC: 0.9864824414253235\nTime: 1.5873286724090576\n\nEpoch 119/800\ntrain loss: 0.010986804962158203\ntrain DSC: 0.9867692589759827\nval loss: 0.01222488284111023\nval DSC: 0.9843155145645142\nBest val DSC: 0.9864824414253235\nTime: 1.5404033660888672\n\nEpoch 120/800\ntrain loss: 0.010993657542056725\ntrain DSC: 0.9868497848510742\nval loss: 0.011581912636756897\nval DSC: 0.9853862524032593\nBest val DSC: 0.9864824414253235\nTime: 1.5459184646606445\n\nEpoch 121/800\ntrain loss: 0.010634716416968674\ntrain DSC: 0.9872474074363708\nval loss: 0.011401578783988953\nval DSC: 0.9857882261276245\nBest val DSC: 0.9864824414253235\nTime: 1.5557050704956055\n\nEpoch 122/800\ntrain loss: 0.01077064432081629\ntrain DSC: 0.9872117638587952\nval loss: 0.011191424727439881\nval DSC: 0.9859192967414856\nBest val DSC: 0.9864824414253235\nTime: 1.5401954650878906\n\nEpoch 123/800\ntrain loss: 0.010483912757185639\ntrain DSC: 0.9875883460044861\nval loss: 0.011163479089736939\nval DSC: 0.9860862493515015\nBest val DSC: 0.9864824414253235\nTime: 1.526906967163086\n\nEpoch 124/800\ntrain loss: 0.01066853081593748\ntrain DSC: 0.987065315246582\nval loss: 0.011489889025688172\nval DSC: 0.9854995608329773\nBest val DSC: 0.9864824414253235\nTime: 1.5417981147766113\n\nEpoch 125/800\ntrain loss: 0.010566307873022361\ntrain DSC: 0.9873349666595459\nval loss: 0.010947337746620179\nval DSC: 0.986985981464386\nBest val DSC: 0.986985981464386\nTime: 1.6537487506866455\n\nEpoch 126/800\ntrain loss: 0.010592723479036425\ntrain DSC: 0.987396240234375\nval loss: 0.012083709239959717\nval DSC: 0.984704852104187\nBest val DSC: 0.986985981464386\nTime: 1.5435755252838135\n\nEpoch 127/800\ntrain loss: 0.011290925448058082\ntrain DSC: 0.9859626293182373\nval loss: 0.013570913672447204\nval DSC: 0.9821421504020691\nBest val DSC: 0.986985981464386\nTime: 1.5390172004699707\n\nEpoch 128/800\ntrain loss: 0.01269509069255141\ntrain DSC: 0.9840454459190369\nval loss: 0.014427414536476136\nval DSC: 0.9820417165756226\nBest val DSC: 0.986985981464386\nTime: 1.537752389907837\n\nEpoch 129/800\ntrain loss: 0.011715461973284111\ntrain DSC: 0.9852777719497681\nval loss: 0.014424961805343629\nval DSC: 0.9817166328430176\nBest val DSC: 0.986985981464386\nTime: 1.5580720901489258\n\nEpoch 130/800\ntrain loss: 0.011566814829091557\ntrain DSC: 0.9856935143470764\nval loss: 0.011068886518478394\nval DSC: 0.9861451387405396\nBest val DSC: 0.986985981464386\nTime: 1.5223770141601562\n\nEpoch 131/800\ntrain loss: 0.01094056348331639\ntrain DSC: 0.9865768551826477\nval loss: 0.01096859872341156\nval DSC: 0.9866140484809875\nBest val DSC: 0.986985981464386\nTime: 1.5415024757385254\n\nEpoch 132/800\ntrain loss: 0.01022649299902994\ntrain DSC: 0.9876897931098938\nval loss: 0.0116105318069458\nval DSC: 0.9855872392654419\nBest val DSC: 0.986985981464386\nTime: 1.5255396366119385\n\nEpoch 133/800\ntrain loss: 0.010294056329570834\ntrain DSC: 0.9874458909034729\nval loss: 0.011077278852462768\nval DSC: 0.9862030148506165\nBest val DSC: 0.986985981464386\nTime: 1.5710015296936035\n\nEpoch 134/800\ntrain loss: 0.010147927237338707\ntrain DSC: 0.9876644015312195\nval loss: 0.011196702718734741\nval DSC: 0.9854714274406433\nBest val DSC: 0.986985981464386\nTime: 1.508450984954834\n\nEpoch 135/800\ntrain loss: 0.010152133761859332\ntrain DSC: 0.9875915050506592\nval loss: 0.011478719115257264\nval DSC: 0.9853014945983887\nBest val DSC: 0.986985981464386\nTime: 1.5400676727294922\n\nEpoch 136/800\ntrain loss: 0.010140708235443616\ntrain DSC: 0.9875848293304443\nval loss: 0.010821202397346496\nval DSC: 0.9863876104354858\nBest val DSC: 0.986985981464386\nTime: 1.582650899887085\n\nEpoch 137/800\ntrain loss: 0.010033767731463323\ntrain DSC: 0.9877588152885437\nval loss: 0.011169061064720154\nval DSC: 0.9858272671699524\nBest val DSC: 0.986985981464386\nTime: 1.5729899406433105\n\nEpoch 138/800\ntrain loss: 0.009856893390905662\ntrain DSC: 0.9879674315452576\nval loss: 0.01075991690158844\nval DSC: 0.9865446090698242\nBest val DSC: 0.986985981464386\nTime: 1.5491821765899658\n\nEpoch 139/800\ntrain loss: 0.009855474604934942\ntrain DSC: 0.987880289554596\nval loss: 0.010999426245689392\nval DSC: 0.9864104390144348\nBest val DSC: 0.986985981464386\nTime: 1.5619242191314697\n\nEpoch 140/800\ntrain loss: 0.009994499018934906\ntrain DSC: 0.9878653883934021\nval loss: 0.010636195540428162\nval DSC: 0.9865352511405945\nBest val DSC: 0.986985981464386\nTime: 1.5739455223083496\n\nEpoch 141/800\ntrain loss: 0.009925783657636798\ntrain DSC: 0.9878720045089722\nval loss: 0.010806739330291748\nval DSC: 0.9862273335456848\nBest val DSC: 0.986985981464386\nTime: 1.5451343059539795\n\nEpoch 142/800\ntrain loss: 0.010226855512525215\ntrain DSC: 0.9874304533004761\nval loss: 0.010951012372970581\nval DSC: 0.9859598278999329\nBest val DSC: 0.986985981464386\nTime: 1.5693049430847168\n\nEpoch 143/800\ntrain loss: 0.009823589051356081\ntrain DSC: 0.9882025718688965\nval loss: 0.011001944541931152\nval DSC: 0.9862076640129089\nBest val DSC: 0.986985981464386\nTime: 1.5584840774536133\n\nEpoch 144/800\ntrain loss: 0.009739993048495934\ntrain DSC: 0.9881624579429626\nval loss: 0.01063341200351715\nval DSC: 0.9867633581161499\nBest val DSC: 0.986985981464386\nTime: 1.5305101871490479\n\nEpoch 145/800\ntrain loss: 0.009786973234082832\ntrain DSC: 0.9881332516670227\nval loss: 0.010746806859970093\nval DSC: 0.9862979650497437\nBest val DSC: 0.986985981464386\nTime: 1.524982213973999\n\nEpoch 146/800\ntrain loss: 0.00996166956229288\ntrain DSC: 0.9874340295791626\nval loss: 0.010586965084075927\nval DSC: 0.9866812825202942\nBest val DSC: 0.986985981464386\nTime: 1.5223431587219238\n\nEpoch 147/800\ntrain loss: 0.009850505922661453\ntrain DSC: 0.9876306056976318\nval loss: 0.01128697395324707\nval DSC: 0.9852649569511414\nBest val DSC: 0.986985981464386\nTime: 1.5420777797698975\n\nEpoch 148/800\ntrain loss: 0.009652581371244837\ntrain DSC: 0.9880710244178772\nval loss: 0.010861244797706605\nval DSC: 0.986097514629364\nBest val DSC: 0.986985981464386\nTime: 1.558828592300415\n\nEpoch 149/800\ntrain loss: 0.010032438841022428\ntrain DSC: 0.9874266982078552\nval loss: 0.010629495978355408\nval DSC: 0.9865171313285828\nBest val DSC: 0.986985981464386\nTime: 1.5487761497497559\n\nEpoch 150/800\ntrain loss: 0.010046476223429695\ntrain DSC: 0.9874677062034607\nval loss: 0.01137661635875702\nval DSC: 0.9854829907417297\nBest val DSC: 0.986985981464386\nTime: 1.5464026927947998\n\nEpoch 151/800\ntrain loss: 0.009814173471732218\ntrain DSC: 0.9878106117248535\nval loss: 0.01050005853176117\nval DSC: 0.9867674112319946\nBest val DSC: 0.986985981464386\nTime: 1.5283851623535156\n\nEpoch 152/800\ntrain loss: 0.009784475701754211\ntrain DSC: 0.9878565669059753\nval loss: 0.01047002375125885\nval DSC: 0.9865709543228149\nBest val DSC: 0.986985981464386\nTime: 1.5577261447906494\n\nEpoch 153/800\ntrain loss: 0.009509788184869484\ntrain DSC: 0.9881376028060913\nval loss: 0.010522165894508361\nval DSC: 0.9865039587020874\nBest val DSC: 0.986985981464386\nTime: 1.5584757328033447\n\nEpoch 154/800\ntrain loss: 0.009721540036748667\ntrain DSC: 0.9878578186035156\nval loss: 0.010309064388275146\nval DSC: 0.9868100881576538\nBest val DSC: 0.986985981464386\nTime: 1.5297396183013916\n\nEpoch 155/800\ntrain loss: 0.009371030526083023\ntrain DSC: 0.9885047078132629\nval loss: 0.010758045315742492\nval DSC: 0.9864198565483093\nBest val DSC: 0.986985981464386\nTime: 1.5245580673217773\n\nEpoch 156/800\ntrain loss: 0.009987109997233406\ntrain DSC: 0.9875444769859314\nval loss: 0.010477975010871887\nval DSC: 0.9863796234130859\nBest val DSC: 0.986985981464386\nTime: 1.5411765575408936\n\nEpoch 157/800\ntrain loss: 0.009502624879117872\ntrain DSC: 0.9881406426429749\nval loss: 0.010951974987983703\nval DSC: 0.9855225682258606\nBest val DSC: 0.986985981464386\nTime: 1.548368215560913\n\nEpoch 158/800\ntrain loss: 0.009861511285187767\ntrain DSC: 0.987546443939209\nval loss: 0.01044812798500061\nval DSC: 0.9863861203193665\nBest val DSC: 0.986985981464386\nTime: 1.5380029678344727\n\nEpoch 159/800\ntrain loss: 0.009547083104243044\ntrain DSC: 0.9879799485206604\nval loss: 0.010764852166175842\nval DSC: 0.9859308004379272\nBest val DSC: 0.986985981464386\nTime: 1.543285846710205\n\nEpoch 160/800\ntrain loss: 0.009685736210619817\ntrain DSC: 0.9874956607818604\nval loss: 0.010463547706604005\nval DSC: 0.9865115284919739\nBest val DSC: 0.986985981464386\nTime: 1.551680088043213\n\nEpoch 161/800\ntrain loss: 0.009356043377860647\ntrain DSC: 0.9883016347885132\nval loss: 0.010502344369888306\nval DSC: 0.9864214062690735\nBest val DSC: 0.986985981464386\nTime: 1.5435023307800293\n\nEpoch 162/800\ntrain loss: 0.009764701616568644\ntrain DSC: 0.9875474572181702\nval loss: 0.010772284865379334\nval DSC: 0.985795795917511\nBest val DSC: 0.986985981464386\nTime: 1.5599603652954102\n\nEpoch 163/800\ntrain loss: 0.009696900844573975\ntrain DSC: 0.9877970814704895\nval loss: 0.010144686698913575\nval DSC: 0.9871786832809448\nBest val DSC: 0.9871786832809448\nTime: 1.5850028991699219\n\nEpoch 164/800\ntrain loss: 0.009502397208917336\ntrain DSC: 0.9879053831100464\nval loss: 0.010464721918106079\nval DSC: 0.9862850308418274\nBest val DSC: 0.9871786832809448\nTime: 1.5550203323364258\n\nEpoch 165/800\ntrain loss: 0.009555676921469266\ntrain DSC: 0.9877826571464539\nval loss: 0.010361996293067933\nval DSC: 0.9865198135375977\nBest val DSC: 0.9871786832809448\nTime: 1.541064739227295\n\nEpoch 166/800\ntrain loss: 0.00922293448057331\ntrain DSC: 0.9884344339370728\nval loss: 0.010217729210853576\nval DSC: 0.9867957830429077\nBest val DSC: 0.9871786832809448\nTime: 1.55743408203125\n\nEpoch 167/800\ntrain loss: 0.009332636340719755\ntrain DSC: 0.9881932139396667\nval loss: 0.010151863098144531\nval DSC: 0.9871290922164917\nBest val DSC: 0.9871786832809448\nTime: 1.5363128185272217\n\nEpoch 168/800\ntrain loss: 0.009285020046546811\ntrain DSC: 0.9884172677993774\nval loss: 0.010347133874893189\nval DSC: 0.9865341186523438\nBest val DSC: 0.9871786832809448\nTime: 1.537379264831543\n\nEpoch 169/800\ntrain loss: 0.009354013888562312\ntrain DSC: 0.9882454872131348\nval loss: 0.010169017314910888\nval DSC: 0.9867812991142273\nBest val DSC: 0.9871786832809448\nTime: 1.5329811573028564\n\nEpoch 170/800\ntrain loss: 0.009149351080910105\ntrain DSC: 0.9884313344955444\nval loss: 0.010194930434226989\nval DSC: 0.986669659614563\nBest val DSC: 0.9871786832809448\nTime: 1.5295674800872803\n\nEpoch 171/800\ntrain loss: 0.009117189000864497\ntrain DSC: 0.9885838031768799\nval loss: 0.010178226232528686\nval DSC: 0.9867616891860962\nBest val DSC: 0.9871786832809448\nTime: 1.5644235610961914\n\nEpoch 172/800\ntrain loss: 0.009066266114594506\ntrain DSC: 0.9886718392372131\nval loss: 0.010169333219528199\nval DSC: 0.9868623614311218\nBest val DSC: 0.9871786832809448\nTime: 1.5437324047088623\n\nEpoch 173/800\ntrain loss: 0.009104926078045954\ntrain DSC: 0.9885346293449402\nval loss: 0.010065814852714539\nval DSC: 0.9868413805961609\nBest val DSC: 0.9871786832809448\nTime: 1.5462641716003418\n\nEpoch 174/800\ntrain loss: 0.009238669129668689\ntrain DSC: 0.9882630109786987\nval loss: 0.010401272773742675\nval DSC: 0.9861907958984375\nBest val DSC: 0.9871786832809448\nTime: 1.5689170360565186\n\nEpoch 175/800\ntrain loss: 0.009025914747206891\ntrain DSC: 0.9885925650596619\nval loss: 0.01025117039680481\nval DSC: 0.9868265390396118\nBest val DSC: 0.9871786832809448\nTime: 1.5687613487243652\n\nEpoch 176/800\ntrain loss: 0.009074843320690218\ntrain DSC: 0.9884805679321289\nval loss: 0.010546353459358216\nval DSC: 0.9858039021492004\nBest val DSC: 0.9871786832809448\nTime: 1.554128646850586\n\nEpoch 177/800\ntrain loss: 0.009219370904516001\ntrain DSC: 0.9881696701049805\nval loss: 0.009998440742492676\nval DSC: 0.9869775772094727\nBest val DSC: 0.9871786832809448\nTime: 1.583176612854004\n\nEpoch 178/800\ntrain loss: 0.009065394518805331\ntrain DSC: 0.9886239767074585\nval loss: 0.010071289539337159\nval DSC: 0.9868847131729126\nBest val DSC: 0.9871786832809448\nTime: 1.5717332363128662\n\nEpoch 179/800\ntrain loss: 0.009233736601032194\ntrain DSC: 0.9881967902183533\nval loss: 0.010047915577888488\nval DSC: 0.9868410229682922\nBest val DSC: 0.9871786832809448\nTime: 1.5378808975219727\n\nEpoch 180/800\ntrain loss: 0.008946786161328926\ntrain DSC: 0.9887046217918396\nval loss: 0.009934726357460021\nval DSC: 0.9870702624320984\nBest val DSC: 0.9871786832809448\nTime: 1.5800342559814453\n\nEpoch 181/800\ntrain loss: 0.009069309859979348\ntrain DSC: 0.9884803891181946\nval loss: 0.010147026181221009\nval DSC: 0.9865771532058716\nBest val DSC: 0.9871786832809448\nTime: 1.5448966026306152\n\nEpoch 182/800\ntrain loss: 0.00890920005860876\ntrain DSC: 0.9888638257980347\nval loss: 0.009877872467041016\nval DSC: 0.9872142672538757\nBest val DSC: 0.9872142672538757\nTime: 1.6511430740356445\n\nEpoch 183/800\ntrain loss: 0.009038401431724673\ntrain DSC: 0.988483726978302\nval loss: 0.010287272930145263\nval DSC: 0.9864786267280579\nBest val DSC: 0.9872142672538757\nTime: 1.5136795043945312\n\nEpoch 184/800\ntrain loss: 0.009152093871695096\ntrain DSC: 0.9879989624023438\nval loss: 0.01069812774658203\nval DSC: 0.9856133460998535\nBest val DSC: 0.9872142672538757\nTime: 1.5266625881195068\n\nEpoch 185/800\ntrain loss: 0.008968877987783463\ntrain DSC: 0.9885606169700623\nval loss: 0.010317167639732361\nval DSC: 0.9862877726554871\nBest val DSC: 0.9872142672538757\nTime: 1.5481750965118408\n\nEpoch 186/800\ntrain loss: 0.009046670843343266\ntrain DSC: 0.9884349703788757\nval loss: 0.010103365778923035\nval DSC: 0.9866946339607239\nBest val DSC: 0.9872142672538757\nTime: 1.5309128761291504\n\nEpoch 187/800\ntrain loss: 0.008789639981066594\ntrain DSC: 0.9887807369232178\nval loss: 0.01015315055847168\nval DSC: 0.9865508079528809\nBest val DSC: 0.9872142672538757\nTime: 1.530334711074829\n\nEpoch 188/800\ntrain loss: 0.008908570789899982\ntrain DSC: 0.9886621236801147\nval loss: 0.01002349853515625\nval DSC: 0.9870305061340332\nBest val DSC: 0.9872142672538757\nTime: 1.5295088291168213\n\nEpoch 189/800\ntrain loss: 0.00894433650814119\ntrain DSC: 0.9885619282722473\nval loss: 0.009917408227920532\nval DSC: 0.9871795773506165\nBest val DSC: 0.9872142672538757\nTime: 1.5469257831573486\n\nEpoch 190/800\ntrain loss: 0.008758328977178355\ntrain DSC: 0.9888849854469299\nval loss: 0.009993019700050353\nval DSC: 0.9870620965957642\nBest val DSC: 0.9872142672538757\nTime: 1.5199239253997803\n\nEpoch 191/800\ntrain loss: 0.008696089025403633\ntrain DSC: 0.9889731407165527\nval loss: 0.010011091828346252\nval DSC: 0.9867008924484253\nBest val DSC: 0.9872142672538757\nTime: 1.5529680252075195\n\nEpoch 192/800\ntrain loss: 0.008917943376009582\ntrain DSC: 0.9886875748634338\nval loss: 0.010035738348960876\nval DSC: 0.9866835474967957\nBest val DSC: 0.9872142672538757\nTime: 1.5624167919158936\n\nEpoch 193/800\ntrain loss: 0.00877045021682489\ntrain DSC: 0.9888553023338318\nval loss: 0.009820196032524108\nval DSC: 0.9873906373977661\nBest val DSC: 0.9873906373977661\nTime: 1.6289823055267334\n\nEpoch 194/800\ntrain loss: 0.008749728320074862\ntrain DSC: 0.9888606667518616\nval loss: 0.010050177574157715\nval DSC: 0.9867011904716492\nBest val DSC: 0.9873906373977661\nTime: 1.544851303100586\n\nEpoch 195/800\ntrain loss: 0.008578539871778644\ntrain DSC: 0.9890723824501038\nval loss: 0.009863483905792236\nval DSC: 0.9873707890510559\nBest val DSC: 0.9873906373977661\nTime: 1.549041509628296\n\nEpoch 196/800\ntrain loss: 0.00875111388378456\ntrain DSC: 0.9887887239456177\nval loss: 0.010182514786720276\nval DSC: 0.9862392544746399\nBest val DSC: 0.9873906373977661\nTime: 1.6131279468536377\n\nEpoch 197/800\ntrain loss: 0.008696595176321561\ntrain DSC: 0.9889814853668213\nval loss: 0.010188481211662293\nval DSC: 0.9863231778144836\nBest val DSC: 0.9873906373977661\nTime: 1.5558438301086426\n\nEpoch 198/800\ntrain loss: 0.008702377803990097\ntrain DSC: 0.9887458682060242\nval loss: 0.010034388303756714\nval DSC: 0.9868513345718384\nBest val DSC: 0.9873906373977661\nTime: 1.5333654880523682\n\nEpoch 199/800\ntrain loss: 0.00877385745283033\ntrain DSC: 0.9885884523391724\nval loss: 0.010763385891914367\nval DSC: 0.9853038787841797\nBest val DSC: 0.9873906373977661\nTime: 1.6018562316894531\n\nEpoch 200/800\ntrain loss: 0.008748106292036713\ntrain DSC: 0.9887660145759583\nval loss: 0.009935176372528077\nval DSC: 0.9869060516357422\nBest val DSC: 0.9873906373977661\nTime: 1.5346686840057373\n\nEpoch 201/800\ntrain loss: 0.008580876178428775\ntrain DSC: 0.9889313578605652\nval loss: 0.010112732648849487\nval DSC: 0.9863764047622681\nBest val DSC: 0.9873906373977661\nTime: 1.5505211353302002\n\nEpoch 202/800\ntrain loss: 0.008890619043444023\ntrain DSC: 0.9884696006774902\nval loss: 0.010037961602210998\nval DSC: 0.98655766248703\nBest val DSC: 0.9873906373977661\nTime: 1.5220935344696045\n\nEpoch 203/800\ntrain loss: 0.008632747853388552\ntrain DSC: 0.9890563488006592\nval loss: 0.009970107674598694\nval DSC: 0.9868081212043762\nBest val DSC: 0.9873906373977661\nTime: 1.5855293273925781\n\nEpoch 204/800\ntrain loss: 0.008712891672478348\ntrain DSC: 0.9887935519218445\nval loss: 0.009882032871246338\nval DSC: 0.9868515729904175\nBest val DSC: 0.9873906373977661\nTime: 1.5398147106170654\n\nEpoch 205/800\ntrain loss: 0.00852913348401179\ntrain DSC: 0.9890889525413513\nval loss: 0.009799429774284362\nval DSC: 0.9870920181274414\nBest val DSC: 0.9873906373977661\nTime: 1.5396308898925781\n\nEpoch 206/800\ntrain loss: 0.008626014482779581\ntrain DSC: 0.9888050556182861\nval loss: 0.009893253445625305\nval DSC: 0.9872061014175415\nBest val DSC: 0.9873906373977661\nTime: 1.5600476264953613\n\nEpoch 207/800\ntrain loss: 0.008686987103008833\ntrain DSC: 0.9887152314186096\nval loss: 0.009830129146575928\nval DSC: 0.9869439005851746\nBest val DSC: 0.9873906373977661\nTime: 1.5336217880249023\n\nEpoch 208/800\ntrain loss: 0.00873148832164827\ntrain DSC: 0.9886259436607361\nval loss: 0.009780913591384888\nval DSC: 0.9873932003974915\nBest val DSC: 0.9873932003974915\nTime: 1.6301548480987549\n\nEpoch 209/800\ntrain loss: 0.008573550669873347\ntrain DSC: 0.9888924956321716\nval loss: 0.009815031290054321\nval DSC: 0.9870268106460571\nBest val DSC: 0.9873932003974915\nTime: 1.5465779304504395\n\nEpoch 210/800\ntrain loss: 0.0085465741939232\ntrain DSC: 0.9890027046203613\nval loss: 0.009975892305374146\nval DSC: 0.9868057370185852\nBest val DSC: 0.9873932003974915\nTime: 1.5193471908569336\n\nEpoch 211/800\ntrain loss: 0.008846809629534111\ntrain DSC: 0.988472044467926\nval loss: 0.010586515069007874\nval DSC: 0.9854726791381836\nBest val DSC: 0.9873932003974915\nTime: 1.5909576416015625\n\nEpoch 212/800\ntrain loss: 0.008758471637475685\ntrain DSC: 0.9886924028396606\nval loss: 0.009747284650802612\nval DSC: 0.9872035980224609\nBest val DSC: 0.9873932003974915\nTime: 1.571056842803955\n\nEpoch 213/800\ntrain loss: 0.008535144759006187\ntrain DSC: 0.9889703989028931\nval loss: 0.009784749150276184\nval DSC: 0.9870867729187012\nBest val DSC: 0.9873932003974915\nTime: 1.5274803638458252\n\nEpoch 214/800\ntrain loss: 0.008452705672529877\ntrain DSC: 0.9890592694282532\nval loss: 0.009692811965942382\nval DSC: 0.9874247312545776\nBest val DSC: 0.9874247312545776\nTime: 1.621433973312378\n\nEpoch 215/800\ntrain loss: 0.008328039138043513\ntrain DSC: 0.9892762899398804\nval loss: 0.01021374762058258\nval DSC: 0.9862843751907349\nBest val DSC: 0.9874247312545776\nTime: 1.5377614498138428\n\nEpoch 216/800\ntrain loss: 0.008487988690860936\ntrain DSC: 0.9891617298126221\nval loss: 0.009723001718521118\nval DSC: 0.9872848391532898\nBest val DSC: 0.9874247312545776\nTime: 1.5451383590698242\n\nEpoch 217/800\ntrain loss: 0.008350418239343361\ntrain DSC: 0.989223301410675\nval loss: 0.009822556376457214\nval DSC: 0.9870813488960266\nBest val DSC: 0.9874247312545776\nTime: 1.528817892074585\n\nEpoch 218/800\ntrain loss: 0.008599287173787102\ntrain DSC: 0.9889158010482788\nval loss: 0.00983743667602539\nval DSC: 0.9872169494628906\nBest val DSC: 0.9874247312545776\nTime: 1.5361583232879639\n\nEpoch 219/800\ntrain loss: 0.008463517564242004\ntrain DSC: 0.9890338182449341\nval loss: 0.009697309136390686\nval DSC: 0.9871736764907837\nBest val DSC: 0.9874247312545776\nTime: 1.5380973815917969\n\nEpoch 220/800\ntrain loss: 0.008567392826080322\ntrain DSC: 0.9888685941696167\nval loss: 0.009718474745750428\nval DSC: 0.9874351620674133\nBest val DSC: 0.9874351620674133\nTime: 1.6150660514831543\n\nEpoch 221/800\ntrain loss: 0.008533029282679324\ntrain DSC: 0.9890527129173279\nval loss: 0.009786280989646911\nval DSC: 0.9872452020645142\nBest val DSC: 0.9874351620674133\nTime: 1.5692479610443115\n\nEpoch 222/800\ntrain loss: 0.008472603852631615\ntrain DSC: 0.9889819025993347\nval loss: 0.009667232632637024\nval DSC: 0.9872363805770874\nBest val DSC: 0.9874351620674133\nTime: 1.541914939880371\n\nEpoch 223/800\ntrain loss: 0.008646345529399935\ntrain DSC: 0.9886727929115295\nval loss: 0.009834814071655273\nval DSC: 0.9870025515556335\nBest val DSC: 0.9874351620674133\nTime: 1.5454821586608887\n\nEpoch 224/800\ntrain loss: 0.008371360966416656\ntrain DSC: 0.9892206788063049\nval loss: 0.009704905748367309\nval DSC: 0.9871187210083008\nBest val DSC: 0.9874351620674133\nTime: 1.5339667797088623\n\nEpoch 225/800\ntrain loss: 0.008468610341431664\ntrain DSC: 0.9891068339347839\nval loss: 0.00972886085510254\nval DSC: 0.9873026609420776\nBest val DSC: 0.9874351620674133\nTime: 1.5490853786468506\n\nEpoch 226/800\ntrain loss: 0.008292173753019239\ntrain DSC: 0.9893020391464233\nval loss: 0.009740683436393737\nval DSC: 0.9870485067367554\nBest val DSC: 0.9874351620674133\nTime: 1.5532174110412598\n\nEpoch 227/800\ntrain loss: 0.008340643077600197\ntrain DSC: 0.9892032742500305\nval loss: 0.009871134161949157\nval DSC: 0.9866711497306824\nBest val DSC: 0.9874351620674133\nTime: 1.5763118267059326\n\nEpoch 228/800\ntrain loss: 0.00832298349161617\ntrain DSC: 0.989229142665863\nval loss: 0.009634485840797425\nval DSC: 0.9872220158576965\nBest val DSC: 0.9874351620674133\nTime: 1.5754120349884033\n\nEpoch 229/800\ntrain loss: 0.008227402069529549\ntrain DSC: 0.9894658327102661\nval loss: 0.009678781032562256\nval DSC: 0.9870680570602417\nBest val DSC: 0.9874351620674133\nTime: 1.5456857681274414\n\nEpoch 230/800\ntrain loss: 0.008445096797630435\ntrain DSC: 0.989078938961029\nval loss: 0.009664252400398254\nval DSC: 0.9873781204223633\nBest val DSC: 0.9874351620674133\nTime: 1.5265769958496094\n\nEpoch 231/800\ntrain loss: 0.008393847551502165\ntrain DSC: 0.9891742467880249\nval loss: 0.009610366821289063\nval DSC: 0.9872849583625793\nBest val DSC: 0.9874351620674133\nTime: 1.5410752296447754\n\nEpoch 232/800\ntrain loss: 0.00826270169899112\ntrain DSC: 0.9893167614936829\nval loss: 0.009685209393501282\nval DSC: 0.987047016620636\nBest val DSC: 0.9874351620674133\nTime: 1.591775894165039\n\nEpoch 233/800\ntrain loss: 0.008253692603502118\ntrain DSC: 0.9893248081207275\nval loss: 0.009620249271392822\nval DSC: 0.9872272610664368\nBest val DSC: 0.9874351620674133\nTime: 1.5584189891815186\n\nEpoch 234/800\ntrain loss: 0.008345354775913427\ntrain DSC: 0.9892022609710693\nval loss: 0.009635773301124573\nval DSC: 0.9872785806655884\nBest val DSC: 0.9874351620674133\nTime: 1.5590755939483643\n\nEpoch 235/800\ntrain loss: 0.008225379420108482\ntrain DSC: 0.9893655180931091\nval loss: 0.009690141677856446\nval DSC: 0.9871028065681458\nBest val DSC: 0.9874351620674133\nTime: 1.5232787132263184\n\nEpoch 236/800\ntrain loss: 0.008261102144835426\ntrain DSC: 0.9893788695335388\nval loss: 0.009659436345100404\nval DSC: 0.9872797727584839\nBest val DSC: 0.9874351620674133\nTime: 1.536916732788086\n\nEpoch 237/800\ntrain loss: 0.00832658224418515\ntrain DSC: 0.9891403913497925\nval loss: 0.009729158878326417\nval DSC: 0.9870277643203735\nBest val DSC: 0.9874351620674133\nTime: 1.5317504405975342\n\nEpoch 238/800\ntrain loss: 0.008210462624909447\ntrain DSC: 0.9893753528594971\nval loss: 0.009714087843894959\nval DSC: 0.9871218800544739\nBest val DSC: 0.9874351620674133\nTime: 1.5487582683563232\n\nEpoch 239/800\ntrain loss: 0.0082701425083348\ntrain DSC: 0.9892752170562744\nval loss: 0.009681490063667298\nval DSC: 0.9873622059822083\nBest val DSC: 0.9874351620674133\nTime: 1.5698530673980713\n\nEpoch 240/800\ntrain loss: 0.008385264482654508\ntrain DSC: 0.9890851974487305\nval loss: 0.009626662731170655\nval DSC: 0.9872575998306274\nBest val DSC: 0.9874351620674133\nTime: 1.5608625411987305\n\nEpoch 241/800\ntrain loss: 0.00838564751578159\ntrain DSC: 0.9889319539070129\nval loss: 0.00971468985080719\nval DSC: 0.9871162176132202\nBest val DSC: 0.9874351620674133\nTime: 1.540856122970581\n\nEpoch 242/800\ntrain loss: 0.008320088269280606\ntrain DSC: 0.9891912341117859\nval loss: 0.009592187404632569\nval DSC: 0.9873024821281433\nBest val DSC: 0.9874351620674133\nTime: 1.52813720703125\n\nEpoch 243/800\ntrain loss: 0.008244073781810824\ntrain DSC: 0.9892832040786743\nval loss: 0.009822946786880494\nval DSC: 0.9867750406265259\nBest val DSC: 0.9874351620674133\nTime: 1.537649393081665\n\nEpoch 244/800\ntrain loss: 0.008232504617972453\ntrain DSC: 0.9892799258232117\nval loss: 0.009607776999473572\nval DSC: 0.9873725175857544\nBest val DSC: 0.9874351620674133\nTime: 1.5340638160705566\n\nEpoch 245/800\ntrain loss: 0.00812257020199885\ntrain DSC: 0.9894643425941467\nval loss: 0.009715795516967773\nval DSC: 0.9868850708007812\nBest val DSC: 0.9874351620674133\nTime: 1.551034688949585\n\nEpoch 246/800\ntrain loss: 0.008409410226540487\ntrain DSC: 0.9890424609184265\nval loss: 0.009586653113365174\nval DSC: 0.9873378872871399\nBest val DSC: 0.9874351620674133\nTime: 1.543525218963623\n\nEpoch 247/800\ntrain loss: 0.00823215676135704\ntrain DSC: 0.989252507686615\nval loss: 0.009701848030090332\nval DSC: 0.9870272874832153\nBest val DSC: 0.9874351620674133\nTime: 1.5379106998443604\n\nEpoch 248/800\ntrain loss: 0.008179256173430896\ntrain DSC: 0.9893837571144104\nval loss: 0.009553512930870056\nval DSC: 0.9873803853988647\nBest val DSC: 0.9874351620674133\nTime: 1.5427789688110352\n\nEpoch 249/800\ntrain loss: 0.008142924699626986\ntrain DSC: 0.9894880652427673\nval loss: 0.009533438086509704\nval DSC: 0.9874140620231628\nBest val DSC: 0.9874351620674133\nTime: 1.5287604331970215\n\nEpoch 250/800\ntrain loss: 0.008158519619801005\ntrain DSC: 0.9894744157791138\nval loss: 0.009586179256439209\nval DSC: 0.9871169328689575\nBest val DSC: 0.9874351620674133\nTime: 1.5452067852020264\n\nEpoch 251/800\ntrain loss: 0.008151107147091725\ntrain DSC: 0.9894534349441528\nval loss: 0.009571459889411927\nval DSC: 0.9872411489486694\nBest val DSC: 0.9874351620674133\nTime: 1.512343406677246\n\nEpoch 252/800\ntrain loss: 0.008133882381876961\ntrain DSC: 0.9894217848777771\nval loss: 0.009567058086395264\nval DSC: 0.9873107671737671\nBest val DSC: 0.9874351620674133\nTime: 1.5464260578155518\n\nEpoch 253/800\ntrain loss: 0.00820896957741409\ntrain DSC: 0.9892655611038208\nval loss: 0.009574833512306213\nval DSC: 0.9872564077377319\nBest val DSC: 0.9874351620674133\nTime: 1.5386326313018799\n\nEpoch 254/800\ntrain loss: 0.00813794038334831\ntrain DSC: 0.9894242882728577\nval loss: 0.009668338298797607\nval DSC: 0.9869555234909058\nBest val DSC: 0.9874351620674133\nTime: 1.5302813053131104\n\nEpoch 255/800\ntrain loss: 0.008092065326503067\ntrain DSC: 0.9894717931747437\nval loss: 0.009610486030578614\nval DSC: 0.987133800983429\nBest val DSC: 0.9874351620674133\nTime: 1.5271344184875488\n\nEpoch 256/800\ntrain loss: 0.008066303417330882\ntrain DSC: 0.9895561337471008\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3973/3899752891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3973/242952138.py\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, dataloaders, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;31m# wrong, we set a timeout and if the workers fail to join,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                     \u001b[0;31m# they are killed in the `finally` block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_join_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/shapeworks/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677800436418
        }
      },
      "id": "9e895368-b239-4928-a255-0c3f7ad4bbb6"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloaders):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    true = []\n",
        "    for mode in ['train', 'val']:\n",
        "        for image, radius in dataloaders[mode]:\n",
        "            image = image.to(device)\n",
        "            true.append(radius.item())\n",
        "            pred_radius = model.fc1(torch.flatten(model.autoencoder.encode(image), start_dim=1))\n",
        "            pred.append(pred_radius.item())\n",
        "    return pred, true"
      ],
      "outputs": [],
      "execution_count": 64,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677689508798
        }
      },
      "id": "6ec49871-1476-47c2-b7b6-d5368ce50bba"
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Autoencoder().to(device)\n",
        "autoencoder.load_state_dict(torch.load(f'{model_dir}/best_antocoder.torch'))\n",
        "pred, true = predict(model=autoencoder, dataloaders=dataloaders)"
      ],
      "outputs": [],
      "execution_count": 65,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677689509953
        }
      },
      "id": "c3758c79-cfd4-4452-84dd-042d2e5d5b3b"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(true, pred)\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677689529060
        }
      },
      "id": "3abb56e9-2db5-40f2-baf9-dbb6ce244921"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "shapeworks"
    },
    "kernelspec": {
      "name": "shapeworks",
      "language": "python",
      "display_name": "Python (shapeworks)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}