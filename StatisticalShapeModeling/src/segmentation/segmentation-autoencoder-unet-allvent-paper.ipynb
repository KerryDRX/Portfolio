{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import scipy.ndimage as ndimage\n",
        "import nrrd\n",
        "import torchio as tio\n",
        "import monai\n",
        "import nibabel as nib\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1678418531398
        },
        "id": "8246ca62",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "id": "8246ca62"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_cuda_memory():\n",
        "    t = torch.cuda.get_device_properties(0).total_memory\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    f = r-a  # free inside reserved\n",
        "    print('Total:     {:0.2f} GiB'.format(t / 2**30))\n",
        "    print('Reserved:  {:0.2f} GiB'.format(r / 2**30))\n",
        "    print('Allocated: {:0.2f} GiB'.format(a / 2**30))\n",
        "    print('Free:      {:0.2f} GiB'.format(f / 2**30))\n",
        "\n",
        "show_cuda_memory()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total:     11.17 GiB\nReserved:  0.00 GiB\nAllocated: 0.00 GiB\nFree:      0.00 GiB\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678418531567
        }
      },
      "id": "fa84b34d-4438-4a73-823d-59c42e0e5a02"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b2836ef1-c66c-4d38-9b47-a0b62c33be9f"
    },
    {
      "cell_type": "code",
      "source": [
        "city = 'Beijing_Zang'\n",
        "\n",
        "modes = ['train', 'test']\n",
        "total_size = 197\n",
        "train_size, test_size = 158, 39\n",
        "num_classes = 3\n",
        "\n",
        "image_dir = f'../dataset/{city}/MRI'\n",
        "image_arti_dir = f'../dataset/{city}/MRI_arti'\n",
        "label_dir = f'../dataset/{city}/Segmentation'\n",
        "autoencoder_gm_dir = f'../results/SCAE_GM_temp2/best_autoencoder.torch'\n",
        "\n",
        "model_dir = f'../results/{city}_unet_autoencoder_gmwm'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678418531729
        }
      },
      "id": "52a47ac5-f5b3-4059-9dd7-05a4214e1f03"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "random_state = np.random.RandomState(seed=seed)\n",
        "perm = random_state.permutation(total_size)\n",
        "perm = {\n",
        "    'train': perm[:train_size],\n",
        "    'test': perm[-test_size:],\n",
        "}\n",
        "\n",
        "def get_subjects(mode):\n",
        "    subjects = []\n",
        "    image_src = image_dir #image_dir if mode == 'train' else image_arti_dir\n",
        "    image_paths = [sorted(glob(f'{image_src}/*.nii.gz'))[i] for i in perm[mode]]\n",
        "    for image_path in tqdm(image_paths, desc=mode):\n",
        "        fn = image_path.split('/')[-1]\n",
        "        label_path = f'{label_dir}/{fn}'\n",
        "        subject = tio.Subject(\n",
        "            image=tio.ScalarImage(image_path),\n",
        "            label=tio.LabelMap(label_path),\n",
        "        )\n",
        "        subjects.append(subject)\n",
        "    return subjects\n",
        "\n",
        "def get_transform():\n",
        "    resample = tio.Compose([\n",
        "        tio.Resample(2),\n",
        "        tio.CropOrPad((96, 128, 128)),\n",
        "    ])\n",
        "    signal = tio.Compose([ \n",
        "        tio.RescaleIntensity(percentiles=(0.1, 99.9), out_min_max=(0, 1)),\n",
        "    ])\n",
        "    # spatial = tio.Compose([\n",
        "    #     tio.RandomAffine(translation=1),\n",
        "    # ])\n",
        "    remapping = dict()\n",
        "    for i in range(139):\n",
        "        # remapping[i] = 1 if (3<=i<=11 or 19<=i<=20 or 25<=i<=32 or 35<=i) else 0\n",
        "        # remapping[i] = 1 if i in {12, 13, 16, 17} else 0\n",
        "        remapping[i] = 1 if (3<=i<=11 or 19<=i<=20 or 25<=i<=32 or 35<=i) else 2 if i in {12, 13, 16, 17} else 0\n",
        "    remapping = tio.RemapLabels(remapping)\n",
        "\n",
        "    onehot = tio.OneHot(num_classes=num_classes)\n",
        "    transform = {\n",
        "        'train': tio.Compose([\n",
        "            resample,\n",
        "            # spatial,\n",
        "            signal,\n",
        "            remapping,\n",
        "            onehot,\n",
        "        ]),\n",
        "        'test': tio.Compose([\n",
        "            resample,\n",
        "            signal,\n",
        "            remapping,\n",
        "            onehot,\n",
        "        ]),\n",
        "    }\n",
        "    return transform\n",
        "\n",
        "def get_dataloader(transform):\n",
        "    dataloader = dict()\n",
        "    for mode in modes:\n",
        "        dataloader[mode] = torch.utils.data.DataLoader(\n",
        "            tio.SubjectsDataset(\n",
        "                subjects[mode], \n",
        "                transform=transform[mode]\n",
        "            ),\n",
        "            batch_size=1, \n",
        "            num_workers=os.cpu_count(),\n",
        "            shuffle=(mode == 'train'),\n",
        "        )\n",
        "    return dataloader\n",
        "\n",
        "subjects = {mode: get_subjects(mode) for mode in modes}\n",
        "transform = get_transform()\n",
        "dataloaders = get_dataloader(transform)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "train: 100%|██████████| 158/158 [00:01<00:00, 88.90it/s]\ntest: 100%|██████████| 39/39 [00:00<00:00, 92.85it/s]\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1678418538149
        }
      },
      "id": "fe9e2ac9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e247b1c8-c347-4528-8d76-8e1981fa3770"
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution(in_channels, out_channels, stride):\n",
        "    return torch.nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "\n",
        "def deconvolution(in_channels, out_channels, stride):\n",
        "    return torch.nn.ConvTranspose3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, output_padding=1)\n",
        "\n",
        "def normalization(channel):\n",
        "    return torch.nn.BatchNorm3d(channel)\n",
        "\n",
        "def activation():\n",
        "    return torch.nn.PReLU()\n",
        "\n",
        "def pooling(kernel_size):\n",
        "    return torch.nn.MaxPool3d(kernel_size=kernel_size)\n",
        "\n",
        "def upsampling(scale_factor):\n",
        "    return torch.nn.Upsample(scale_factor=scale_factor, mode='trilinear', align_corners=True)\n",
        "\n",
        "class Autoencoder(torch.nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            convolution(in_channels=1, out_channels=channels[0], stride=2),\n",
        "            normalization(channels[0]),\n",
        "            activation(),\n",
        "            \n",
        "            convolution(in_channels=channels[0], out_channels=channels[1], stride=2),\n",
        "            normalization(channels[1]),\n",
        "            activation(),\n",
        "            \n",
        "            convolution(in_channels=channels[1], out_channels=channels[2], stride=2),\n",
        "            normalization(channels[2]),\n",
        "            activation(),\n",
        "\n",
        "            convolution(in_channels=channels[2], out_channels=channels[3], stride=2),\n",
        "            normalization(channels[3]),\n",
        "            activation(),\n",
        "        )\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            deconvolution(in_channels=channels[3], out_channels=channels[2], stride=2),\n",
        "            normalization(channels[2]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[2], out_channels=channels[1], stride=2),\n",
        "            normalization(channels[1]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[1], out_channels=channels[0], stride=2),\n",
        "            normalization(channels[0]),\n",
        "            activation(),\n",
        "            \n",
        "            deconvolution(in_channels=channels[0], out_channels=1, stride=2),\n",
        "            normalization(1),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "autoencoder = Autoencoder(channels=[128,256,512,1024]).to(device)\n",
        "autoencoder.load_state_dict(torch.load(autoencoder_gm_dir))\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678418569279
        }
      },
      "id": "15f056d2-eb79-40bd-8ff0-6a66544a31f2"
    },
    {
      "cell_type": "code",
      "source": [
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n",
        "\n",
        "class StraightThroughEstimator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StraightThroughEstimator, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = STEFunction.apply(x)\n",
        "        return x\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.segmentation = monai.networks.nets.UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=num_classes,\n",
        "            channels=(16, 32, 64, 128),\n",
        "            strides=(2, 2, 2,),\n",
        "            norm=monai.networks.layers.Norm.BATCH,\n",
        "            dropout=0.3,\n",
        "        )\n",
        "        self.ste = StraightThroughEstimator()\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, image):\n",
        "        prob = self.softmax(self.segmentation(image))\n",
        "        mask = prob[:, 1:2]#self.ste(prob[:, 2:]-0.5)\n",
        "        shape = autoencoder.encoder(mask)\n",
        "        recon = autoencoder.decoder(shape)\n",
        "        return prob, mask, shape, recon"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678419409535
        }
      },
      "id": "570a53f5-d51b-4320-9d01-c8a12a1f005d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9689e3b3-ea5d-426d-b3cb-091f185d8133"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(model):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.names = ['loss', 'loss_segm', 'loss_shape', 'loss_recon']\n",
        "    def log(self, mode, epoch, values):\n",
        "        for name, value in zip(self.names, values):\n",
        "            self.metrics[(mode, epoch, name)].append(0 if value == 0 else value.item())\n",
        "    def show(self, mode, epoch):\n",
        "        print()\n",
        "        for name in self.names:\n",
        "            mean = np.mean(self.metrics[(mode, epoch, name)])\n",
        "            print(f'{mode} {name}: {mean}')\n",
        "            if name == 'loss':\n",
        "                mean_loss = mean\n",
        "        return mean_loss\n",
        "'''\n",
        "def predict(dataloaders):\n",
        "    mode = 'test'\n",
        "    model = Model().to(device)\n",
        "    model.load_state_dict(torch.load(f'{model_dir}/best_model.torch'))\n",
        "    model.eval()\n",
        "\n",
        "    for i, subject in enumerate(tqdm(dataloaders[mode])):\n",
        "        image = subject['image'][tio.DATA].to(device)\n",
        "        label = subject['label'][tio.DATA].to(device)\n",
        "        prob, mask, shape, recon = model(image)\n",
        "        \n",
        "        nrrd.write(f'{model_dir}/true{i}.nrrd', label[0, 0].detach().cpu().numpy())\n",
        "        nrrd.write(f'{model_dir}/pred{i}.nrrd', mask[0, 0].detach().cpu().numpy())\n",
        "        nrrd.write(f'{model_dir}/recon{i}.nrrd', recon[0, 0].detach().cpu().numpy())\n",
        "\n",
        "    !rm file.zip\n",
        "    !zip -r file.zip $model_dir\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "'''\n",
        "def train(model, n_epochs, dataloaders, learning_rate, lambda_shape, lambda_recon):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\n",
        "\n",
        "    loss_dice = monai.losses.DiceLoss(squared_pred=True)\n",
        "    loss_mse = torch.nn.MSELoss()\n",
        "    \n",
        "    metric = monai.metrics.DiceMetric(reduction='mean_batch')\n",
        "    metrics = Metrics()\n",
        "    best_val_loss = np.Inf\n",
        "\n",
        "    tol, tol_total = 0, 0\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        t0 = time.time()\n",
        "        print(f'\\nEpoch {epoch}/{n_epochs}')\n",
        "        for mode in modes:\n",
        "            if mode == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            for subject in dataloaders[mode]:\n",
        "                image = subject['image'][tio.DATA].to(device)\n",
        "                label = subject['label'][tio.DATA].to(device).float()\n",
        "                label_gm = label[:, 1:2]\n",
        "                \n",
        "                prob, mask, shape, recon = model(image)\n",
        "                loss_segm = loss_dice(prob, label)\n",
        "                loss = loss_segm\n",
        "                loss_shape, loss_recon = 0, 0\n",
        "                if lambda_shape > 0:\n",
        "                    loss_shape = loss_mse(shape, autoencoder.encoder(label_gm))\n",
        "                    loss += lambda_shape * loss_shape\n",
        "                if lambda_recon > 0:\n",
        "                    loss_recon = loss_dice(recon, label_gm)\n",
        "                    loss += lambda_recon * loss_recon\n",
        "\n",
        "                if mode == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                pred = monai.networks.utils.one_hot(\n",
        "                    torch.argmax(prob, dim=1, keepdim=True), \n",
        "                    num_classes=num_classes, \n",
        "                    dim=1\n",
        "                ).to(device)\n",
        "                metric(pred, label)\n",
        "                metrics.log(mode, epoch, [loss, loss_segm, loss_shape, loss_recon])\n",
        "\n",
        "            dsc = metric.aggregate().tolist()\n",
        "            metric.reset()\n",
        "            mean_loss = metrics.show(mode, epoch)\n",
        "            print(f'{mode} unet DSC: {dsc}')\n",
        "        \n",
        "        if mean_loss <= best_val_loss:\n",
        "            best_val_loss = mean_loss\n",
        "            best_epoch = epoch\n",
        "            tol = 0\n",
        "            tol_total = 0\n",
        "        else:\n",
        "            tol += 1\n",
        "            tol_total += 1\n",
        "        print(f'Best val loss: {best_val_loss}')\n",
        "        \n",
        "        if tol == 10:\n",
        "            scheduler.step()\n",
        "            print('Validation loss stopped to decrease for 10 epochs (LR /= 5).')\n",
        "            tol = 0\n",
        "        \n",
        "        time_elapsed = time.time() - t0\n",
        "        print(f'Time: {time_elapsed}\\n')\n",
        "\n",
        "        if tol_total == 22:\n",
        "            print('Validation loss stopped to decrease for 30 epochs. Training terminated.')\n",
        "            break\n",
        "    print(f'Best epoch: {best_epoch}')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678418569631
        }
      },
      "id": "86731a57-1ac6-4dfd-b48a-53fad0257e88"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0, lambda_recon=0,\n",
        "    # lambda_shape=0.01, lambda_recon=0.001,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/300\n\ntrain loss: 0.3140457736540444\ntrain loss_segm: 0.3140457736540444\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9389461278915405, 0.5425962805747986, 0.5318793654441833]\n\ntest loss: 0.1394583529386765\ntest loss_segm: 0.1394583529386765\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9900426864624023, 0.7536183595657349, 0.7919955253601074]\nBest val loss: 0.1394583529386765\nTime: 63.4544415473938\n\n\nEpoch 2/300\n\ntrain loss: 0.1329192851163164\ntrain loss_segm: 0.1329192851163164\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9907858967781067, 0.7636953592300415, 0.7887741327285767]\n\ntest loss: 0.09654399905449305\ntest loss_segm: 0.09654399905449305\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9930020570755005, 0.8238855004310608, 0.8505937457084656]\nBest val loss: 0.09654399905449305\nTime: 65.96640419960022\n\n\nEpoch 3/300\n\ntrain loss: 0.1064275563989259\ntrain loss_segm: 0.1064275563989259\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9931491613388062, 0.8085648417472839, 0.8203288912773132]\n\ntest loss: 0.08317062831841983\ntest loss_segm: 0.08317062831841983\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.994731605052948, 0.848403811454773, 0.8595898747444153]\nBest val loss: 0.08317062831841983\nTime: 72.50952696800232\n\n\nEpoch 4/300\n\ntrain loss: 0.09582195482865165\ntrain loss_segm: 0.09582195482865165\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9939309358596802, 0.8254576325416565, 0.8340150713920593]\n\ntest loss: 0.07550185135541818\ntest loss_segm: 0.07550185135541818\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9953115582466125, 0.8616364598274231, 0.8736274838447571]\nBest val loss: 0.07550185135541818\nTime: 71.33832812309265\n\n\nEpoch 5/300\n\ntrain loss: 0.08886298598556579\ntrain loss_segm: 0.08886298598556579\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.994360625743866, 0.8360902070999146, 0.8443211317062378]\n\ntest loss: 0.0762242371073136\ntest loss_segm: 0.0762242371073136\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9946985244750977, 0.8501672148704529, 0.8720726370811462]\nBest val loss: 0.07550185135541818\nTime: 65.5424485206604\n\n\nEpoch 6/300\n\ntrain loss: 0.08485852582733842\ntrain loss_segm: 0.08485852582733842\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9945979118347168, 0.8419641256332397, 0.8499780893325806]\n\ntest loss: 0.06726616917130275\ntest loss_segm: 0.06726616917130275\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9955892562866211, 0.8718993067741394, 0.881065845489502]\nBest val loss: 0.06726616917130275\nTime: 65.68485856056213\n\n\nEpoch 7/300\n\ntrain loss: 0.08113301568959333\ntrain loss_segm: 0.08113301568959333\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9948112368583679, 0.8477743864059448, 0.8559490442276001]\n\ntest loss: 0.0659853047094284\ntest loss_segm: 0.0659853047094284\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9955623149871826, 0.8716490268707275, 0.8848810195922852]\nBest val loss: 0.0659853047094284\nTime: 65.89986848831177\n\n\nEpoch 8/300\n\ntrain loss: 0.0788362979228738\ntrain loss_segm: 0.0788362979228738\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9948926568031311, 0.851033091545105, 0.859969437122345]\n\ntest loss: 0.06599716039804313\ntest loss_segm: 0.06599716039804313\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9957050681114197, 0.8739528059959412, 0.8817439675331116]\nBest val loss: 0.0659853047094284\nTime: 68.33488488197327\n\n\nEpoch 9/300\n\ntrain loss: 0.07668295662991609\ntrain loss_segm: 0.07668295662991609\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995036244392395, 0.8545523285865784, 0.8631665110588074]\n\ntest loss: 0.0645245244869819\ntest loss_segm: 0.0645245244869819\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9957510232925415, 0.8747450709342957, 0.8843949437141418]\nBest val loss: 0.0645245244869819\nTime: 64.98820066452026\n\n\nEpoch 10/300\n\ntrain loss: 0.07558722459251367\ntrain loss_segm: 0.07558722459251367\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9950789213180542, 0.8560915589332581, 0.8651052713394165]\n\ntest loss: 0.06599145640547459\ntest loss_segm: 0.06599145640547459\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9956733584403992, 0.8721514940261841, 0.8820494413375854]\nBest val loss: 0.0645245244869819\nTime: 69.43847608566284\n\n\nEpoch 11/300\n\ntrain loss: 0.07516034617076946\ntrain loss_segm: 0.07516034617076946\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9951352477073669, 0.8568716049194336, 0.8654518723487854]\n\ntest loss: 0.06310340551993786\ntest loss_segm: 0.06310340551993786\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.995677649974823, 0.8743078708648682, 0.8885928392410278]\nBest val loss: 0.06310340551993786\nTime: 68.73178482055664\n\n\nEpoch 12/300\n\ntrain loss: 0.07385534283858311\ntrain loss_segm: 0.07385534283858311\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995198130607605, 0.8587793111801147, 0.8677600026130676]\n\ntest loss: 0.06193916881695772\ntest loss_segm: 0.06193916881695772\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9958207607269287, 0.8783444166183472, 0.8888707160949707]\nBest val loss: 0.06193916881695772\nTime: 64.84584212303162\n\n\nEpoch 13/300\n\ntrain loss: 0.07270569448606877\ntrain loss_segm: 0.07270569448606877\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9952589869499207, 0.860734760761261, 0.8697364926338196]\n\ntest loss: 0.06190712415637114\ntest loss_segm: 0.06190712415637114\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9955163598060608, 0.8749333024024963, 0.8915702700614929]\nBest val loss: 0.06190712415637114\nTime: 66.93747973442078\n\n\nEpoch 14/300\n\ntrain loss: 0.07182607710172859\ntrain loss_segm: 0.07182607710172859\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9953212738037109, 0.8623518943786621, 0.8709954619407654]\n\ntest loss: 0.060675293302688844\ntest loss_segm: 0.060675293302688844\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.99599689245224, 0.881262481212616, 0.890202522277832]\nBest val loss: 0.060675293302688844\nTime: 66.13864755630493\n\n\nEpoch 15/300\n\ntrain loss: 0.07111958468545086\ntrain loss_segm: 0.07111958468545086\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9953683614730835, 0.8635378479957581, 0.8721117377281189]\n\ntest loss: 0.06810964376498492\ntest loss_segm: 0.06810964376498492\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9958131313323975, 0.8728014826774597, 0.8745101690292358]\nBest val loss: 0.060675293302688844\nTime: 64.89618587493896\n\n\nEpoch 16/300\n\ntrain loss: 0.07071085455768471\ntrain loss_segm: 0.07071085455768471\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995403528213501, 0.8641765117645264, 0.8727767467498779]\n\ntest loss: 0.060774957235807024\ntest loss_segm: 0.060774957235807024\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959034323692322, 0.8803431391716003, 0.8901219367980957]\nBest val loss: 0.060675293302688844\nTime: 64.7089011669159\n\n\nEpoch 17/300\n\ntrain loss: 0.07001531100537203\ntrain loss_segm: 0.07001531100537203\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9954345226287842, 0.8652955889701843, 0.8739786148071289]\n\ntest loss: 0.06025744649844292\ntest loss_segm: 0.06025744649844292\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960737824440002, 0.8837196230888367, 0.8891812562942505]\nBest val loss: 0.06025744649844292\nTime: 65.04535722732544\n\n\nEpoch 18/300\n\ntrain loss: 0.07013612002417256\ntrain loss_segm: 0.07013612002417256\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9954335689544678, 0.8651348948478699, 0.8736603856086731]\n\ntest loss: 0.061195671367339596\ntest loss_segm: 0.061195671367339596\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960755705833435, 0.8827746510505676, 0.8866654634475708]\nBest val loss: 0.06025744649844292\nTime: 72.24814414978027\n\n\nEpoch 19/300\n\ntrain loss: 0.06925153569613077\ntrain loss_segm: 0.06925153569613077\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9954792261123657, 0.8665846586227417, 0.8753201365470886]\n\ntest loss: 0.05940524479135489\ntest loss_segm: 0.05940524479135489\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960099458694458, 0.8826981782913208, 0.8927949070930481]\nBest val loss: 0.05940524479135489\nTime: 69.02133321762085\n\n\nEpoch 20/300\n\ntrain loss: 0.06888315517785429\ntrain loss_segm: 0.06888315517785429\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955065846443176, 0.867232620716095, 0.8758548498153687]\n\ntest loss: 0.060062447897134684\ntest loss_segm: 0.060062447897134684\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960675239562988, 0.8830598592758179, 0.8903451561927795]\nBest val loss: 0.05940524479135489\nTime: 67.41953802108765\n\n\nEpoch 21/300\n\ntrain loss: 0.0687413073698931\ntrain loss_segm: 0.0687413073698931\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955393075942993, 0.8676535487174988, 0.876032292842865]\n\ntest loss: 0.05886246416813288\ntest loss_segm: 0.05886246416813288\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959391951560974, 0.8831718564033508, 0.8933117985725403]\nBest val loss: 0.05886246416813288\nTime: 66.42694187164307\n\n\nEpoch 22/300\n\ntrain loss: 0.06847897735483284\ntrain loss_segm: 0.06847897735483284\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955320954322815, 0.86794513463974, 0.8766012787818909]\n\ntest loss: 0.059694248609817945\ntest loss_segm: 0.059694248609817945\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960783123970032, 0.8825297951698303, 0.89218670129776]\nBest val loss: 0.05886246416813288\nTime: 67.53500151634216\n\n\nEpoch 23/300\n\ntrain loss: 0.06791734544536736\ntrain loss_segm: 0.06791734544536736\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955673217773438, 0.8688605427742004, 0.8774198889732361]\n\ntest loss: 0.060118239564009204\ntest loss_segm: 0.060118239564009204\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960684180259705, 0.882550060749054, 0.8904608488082886]\nBest val loss: 0.05886246416813288\nTime: 65.40301156044006\n\n\nEpoch 24/300\n\ntrain loss: 0.06751198814356629\ntrain loss_segm: 0.06751198814356629\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955921173095703, 0.8696762323379517, 0.8781048655509949]\n\ntest loss: 0.05872564772382761\ntest loss_segm: 0.05872564772382761\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960273504257202, 0.8827665448188782, 0.8943954110145569]\nBest val loss: 0.05872564772382761\nTime: 73.67362928390503\n\n\nEpoch 25/300\n\ntrain loss: 0.06723012819980519\ntrain loss_segm: 0.06723012819980519\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995609700679779, 0.8700843453407288, 0.8786885738372803]\n\ntest loss: 0.06058391804496447\ntest loss_segm: 0.06058391804496447\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960477948188782, 0.8810652494430542, 0.8900225758552551]\nBest val loss: 0.05872564772382761\nTime: 63.67894768714905\n\n\nEpoch 26/300\n\ntrain loss: 0.06716193051277837\ntrain loss_segm: 0.06716193051277837\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9955995678901672, 0.8701010346412659, 0.8789506554603577]\n\ntest loss: 0.059414773128735714\ntest loss_segm: 0.059414773128735714\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961254596710205, 0.8846907019615173, 0.8905567526817322]\nBest val loss: 0.05872564772382761\nTime: 63.26229500770569\n\n\nEpoch 27/300\n\ntrain loss: 0.06688268874076349\ntrain loss_segm: 0.06688268874076349\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956309795379639, 0.8706979155540466, 0.879250705242157]\n\ntest loss: 0.06057690208156904\ntest loss_segm: 0.06057690208156904\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960970878601074, 0.8822091817855835, 0.8891984820365906]\nBest val loss: 0.05872564772382761\nTime: 63.366562366485596\n\n\nEpoch 28/300\n\ntrain loss: 0.0666264352164691\ntrain loss_segm: 0.0666264352164691\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956502914428711, 0.8710721135139465, 0.8797109127044678]\n\ntest loss: 0.05982754742487883\ntest loss_segm: 0.05982754742487883\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961283802986145, 0.8849110007286072, 0.8893082737922668]\nBest val loss: 0.05872564772382761\nTime: 66.22692465782166\n\n\nEpoch 29/300\n\ntrain loss: 0.06675762279973\ntrain loss_segm: 0.06675762279973\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956297278404236, 0.8708009123802185, 0.8794909119606018]\n\ntest loss: 0.06527944845266831\ntest loss_segm: 0.06527944845266831\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959205389022827, 0.876633882522583, 0.8798834085464478]\nBest val loss: 0.05872564772382761\nTime: 72.53090810775757\n\n\nEpoch 30/300\n\ntrain loss: 0.06596988385331028\ntrain loss_segm: 0.06596988385331028\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956929683685303, 0.8722647428512573, 0.8808389902114868]\n\ntest loss: 0.06130765454891401\ntest loss_segm: 0.06130765454891401\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960726499557495, 0.8826767206192017, 0.8871318101882935]\nBest val loss: 0.05872564772382761\nTime: 66.68493127822876\n\n\nEpoch 31/300\n\ntrain loss: 0.0656898851400312\ntrain loss_segm: 0.0656898851400312\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956945180892944, 0.8726292848587036, 0.8813655972480774]\n\ntest loss: 0.059148799341458544\ntest loss_segm: 0.059148799341458544\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996073305606842, 0.8825506567955017, 0.8932982087135315]\nBest val loss: 0.05872564772382761\nTime: 63.777137994766235\n\n\nEpoch 32/300\n\ntrain loss: 0.06570187229898912\ntrain loss_segm: 0.06570187229898912\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9956915974617004, 0.8726362586021423, 0.8813521862030029]\n\ntest loss: 0.05793605162165104\ntest loss_segm: 0.05793605162165104\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961308240890503, 0.8848681449890137, 0.8954232335090637]\nBest val loss: 0.05793605162165104\nTime: 64.61495423316956\n\n\nEpoch 33/300\n\ntrain loss: 0.06527126844547972\ntrain loss_segm: 0.06527126844547972\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995712161064148, 0.8732413053512573, 0.8820891380310059]\n\ntest loss: 0.058841253989017926\ntest loss_segm: 0.058841253989017926\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961313009262085, 0.8836998343467712, 0.8931573629379272]\nBest val loss: 0.05793605162165104\nTime: 64.9469587802887\n\n\nEpoch 34/300\n\ntrain loss: 0.06532636992161787\ntrain loss_segm: 0.06532636992161787\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957155585289001, 0.8732667565345764, 0.8819966316223145]\n\ntest loss: 0.06035345725906201\ntest loss_segm: 0.06035345725906201\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960395693778992, 0.8820949196815491, 0.8905139565467834]\nBest val loss: 0.05793605162165104\nTime: 62.7451708316803\n\n\nEpoch 35/300\n\ntrain loss: 0.06507803973612152\ntrain loss_segm: 0.06507803973612152\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957283139228821, 0.8736633062362671, 0.8825280070304871]\n\ntest loss: 0.06447228569632922\ntest loss_segm: 0.06447228569632922\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959202408790588, 0.8769552707672119, 0.8817774653434753]\nBest val loss: 0.05793605162165104\nTime: 62.99800252914429\n\n\nEpoch 36/300\n\ntrain loss: 0.0648358391149889\ntrain loss_segm: 0.0648358391149889\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957404732704163, 0.874077320098877, 0.8827481865882874]\n\ntest loss: 0.059360584865013756\ntest loss_segm: 0.059360584865013756\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960797429084778, 0.8820462226867676, 0.8935585021972656]\nBest val loss: 0.05793605162165104\nTime: 62.78767371177673\n\n\nEpoch 37/300\n\ntrain loss: 0.06495126725846453\ntrain loss_segm: 0.06495126725846453\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957286715507507, 0.8737130761146545, 0.8826673626899719]\n\ntest loss: 0.05731899577837724\ntest loss_segm: 0.05731899577837724\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960880875587463, 0.8846850395202637, 0.8966403603553772]\nBest val loss: 0.05731899577837724\nTime: 64.03966212272644\n\n\nEpoch 38/300\n\ntrain loss: 0.06459433524103105\ntrain loss_segm: 0.06459433524103105\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957568049430847, 0.8744995594024658, 0.8832243084907532]\n\ntest loss: 0.05905994858879309\ntest loss_segm: 0.05905994858879309\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961652755737305, 0.8857417106628418, 0.8910856246948242]\nBest val loss: 0.05731899577837724\nTime: 68.86888909339905\n\n\nEpoch 39/300\n\ntrain loss: 0.06449735329686841\ntrain loss_segm: 0.06449735329686841\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957528710365295, 0.8745841383934021, 0.8834400177001953]\n\ntest loss: 0.05770073917049628\ntest loss_segm: 0.05770073917049628\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960572719573975, 0.8842495679855347, 0.8963698148727417]\nBest val loss: 0.05731899577837724\nTime: 66.84024906158447\n\n\nEpoch 40/300\n\ntrain loss: 0.06422073974073687\ntrain loss_segm: 0.06422073974073687\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957759976387024, 0.8751155138015747, 0.8839729428291321]\n\ntest loss: 0.057306499244310916\ntest loss_segm: 0.057306499244310916\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961743354797363, 0.8857777118682861, 0.8960577249526978]\nBest val loss: 0.057306499244310916\nTime: 62.95885443687439\n\n\nEpoch 41/300\n\ntrain loss: 0.06407503043360348\ntrain loss_segm: 0.06407503043360348\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957748651504517, 0.875271737575531, 0.8841230273246765]\n\ntest loss: 0.06224993530374307\ntest loss_segm: 0.06224993530374307\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959009885787964, 0.8780370950698853, 0.8878960013389587]\nBest val loss: 0.057306499244310916\nTime: 63.743518114089966\n\n\nEpoch 42/300\n\ntrain loss: 0.06427381480041938\ntrain loss_segm: 0.06427381480041938\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957729578018188, 0.8749797940254211, 0.88386070728302]\n\ntest loss: 0.0628270471516328\ntest loss_segm: 0.0628270471516328\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959405660629272, 0.8793655037879944, 0.8850345015525818]\nBest val loss: 0.057306499244310916\nTime: 64.1445198059082\n\n\nEpoch 43/300\n\ntrain loss: 0.06389184470606755\ntrain loss_segm: 0.06389184470606755\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957827925682068, 0.8755517601966858, 0.8846035599708557]\n\ntest loss: 0.061907827567595705\ntest loss_segm: 0.061907827567595705\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960704445838928, 0.8811039328575134, 0.885768711566925]\nBest val loss: 0.057306499244310916\nTime: 65.69505882263184\n\n\nEpoch 44/300\n\ntrain loss: 0.06379647958505003\ntrain loss_segm: 0.06379647958505003\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995785653591156, 0.8756900429725647, 0.8847723603248596]\n\ntest loss: 0.05876856583815355\ntest loss_segm: 0.05876856583815355\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996125340461731, 0.8845695853233337, 0.8928130865097046]\nBest val loss: 0.057306499244310916\nTime: 65.54303431510925\n\n\nEpoch 45/300\n\ntrain loss: 0.06357026432605484\ntrain loss_segm: 0.06357026432605484\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957951307296753, 0.8760814666748047, 0.8851214051246643]\n\ntest loss: 0.058610545232509956\ntest loss_segm: 0.058610545232509956\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961923360824585, 0.8858742117881775, 0.8919392228126526]\nBest val loss: 0.057306499244310916\nTime: 72.05052065849304\n\n\nEpoch 46/300\n\ntrain loss: 0.06338983220107193\ntrain loss_segm: 0.06338983220107193\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958116412162781, 0.8763719201087952, 0.8852906823158264]\n\ntest loss: 0.058254989580466196\ntest loss_segm: 0.058254989580466196\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961468577384949, 0.8849273324012756, 0.8937327265739441]\nBest val loss: 0.057306499244310916\nTime: 65.50627207756042\n\n\nEpoch 47/300\n\ntrain loss: 0.06346488958578321\ntrain loss_segm: 0.06346488958578321\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995785117149353, 0.8761385083198547, 0.8855094313621521]\n\ntest loss: 0.057938033571610086\ntest loss_segm: 0.057938033571610086\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961500763893127, 0.8852459788322449, 0.8944422602653503]\nBest val loss: 0.057306499244310916\nTime: 63.74824261665344\n\n\nEpoch 48/300\n\ntrain loss: 0.0635387388918596\ntrain loss_segm: 0.0635387388918596\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9957972764968872, 0.8761096596717834, 0.885191023349762]\n\ntest loss: 0.05982343575511223\ntest loss_segm: 0.05982343575511223\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961453676223755, 0.8838458061218262, 0.8897855877876282]\nBest val loss: 0.057306499244310916\nTime: 65.35371971130371\n\n\nEpoch 49/300\n\ntrain loss: 0.06331705393953414\ntrain loss_segm: 0.06331705393953414\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958094358444214, 0.8764947652816772, 0.8856021761894226]\n\ntest loss: 0.05984240655715649\ntest loss_segm: 0.05984240655715649\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961031675338745, 0.8839228749275208, 0.8904037475585938]\nBest val loss: 0.057306499244310916\nTime: 63.954336166381836\n\n\nEpoch 50/300\n\ntrain loss: 0.06309488382709177\ntrain loss_segm: 0.06309488382709177\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958174228668213, 0.876832127571106, 0.8860365748405457]\n\ntest loss: 0.05761087055389698\ntest loss_segm: 0.05761087055389698\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960711598396301, 0.8829240202903748, 0.8974195718765259]\nBest val loss: 0.057306499244310916\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 76.00857472419739\n\n\nEpoch 51/300\n\ntrain loss: 0.06216352185399472\ntrain loss_segm: 0.06216352185399472\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958691000938416, 0.8784899115562439, 0.887627899646759]\n\ntest loss: 0.06090713531160966\ntest loss_segm: 0.06090713531160966\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960459470748901, 0.8819020390510559, 0.8883432149887085]\nBest val loss: 0.057306499244310916\nTime: 67.3121280670166\n\n\nEpoch 52/300\n\ntrain loss: 0.06204271637186219\ntrain loss_segm: 0.06204271637186219\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.99587082862854, 0.878610372543335, 0.8877814412117004]\n\ntest loss: 0.06521373681533031\ntest loss_segm: 0.06521373681533031\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.995779275894165, 0.8752246499061584, 0.881043553352356]\nBest val loss: 0.057306499244310916\nTime: 63.874207496643066\n\n\nEpoch 53/300\n\ntrain loss: 0.06193073980415924\ntrain loss_segm: 0.06193073980415924\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995884358882904, 0.8788436651229858, 0.8879072070121765]\n\ntest loss: 0.05757533367245625\ntest loss_segm: 0.05757533367245625\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961966276168823, 0.8869572877883911, 0.8943228125572205]\nBest val loss: 0.057306499244310916\nTime: 64.10387635231018\n\n\nEpoch 54/300\n\ntrain loss: 0.06190391644080983\ntrain loss_segm: 0.06190391644080983\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958809018135071, 0.8788965940475464, 0.8880226016044617]\n\ntest loss: 0.06417416341793843\ntest loss_segm: 0.06417416341793843\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9958981275558472, 0.8778724670410156, 0.881618082523346]\nBest val loss: 0.057306499244310916\nTime: 65.1972107887268\n\n\nEpoch 55/300\n\ntrain loss: 0.061878080279389513\ntrain loss_segm: 0.061878080279389513\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958844780921936, 0.8789389729499817, 0.8880778551101685]\n\ntest loss: 0.056646241209445856\ntest loss_segm: 0.056646241209445856\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961838126182556, 0.8871355056762695, 0.8971306085586548]\nBest val loss: 0.056646241209445856\nTime: 73.16667699813843\n\n\nEpoch 56/300\n\ntrain loss: 0.061805151756617084\ntrain loss_segm: 0.061805151756617084\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958849549293518, 0.8790358304977417, 0.8881733417510986]\n\ntest loss: 0.05623368355326164\ntest loss_segm: 0.05623368355326164\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962042570114136, 0.8871763348579407, 0.8983334302902222]\nBest val loss: 0.05623368355326164\nTime: 64.11524677276611\n\n\nEpoch 57/300\n\ntrain loss: 0.06180212000691438\ntrain loss_segm: 0.06180212000691438\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958844780921936, 0.879037618637085, 0.8882343769073486]\n\ntest loss: 0.05655241127197559\ntest loss_segm: 0.05655241127197559\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996160089969635, 0.8873302936553955, 0.8971388339996338]\nBest val loss: 0.05623368355326164\nTime: 65.38204598426819\n\n\nEpoch 58/300\n\ntrain loss: 0.0617423465923418\ntrain loss_segm: 0.0617423465923418\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958884119987488, 0.8791999220848083, 0.8883606195449829]\n\ntest loss: 0.057845709224541984\ntest loss_segm: 0.057845709224541984\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961779713630676, 0.8864952325820923, 0.8939076662063599]\nBest val loss: 0.05623368355326164\nTime: 64.41637229919434\n\n\nEpoch 59/300\n\ntrain loss: 0.06172926594279235\ntrain loss_segm: 0.06172926594279235\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958970546722412, 0.8792338371276855, 0.8882350921630859]\n\ntest loss: 0.05720089576565302\ntest loss_segm: 0.05720089576565302\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962043762207031, 0.887079656124115, 0.8952476382255554]\nBest val loss: 0.05623368355326164\nTime: 63.930413246154785\n\n\nEpoch 60/300\n\ntrain loss: 0.06163248481063903\ntrain loss_segm: 0.06163248481063903\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958915114402771, 0.8793814182281494, 0.8885074853897095]\n\ntest loss: 0.05834744612757976\ntest loss_segm: 0.05834744612757976\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996136486530304, 0.8852593898773193, 0.8933876156806946]\nBest val loss: 0.05623368355326164\nTime: 63.816699266433716\n\n\nEpoch 61/300\n\ntrain loss: 0.06172376097757605\ntrain loss_segm: 0.06172376097757605\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958911538124084, 0.8792018890380859, 0.8883501887321472]\n\ntest loss: 0.05688664785180336\ntest loss_segm: 0.05688664785180336\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961645603179932, 0.8864007592201233, 0.897057831287384]\nBest val loss: 0.05623368355326164\nTime: 63.56012272834778\n\n\nEpoch 62/300\n\ntrain loss: 0.061634638806498505\ntrain loss_segm: 0.061634638806498505\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958922863006592, 0.8793171048164368, 0.8884045481681824]\n\ntest loss: 0.056520094378636435\ntest loss_segm: 0.056520094378636435\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961909651756287, 0.8869940638542175, 0.8976765275001526]\nBest val loss: 0.05623368355326164\nTime: 66.73489332199097\n\n\nEpoch 63/300\n\ntrain loss: 0.06165599035499971\ntrain loss_segm: 0.06165599035499971\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958924651145935, 0.8792897462844849, 0.8884527683258057]\n\ntest loss: 0.05742176163655061\ntest loss_segm: 0.05742176163655061\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961647391319275, 0.8864387273788452, 0.8953368067741394]\nBest val loss: 0.05623368355326164\nTime: 66.67765831947327\n\n\nEpoch 64/300\n\ntrain loss: 0.06166091653270812\ntrain loss_segm: 0.06166091653270812\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958964586257935, 0.87932288646698, 0.8883768320083618]\n\ntest loss: 0.057800070788615786\ntest loss_segm: 0.057800070788615786\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961429238319397, 0.88577800989151, 0.8946833610534668]\nBest val loss: 0.05623368355326164\nTime: 64.14587450027466\n\n\nEpoch 65/300\n\ntrain loss: 0.06159981863596772\ntrain loss_segm: 0.06159981863596772\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9958980679512024, 0.8794142603874207, 0.8885014057159424]\n\ntest loss: 0.056414284862768956\ntest loss_segm: 0.056414284862768956\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961903095245361, 0.8875643610954285, 0.8973274230957031]\nBest val loss: 0.05623368355326164\nTime: 64.03596520423889\n\n\nEpoch 66/300\n\ntrain loss: 0.061496232789528524\ntrain loss_segm: 0.061496232789528524\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959009885787964, 0.8795614242553711, 0.8886809349060059]\n\ntest loss: 0.05620074539612501\ntest loss_segm: 0.05620074539612501\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962350130081177, 0.8878576755523682, 0.8978545069694519]\nBest val loss: 0.05620074539612501\nTime: 62.759198904037476\n\n\nEpoch 67/300\n\ntrain loss: 0.06144891974009291\ntrain loss_segm: 0.06144891974009291\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959023594856262, 0.8796408176422119, 0.8888037800788879]\n\ntest loss: 0.05700327733006233\ntest loss_segm: 0.05700327733006233\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962027072906494, 0.8868550062179565, 0.8961883783340454]\nBest val loss: 0.05620074539612501\nTime: 63.87593913078308\n\n\nEpoch 68/300\n\ntrain loss: 0.061439268031640897\ntrain loss_segm: 0.061439268031640897\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959073662757874, 0.879689633846283, 0.8888384699821472]\n\ntest loss: 0.0602358425847995\ntest loss_segm: 0.0602358425847995\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960616827011108, 0.8829214572906494, 0.8893690705299377]\nBest val loss: 0.05620074539612501\nTime: 64.74942779541016\n\n\nEpoch 69/300\n\ntrain loss: 0.0614218345454222\ntrain loss_segm: 0.0614218345454222\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959036707878113, 0.8797640204429626, 0.888902485370636]\n\ntest loss: 0.05715342811667002\ntest loss_segm: 0.05715342811667002\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961754083633423, 0.8862359523773193, 0.8965671062469482]\nBest val loss: 0.05620074539612501\nTime: 67.34326910972595\n\n\nEpoch 70/300\n\ntrain loss: 0.06146014861385279\ntrain loss_segm: 0.06146014861385279\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959008097648621, 0.8796400427818298, 0.8888270854949951]\n\ntest loss: 0.05843625217676163\ntest loss_segm: 0.05843625217676163\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960954189300537, 0.884672999382019, 0.8939124941825867]\nBest val loss: 0.05620074539612501\nTime: 64.94751358032227\n\n\nEpoch 71/300\n\ntrain loss: 0.06148360563512845\ntrain loss_segm: 0.06148360563512845\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995903491973877, 0.8795616030693054, 0.8887207508087158]\n\ntest loss: 0.05707848052947949\ntest loss_segm: 0.05707848052947949\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961829781532288, 0.887100338935852, 0.8956973552703857]\nBest val loss: 0.05620074539612501\nTime: 63.72251605987549\n\n\nEpoch 72/300\n\ntrain loss: 0.0613229077639459\ntrain loss_segm: 0.0613229077639459\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959095120429993, 0.8798061609268188, 0.8889873027801514]\n\ntest loss: 0.05681852719340569\ntest loss_segm: 0.05681852719340569\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961869716644287, 0.8870014548301697, 0.8966898918151855]\nBest val loss: 0.05620074539612501\nTime: 63.75097131729126\n\n\nEpoch 73/300\n\ntrain loss: 0.061320476540470424\ntrain loss_segm: 0.061320476540470424\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959113001823425, 0.879838764667511, 0.8889787793159485]\n\ntest loss: 0.06189317781573687\ntest loss_segm: 0.06189317781573687\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.99593585729599, 0.8797532916069031, 0.8872408866882324]\nBest val loss: 0.05620074539612501\nTime: 64.19871997833252\n\n\nEpoch 74/300\n\ntrain loss: 0.06128590384238883\ntrain loss_segm: 0.06128590384238883\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959110021591187, 0.8798574805259705, 0.8890893459320068]\n\ntest loss: 0.059055604613744296\ntest loss_segm: 0.059055604613744296\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960830807685852, 0.8843199014663696, 0.8921685814857483]\nBest val loss: 0.05620074539612501\nTime: 63.86544871330261\n\n\nEpoch 75/300\n\ntrain loss: 0.06132735241251656\ntrain loss_segm: 0.06132735241251656\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959087371826172, 0.8798728585243225, 0.8890098929405212]\n\ntest loss: 0.05965316563080519\ntest loss_segm: 0.05965316563080519\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960636496543884, 0.8831062316894531, 0.8912991285324097]\nBest val loss: 0.05620074539612501\nTime: 63.6589093208313\n\n\nEpoch 76/300\n\ntrain loss: 0.06126634396989889\ntrain loss_segm: 0.06126634396989889\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959191679954529, 0.879983127117157, 0.8890717029571533]\n\ntest loss: 0.05637674205578291\ntest loss_segm: 0.05637674205578291\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961807131767273, 0.8874558806419373, 0.8974537253379822]\nBest val loss: 0.05620074539612501\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 65.00040936470032\n\n\nEpoch 77/300\n\ntrain loss: 0.061063687847573545\ntrain loss_segm: 0.061063687847573545\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959146976470947, 0.8802211284637451, 0.8894699811935425]\n\ntest loss: 0.056004661111495435\ntest loss_segm: 0.056004661111495435\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961453676223755, 0.8868983387947083, 0.8991593718528748]\nBest val loss: 0.056004661111495435\nTime: 63.5689902305603\n\n\nEpoch 78/300\n\ntrain loss: 0.06099963499398171\ntrain loss_segm: 0.06099963499398171\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959262013435364, 0.880458414554596, 0.8895086050033569]\n\ntest loss: 0.0591499272447366\ntest loss_segm: 0.0591499272447366\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961298704147339, 0.8847556710243225, 0.8912087678909302]\nBest val loss: 0.056004661111495435\nTime: 64.98668098449707\n\n\nEpoch 79/300\n\ntrain loss: 0.0609716599500632\ntrain loss_segm: 0.0609716599500632\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995931088924408, 0.8804447650909424, 0.8897060751914978]\n\ntest loss: 0.0564395689811462\ntest loss_segm: 0.0564395689811462\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960616827011108, 0.8850736618041992, 0.8992838859558105]\nBest val loss: 0.056004661111495435\nTime: 64.18923449516296\n\n\nEpoch 80/300\n\ntrain loss: 0.06104843070895612\ntrain loss_segm: 0.06104843070895612\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959225058555603, 0.8803600668907166, 0.8894580006599426]\n\ntest loss: 0.056773724941871107\ntest loss_segm: 0.056773724941871107\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961826801300049, 0.8871879577636719, 0.8966670036315918]\nBest val loss: 0.056004661111495435\nTime: 63.55037021636963\n\n\nEpoch 81/300\n\ntrain loss: 0.06095767495092712\ntrain loss_segm: 0.06095767495092712\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959316849708557, 0.8805179595947266, 0.8896557092666626]\n\ntest loss: 0.05697239744357574\ntest loss_segm: 0.05697239744357574\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962007403373718, 0.8869490027427673, 0.8962153792381287]\nBest val loss: 0.056004661111495435\nTime: 65.43127489089966\n\n\nEpoch 82/300\n\ntrain loss: 0.06096782483443429\ntrain loss_segm: 0.06096782483443429\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959319829940796, 0.8804782629013062, 0.8896328210830688]\n\ntest loss: 0.05748824393137907\ntest loss_segm: 0.05748824393137907\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961715340614319, 0.88638836145401, 0.8950329422950745]\nBest val loss: 0.056004661111495435\nTime: 68.79242753982544\n\n\nEpoch 83/300\n\ntrain loss: 0.06096958930167971\ntrain loss_segm: 0.06096958930167971\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959341287612915, 0.8804895877838135, 0.8896529674530029]\n\ntest loss: 0.056181205675387993\ntest loss_segm: 0.056181205675387993\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961972236633301, 0.8876631259918213, 0.8980210423469543]\nBest val loss: 0.056004661111495435\nTime: 63.9823522567749\n\n\nEpoch 84/300\n\ntrain loss: 0.060986444967079764\ntrain loss_segm: 0.060986444967079764\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959254264831543, 0.8803622126579285, 0.8895626664161682]\n\ntest loss: 0.057398416579533845\ntest loss_segm: 0.057398416579533845\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961592555046082, 0.8866479396820068, 0.8950099349021912]\nBest val loss: 0.056004661111495435\nTime: 64.32663679122925\n\n\nEpoch 85/300\n\ntrain loss: 0.060992115141847464\ntrain loss_segm: 0.060992115141847464\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959352016448975, 0.8804790377616882, 0.8894938230514526]\n\ntest loss: 0.06370144098615035\ntest loss_segm: 0.06370144098615035\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9958304166793823, 0.877368152141571, 0.8837684988975525]\nBest val loss: 0.056004661111495435\nTime: 64.17470860481262\n\n\nEpoch 86/300\n\ntrain loss: 0.060960161558623556\ntrain loss_segm: 0.060960161558623556\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959250092506409, 0.8804508447647095, 0.8896432518959045]\n\ntest loss: 0.056891787415131546\ntest loss_segm: 0.056891787415131546\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962064027786255, 0.8874786496162415, 0.8959420919418335]\nBest val loss: 0.056004661111495435\nTime: 64.76953935623169\n\n\nEpoch 87/300\n\ntrain loss: 0.06096442142807985\ntrain loss_segm: 0.06096442142807985\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959313273429871, 0.8804764747619629, 0.8895143866539001]\n\ntest loss: 0.05603608288444006\ntest loss_segm: 0.05603608288444006\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961874485015869, 0.887536346912384, 0.8985406756401062]\nBest val loss: 0.056004661111495435\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 65.89597129821777\n\n\nEpoch 88/300\n\ntrain loss: 0.060878057315757\ntrain loss_segm: 0.060878057315757\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959349632263184, 0.880620539188385, 0.8898290991783142]\n\ntest loss: 0.05930416209575457\ntest loss_segm: 0.05930416209575457\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960939288139343, 0.8842283487319946, 0.8912158012390137]\nBest val loss: 0.056004661111495435\nTime: 62.70058250427246\n\n\nEpoch 89/300\n\ntrain loss: 0.060950496010010756\ntrain loss_segm: 0.060950496010010756\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959326386451721, 0.8805481195449829, 0.8896093368530273]\n\ntest loss: 0.05607059464240686\ntest loss_segm: 0.05607059464240686\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962255954742432, 0.8880119323730469, 0.8980722427368164]\nBest val loss: 0.056004661111495435\nTime: 62.90764260292053\n\n\nEpoch 90/300\n\ntrain loss: 0.06091300813080389\ntrain loss_segm: 0.06091300813080389\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959322810173035, 0.8805111050605774, 0.8896751999855042]\n\ntest loss: 0.056802565852801\ntest loss_segm: 0.056802565852801\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961876273155212, 0.8872763514518738, 0.8964956402778625]\nBest val loss: 0.056004661111495435\nTime: 64.50333976745605\n\n\nEpoch 91/300\n\ntrain loss: 0.060935269876182835\ntrain loss_segm: 0.060935269876182835\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959268569946289, 0.8805260062217712, 0.8897208571434021]\n\ntest loss: 0.060284642359385125\ntest loss_segm: 0.060284642359385125\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960231184959412, 0.8825445175170898, 0.8897728323936462]\nBest val loss: 0.056004661111495435\nTime: 64.60814571380615\n\n\nEpoch 92/300\n\ntrain loss: 0.060812158627977855\ntrain loss_segm: 0.060812158627977855\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959378838539124, 0.8807396292686462, 0.8899325728416443]\n\ntest loss: 0.06451648406875439\ntest loss_segm: 0.06451648406875439\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9957630634307861, 0.875815749168396, 0.882713794708252]\nBest val loss: 0.056004661111495435\nTime: 65.27450037002563\n\n\nEpoch 93/300\n\ntrain loss: 0.060928636902495274\ntrain loss_segm: 0.060928636902495274\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959326386451721, 0.8805034756660461, 0.8896727561950684]\n\ntest loss: 0.056350433482573584\ntest loss_segm: 0.056350433482573584\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962132573127747, 0.88783198595047, 0.8974074125289917]\nBest val loss: 0.056004661111495435\nTime: 64.68233513832092\n\n\nEpoch 94/300\n\ntrain loss: 0.06091915936315361\ntrain loss_segm: 0.06091915936315361\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959291815757751, 0.8805468678474426, 0.8897154331207275]\n\ntest loss: 0.06106331027471102\ntest loss_segm: 0.06106331027471102\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960145950317383, 0.8817265629768372, 0.8879485726356506]\nBest val loss: 0.056004661111495435\nTime: 65.4684693813324\n\n\nEpoch 95/300\n\ntrain loss: 0.06089884382259997\ntrain loss_segm: 0.06089884382259997\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959341883659363, 0.8806024789810181, 0.8897599577903748]\n\ntest loss: 0.05595445365477831\ntest loss_segm: 0.05595445365477831\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962176084518433, 0.8881474137306213, 0.8982940316200256]\nBest val loss: 0.05595445365477831\nTime: 65.01510286331177\n\n\nEpoch 96/300\n\ntrain loss: 0.060883651782251615\ntrain loss_segm: 0.060883651782251615\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959349036216736, 0.880630373954773, 0.8897061944007874]\n\ntest loss: 0.05607702535314438\ntest loss_segm: 0.05607702535314438\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962222576141357, 0.8877944946289062, 0.8982893824577332]\nBest val loss: 0.05595445365477831\nTime: 66.70006251335144\n\n\nEpoch 97/300\n\ntrain loss: 0.06093067180695413\ntrain loss_segm: 0.06093067180695413\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959338307380676, 0.8805825710296631, 0.8896710276603699]\n\ntest loss: 0.05583565414716036\ntest loss_segm: 0.05583565414716036\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996185302734375, 0.8873051404953003, 0.8992953300476074]\nBest val loss: 0.05583565414716036\nTime: 66.43280124664307\n\n\nEpoch 98/300\n\ntrain loss: 0.06083700053771086\ntrain loss_segm: 0.06083700053771086\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959352016448975, 0.8807116150856018, 0.889898955821991]\n\ntest loss: 0.06527360232594685\ntest loss_segm: 0.06527360232594685\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9957627654075623, 0.8753236532211304, 0.8807984590530396]\nBest val loss: 0.05583565414716036\nTime: 64.05582165718079\n\n\nEpoch 99/300\n\ntrain loss: 0.06085531137695041\ntrain loss_segm: 0.06085531137695041\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959372282028198, 0.8806766271591187, 0.8898605108261108]\n\ntest loss: 0.05721869386541538\ntest loss_segm: 0.05721869386541538\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961832761764526, 0.886920154094696, 0.8954381942749023]\nBest val loss: 0.05583565414716036\nTime: 64.92503499984741\n\n\nEpoch 100/300\n\ntrain loss: 0.060868489897892446\ntrain loss_segm: 0.060868489897892446\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959352612495422, 0.8807197213172913, 0.8897742629051208]\n\ntest loss: 0.060390191487012766\ntest loss_segm: 0.060390191487012766\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960101246833801, 0.8821603059768677, 0.8898777365684509]\nBest val loss: 0.05583565414716036\nTime: 64.55167961120605\n\n\nEpoch 101/300\n\ntrain loss: 0.06082373112440109\ntrain loss_segm: 0.06082373112440109\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959374070167542, 0.8806857466697693, 0.8898642063140869]\n\ntest loss: 0.05703687448150072\ntest loss_segm: 0.05703687448150072\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961956739425659, 0.8873483538627625, 0.8955813646316528]\nBest val loss: 0.05583565414716036\nTime: 65.92158603668213\n\n\nEpoch 102/300\n\ntrain loss: 0.06084680882624433\ntrain loss_segm: 0.06084680882624433\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959366917610168, 0.880666196346283, 0.8897915482521057]\n\ntest loss: 0.05949950886842532\ntest loss_segm: 0.05949950886842532\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960612654685974, 0.8836644291877747, 0.8912255764007568]\nBest val loss: 0.05583565414716036\nTime: 64.81551313400269\n\n\nEpoch 103/300\n\ntrain loss: 0.06088280548117583\ntrain loss_segm: 0.06088280548117583\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959372282028198, 0.8806778192520142, 0.8897268772125244]\n\ntest loss: 0.05691180874903997\ntest loss_segm: 0.05691180874903997\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961955547332764, 0.887365460395813, 0.8959569931030273]\nBest val loss: 0.05583565414716036\nTime: 73.24253129959106\n\n\nEpoch 104/300\n\ntrain loss: 0.06093156738560411\ntrain loss_segm: 0.06093156738560411\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995931088924408, 0.8805536031723022, 0.8897042870521545]\n\ntest loss: 0.057550573005126074\ntest loss_segm: 0.057550573005126074\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996177077293396, 0.8865227103233337, 0.8947029113769531]\nBest val loss: 0.05583565414716036\nTime: 77.3973867893219\n\n\nEpoch 105/300\n\ntrain loss: 0.06081458251876167\ntrain loss_segm: 0.06081458251876167\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959405660629272, 0.880788266658783, 0.8899234533309937]\n\ntest loss: 0.062200896155375704\ntest loss_segm: 0.062200896155375704\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959462285041809, 0.8799740672111511, 0.8860333561897278]\nBest val loss: 0.05583565414716036\nTime: 65.42172169685364\n\n\nEpoch 106/300\n\ntrain loss: 0.060890052019606664\ntrain loss_segm: 0.060890052019606664\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.99593186378479, 0.8806024789810181, 0.8897714018821716]\n\ntest loss: 0.06245996678868929\ntest loss_segm: 0.06245996678868929\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959132075309753, 0.8794327974319458, 0.8857089281082153]\nBest val loss: 0.05583565414716036\nTime: 64.54580020904541\n\n\nEpoch 107/300\n\ntrain loss: 0.06086780182734321\ntrain loss_segm: 0.06086780182734321\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995944082736969, 0.8807066679000854, 0.889708399772644]\n\ntest loss: 0.06005721653883274\ntest loss_segm: 0.06005721653883274\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960536360740662, 0.883003830909729, 0.8900977969169617]\nBest val loss: 0.05583565414716036\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 64.92498207092285\n\n\nEpoch 108/300\n\ntrain loss: 0.06082959694764282\ntrain loss_segm: 0.06082959694764282\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959391355514526, 0.8807924389839172, 0.8898134827613831]\n\ntest loss: 0.05890984164598661\ntest loss_segm: 0.05890984164598661\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961467385292053, 0.885168194770813, 0.8915225267410278]\nBest val loss: 0.05583565414716036\nTime: 64.4923448562622\n\n\nEpoch 109/300\n\ntrain loss: 0.060841383129546914\ntrain loss_segm: 0.060841383129546914\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959324598312378, 0.8806777000427246, 0.8898723721504211]\n\ntest loss: 0.055789211144049965\ntest loss_segm: 0.055789211144049965\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962264895439148, 0.8879031538963318, 0.8989315629005432]\nBest val loss: 0.055789211144049965\nTime: 63.988524436950684\n\n\nEpoch 110/300\n\ntrain loss: 0.060891128887858574\ntrain loss_segm: 0.060891128887858574\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959312081336975, 0.8805719614028931, 0.8897860646247864]\n\ntest loss: 0.05725318785661306\ntest loss_segm: 0.05725318785661306\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961951971054077, 0.8869274854660034, 0.8953334093093872]\nBest val loss: 0.055789211144049965\nTime: 65.03228855133057\n\n\nEpoch 111/300\n\ntrain loss: 0.06088642787801314\ntrain loss_segm: 0.06088642787801314\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959356784820557, 0.8806765675544739, 0.8897386193275452]\n\ntest loss: 0.05698872395814993\ntest loss_segm: 0.05698872395814993\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961897134780884, 0.8873205184936523, 0.8957205414772034]\nBest val loss: 0.055789211144049965\nTime: 65.68318152427673\n\n\nEpoch 112/300\n\ntrain loss: 0.060915007197026964\ntrain loss_segm: 0.060915007197026964\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959322810173035, 0.88066565990448, 0.8897974491119385]\n\ntest loss: 0.05671242156471962\ntest loss_segm: 0.05671242156471962\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961997866630554, 0.8873695731163025, 0.8966290354728699]\nBest val loss: 0.055789211144049965\nTime: 64.99039721488953\n\n\nEpoch 113/300\n\ntrain loss: 0.06084257137926319\ntrain loss_segm: 0.06084257137926319\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959332346916199, 0.8806496262550354, 0.8898407220840454]\n\ntest loss: 0.05581057854951956\ntest loss_segm: 0.05581057854951956\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962146282196045, 0.8878709077835083, 0.8989501595497131]\nBest val loss: 0.055789211144049965\nTime: 65.23170065879822\n\n\nEpoch 114/300\n\ntrain loss: 0.0608487547245584\ntrain loss_segm: 0.0608487547245584\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959353804588318, 0.880739152431488, 0.8898111581802368]\n\ntest loss: 0.05802849087959681\ntest loss_segm: 0.05802849087959681\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961428642272949, 0.8857197761535645, 0.8939335942268372]\nBest val loss: 0.055789211144049965\nTime: 64.50201535224915\n\n\nEpoch 115/300\n\ntrain loss: 0.06083122296612474\ntrain loss_segm: 0.06083122296612474\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995943009853363, 0.8808311820030212, 0.8898441195487976]\n\ntest loss: 0.05767373950817646\ntest loss_segm: 0.05767373950817646\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961689710617065, 0.8863263130187988, 0.8944846987724304]\nBest val loss: 0.055789211144049965\nTime: 65.8754153251648\n\n\nEpoch 116/300\n\ntrain loss: 0.06083220297682889\ntrain loss_segm: 0.06083220297682889\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959387183189392, 0.8807429075241089, 0.8899191617965698]\n\ntest loss: 0.05619505000038025\ntest loss_segm: 0.05619505000038025\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962089657783508, 0.887863278388977, 0.8978461027145386]\nBest val loss: 0.055789211144049965\nTime: 66.1864287853241\n\n\nEpoch 117/300\n\ntrain loss: 0.060827268075339405\ntrain loss_segm: 0.060827268075339405\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959439635276794, 0.8807586431503296, 0.8898164629936218]\n\ntest loss: 0.05655242302096807\ntest loss_segm: 0.05655242302096807\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996210515499115, 0.887607991695404, 0.8968939185142517]\nBest val loss: 0.055789211144049965\nTime: 83.72890663146973\n\n\nEpoch 118/300\n\ntrain loss: 0.060839274473771264\ntrain loss_segm: 0.060839274473771264\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959338307380676, 0.8806074261665344, 0.8898821473121643]\n\ntest loss: 0.05663415168722471\ntest loss_segm: 0.05663415168722471\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9962242245674133, 0.8877485394477844, 0.8965222239494324]\nBest val loss: 0.055789211144049965\nTime: 81.57003426551819\n\n\nEpoch 119/300\n\ntrain loss: 0.060822299363303786\ntrain loss_segm: 0.060822299363303786\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995936930179596, 0.8807785511016846, 0.889838695526123]\n\ntest loss: 0.05605297592970041\ntest loss_segm: 0.05605297592970041\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961588978767395, 0.8862361311912537, 0.899455726146698]\nBest val loss: 0.055789211144049965\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 64.38702178001404\n\n\nEpoch 120/300\n\ntrain loss: 0.06083777596395981\ntrain loss_segm: 0.06083777596395981\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959372282028198, 0.880766749382019, 0.8898543119430542]\n\ntest loss: 0.061225492698259845\ntest loss_segm: 0.061225492698259845\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959799647331238, 0.8811772465705872, 0.8880453109741211]\nBest val loss: 0.055789211144049965\nTime: 64.19848251342773\n\n\nEpoch 121/300\n\ntrain loss: 0.06085844467618043\ntrain loss_segm: 0.06085844467618043\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959349632263184, 0.8806974291801453, 0.8897887468338013]\n\ntest loss: 0.056244283150403925\ntest loss_segm: 0.056244283150403925\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961866736412048, 0.8877080082893372, 0.8977643251419067]\nBest val loss: 0.055789211144049965\nTime: 64.41810989379883\n\n\nEpoch 122/300\n\ntrain loss: 0.060863669372246236\ntrain loss_segm: 0.060863669372246236\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.995941162109375, 0.8806372284889221, 0.8897714018821716]\n\ntest loss: 0.05913003992575865\ntest loss_segm: 0.05913003992575865\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9960862398147583, 0.8842217922210693, 0.8918402194976807]\nBest val loss: 0.055789211144049965\nTime: 67.59717106819153\n\n\nEpoch 123/300\n\ntrain loss: 0.06084938109203984\ntrain loss_segm: 0.06084938109203984\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959344267845154, 0.8806915283203125, 0.8898587822914124]\n\ntest loss: 0.06399287445805012\ntest loss_segm: 0.06399287445805012\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9958030581474304, 0.8768278360366821, 0.8833941221237183]\nBest val loss: 0.055789211144049965\nTime: 81.66792845726013\n\n\nEpoch 124/300\n\ntrain loss: 0.060783702998033054\ntrain loss_segm: 0.060783702998033054\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959444403648376, 0.8808808922767639, 0.8899438381195068]\n\ntest loss: 0.05583022821408052\ntest loss_segm: 0.05583022821408052\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961943030357361, 0.8874783515930176, 0.8992522954940796]\nBest val loss: 0.055789211144049965\nTime: 66.39768075942993\n\n\nEpoch 125/300\n\ntrain loss: 0.06084039536175094\ntrain loss_segm: 0.06084039536175094\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959344267845154, 0.880649209022522, 0.8898196816444397]\n\ntest loss: 0.0558943238395911\ntest loss_segm: 0.0558943238395911\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961921572685242, 0.8878276348114014, 0.8987049460411072]\nBest val loss: 0.055789211144049965\nTime: 64.29173946380615\n\n\nEpoch 126/300\n\ntrain loss: 0.06095563726428943\ntrain loss_segm: 0.06095563726428943\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959293603897095, 0.8804652690887451, 0.8895953893661499]\n\ntest loss: 0.05998317706279266\ntest loss_segm: 0.05998317706279266\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996029794216156, 0.8829132318496704, 0.890414297580719]\nBest val loss: 0.055789211144049965\nTime: 63.78837466239929\n\n\nEpoch 127/300\n\ntrain loss: 0.060889432301061065\ntrain loss_segm: 0.060889432301061065\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959352612495422, 0.8806328177452087, 0.8897522687911987]\n\ntest loss: 0.05605580686376645\ntest loss_segm: 0.05605580686376645\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996211051940918, 0.8879870772361755, 0.8981403708457947]\nBest val loss: 0.055789211144049965\nTime: 64.55065822601318\n\n\nEpoch 128/300\n\ntrain loss: 0.060853831800101676\ntrain loss_segm: 0.060853831800101676\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959316253662109, 0.8806639313697815, 0.8898307681083679]\n\ntest loss: 0.057625855008761086\ntest loss_segm: 0.057625855008761086\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996180534362793, 0.8864383101463318, 0.8945556879043579]\nBest val loss: 0.055789211144049965\nTime: 73.77405261993408\n\n\nEpoch 129/300\n\ntrain loss: 0.06079839009650146\ntrain loss_segm: 0.06079839009650146\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959409832954407, 0.8808844685554504, 0.8899290561676025]\n\ntest loss: 0.05750934216074455\ntest loss_segm: 0.05750934216074455\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.996167004108429, 0.8864293098449707, 0.8948708772659302]\nBest val loss: 0.055789211144049965\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 88.88987803459167\n\n\nEpoch 130/300\n\ntrain loss: 0.06081945840505105\ntrain loss_segm: 0.06081945840505105\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959389567375183, 0.8807327747344971, 0.8898687362670898]\n\ntest loss: 0.05643533724240768\ntest loss_segm: 0.05643533724240768\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9961014986038208, 0.8850422501564026, 0.8992860317230225]\nBest val loss: 0.055789211144049965\nTime: 86.61692261695862\n\n\nEpoch 131/300\n\ntrain loss: 0.06085511987816684\ntrain loss_segm: 0.06085511987816684\ntrain loss_shape: 0.0\ntrain loss_recon: 0.0\ntrain unet DSC: [0.9959337115287781, 0.8806267976760864, 0.8897987008094788]\n\ntest loss: 0.06097855772345494\ntest loss_segm: 0.06097855772345494\ntest loss_shape: 0.0\ntest loss_recon: 0.0\ntest unet DSC: [0.9959788918495178, 0.881541907787323, 0.8884639143943787]\nBest val loss: 0.055789211144049965\nTime: 81.34277367591858\n\nValidation loss stopped to decrease for 30 epochs. Training terminated.\nBest epoch: 109\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678428146802
        }
      },
      "id": "8b977631-4f72-4cf3-b410-800eedbbecd0"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.01, lambda_recon=0.01,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/300\n\ntrain loss: 0.29476251820974714\ntrain loss_segm: 0.29476251820974714\ntrain loss_shape: 0.20519594933036006\ntrain loss_recon: 0.43805305678633194\ntrain unet DSC: [0.9579780697822571, 0.5785843729972839, 0.5882948637008667]\n\ntest loss: 0.13700910294667268\ntest loss_segm: 0.13700910294667268\ntest loss_shape: 0.10304195032669948\ntest loss_recon: 0.2597254820359059\ntest unet DSC: [0.9906886219978333, 0.765700101852417, 0.7974656820297241]\nBest val loss: 0.13700910294667268\nTime: 86.2115626335144\n\n\nEpoch 2/300\n\ntrain loss: 0.13499636745339708\ntrain loss_segm: 0.13499636745339708\ntrain loss_shape: 0.1342805171314674\ntrain loss_recon: 0.28173125318334075\ntrain unet DSC: [0.9912410974502563, 0.7697640061378479, 0.7840234041213989]\n\ntest loss: 0.10009106000264485\ntest loss_segm: 0.10009106000264485\ntest loss_shape: 0.07873722547904038\ntest loss_recon: 0.20848735784872985\ntest unet DSC: [0.9929248094558716, 0.8198614120483398, 0.8447510004043579]\nBest val loss: 0.10009106000264485\nTime: 87.46421527862549\n\n\nEpoch 3/300\n\ntrain loss: 0.11060568487531022\ntrain loss_segm: 0.11060568487531022\ntrain loss_shape: 0.10169628828386718\ntrain loss_recon: 0.23114606738090515\ntrain unet DSC: [0.9930992722511292, 0.8070318698883057, 0.8160373568534851]\n\ntest loss: 0.09152577263422501\ntest loss_segm: 0.09152577263422501\ntest loss_shape: 0.06219005164427635\ntest loss_recon: 0.16794314139928573\ntest unet DSC: [0.9945193529129028, 0.8385401964187622, 0.8404831290245056]\nBest val loss: 0.09152577263422501\nTime: 90.10895109176636\n\n\nEpoch 4/300\n\ntrain loss: 0.09779652189227599\ntrain loss_segm: 0.09779652189227599\ntrain loss_shape: 0.08421904782328425\ntrain loss_recon: 0.20002608020094376\ntrain unet DSC: [0.9939625263214111, 0.8266299962997437, 0.8341449499130249]\n\ntest loss: 0.08552052195255573\ntest loss_segm: 0.08552052195255573\ntest loss_shape: 0.05652266492446264\ntest loss_recon: 0.1556642193060655\ntest unet DSC: [0.9948875308036804, 0.846247136592865, 0.8558862209320068]\nBest val loss: 0.08552052195255573\nTime: 88.0042712688446\n\n\nEpoch 5/300\n\ntrain loss: 0.091211582663693\ntrain loss_segm: 0.091211582663693\ntrain loss_shape: 0.07430905542230304\ntrain loss_recon: 0.18254099842868274\ntrain unet DSC: [0.9943480491638184, 0.8359485268592834, 0.8436620235443115]\n\ntest loss: 0.07718695853001033\ntest loss_segm: 0.07718695853001033\ntest loss_shape: 0.05054113268852234\ntest loss_recon: 0.14495300482480955\ntest unet DSC: [0.9953008890151978, 0.8609834313392639, 0.8650239109992981]\nBest val loss: 0.07718695853001033\nTime: 88.09233808517456\n\n\nEpoch 6/300\n\ntrain loss: 0.08663267590388467\ntrain loss_segm: 0.08663267590388467\ntrain loss_shape: 0.06786636105160924\ntrain loss_recon: 0.17076438061798674\ntrain unet DSC: [0.9946238994598389, 0.8428922295570374, 0.8505405187606812]\n\ntest loss: 0.07083324304757974\ntest loss_segm: 0.07083324304757974\ntest loss_shape: 0.04996461746020195\ntest loss_recon: 0.14128738030409202\ntest unet DSC: [0.9953967332839966, 0.8694125413894653, 0.87688809633255]\nBest val loss: 0.07083324304757974\nTime: 86.33763313293457\n\n\nEpoch 7/300\n\ntrain loss: 0.08327219210848023\ntrain loss_segm: 0.08327219210848023\ntrain loss_shape: 0.0633025230674804\ntrain loss_recon: 0.16207665619970876\ntrain unet DSC: [0.9947887659072876, 0.8477197289466858, 0.8556451201438904]\n\ntest loss: 0.06747239980942164\ntest loss_segm: 0.06747239980942164\ntest loss_shape: 0.04780009732796596\ntest loss_recon: 0.13738271976128602\ntest unet DSC: [0.9953610301017761, 0.8693852424621582, 0.886586606502533]\nBest val loss: 0.06747239980942164\nTime: 87.54760456085205\n\n\nEpoch 8/300\n\ntrain loss: 0.08129759698728972\ntrain loss_segm: 0.08129759698728972\ntrain loss_shape: 0.06044270499031755\ntrain loss_recon: 0.15726611885843397\ntrain unet DSC: [0.9948933124542236, 0.8504506349563599, 0.858735978603363]\n\ntest loss: 0.06554596957105857\ntest loss_segm: 0.06554596957105857\ntest loss_shape: 0.04510379840548222\ntest loss_recon: 0.13126326065797073\ntest unet DSC: [0.995661735534668, 0.875135064125061, 0.8871158957481384]\nBest val loss: 0.06554596957105857\nTime: 88.39187479019165\n\n\nEpoch 9/300\n\ntrain loss: 0.07896779581338545\ntrain loss_segm: 0.07896779581338545\ntrain loss_shape: 0.05761590050651303\ntrain loss_recon: 0.1515612587144103\ntrain unet DSC: [0.9950387477874756, 0.8541082143783569, 0.862342357635498]\n\ntest loss: 0.06630403013565601\ntest loss_segm: 0.06630403013565601\ntest loss_shape: 0.04468341811727255\ntest loss_recon: 0.12949587595768464\ntest unet DSC: [0.9957377314567566, 0.8747445344924927, 0.8847142457962036]\nBest val loss: 0.06554596957105857\nTime: 86.1061041355133\n\n\nEpoch 10/300\n\ntrain loss: 0.07779358963989004\ntrain loss_segm: 0.07779358963989004\ntrain loss_shape: 0.055912504161271866\ntrain loss_recon: 0.1488498883156837\ntrain unet DSC: [0.9950960278511047, 0.8559014201164246, 0.864159107208252]\n\ntest loss: 0.06469083730226909\ntest loss_segm: 0.06469083730226909\ntest loss_shape: 0.04477746307085722\ntest loss_recon: 0.1302703084089817\ntest unet DSC: [0.9956030249595642, 0.8750007748603821, 0.8891820907592773]\nBest val loss: 0.06469083730226909\nTime: 86.20318865776062\n\n\nEpoch 11/300\n\ntrain loss: 0.0760256064277661\ntrain loss_segm: 0.0760256064277661\ntrain loss_shape: 0.05407796010280712\ntrain loss_recon: 0.14492709953573685\ntrain unet DSC: [0.9952093362808228, 0.8586724400520325, 0.8670903444290161]\n\ntest loss: 0.06303888912766407\ntest loss_segm: 0.06303888912766407\ntest loss_shape: 0.04263306342256375\ntest loss_recon: 0.12592010773145235\ntest unet DSC: [0.9958065748214722, 0.8778784275054932, 0.8910537958145142]\nBest val loss: 0.06303888912766407\nTime: 86.24087452888489\n\n\nEpoch 12/300\n\ntrain loss: 0.07511169685027268\ntrain loss_segm: 0.07511169685027268\ntrain loss_shape: 0.05295386721816244\ntrain loss_recon: 0.14282138890857937\ntrain unet DSC: [0.9952371120452881, 0.8599469065666199, 0.8685696721076965]\n\ntest loss: 0.07191077447854556\ntest loss_segm: 0.07191077447854556\ntest loss_shape: 0.045357516942880094\ntest loss_recon: 0.13231435800210023\ntest unet DSC: [0.9958921670913696, 0.8731729388237, 0.8649026155471802]\nBest val loss: 0.06303888912766407\nTime: 87.83964610099792\n\n\nEpoch 13/300\n\ntrain loss: 0.07419357019700581\ntrain loss_segm: 0.07419357019700581\ntrain loss_shape: 0.05178488941886757\ntrain loss_recon: 0.14072495925275585\ntrain unet DSC: [0.9953121542930603, 0.8615245819091797, 0.8698423504829407]\n\ntest loss: 0.06382303054516132\ntest loss_segm: 0.06382303054516132\ntest loss_shape: 0.040580286333958306\ntest loss_recon: 0.12161338940644875\ntest unet DSC: [0.9960163831710815, 0.8805108666419983, 0.8856346011161804]\nBest val loss: 0.06303888912766407\nTime: 87.68740200996399\n\n\nEpoch 14/300\n\ntrain loss: 0.07339072515127025\ntrain loss_segm: 0.07339072515127025\ntrain loss_shape: 0.05093818476211421\ntrain loss_recon: 0.13930247931540768\ntrain unet DSC: [0.99535071849823, 0.8627265095710754, 0.8713636994361877]\n\ntest loss: 0.06322151269668187\ntest loss_segm: 0.06322151269668187\ntest loss_shape: 0.04139822664169165\ntest loss_recon: 0.1228808256296011\ntest unet DSC: [0.9959660172462463, 0.8805917501449585, 0.8875059485435486]\nBest val loss: 0.06303888912766407\nTime: 86.20310473442078\n\n\nEpoch 15/300\n\ntrain loss: 0.07273236167015909\ntrain loss_segm: 0.07273236167015909\ntrain loss_shape: 0.050140007127878035\ntrain loss_recon: 0.13764981114411656\ntrain unet DSC: [0.995396077632904, 0.8638680577278137, 0.8722683191299438]\n\ntest loss: 0.0629265159368515\ntest loss_segm: 0.0629265159368515\ntest loss_shape: 0.04155916204819313\ntest loss_recon: 0.12277087492820545\ntest unet DSC: [0.995915412902832, 0.8803899884223938, 0.888931393623352]\nBest val loss: 0.0629265159368515\nTime: 86.14643359184265\n\n\nEpoch 16/300\n\ntrain loss: 0.07215520503777492\ntrain loss_segm: 0.07215520503777492\ntrain loss_shape: 0.04953065153824378\ntrain loss_recon: 0.1365975575356544\ntrain unet DSC: [0.9954270720481873, 0.864709198474884, 0.8732684254646301]\n\ntest loss: 0.0631451174043692\ntest loss_segm: 0.0631451174043692\ntest loss_shape: 0.04030267454874821\ntest loss_recon: 0.12129631256445861\ntest unet DSC: [0.9960950016975403, 0.8822924494743347, 0.8856532573699951]\nBest val loss: 0.0629265159368515\nTime: 85.90143299102783\n\n\nEpoch 17/300\n\ntrain loss: 0.0719966511654703\ntrain loss_segm: 0.0719966511654703\ntrain loss_shape: 0.04919309098320671\ntrain loss_recon: 0.13626705468455447\ntrain unet DSC: [0.9954360723495483, 0.8650066256523132, 0.8734895586967468]\n\ntest loss: 0.06180706295447472\ntest loss_segm: 0.06180706295447472\ntest loss_shape: 0.03916159238761816\ntest loss_recon: 0.11949283495927468\ntest unet DSC: [0.9961006045341492, 0.8833944797515869, 0.8887608647346497]\nBest val loss: 0.06180706295447472\nTime: 85.64980864524841\n\n\nEpoch 18/300\n\ntrain loss: 0.07104299384864825\ntrain loss_segm: 0.07104299384864825\ntrain loss_shape: 0.0483609806463311\ntrain loss_recon: 0.13433277116546147\ntrain unet DSC: [0.99549800157547, 0.8666546940803528, 0.8750705718994141]\n\ntest loss: 0.06055901849117035\ntest loss_segm: 0.06055901849117035\ntest loss_shape: 0.03887528085555786\ntest loss_recon: 0.11803880563149086\ntest unet DSC: [0.996070384979248, 0.8828113079071045, 0.8936936855316162]\nBest val loss: 0.06055901849117035\nTime: 86.12585306167603\n\n\nEpoch 19/300\n\ntrain loss: 0.07063161157354524\ntrain loss_segm: 0.07063161157354524\ntrain loss_shape: 0.04784792718253558\ntrain loss_recon: 0.13353096910669834\ntrain unet DSC: [0.9955152869224548, 0.8673073649406433, 0.8757967352867126]\n\ntest loss: 0.06352381799847652\ntest loss_segm: 0.06352381799847652\ntest loss_shape: 0.039550927109443225\ntest loss_recon: 0.12096054126054813\ntest unet DSC: [0.9961234927177429, 0.8827071785926819, 0.883878767490387]\nBest val loss: 0.06055901849117035\nTime: 85.80573678016663\n\n\nEpoch 20/300\n\ntrain loss: 0.07018228327926201\ntrain loss_segm: 0.07018228327926201\ntrain loss_shape: 0.04746148716422576\ntrain loss_recon: 0.13287742982936812\ntrain unet DSC: [0.9955377578735352, 0.8679659962654114, 0.8766552805900574]\n\ntest loss: 0.06420202467304009\ntest loss_segm: 0.06420202467304009\ntest loss_shape: 0.04018064091602961\ntest loss_recon: 0.12155510829045223\ntest unet DSC: [0.9960522055625916, 0.8808300495147705, 0.883016049861908]\nBest val loss: 0.06055901849117035\nTime: 86.12479948997498\n\n\nEpoch 21/300\n\ntrain loss: 0.07032140679185904\ntrain loss_segm: 0.07032140679185904\ntrain loss_shape: 0.04734098706158656\ntrain loss_recon: 0.13290078315553785\ntrain unet DSC: [0.9955494403839111, 0.8678615689277649, 0.8762250542640686]\n\ntest loss: 0.06548760277338517\ntest loss_segm: 0.06548760277338517\ntest loss_shape: 0.04008393534100973\ntest loss_recon: 0.12241924878878471\ntest unet DSC: [0.9959960579872131, 0.8780255317687988, 0.881801187992096]\nBest val loss: 0.06055901849117035\nTime: 86.92570924758911\n\n\nEpoch 22/300\n\ntrain loss: 0.0692748668827588\ntrain loss_segm: 0.0692748668827588\ntrain loss_shape: 0.046514549493035184\ntrain loss_recon: 0.13101699457892887\ntrain unet DSC: [0.9955943822860718, 0.8695392608642578, 0.8780629634857178]\n\ntest loss: 0.060870614380408555\ntest loss_segm: 0.060870614380408555\ntest loss_shape: 0.03977199720266538\ntest loss_recon: 0.11952373920342861\ntest unet DSC: [0.9959874153137207, 0.8807197213172913, 0.8938800096511841]\nBest val loss: 0.06055901849117035\nTime: 87.29958152770996\n\n\nEpoch 23/300\n\ntrain loss: 0.06900748784972142\ntrain loss_segm: 0.06900748784972142\ntrain loss_shape: 0.04621676407471488\ntrain loss_recon: 0.13064307389380056\ntrain unet DSC: [0.9956300854682922, 0.8700720071792603, 0.8782632350921631]\n\ntest loss: 0.06389102406608753\ntest loss_segm: 0.06389102406608753\ntest loss_shape: 0.040159110266428724\ntest loss_recon: 0.12119368864939763\ntest unet DSC: [0.9960325956344604, 0.8805818557739258, 0.8842362761497498]\nBest val loss: 0.06055901849117035\nTime: 86.50606536865234\n\n\nEpoch 24/300\n\ntrain loss: 0.06892183663535721\ntrain loss_segm: 0.06892183663535721\ntrain loss_shape: 0.04602839352115046\ntrain loss_recon: 0.13048745257945\ntrain unet DSC: [0.9956213235855103, 0.8700933456420898, 0.8785976767539978]\n\ntest loss: 0.06268874183297157\ntest loss_segm: 0.06268874183297157\ntest loss_shape: 0.03888682361978751\ntest loss_recon: 0.11948873293705475\ntest unet DSC: [0.9961277842521667, 0.8829789757728577, 0.8865444660186768]\nBest val loss: 0.06055901849117035\nTime: 85.71026635169983\n\n\nEpoch 25/300\n\ntrain loss: 0.06855450907745693\ntrain loss_segm: 0.06855450907745693\ntrain loss_shape: 0.04575001495548441\ntrain loss_recon: 0.12977736207503307\ntrain unet DSC: [0.9956480860710144, 0.8707997798919678, 0.8791390061378479]\n\ntest loss: 0.06033153812854718\ntest loss_segm: 0.06033153812854718\ntest loss_shape: 0.0380914664039245\ntest loss_recon: 0.11610487638375698\ntest unet DSC: [0.996184229850769, 0.8850714564323425, 0.8916937112808228]\nBest val loss: 0.06033153812854718\nTime: 86.06691241264343\n\n\nEpoch 26/300\n\ntrain loss: 0.06817471146394935\ntrain loss_segm: 0.06817471146394935\ntrain loss_shape: 0.04544294748125197\ntrain loss_recon: 0.12911693506603\ntrain unet DSC: [0.9956589937210083, 0.8714143633842468, 0.8800188899040222]\n\ntest loss: 0.06106782341614748\ntest loss_segm: 0.06106782341614748\ntest loss_shape: 0.03913135778827545\ntest loss_recon: 0.11901282041500776\ntest unet DSC: [0.9961549639701843, 0.8849713206291199, 0.8898965716362]\nBest val loss: 0.06033153812854718\nTime: 86.06190586090088\n\n\nEpoch 27/300\n\ntrain loss: 0.06822536029864716\ntrain loss_segm: 0.06822536029864716\ntrain loss_shape: 0.0452940071213849\ntrain loss_recon: 0.12919692902625363\ntrain unet DSC: [0.995659351348877, 0.8711938261985779, 0.8797584176063538]\n\ntest loss: 0.060240281315950245\ntest loss_segm: 0.060240281315950245\ntest loss_shape: 0.038479726952620044\ntest loss_recon: 0.11614432243200448\ntest unet DSC: [0.9961090087890625, 0.8828614354133606, 0.8938972353935242]\nBest val loss: 0.060240281315950245\nTime: 86.40855050086975\n\n\nEpoch 28/300\n\ntrain loss: 0.06812385665370693\ntrain loss_segm: 0.06812385665370693\ntrain loss_shape: 0.04524625828371772\ntrain loss_recon: 0.12901903745494311\ntrain unet DSC: [0.9956607222557068, 0.87139892578125, 0.8799793720245361]\n\ntest loss: 0.05893960193945812\ntest loss_segm: 0.05893960193945812\ntest loss_shape: 0.03816851266683676\ntest loss_recon: 0.11644463355724628\ntest unet DSC: [0.9961352348327637, 0.88547283411026, 0.8960342407226562]\nBest val loss: 0.05893960193945812\nTime: 86.0629711151123\n\n\nEpoch 29/300\n\ntrain loss: 0.0677405679009006\ntrain loss_segm: 0.0677405679009006\ntrain loss_shape: 0.04487132472045059\ntrain loss_recon: 0.12842084033579765\ntrain unet DSC: [0.9956903457641602, 0.8721185326576233, 0.8805557489395142]\n\ntest loss: 0.06077557935928687\ntest loss_segm: 0.06077557935928687\ntest loss_shape: 0.03791396286434088\ntest loss_recon: 0.11668589634773059\ntest unet DSC: [0.9961808919906616, 0.8847638368606567, 0.8908417820930481]\nBest val loss: 0.05893960193945812\nTime: 86.36517357826233\n\n\nEpoch 30/300\n\ntrain loss: 0.06738861801126335\ntrain loss_segm: 0.06738861801126335\ntrain loss_shape: 0.044583524801308595\ntrain loss_recon: 0.12780123496357398\ntrain unet DSC: [0.9957045316696167, 0.8726251125335693, 0.8812398910522461]\n\ntest loss: 0.05931048267162763\ntest loss_segm: 0.05931048267162763\ntest loss_shape: 0.03880920671881773\ntest loss_recon: 0.11729699984574929\ntest unet DSC: [0.9961007833480835, 0.8854407072067261, 0.8950523734092712]\nBest val loss: 0.05893960193945812\nTime: 85.94372844696045\n\n\nEpoch 31/300\n\ntrain loss: 0.06699914341391641\ntrain loss_segm: 0.06699914341391641\ntrain loss_shape: 0.044314147008559374\ntrain loss_recon: 0.1272044189368622\ntrain unet DSC: [0.9957219362258911, 0.8732530474662781, 0.8818721771240234]\n\ntest loss: 0.05892568969955811\ntest loss_segm: 0.05892568969955811\ntest loss_shape: 0.037485661510473646\ntest loss_recon: 0.11464754587564713\ntest unet DSC: [0.9962042570114136, 0.8862336277961731, 0.8952241539955139]\nBest val loss: 0.05892568969955811\nTime: 85.89105916023254\n\n\nEpoch 32/300\n\ntrain loss: 0.06701048317400715\ntrain loss_segm: 0.06701048317400715\ntrain loss_shape: 0.044210164984570276\ntrain loss_recon: 0.12709133421318441\ntrain unet DSC: [0.9957322478294373, 0.8732366561889648, 0.8817852139472961]\n\ntest loss: 0.061635794738928475\ntest loss_segm: 0.061635794738928475\ntest loss_shape: 0.03852857014116568\ntest loss_recon: 0.1178050560828967\ntest unet DSC: [0.9960910081863403, 0.8823285102844238, 0.8905355930328369]\nBest val loss: 0.05892568969955811\nTime: 86.34210467338562\n\n\nEpoch 33/300\n\ntrain loss: 0.06680950421038308\ntrain loss_segm: 0.06680950421038308\ntrain loss_shape: 0.04404378821483896\ntrain loss_recon: 0.1268538383743431\ntrain unet DSC: [0.995731770992279, 0.87344890832901, 0.882258951663971]\n\ntest loss: 0.059921776254971824\ntest loss_segm: 0.059921776254971824\ntest loss_shape: 0.040059065111936666\ntest loss_recon: 0.12043907092167781\ntest unet DSC: [0.9959126114845276, 0.8821051120758057, 0.8958759903907776]\nBest val loss: 0.05892568969955811\nTime: 86.35210156440735\n\n\nEpoch 34/300\n\ntrain loss: 0.0667794111503076\ntrain loss_segm: 0.0667794111503076\ntrain loss_shape: 0.04398668632854389\ntrain loss_recon: 0.12686337021332753\ntrain unet DSC: [0.995731770992279, 0.8735707998275757, 0.8821393847465515]\n\ntest loss: 0.059005046215577006\ntest loss_segm: 0.059005046215577006\ntest loss_shape: 0.038664900339566745\ntest loss_recon: 0.11760669488173264\ntest unet DSC: [0.996132493019104, 0.8867443799972534, 0.8946589827537537]\nBest val loss: 0.05892568969955811\nTime: 86.11484432220459\n\n\nEpoch 35/300\n\ntrain loss: 0.06626703420394583\ntrain loss_segm: 0.06626703420394583\ntrain loss_shape: 0.04370041689069211\ntrain loss_recon: 0.12611655343936967\ntrain unet DSC: [0.9957602620124817, 0.8744673132896423, 0.8831276297569275]\n\ntest loss: 0.0596927030919454\ntest loss_segm: 0.0596927030919454\ntest loss_shape: 0.03756537407827683\ntest loss_recon: 0.11606808044971564\ntest unet DSC: [0.9961361885070801, 0.8844355940818787, 0.8945384621620178]\nBest val loss: 0.05892568969955811\nTime: 86.19599556922913\n\n\nEpoch 36/300\n\ntrain loss: 0.06623672040863128\ntrain loss_segm: 0.06623672040863128\ntrain loss_shape: 0.04361214891924888\ntrain loss_recon: 0.12596927224835264\ntrain unet DSC: [0.9957486987113953, 0.8743311166763306, 0.8833853006362915]\n\ntest loss: 0.0607080370760881\ntest loss_segm: 0.0607080370760881\ntest loss_shape: 0.03912062541796611\ntest loss_recon: 0.11795424956541795\ntest unet DSC: [0.9960298538208008, 0.8805757164955139, 0.8944481015205383]\nBest val loss: 0.05892568969955811\nTime: 85.76469945907593\n\n\nEpoch 37/300\n\ntrain loss: 0.06601598219875293\ntrain loss_segm: 0.06601598219875293\ntrain loss_shape: 0.043479224879153164\ntrain loss_recon: 0.1256604530388796\ntrain unet DSC: [0.9957671165466309, 0.8747510313987732, 0.8836056590080261]\n\ntest loss: 0.05903221332491972\ntest loss_segm: 0.05903221332491972\ntest loss_shape: 0.037854768049258455\ntest loss_recon: 0.11629046537937263\ntest unet DSC: [0.9961047172546387, 0.8844682574272156, 0.8966087698936462]\nBest val loss: 0.05892568969955811\nTime: 87.75011563301086\n\n\nEpoch 38/300\n\ntrain loss: 0.06571862966859643\ntrain loss_segm: 0.06571862966859643\ntrain loss_shape: 0.04322852621067174\ntrain loss_recon: 0.12507106344911118\ntrain unet DSC: [0.9957869052886963, 0.8753306865692139, 0.8841158747673035]\n\ntest loss: 0.05995421703809347\ntest loss_segm: 0.05995421703809347\ntest loss_shape: 0.037788479565045774\ntest loss_recon: 0.1163730881153009\ntest unet DSC: [0.9961581826210022, 0.8835927248001099, 0.8937272429466248]\nBest val loss: 0.05892568969955811\nTime: 85.79977583885193\n\n\nEpoch 39/300\n\ntrain loss: 0.06584702000682112\ntrain loss_segm: 0.06584702000682112\ntrain loss_shape: 0.04326690613186058\ntrain loss_recon: 0.12528895728195769\ntrain unet DSC: [0.9957770109176636, 0.8750621676445007, 0.883877158164978]\n\ntest loss: 0.059823062079839215\ntest loss_segm: 0.059823062079839215\ntest loss_shape: 0.03840119086014919\ntest loss_recon: 0.11626679775042412\ntest unet DSC: [0.9961283802986145, 0.8841243386268616, 0.8939962983131409]\nBest val loss: 0.05892568969955811\nTime: 86.13032341003418\n\n\nEpoch 40/300\n\ntrain loss: 0.06547189284634741\ntrain loss_segm: 0.06547189284634741\ntrain loss_shape: 0.04308245927566969\ntrain loss_recon: 0.1247375388688679\ntrain unet DSC: [0.995779812335968, 0.8755578398704529, 0.8847217559814453]\n\ntest loss: 0.05898429587101325\ntest loss_segm: 0.05898429587101325\ntest loss_shape: 0.037883879855657235\ntest loss_recon: 0.11520740007742858\ntest unet DSC: [0.9961824417114258, 0.8861262202262878, 0.8947401642799377]\nBest val loss: 0.05892568969955811\nTime: 86.57852959632874\n\n\nEpoch 41/300\n\ntrain loss: 0.0654717157771693\ntrain loss_segm: 0.0654717157771693\ntrain loss_shape: 0.04306415249180945\ntrain loss_recon: 0.12469879698149766\ntrain unet DSC: [0.9957946538925171, 0.8756515979766846, 0.8846136331558228]\n\ntest loss: 0.05831048752252872\ntest loss_segm: 0.05831048752252872\ntest loss_shape: 0.03813834474063837\ntest loss_recon: 0.1161583662033081\ntest unet DSC: [0.9960753321647644, 0.8851973414421082, 0.8979443311691284]\nBest val loss: 0.05831048752252872\nTime: 85.78988361358643\n\n\nEpoch 42/300\n\ntrain loss: 0.06558759765157217\ntrain loss_segm: 0.06558759765157217\ntrain loss_shape: 0.043084906532040126\ntrain loss_recon: 0.12494478270977358\ntrain unet DSC: [0.9957742691040039, 0.8754210472106934, 0.8844964504241943]\n\ntest loss: 0.06101379037285463\ntest loss_segm: 0.06101379037285463\ntest loss_shape: 0.038044411020401195\ntest loss_recon: 0.11808176529713166\ntest unet DSC: [0.9961325526237488, 0.8843500018119812, 0.8907387852668762]\nBest val loss: 0.05831048752252872\nTime: 86.03405904769897\n\n\nEpoch 43/300\n\ntrain loss: 0.06523951119448565\ntrain loss_segm: 0.06523951119448565\ntrain loss_shape: 0.04282400580240956\ntrain loss_recon: 0.1242034518266026\ntrain unet DSC: [0.9958071112632751, 0.8760084509849548, 0.8850657939910889]\n\ntest loss: 0.05944297051964662\ntest loss_segm: 0.05944297051964662\ntest loss_shape: 0.037983604826224156\ntest loss_recon: 0.11645730030842316\ntest unet DSC: [0.9961829781532288, 0.8861221075057983, 0.8934659957885742]\nBest val loss: 0.05831048752252872\nTime: 85.80514931678772\n\n\nEpoch 44/300\n\ntrain loss: 0.06491767841426632\ntrain loss_segm: 0.06491767841426632\ntrain loss_shape: 0.04268040050622783\ntrain loss_recon: 0.12398369365100619\ntrain unet DSC: [0.9957998991012573, 0.8763347864151001, 0.8857453465461731]\n\ntest loss: 0.06155347738128442\ntest loss_segm: 0.06155347738128442\ntest loss_shape: 0.03930533237946339\ntest loss_recon: 0.11932841478249966\ntest unet DSC: [0.9961516261100769, 0.8849798440933228, 0.8869988918304443]\nBest val loss: 0.05831048752252872\nTime: 86.41670942306519\n\n\nEpoch 45/300\n\ntrain loss: 0.06492783407433124\ntrain loss_segm: 0.06492783407433124\ntrain loss_shape: 0.04265583353706553\ntrain loss_recon: 0.12392731407020666\ntrain unet DSC: [0.9958047866821289, 0.8764379024505615, 0.8856240510940552]\n\ntest loss: 0.06099414510222582\ntest loss_segm: 0.06099414510222582\ntest loss_shape: 0.03798749168904928\ntest loss_recon: 0.11704285328204815\ntest unet DSC: [0.9961430430412292, 0.8842082023620605, 0.8901402354240417]\nBest val loss: 0.05831048752252872\nTime: 85.95008635520935\n\n\nEpoch 46/300\n\ntrain loss: 0.06485251367940933\ntrain loss_segm: 0.06485251367940933\ntrain loss_shape: 0.04260114639337304\ntrain loss_recon: 0.12369939194449896\ntrain unet DSC: [0.9958125948905945, 0.8765401840209961, 0.8857272863388062]\n\ntest loss: 0.06228662540133183\ntest loss_segm: 0.06228662540133183\ntest loss_shape: 0.03820804611612589\ntest loss_recon: 0.11871505547792484\ntest unet DSC: [0.9962393641471863, 0.8848825097084045, 0.8850065469741821]\nBest val loss: 0.05831048752252872\nTime: 86.89917802810669\n\n\nEpoch 47/300\n\ntrain loss: 0.06462569475834128\ntrain loss_segm: 0.06462569475834128\ntrain loss_shape: 0.0423827198911694\ntrain loss_recon: 0.12328344583511353\ntrain unet DSC: [0.9958212375640869, 0.8768969178199768, 0.8861331343650818]\n\ntest loss: 0.060770088663467996\ntest loss_segm: 0.060770088663467996\ntest loss_shape: 0.038512773238695584\ntest loss_recon: 0.11815407031621689\ntest unet DSC: [0.9960829615592957, 0.884240448474884, 0.8910911679267883]\nBest val loss: 0.05831048752252872\nTime: 86.21141695976257\n\n\nEpoch 48/300\n\ntrain loss: 0.06476988714141181\ntrain loss_segm: 0.06476988714141181\ntrain loss_shape: 0.042530961122505276\ntrain loss_recon: 0.12365207113797151\ntrain unet DSC: [0.9958128333091736, 0.8767055869102478, 0.8859111666679382]\n\ntest loss: 0.059973577658335366\ntest loss_segm: 0.059973577658335366\ntest loss_shape: 0.03797642714702166\ntest loss_recon: 0.11546601240451519\ntest unet DSC: [0.9961363673210144, 0.8837279081344604, 0.8934292793273926]\nBest val loss: 0.05831048752252872\nTime: 85.96834516525269\n\n\nEpoch 49/300\n\ntrain loss: 0.06443868112997918\ntrain loss_segm: 0.06443868112997918\ntrain loss_shape: 0.04232360693755784\ntrain loss_recon: 0.12305843716935266\ntrain unet DSC: [0.9958199858665466, 0.8771841526031494, 0.8865667581558228]\n\ntest loss: 0.05923929619483459\ntest loss_segm: 0.05923929619483459\ntest loss_shape: 0.03775915622902222\ntest loss_recon: 0.11549074527544853\ntest unet DSC: [0.9961753487586975, 0.8853998184204102, 0.8942520022392273]\nBest val loss: 0.05831048752252872\nTime: 86.53648900985718\n\n\nEpoch 50/300\n\ntrain loss: 0.06460653408135794\ntrain loss_segm: 0.06460653408135794\ntrain loss_shape: 0.04235081377003012\ntrain loss_recon: 0.12323552520969246\ntrain unet DSC: [0.9958276152610779, 0.8769792914390564, 0.8861947655677795]\n\ntest loss: 0.05997156638365526\ntest loss_segm: 0.05997156638365526\ntest loss_shape: 0.03891850277208365\ntest loss_recon: 0.11855232104277\ntest unet DSC: [0.9959872364997864, 0.8816445469856262, 0.8957149982452393]\nBest val loss: 0.05831048752252872\nTime: 86.77284717559814\n\n\nEpoch 51/300\n\ntrain loss: 0.06443715524635737\ntrain loss_segm: 0.06443715524635737\ntrain loss_shape: 0.042239489976929716\ntrain loss_recon: 0.12301240050340001\ntrain unet DSC: [0.9958295226097107, 0.8771307468414307, 0.886504054069519]\n\ntest loss: 0.05922975964271105\ntest loss_segm: 0.05922975964271105\ntest loss_shape: 0.0385506462592345\ntest loss_recon: 0.11848955429517306\ntest unet DSC: [0.9960963726043701, 0.8857330083847046, 0.8945629596710205]\nBest val loss: 0.05831048752252872\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.36255288124084\n\n\nEpoch 52/300\n\ntrain loss: 0.06340645826598511\ntrain loss_segm: 0.06340645826598511\ntrain loss_shape: 0.04163297723177113\ntrain loss_recon: 0.12144681547261492\ntrain unet DSC: [0.9958878755569458, 0.8790878057479858, 0.8881421685218811]\n\ntest loss: 0.060076408565808564\ntest loss_segm: 0.060076408565808564\ntest loss_shape: 0.037376891678342454\ntest loss_recon: 0.11529760330151288\ntest unet DSC: [0.9962216019630432, 0.8857289552688599, 0.8913458585739136]\nBest val loss: 0.05831048752252872\nTime: 85.8647289276123\n\n\nEpoch 53/300\n\ntrain loss: 0.0633154513573722\ntrain loss_segm: 0.0633154513573722\ntrain loss_shape: 0.041504849149267886\ntrain loss_recon: 0.12105895974968053\ntrain unet DSC: [0.9958990812301636, 0.8791900873184204, 0.8882442712783813]\n\ntest loss: 0.05802275861303011\ntest loss_segm: 0.05802275861303011\ntest loss_shape: 0.03691266142787077\ntest loss_recon: 0.11365791314687484\ntest unet DSC: [0.9962197542190552, 0.8868311643600464, 0.8969755172729492]\nBest val loss: 0.05802275861303011\nTime: 87.04199647903442\n\n\nEpoch 54/300\n\ntrain loss: 0.06311257706978653\ntrain loss_segm: 0.06311257706978653\ntrain loss_shape: 0.041409347465709796\ntrain loss_recon: 0.12071998851208747\ntrain unet DSC: [0.9959019422531128, 0.8794952630996704, 0.8886992931365967]\n\ntest loss: 0.06089668816480881\ntest loss_segm: 0.06089668816480881\ntest loss_shape: 0.03778868259336704\ntest loss_recon: 0.11636583621685322\ntest unet DSC: [0.9962086081504822, 0.8853013515472412, 0.888769268989563]\nBest val loss: 0.05802275861303011\nTime: 88.67411017417908\n\n\nEpoch 55/300\n\ntrain loss: 0.0631177674365949\ntrain loss_segm: 0.0631177674365949\ntrain loss_shape: 0.04138822134442722\ntrain loss_recon: 0.12071295629573774\ntrain unet DSC: [0.9959039688110352, 0.8794516324996948, 0.8886387348175049]\n\ntest loss: 0.059168901963111684\ntest loss_segm: 0.059168901963111684\ntest loss_shape: 0.03705601790585579\ntest loss_recon: 0.11448286741207807\ntest unet DSC: [0.9962678551673889, 0.887311577796936, 0.8927581906318665]\nBest val loss: 0.05802275861303011\nTime: 87.21275067329407\n\n\nEpoch 56/300\n\ntrain loss: 0.06306982441227647\ntrain loss_segm: 0.06306982441227647\ntrain loss_shape: 0.041317723102018804\ntrain loss_recon: 0.12063390393800373\ntrain unet DSC: [0.9959089756011963, 0.8795892000198364, 0.8886604905128479]\n\ntest loss: 0.05882565591197748\ntest loss_segm: 0.05882565591197748\ntest loss_shape: 0.03746154961677698\ntest loss_recon: 0.11474796900382409\ntest unet DSC: [0.9962101578712463, 0.8870570063591003, 0.8943377733230591]\nBest val loss: 0.05802275861303011\nTime: 86.33781433105469\n\n\nEpoch 57/300\n\ntrain loss: 0.06297748094966894\ntrain loss_segm: 0.06297748094966894\ntrain loss_shape: 0.04127474499371233\ntrain loss_recon: 0.12050616363935833\ntrain unet DSC: [0.995913565158844, 0.8797690272331238, 0.8888387680053711]\n\ntest loss: 0.0647621211142112\ntest loss_segm: 0.0647621211142112\ntest loss_shape: 0.0394465454782431\ntest loss_recon: 0.12116297238912338\ntest unet DSC: [0.9960745573043823, 0.8807697892189026, 0.8807289004325867]\nBest val loss: 0.05802275861303011\nTime: 85.7252266407013\n\n\nEpoch 58/300\n\ntrain loss: 0.06300003866700432\ntrain loss_segm: 0.06300003866700432\ntrain loss_shape: 0.04134088438711589\ntrain loss_recon: 0.12057062794890584\ntrain unet DSC: [0.9959089756011963, 0.8796287178993225, 0.8888427019119263]\n\ntest loss: 0.05779383178704824\ntest loss_segm: 0.05779383178704824\ntest loss_shape: 0.036815317443166025\ntest loss_recon: 0.11364846810316429\ntest unet DSC: [0.9962190389633179, 0.8875247240066528, 0.8973420262336731]\nBest val loss: 0.05779383178704824\nTime: 86.38327980041504\n\n\nEpoch 59/300\n\ntrain loss: 0.0628898676390512\ntrain loss_segm: 0.0628898676390512\ntrain loss_shape: 0.041194677682994285\ntrain loss_recon: 0.1203140623961823\ntrain unet DSC: [0.9959168434143066, 0.8798484206199646, 0.8889523148536682]\n\ntest loss: 0.060254325660375446\ntest loss_segm: 0.060254325660375446\ntest loss_shape: 0.03740562149920525\ntest loss_recon: 0.11563236132646218\ntest unet DSC: [0.9962190389633179, 0.8862536549568176, 0.8902837038040161]\nBest val loss: 0.05779383178704824\nTime: 86.27266359329224\n\n\nEpoch 60/300\n\ntrain loss: 0.06302478496881225\ntrain loss_segm: 0.06302478496881225\ntrain loss_shape: 0.04128651601509958\ntrain loss_recon: 0.12053480216219456\ntrain unet DSC: [0.9959112405776978, 0.8797004818916321, 0.8887760043144226]\n\ntest loss: 0.05802690428801072\ntest loss_segm: 0.05802690428801072\ntest loss_shape: 0.037072791694066465\ntest loss_recon: 0.11384159479385768\ntest unet DSC: [0.9961914420127869, 0.8862228393554688, 0.8975464105606079]\nBest val loss: 0.05779383178704824\nTime: 86.71380925178528\n\n\nEpoch 61/300\n\ntrain loss: 0.06289553951141955\ntrain loss_segm: 0.06289553951141955\ntrain loss_shape: 0.04124394669845889\ntrain loss_recon: 0.12036516583418544\ntrain unet DSC: [0.9959160685539246, 0.8798598051071167, 0.8890160918235779]\n\ntest loss: 0.057918390880028404\ntest loss_segm: 0.057918390880028404\ntest loss_shape: 0.03685572657447595\ntest loss_recon: 0.11397487077957545\ntest unet DSC: [0.9962284564971924, 0.887596607208252, 0.8968075513839722]\nBest val loss: 0.05779383178704824\nTime: 86.46480321884155\n\n\nEpoch 62/300\n\ntrain loss: 0.06285859375626227\ntrain loss_segm: 0.06285859375626227\ntrain loss_shape: 0.04115184523825404\ntrain loss_recon: 0.12027595050727265\ntrain unet DSC: [0.9959193468093872, 0.8799407482147217, 0.8890441656112671]\n\ntest loss: 0.057801290486867614\ntest loss_segm: 0.057801290486867614\ntest loss_shape: 0.036387838662052766\ntest loss_recon: 0.1128276173885052\ntest unet DSC: [0.9962445497512817, 0.8869795203208923, 0.8977066278457642]\nBest val loss: 0.05779383178704824\nTime: 86.41054725646973\n\n\nEpoch 63/300\n\ntrain loss: 0.06281801091530655\ntrain loss_segm: 0.06281801091530655\ntrain loss_shape: 0.04111919358749933\ntrain loss_recon: 0.12018835846381852\ntrain unet DSC: [0.9959179759025574, 0.8799926042556763, 0.8891556859016418]\n\ntest loss: 0.07152770650692475\ntest loss_segm: 0.07152770650692475\ntest loss_shape: 0.04228273110511976\ntest loss_recon: 0.1296255573248252\ntest unet DSC: [0.9957584738731384, 0.8712866306304932, 0.8680404424667358]\nBest val loss: 0.05779383178704824\nTime: 85.70989656448364\n\n\nEpoch 64/300\n\ntrain loss: 0.06283868865781947\ntrain loss_segm: 0.06283868865781947\ntrain loss_shape: 0.04112393800405007\ntrain loss_recon: 0.12011188495008251\ntrain unet DSC: [0.9959233999252319, 0.8799693584442139, 0.8891056776046753]\n\ntest loss: 0.06093971880200582\ntest loss_segm: 0.06093971880200582\ntest loss_shape: 0.03793281120940661\ntest loss_recon: 0.11696138748755822\ntest unet DSC: [0.9961700439453125, 0.8848753571510315, 0.8892102241516113]\nBest val loss: 0.05779383178704824\nTime: 85.73688220977783\n\n\nEpoch 65/300\n\ntrain loss: 0.06278635252597212\ntrain loss_segm: 0.06278635252597212\ntrain loss_shape: 0.04111381299510787\ntrain loss_recon: 0.12017083847070043\ntrain unet DSC: [0.9959180951118469, 0.8800243139266968, 0.8892082571983337]\n\ntest loss: 0.06399047145476708\ntest loss_segm: 0.06399047145476708\ntest loss_shape: 0.03903023220407657\ntest loss_recon: 0.12014720837275188\ntest unet DSC: [0.9960619807243347, 0.8811888098716736, 0.882879376411438]\nBest val loss: 0.05779383178704824\nTime: 85.7419786453247\n\n\nEpoch 66/300\n\ntrain loss: 0.06277521501613569\ntrain loss_segm: 0.06277521501613569\ntrain loss_shape: 0.04109290025279492\ntrain loss_recon: 0.12009612367122988\ntrain unet DSC: [0.9959228038787842, 0.8800718784332275, 0.8891637325286865]\n\ntest loss: 0.058080556300970226\ntest loss_segm: 0.058080556300970226\ntest loss_shape: 0.03771083171551044\ntest loss_recon: 0.11530680992664435\ntest unet DSC: [0.9960940480232239, 0.8854110836982727, 0.8982867002487183]\nBest val loss: 0.05779383178704824\nTime: 85.87497448921204\n\n\nEpoch 67/300\n\ntrain loss: 0.06274301839308648\ntrain loss_segm: 0.06274301839308648\ntrain loss_shape: 0.04108856649055511\ntrain loss_recon: 0.12008326453498647\ntrain unet DSC: [0.9959225058555603, 0.8801192045211792, 0.889265239238739]\n\ntest loss: 0.05810134714612594\ntest loss_segm: 0.05810134714612594\ntest loss_shape: 0.036989391375428594\ntest loss_recon: 0.11366887887318929\ntest unet DSC: [0.9962052702903748, 0.8865774869918823, 0.8968867659568787]\nBest val loss: 0.05779383178704824\nTime: 85.91765308380127\n\n\nEpoch 68/300\n\ntrain loss: 0.06272823205571386\ntrain loss_segm: 0.06272823205571386\ntrain loss_shape: 0.041064664084888715\ntrain loss_recon: 0.119999457009231\ntrain unet DSC: [0.9959235787391663, 0.8800978660583496, 0.8892495632171631]\n\ntest loss: 0.058555273386912465\ntest loss_segm: 0.058555273386912465\ntest loss_shape: 0.03674814907404093\ntest loss_recon: 0.11349161313130306\ntest unet DSC: [0.996250569820404, 0.8871211409568787, 0.895064651966095]\nBest val loss: 0.05779383178704824\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.71954655647278\n\n\nEpoch 69/300\n\ntrain loss: 0.06254597074246104\ntrain loss_segm: 0.06254597074246104\ntrain loss_shape: 0.04090947011792207\ntrain loss_recon: 0.11955037448979632\ntrain unet DSC: [0.9959433674812317, 0.8804442286491394, 0.8895989060401917]\n\ntest loss: 0.05963962687513767\ntest loss_segm: 0.05963962687513767\ntest loss_shape: 0.0372092450180879\ntest loss_recon: 0.11507580372003409\ntest unet DSC: [0.9962377548217773, 0.886834979057312, 0.891725480556488]\nBest val loss: 0.05779383178704824\nTime: 86.00693273544312\n\n\nEpoch 70/300\n\ntrain loss: 0.06239997286679624\ntrain loss_segm: 0.06239997286679624\ntrain loss_shape: 0.04079918301652504\ntrain loss_recon: 0.1194110735307766\ntrain unet DSC: [0.9959496855735779, 0.8807471394538879, 0.8897625207901001]\n\ntest loss: 0.05793178052856372\ntest loss_segm: 0.05793178052856372\ntest loss_shape: 0.03714016371239454\ntest loss_recon: 0.11399278426781678\ntest unet DSC: [0.9961696267127991, 0.8862085342407227, 0.8980021476745605]\nBest val loss: 0.05779383178704824\nTime: 86.52201223373413\n\n\nEpoch 71/300\n\ntrain loss: 0.062401175970518134\ntrain loss_segm: 0.062401175970518134\ntrain loss_shape: 0.040919332564631594\ntrain loss_recon: 0.11951394812970222\ntrain unet DSC: [0.9959424138069153, 0.880703330039978, 0.8897586464881897]\n\ntest loss: 0.06483066702882449\ntest loss_segm: 0.06483066702882449\ntest loss_shape: 0.03922047709616331\ntest loss_recon: 0.12113209718312973\ntest unet DSC: [0.9960418939590454, 0.8802253603935242, 0.881172776222229]\nBest val loss: 0.05779383178704824\nTime: 85.94551038742065\n\n\nEpoch 72/300\n\ntrain loss: 0.06238523493461971\ntrain loss_segm: 0.06238523493461971\ntrain loss_shape: 0.04083231765824028\ntrain loss_recon: 0.11939244179786006\ntrain unet DSC: [0.9959449172019958, 0.8807488083839417, 0.8898642063140869]\n\ntest loss: 0.07021493120835377\ntest loss_segm: 0.07021493120835377\ntest loss_shape: 0.041970038786530495\ntest loss_recon: 0.12824534147213668\ntest unet DSC: [0.9957671761512756, 0.8725282549858093, 0.8710225224494934]\nBest val loss: 0.05779383178704824\nTime: 86.44834184646606\n\n\nEpoch 73/300\n\ntrain loss: 0.06239322877099997\ntrain loss_segm: 0.06239322877099997\ntrain loss_shape: 0.04085347848602488\ntrain loss_recon: 0.11938465660131431\ntrain unet DSC: [0.9959496855735779, 0.8807967901229858, 0.8898443579673767]\n\ntest loss: 0.06379585455243404\ntest loss_segm: 0.06379585455243404\ntest loss_shape: 0.03891661151861533\ntest loss_recon: 0.12000110516181359\ntest unet DSC: [0.9960904121398926, 0.8818106055259705, 0.8830284476280212]\nBest val loss: 0.05779383178704824\nTime: 85.73912715911865\n\n\nEpoch 74/300\n\ntrain loss: 0.062407759289386903\ntrain loss_segm: 0.062407759289386903\ntrain loss_shape: 0.0408754899767758\ntrain loss_recon: 0.11950469356548937\ntrain unet DSC: [0.9959416389465332, 0.8806973099708557, 0.889779269695282]\n\ntest loss: 0.05754134737146206\ntest loss_segm: 0.05754134737146206\ntest loss_shape: 0.03634473614585705\ntest loss_recon: 0.11250735857547858\ntest unet DSC: [0.9962475299835205, 0.8876063823699951, 0.8978723287582397]\nBest val loss: 0.05754134737146206\nTime: 86.07222771644592\n\n\nEpoch 75/300\n\ntrain loss: 0.06245697997038877\ntrain loss_segm: 0.06245697997038877\ntrain loss_shape: 0.04083640760258783\ntrain loss_recon: 0.11948848336557799\ntrain unet DSC: [0.9959408044815063, 0.8806133270263672, 0.8897024989128113]\n\ntest loss: 0.05852223216341092\ntest loss_segm: 0.05852223216341092\ntest loss_shape: 0.036732917508253686\ntest loss_recon: 0.11340696689410087\ntest unet DSC: [0.9962454438209534, 0.887469470500946, 0.8947537541389465]\nBest val loss: 0.05754134737146206\nTime: 86.18115735054016\n\n\nEpoch 76/300\n\ntrain loss: 0.06243077748089652\ntrain loss_segm: 0.06243077748089652\ntrain loss_shape: 0.04088856360014481\ntrain loss_recon: 0.11961283185814\ntrain unet DSC: [0.9959412813186646, 0.8806208372116089, 0.8897256851196289]\n\ntest loss: 0.05785662241471119\ntest loss_segm: 0.05785662241471119\ntest loss_shape: 0.036879442632198334\ntest loss_recon: 0.11358297024017726\ntest unet DSC: [0.9961966276168823, 0.8863767981529236, 0.8979806900024414]\nBest val loss: 0.05754134737146206\nTime: 87.03100538253784\n\n\nEpoch 77/300\n\ntrain loss: 0.06239107687356351\ntrain loss_segm: 0.06239107687356351\ntrain loss_shape: 0.0408752783189846\ntrain loss_recon: 0.11949083586282368\ntrain unet DSC: [0.9959458708763123, 0.8807659149169922, 0.88981032371521]\n\ntest loss: 0.05914088080708797\ntest loss_segm: 0.05914088080708797\ntest loss_shape: 0.03674813999961584\ntest loss_recon: 0.11402570131497505\ntest unet DSC: [0.9962583780288696, 0.8872871398925781, 0.8929611444473267]\nBest val loss: 0.05754134737146206\nTime: 85.94070744514465\n\n\nEpoch 78/300\n\ntrain loss: 0.06244067786426484\ntrain loss_segm: 0.06244067786426484\ntrain loss_shape: 0.04087315073024623\ntrain loss_recon: 0.11949787185161928\ntrain unet DSC: [0.9959449172019958, 0.8806523084640503, 0.8897455930709839]\n\ntest loss: 0.05777877960831691\ntest loss_segm: 0.05777877960831691\ntest loss_shape: 0.03622368035408167\ntest loss_recon: 0.11257178813983233\ntest unet DSC: [0.9962774515151978, 0.8882615566253662, 0.8965606689453125]\nBest val loss: 0.05754134737146206\nTime: 85.88027787208557\n\n\nEpoch 79/300\n\ntrain loss: 0.062355930598664885\ntrain loss_segm: 0.062355930598664885\ntrain loss_shape: 0.040852630369459526\ntrain loss_recon: 0.119407840921909\ntrain unet DSC: [0.9959484934806824, 0.8807921409606934, 0.8898689150810242]\n\ntest loss: 0.05904018563719896\ntest loss_segm: 0.05904018563719896\ntest loss_shape: 0.03695374240095799\ntest loss_recon: 0.11434245720887795\ntest unet DSC: [0.9962484836578369, 0.8873758316040039, 0.8931992650032043]\nBest val loss: 0.05754134737146206\nTime: 86.30481266975403\n\n\nEpoch 80/300\n\ntrain loss: 0.06239505984549281\ntrain loss_segm: 0.06239505984549281\ntrain loss_shape: 0.04082303944550737\ntrain loss_recon: 0.11946956568126436\ntrain unet DSC: [0.9959394335746765, 0.880664587020874, 0.889889121055603]\n\ntest loss: 0.06560692897973916\ntest loss_segm: 0.06560692897973916\ntest loss_shape: 0.039713479148653835\ntest loss_recon: 0.12246622030551617\ntest unet DSC: [0.9959617257118225, 0.8786687850952148, 0.8802949786186218]\nBest val loss: 0.05754134737146206\nTime: 85.33722925186157\n\n\nEpoch 81/300\n\ntrain loss: 0.06232787475367136\ntrain loss_segm: 0.06232787475367136\ntrain loss_shape: 0.04079266522032551\ntrain loss_recon: 0.11932328682911547\ntrain unet DSC: [0.9959471225738525, 0.8808721899986267, 0.8899308443069458]\n\ntest loss: 0.05835846419899891\ntest loss_segm: 0.05835846419899891\ntest loss_shape: 0.036861345840570256\ntest loss_recon: 0.11379929230763362\ntest unet DSC: [0.9962495565414429, 0.8879336714744568, 0.8950090408325195]\nBest val loss: 0.05754134737146206\nTime: 86.00794529914856\n\n\nEpoch 82/300\n\ntrain loss: 0.06237996690258195\ntrain loss_segm: 0.06237996690258195\ntrain loss_shape: 0.040809858213119866\ntrain loss_recon: 0.11943379534950739\ntrain unet DSC: [0.9959463477134705, 0.8807348012924194, 0.889779269695282]\n\ntest loss: 0.05813202118644348\ntest loss_segm: 0.05813202118644348\ntest loss_shape: 0.036555490480401576\ntest loss_recon: 0.11312678991219936\ntest unet DSC: [0.9962543845176697, 0.8878593444824219, 0.8957791328430176]\nBest val loss: 0.05754134737146206\nTime: 85.59980034828186\n\n\nEpoch 83/300\n\ntrain loss: 0.06234608194496058\ntrain loss_segm: 0.06234608194496058\ntrain loss_shape: 0.04076282149534437\ntrain loss_recon: 0.11927816498128674\ntrain unet DSC: [0.995954692363739, 0.8808006644248962, 0.8899009823799133]\n\ntest loss: 0.0578115100088792\ntest loss_segm: 0.0578115100088792\ntest loss_shape: 0.03712881948703375\ntest loss_recon: 0.11417300884540264\ntest unet DSC: [0.9961647391319275, 0.8863839507102966, 0.8981907963752747]\nBest val loss: 0.05754134737146206\nTime: 85.73421740531921\n\n\nEpoch 84/300\n\ntrain loss: 0.062348202625407446\ntrain loss_segm: 0.062348202625407446\ntrain loss_shape: 0.0408090159674234\ntrain loss_recon: 0.11938879120198986\ntrain unet DSC: [0.9959498643875122, 0.8808650970458984, 0.8898581862449646]\n\ntest loss: 0.061256131205039144\ntest loss_segm: 0.061256131205039144\ntest loss_shape: 0.037803979781575694\ntest loss_recon: 0.11656374809069511\ntest unet DSC: [0.996186375617981, 0.8849297761917114, 0.8880633115768433]\nBest val loss: 0.05754134737146206\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 86.00714612007141\n\n\nEpoch 85/300\n\ntrain loss: 0.06229439648929276\ntrain loss_segm: 0.06229439648929276\ntrain loss_shape: 0.040736650883019726\ntrain loss_recon: 0.11927263344390483\ntrain unet DSC: [0.9959512948989868, 0.8808882236480713, 0.8899329304695129]\n\ntest loss: 0.06083293201831671\ntest loss_segm: 0.06083293201831671\ntest loss_shape: 0.03771502925799443\ntest loss_recon: 0.11643874798065577\ntest unet DSC: [0.9961697459220886, 0.8849543929100037, 0.8895120620727539]\nBest val loss: 0.05754134737146206\nTime: 85.63511991500854\n\n\nEpoch 86/300\n\ntrain loss: 0.06222789847775351\ntrain loss_segm: 0.06222789847775351\ntrain loss_shape: 0.04075373650257346\ntrain loss_recon: 0.11919766589056087\ntrain unet DSC: [0.9959500432014465, 0.8809646368026733, 0.8901370763778687]\n\ntest loss: 0.058182518069560714\ntest loss_segm: 0.058182518069560714\ntest loss_shape: 0.036438641544335924\ntest loss_recon: 0.1131554765579028\ntest unet DSC: [0.9962759017944336, 0.8882218599319458, 0.895307183265686]\nBest val loss: 0.05754134737146206\nTime: 86.57436490058899\n\n\nEpoch 87/300\n\ntrain loss: 0.06225325509057015\ntrain loss_segm: 0.06225325509057015\ntrain loss_shape: 0.04077167488351653\ntrain loss_recon: 0.11917924805532527\ntrain unet DSC: [0.9959500432014465, 0.8809107542037964, 0.890101969242096]\n\ntest loss: 0.05890278231639128\ntest loss_segm: 0.05890278231639128\ntest loss_shape: 0.03696661246701693\ntest loss_recon: 0.11422146130830814\ntest unet DSC: [0.996238112449646, 0.887367844581604, 0.8937234878540039]\nBest val loss: 0.05754134737146206\nTime: 85.26443099975586\n\n\nEpoch 88/300\n\ntrain loss: 0.06227009561903114\ntrain loss_segm: 0.06227009561903114\ntrain loss_shape: 0.04073114811053759\ntrain loss_recon: 0.11923292920559267\ntrain unet DSC: [0.9959496855735779, 0.8809289336204529, 0.8900324106216431]\n\ntest loss: 0.05766515892285567\ntest loss_segm: 0.05766515892285567\ntest loss_shape: 0.03664731549528929\ntest loss_recon: 0.1132718691459069\ntest unet DSC: [0.9962088465690613, 0.8869079947471619, 0.8981540203094482]\nBest val loss: 0.05754134737146206\nTime: 85.72660183906555\n\n\nEpoch 89/300\n\ntrain loss: 0.06232305184666869\ntrain loss_segm: 0.06232305184666869\ntrain loss_shape: 0.04081639152350305\ntrain loss_recon: 0.1193701398523548\ntrain unet DSC: [0.9959480166435242, 0.8808696866035461, 0.8899277448654175]\n\ntest loss: 0.05962388312969452\ntest loss_segm: 0.05962388312969452\ntest loss_shape: 0.037178872439723745\ntest loss_recon: 0.11492817524151924\ntest unet DSC: [0.996229887008667, 0.8867071270942688, 0.8918101191520691]\nBest val loss: 0.05754134737146206\nTime: 85.633229970932\n\n\nEpoch 90/300\n\ntrain loss: 0.06225125727396977\ntrain loss_segm: 0.06225125727396977\ntrain loss_shape: 0.04075195369180999\ntrain loss_recon: 0.11921586416944673\ntrain unet DSC: [0.9959526658058167, 0.8809900283813477, 0.8901228904724121]\n\ntest loss: 0.05777535109947889\ntest loss_segm: 0.05777535109947889\ntest loss_shape: 0.0369473141259872\ntest loss_recon: 0.11363216088368343\ntest unet DSC: [0.996218740940094, 0.8876311182975769, 0.8971304297447205]\nBest val loss: 0.05754134737146206\nTime: 85.91249656677246\n\n\nEpoch 91/300\n\ntrain loss: 0.06224676610642596\ntrain loss_segm: 0.06224676610642596\ntrain loss_shape: 0.040720768390765674\ntrain loss_recon: 0.11916990597036821\ntrain unet DSC: [0.995951235294342, 0.8809608817100525, 0.8900031447410583]\n\ntest loss: 0.05987365295489629\ntest loss_segm: 0.05987365295489629\ntest loss_shape: 0.0371912847726773\ntest loss_recon: 0.11502852348180917\ntest unet DSC: [0.9962350726127625, 0.8865881562232971, 0.8910432457923889]\nBest val loss: 0.05754134737146206\nTime: 85.98270034790039\n\n\nEpoch 92/300\n\ntrain loss: 0.062301465557723104\ntrain loss_segm: 0.062301465557723104\ntrain loss_shape: 0.04074037508874\ntrain loss_recon: 0.11920188801198066\ntrain unet DSC: [0.9959523677825928, 0.8809559345245361, 0.8899939656257629]\n\ntest loss: 0.0576473980759963\ntest loss_segm: 0.0576473980759963\ntest loss_shape: 0.03674027858636318\ntest loss_recon: 0.11326792301275791\ntest unet DSC: [0.9962376356124878, 0.8880466818809509, 0.8970991969108582]\nBest val loss: 0.05754134737146206\nTime: 85.80671858787537\n\n\nEpoch 93/300\n\ntrain loss: 0.06220767833292484\ntrain loss_segm: 0.06220767833292484\ntrain loss_shape: 0.04076716277889813\ntrain loss_recon: 0.11929349582406538\ntrain unet DSC: [0.9959537982940674, 0.8810680508613586, 0.8901277184486389]\n\ntest loss: 0.05785698204850539\ntest loss_segm: 0.05785698204850539\ntest loss_shape: 0.03666357930081013\ntest loss_recon: 0.11323946255903977\ntest unet DSC: [0.9962469339370728, 0.8878433108329773, 0.8965628147125244]\nBest val loss: 0.05754134737146206\nTime: 85.50284934043884\n\n\nEpoch 94/300\n\ntrain loss: 0.06222117515398732\ntrain loss_segm: 0.06222117515398732\ntrain loss_shape: 0.04066671834244758\ntrain loss_recon: 0.11908426654489734\ntrain unet DSC: [0.9959571957588196, 0.881037175655365, 0.8901047706604004]\n\ntest loss: 0.057820955625711344\ntest loss_segm: 0.057820955625711344\ntest loss_shape: 0.03705873722449327\ntest loss_recon: 0.11392816060628647\ntest unet DSC: [0.9961734414100647, 0.8863624930381775, 0.898141622543335]\nBest val loss: 0.05754134737146206\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.27316069602966\n\n\nEpoch 95/300\n\ntrain loss: 0.062256446414733234\ntrain loss_segm: 0.062256446414733234\ntrain loss_shape: 0.04076545350725138\ntrain loss_recon: 0.11918477582026131\ntrain unet DSC: [0.9959521889686584, 0.8809823989868164, 0.8900479674339294]\n\ntest loss: 0.06180073903539242\ntest loss_segm: 0.06180073903539242\ntest loss_shape: 0.03793228670763664\ntest loss_recon: 0.11730098724365234\ntest unet DSC: [0.9961584806442261, 0.8840765357017517, 0.8871985077857971]\nBest val loss: 0.05754134737146206\nTime: 85.5312123298645\n\n\nEpoch 96/300\n\ntrain loss: 0.06230340086961094\ntrain loss_segm: 0.06230340086961094\ntrain loss_shape: 0.04079229389376278\ntrain loss_recon: 0.11930707285675822\ntrain unet DSC: [0.995951771736145, 0.8808931708335876, 0.8899598121643066]\n\ntest loss: 0.05890361630381682\ntest loss_segm: 0.05890361630381682\ntest loss_shape: 0.03694395539470208\ntest loss_recon: 0.11417308984658657\ntest unet DSC: [0.9962353706359863, 0.8872660398483276, 0.8937780857086182]\nBest val loss: 0.05754134737146206\nTime: 85.09683036804199\n\nValidation loss stopped to decrease for 30 epochs. Training terminated.\nBest epoch: 74\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678436431994
        }
      },
      "id": "dc35205b-d514-4739-910c-d4102a774959"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.01, lambda_recon=0.001,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/300\n\ntrain loss: 0.30170830744731275\ntrain loss_segm: 0.30170830744731275\ntrain loss_shape: 0.2228647405399552\ntrain loss_recon: 0.46985984603060954\ntrain unet DSC: [0.9415330290794373, 0.5637030005455017, 0.5453333854675293]\n\ntest loss: 0.16016409565240908\ntest loss_segm: 0.16016409565240908\ntest loss_shape: 0.10993720782108796\ntest loss_recon: 0.2989827470901685\ntest unet DSC: [0.9884563088417053, 0.7014467716217041, 0.7868847846984863]\nBest val loss: 0.16016409565240908\nTime: 86.01402878761292\n\n\nEpoch 2/300\n\ntrain loss: 0.1422858934236478\ntrain loss_segm: 0.1422858934236478\ntrain loss_shape: 0.1412860945433001\ntrain loss_recon: 0.3102436627768263\ntrain unet DSC: [0.9908702373504639, 0.7530251741409302, 0.7671416401863098]\n\ntest loss: 0.10539026138110039\ntest loss_segm: 0.10539026138110039\ntest loss_shape: 0.0753032454313376\ntest loss_recon: 0.19482113612003815\ntest unet DSC: [0.9934009909629822, 0.8073424696922302, 0.8342234492301941]\nBest val loss: 0.10539026138110039\nTime: 85.65938448905945\n\n\nEpoch 3/300\n\ntrain loss: 0.11403813590354557\ntrain loss_segm: 0.11403813590354557\ntrain loss_shape: 0.1082533803544467\ntrain loss_recon: 0.25149192953411537\ntrain unet DSC: [0.9927022457122803, 0.7963302731513977, 0.8094137907028198]\n\ntest loss: 0.08638198902973762\ntest loss_segm: 0.08638198902973762\ntest loss_shape: 0.06648435386327597\ntest loss_recon: 0.1792198297304985\ntest unet DSC: [0.9939848780632019, 0.8391711711883545, 0.8582884073257446]\nBest val loss: 0.08638198902973762\nTime: 85.94313716888428\n\n\nEpoch 4/300\n\ntrain loss: 0.10039214157982718\ntrain loss_segm: 0.10039214157982718\ntrain loss_shape: 0.09074451213207425\ntrain loss_recon: 0.21861956202531163\ntrain unet DSC: [0.9936227202415466, 0.8178302645683289, 0.8290345668792725]\n\ntest loss: 0.07760353596546711\ntest loss_segm: 0.07760353596546711\ntest loss_shape: 0.056381285668183595\ntest loss_recon: 0.15998534361521402\ntest unet DSC: [0.994996190071106, 0.8588175773620605, 0.8647222518920898]\nBest val loss: 0.07760353596546711\nTime: 85.57980418205261\n\n\nEpoch 5/300\n\ntrain loss: 0.09148509757994097\ntrain loss_segm: 0.09148509757994097\ntrain loss_shape: 0.07960058220579655\ntrain loss_recon: 0.19643017951446243\ntrain unet DSC: [0.9941779971122742, 0.8320237994194031, 0.8421258330345154]\n\ntest loss: 0.07125255809380458\ntest loss_segm: 0.07125255809380458\ntest loss_shape: 0.05158208912381759\ntest loss_recon: 0.14704115421344072\ntest unet DSC: [0.9951313734054565, 0.8620762825012207, 0.8800522089004517]\nBest val loss: 0.07125255809380458\nTime: 85.83481740951538\n\n\nEpoch 6/300\n\ntrain loss: 0.08633549935832809\ntrain loss_segm: 0.08633549935832809\ntrain loss_shape: 0.07268098713476447\ntrain loss_recon: 0.18256272243548044\ntrain unet DSC: [0.9944602847099304, 0.8397243618965149, 0.8499153256416321]\n\ntest loss: 0.0709735967983038\ntest loss_segm: 0.0709735967983038\ntest loss_shape: 0.04802527183141464\ntest loss_recon: 0.13991618920595217\ntest unet DSC: [0.9954791069030762, 0.8675034642219543, 0.8768635988235474]\nBest val loss: 0.0709735967983038\nTime: 85.85081624984741\n\n\nEpoch 7/300\n\ntrain loss: 0.0828286933748028\ntrain loss_segm: 0.0828286933748028\ntrain loss_shape: 0.06755924326237998\ntrain loss_recon: 0.17242568391787855\ntrain unet DSC: [0.9946641325950623, 0.8451985120773315, 0.8551212549209595]\n\ntest loss: 0.07339519940507717\ntest loss_segm: 0.07339519940507717\ntest loss_shape: 0.04914376053672571\ntest loss_recon: 0.14506374108485687\ntest unet DSC: [0.9956320524215698, 0.8684183955192566, 0.8679949641227722]\nBest val loss: 0.0709735967983038\nTime: 85.72595238685608\n\n\nEpoch 8/300\n\ntrain loss: 0.08002159065460857\ntrain loss_segm: 0.08002159065460857\ntrain loss_shape: 0.06327589905431753\ntrain loss_recon: 0.1637027539029906\ntrain unet DSC: [0.9948551058769226, 0.8497958779335022, 0.8592731356620789]\n\ntest loss: 0.06553426327613684\ntest loss_segm: 0.06553426327613684\ntest loss_shape: 0.04474076867485658\ntest loss_recon: 0.1311283814601409\ntest unet DSC: [0.9957217574119568, 0.8735228180885315, 0.8843797445297241]\nBest val loss: 0.06553426327613684\nTime: 86.23039722442627\n\n\nEpoch 9/300\n\ntrain loss: 0.07805781113573268\ntrain loss_segm: 0.07805781113573268\ntrain loss_shape: 0.06049926597860795\ntrain loss_recon: 0.1582989583287058\ntrain unet DSC: [0.9949711561203003, 0.8528870940208435, 0.8622758984565735]\n\ntest loss: 0.06348189109793076\ntest loss_segm: 0.06348189109793076\ntest loss_shape: 0.043426438115346126\ntest loss_recon: 0.1274824463404142\ntest unet DSC: [0.9958087205886841, 0.8776841759681702, 0.8874221444129944]\nBest val loss: 0.06348189109793076\nTime: 85.06250858306885\n\n\nEpoch 10/300\n\ntrain loss: 0.07652460981773425\ntrain loss_segm: 0.07652460981773425\ntrain loss_shape: 0.058227006039476094\ntrain loss_recon: 0.15361605490310282\ntrain unet DSC: [0.9950644373893738, 0.8553465604782104, 0.8647703528404236]\n\ntest loss: 0.0631908818315237\ntest loss_segm: 0.0631908818315237\ntest loss_shape: 0.04470017313575133\ntest loss_recon: 0.1302795287890312\ntest unet DSC: [0.9957259893417358, 0.8772709369659424, 0.8888440132141113]\nBest val loss: 0.0631908818315237\nTime: 85.91499137878418\n\n\nEpoch 11/300\n\ntrain loss: 0.0751301747145532\ntrain loss_segm: 0.0751301747145532\ntrain loss_shape: 0.056307700167916995\ntrain loss_recon: 0.1497575806666024\ntrain unet DSC: [0.9951528310775757, 0.8576251864433289, 0.8669363856315613]\n\ntest loss: 0.06236332903305689\ntest loss_segm: 0.06236332903305689\ntest loss_shape: 0.04317368232668974\ntest loss_recon: 0.12797247293667915\ntest unet DSC: [0.9958433508872986, 0.8792526125907898, 0.888733983039856]\nBest val loss: 0.06236332903305689\nTime: 85.71411895751953\n\n\nEpoch 12/300\n\ntrain loss: 0.0745618679100954\ntrain loss_segm: 0.0745618679100954\ntrain loss_shape: 0.055062187576218495\ntrain loss_recon: 0.14779851330986507\ntrain unet DSC: [0.995182454586029, 0.8584386110305786, 0.8678581118583679]\n\ntest loss: 0.062059737455386385\ntest loss_segm: 0.062059737455386385\ntest loss_shape: 0.04126629921106192\ntest loss_recon: 0.12209281096091637\ntest unet DSC: [0.9959740042686462, 0.8796403408050537, 0.8889627456665039]\nBest val loss: 0.062059737455386385\nTime: 85.8001356124878\n\n\nEpoch 13/300\n\ntrain loss: 0.07352680524316015\ntrain loss_segm: 0.07352680524316015\ntrain loss_shape: 0.053784557347056235\ntrain loss_recon: 0.14515232851233664\ntrain unet DSC: [0.9952537417411804, 0.8601968288421631, 0.8694724440574646]\n\ntest loss: 0.06916836582315274\ntest loss_segm: 0.06916836582315274\ntest loss_shape: 0.043046911079914145\ntest loss_recon: 0.12968697914710411\ntest unet DSC: [0.9956498742103577, 0.8700591921806335, 0.8758718967437744]\nBest val loss: 0.062059737455386385\nTime: 85.74372673034668\n\n\nEpoch 14/300\n\ntrain loss: 0.07243317521259754\ntrain loss_segm: 0.07243317521259754\ntrain loss_shape: 0.05255261796844911\ntrain loss_recon: 0.14259197817573063\ntrain unet DSC: [0.9953217506408691, 0.8620882630348206, 0.8712459802627563]\n\ntest loss: 0.06195215909526898\ntest loss_segm: 0.06195215909526898\ntest loss_shape: 0.042734201137836166\ntest loss_recon: 0.12535222371419272\ntest unet DSC: [0.9957704544067383, 0.8761381506919861, 0.8917344808578491]\nBest val loss: 0.06195215909526898\nTime: 86.06823301315308\n\n\nEpoch 15/300\n\ntrain loss: 0.07217680238470246\ntrain loss_segm: 0.07217680238470246\ntrain loss_shape: 0.051876327376576915\ntrain loss_recon: 0.14169122071205814\ntrain unet DSC: [0.995324432849884, 0.8624072670936584, 0.8717073798179626]\n\ntest loss: 0.06008625775575638\ntest loss_segm: 0.06008625775575638\ntest loss_shape: 0.040777149490821056\ntest loss_recon: 0.12112320844943707\ntest unet DSC: [0.9959203004837036, 0.87983638048172, 0.89478999376297]\nBest val loss: 0.06008625775575638\nTime: 86.17212772369385\n\n\nEpoch 16/300\n\ntrain loss: 0.07122863963529279\ntrain loss_segm: 0.07122863963529279\ntrain loss_shape: 0.050895457332835924\ntrain loss_recon: 0.139653913205183\ntrain unet DSC: [0.9953849911689758, 0.8638514280319214, 0.8732815384864807]\n\ntest loss: 0.06030127472984485\ntest loss_segm: 0.06030127472984485\ntest loss_shape: 0.03918524764669247\ntest loss_recon: 0.11938720941543579\ntest unet DSC: [0.9960842728614807, 0.8829672336578369, 0.8919221758842468]\nBest val loss: 0.06008625775575638\nTime: 85.78523826599121\n\n\nEpoch 17/300\n\ntrain loss: 0.07092360532076299\ntrain loss_segm: 0.07092360532076299\ntrain loss_shape: 0.050361056045829494\ntrain loss_recon: 0.1385879086542733\ntrain unet DSC: [0.9954156875610352, 0.8645819425582886, 0.873684287071228]\n\ntest loss: 0.06307173644502957\ntest loss_segm: 0.06307173644502957\ntest loss_shape: 0.0437716910472283\ntest loss_recon: 0.12960815888184768\ntest unet DSC: [0.9955406188964844, 0.8715452551841736, 0.8915943503379822]\nBest val loss: 0.06008625775575638\nTime: 85.65146279335022\n\n\nEpoch 18/300\n\ntrain loss: 0.07047021327705323\ntrain loss_segm: 0.07047021327705323\ntrain loss_shape: 0.049743091522515576\ntrain loss_recon: 0.1375839698918258\ntrain unet DSC: [0.9954505562782288, 0.8653590679168701, 0.8744245171546936]\n\ntest loss: 0.0593467751183571\ntest loss_segm: 0.0593467751183571\ntest loss_shape: 0.03953900914161633\ntest loss_recon: 0.12093004355063805\ntest unet DSC: [0.9961498975753784, 0.885607898235321, 0.8922991752624512]\nBest val loss: 0.0593467751183571\nTime: 85.59588670730591\n\n\nEpoch 19/300\n\ntrain loss: 0.06977326649276516\ntrain loss_segm: 0.06977326649276516\ntrain loss_shape: 0.049119431761246696\ntrain loss_recon: 0.136149449816233\ntrain unet DSC: [0.9954851269721985, 0.8665494322776794, 0.8755608201026917]\n\ntest loss: 0.06472008742201023\ntest loss_segm: 0.06472008742201023\ntest loss_shape: 0.040507746048462696\ntest loss_recon: 0.1234515355183528\ntest unet DSC: [0.9959286451339722, 0.8773991465568542, 0.8828248381614685]\nBest val loss: 0.0593467751183571\nTime: 86.11354637145996\n\n\nEpoch 20/300\n\ntrain loss: 0.06921590423753744\ntrain loss_segm: 0.06921590423753744\ntrain loss_shape: 0.04841506250108345\ntrain loss_recon: 0.13475608561612382\ntrain unet DSC: [0.9955247044563293, 0.8674495816230774, 0.8764643669128418]\n\ntest loss: 0.05873154103755951\ntest loss_segm: 0.05873154103755951\ntest loss_shape: 0.03983445809437679\ntest loss_recon: 0.11973425363882995\ntest unet DSC: [0.996078372001648, 0.8845790028572083, 0.8950629830360413]\nBest val loss: 0.05873154103755951\nTime: 86.04561591148376\n\n\nEpoch 21/300\n\ntrain loss: 0.06892289893253695\ntrain loss_segm: 0.06892289893253695\ntrain loss_shape: 0.04804460697253294\ntrain loss_recon: 0.13423655267003215\ntrain unet DSC: [0.9955463409423828, 0.8679742813110352, 0.8769569993019104]\n\ntest loss: 0.05950947764974374\ntest loss_segm: 0.05950947764974374\ntest loss_shape: 0.04010514666636785\ntest loss_recon: 0.1197823316623003\ntest unet DSC: [0.9959771633148193, 0.8805130124092102, 0.8949597477912903]\nBest val loss: 0.05873154103755951\nTime: 85.62129759788513\n\n\nEpoch 22/300\n\ntrain loss: 0.06871217666075954\ntrain loss_segm: 0.06871217666075954\ntrain loss_shape: 0.047758939499153365\ntrain loss_recon: 0.13378647082968603\ntrain unet DSC: [0.9955621957778931, 0.868423342704773, 0.87727952003479]\n\ntest loss: 0.06085527917513481\ntest loss_segm: 0.06085527917513481\ntest loss_shape: 0.03849267481993406\ntest loss_recon: 0.11778346391824576\ntest unet DSC: [0.9960426092147827, 0.8815599083900452, 0.890579879283905]\nBest val loss: 0.05873154103755951\nTime: 85.74695539474487\n\n\nEpoch 23/300\n\ntrain loss: 0.06844359042146538\ntrain loss_segm: 0.06844359042146538\ntrain loss_shape: 0.04740462763400018\ntrain loss_recon: 0.1332810472083997\ntrain unet DSC: [0.9955605864524841, 0.8686643242835999, 0.8778850436210632]\n\ntest loss: 0.0633389165577216\ntest loss_segm: 0.0633389165577216\ntest loss_shape: 0.03959770538868048\ntest loss_recon: 0.12063353489606808\ntest unet DSC: [0.9960275292396545, 0.8796961903572083, 0.8842986822128296]\nBest val loss: 0.05873154103755951\nTime: 85.399667263031\n\n\nEpoch 24/300\n\ntrain loss: 0.06809346615985225\ntrain loss_segm: 0.06809346615985225\ntrain loss_shape: 0.04692711709420892\ntrain loss_recon: 0.13233033124404617\ntrain unet DSC: [0.9956005215644836, 0.8694236874580383, 0.8782538175582886]\n\ntest loss: 0.05836980197674189\ntest loss_segm: 0.05836980197674189\ntest loss_shape: 0.038909074874260485\ntest loss_recon: 0.11799537065701607\ntest unet DSC: [0.9960846304893494, 0.8841493129730225, 0.8963691592216492]\nBest val loss: 0.05836980197674189\nTime: 85.88735103607178\n\n\nEpoch 25/300\n\ntrain loss: 0.06733433159563361\ntrain loss_segm: 0.06733433159563361\ntrain loss_shape: 0.04636859016705163\ntrain loss_recon: 0.13097338510465018\ntrain unet DSC: [0.9956330060958862, 0.870649516582489, 0.8797383308410645]\n\ntest loss: 0.05834232499966255\ntest loss_segm: 0.05834232499966255\ntest loss_shape: 0.03868905817851042\ntest loss_recon: 0.11689269542694092\ntest unet DSC: [0.9961420893669128, 0.8856695294380188, 0.8948391675949097]\nBest val loss: 0.05834232499966255\nTime: 85.7782564163208\n\n\nEpoch 26/300\n\ntrain loss: 0.0672058689443371\ntrain loss_segm: 0.0672058689443371\ntrain loss_shape: 0.04612213535870932\ntrain loss_recon: 0.130772032315218\ntrain unet DSC: [0.9956442713737488, 0.8708590865135193, 0.8799066543579102]\n\ntest loss: 0.05824093214976482\ntest loss_segm: 0.05824093214976482\ntest loss_shape: 0.03714804341777777\ntest loss_recon: 0.11399220044796284\ntest unet DSC: [0.9961938858032227, 0.8852459788322449, 0.8952540755271912]\nBest val loss: 0.05824093214976482\nTime: 85.63301658630371\n\n\nEpoch 27/300\n\ntrain loss: 0.0669054495335757\ntrain loss_segm: 0.0669054495335757\ntrain loss_shape: 0.045926191950146154\ntrain loss_recon: 0.13017890687230266\ntrain unet DSC: [0.995652973651886, 0.8713982701301575, 0.8803929090499878]\n\ntest loss: 0.059363665393529795\ntest loss_segm: 0.059363665393529795\ntest loss_shape: 0.04033628631478701\ntest loss_recon: 0.12123218407997718\ntest unet DSC: [0.9958688020706177, 0.8795918226242065, 0.8965251445770264]\nBest val loss: 0.05824093214976482\nTime: 85.51782512664795\n\n\nEpoch 28/300\n\ntrain loss: 0.06692002734900275\ntrain loss_segm: 0.06692002734900275\ntrain loss_shape: 0.045846115161157865\ntrain loss_recon: 0.13019978170153462\ntrain unet DSC: [0.9956690073013306, 0.8713940978050232, 0.8803656697273254]\n\ntest loss: 0.05795176374988678\ntest loss_segm: 0.05795176374988678\ntest loss_shape: 0.03778310932027988\ntest loss_recon: 0.11712193794739552\ntest unet DSC: [0.9961798191070557, 0.8855586647987366, 0.895667552947998]\nBest val loss: 0.05795176374988678\nTime: 85.28454852104187\n\n\nEpoch 29/300\n\ntrain loss: 0.06668665354387669\ntrain loss_segm: 0.06668665354387669\ntrain loss_shape: 0.045627572868443746\ntrain loss_recon: 0.12976837384549877\ntrain unet DSC: [0.9956763386726379, 0.8718059659004211, 0.8808249831199646]\n\ntest loss: 0.058502643154217646\ntest loss_segm: 0.058502643154217646\ntest loss_shape: 0.03849988201489815\ntest loss_recon: 0.1157838258987818\ntest unet DSC: [0.9961627125740051, 0.8845018148422241, 0.894757091999054]\nBest val loss: 0.05795176374988678\nTime: 85.38092732429504\n\n\nEpoch 30/300\n\ntrain loss: 0.06637725146793866\ntrain loss_segm: 0.06637725146793866\ntrain loss_shape: 0.0452842098672556\ntrain loss_recon: 0.12915614662291128\ntrain unet DSC: [0.9956933259963989, 0.8722948431968689, 0.8812445402145386]\n\ntest loss: 0.05770588885897245\ntest loss_segm: 0.05770588885897245\ntest loss_shape: 0.037940690628229044\ntest loss_recon: 0.11510637631783119\ntest unet DSC: [0.9961767792701721, 0.8861929178237915, 0.896458089351654]\nBest val loss: 0.05770588885897245\nTime: 85.62130188941956\n\n\nEpoch 31/300\n\ntrain loss: 0.06593329119908659\ntrain loss_segm: 0.06593329119908659\ntrain loss_shape: 0.0449514431478102\ntrain loss_recon: 0.12835120626642735\ntrain unet DSC: [0.9957154393196106, 0.873013436794281, 0.8821370601654053]\n\ntest loss: 0.05672091828324856\ntest loss_segm: 0.05672091828324856\ntest loss_shape: 0.03763539716601372\ntest loss_recon: 0.11592579957766411\ntest unet DSC: [0.9961780905723572, 0.8876718282699585, 0.8979918360710144]\nBest val loss: 0.05672091828324856\nTime: 85.77383279800415\n\n\nEpoch 32/300\n\ntrain loss: 0.06614494165759298\ntrain loss_segm: 0.06614494165759298\ntrain loss_shape: 0.0449142393101997\ntrain loss_recon: 0.1285494188719158\ntrain unet DSC: [0.9957157373428345, 0.8727672100067139, 0.8816455602645874]\n\ntest loss: 0.058546392199320674\ntest loss_segm: 0.058546392199320674\ntest loss_shape: 0.03931964083741873\ntest loss_recon: 0.1190815690236214\ntest unet DSC: [0.9960870742797852, 0.8850845098495483, 0.8943549990653992]\nBest val loss: 0.05672091828324856\nTime: 86.49126935005188\n\n\nEpoch 33/300\n\ntrain loss: 0.06632730494477326\ntrain loss_segm: 0.06632730494477326\ntrain loss_shape: 0.045116787870662124\ntrain loss_recon: 0.12911552454851852\ntrain unet DSC: [0.9956854581832886, 0.8722677826881409, 0.8815261125564575]\n\ntest loss: 0.061042205358927064\ntest loss_segm: 0.061042205358927064\ntest loss_shape: 0.041324493976739735\ntest loss_recon: 0.12404853869707157\ntest unet DSC: [0.9957810640335083, 0.8762753009796143, 0.8937748670578003]\nBest val loss: 0.05672091828324856\nTime: 86.04839968681335\n\n\nEpoch 34/300\n\ntrain loss: 0.0657017423853844\ntrain loss_segm: 0.0657017423853844\ntrain loss_shape: 0.04464750258322758\ntrain loss_recon: 0.1279297757752334\ntrain unet DSC: [0.9957217574119568, 0.8733819723129272, 0.8825006484985352]\n\ntest loss: 0.05753280337040241\ntest loss_segm: 0.05753280337040241\ntest loss_shape: 0.03848241804501949\ntest loss_recon: 0.11640975261345887\ntest unet DSC: [0.9961233139038086, 0.8857368230819702, 0.8972588777542114]\nBest val loss: 0.05672091828324856\nTime: 86.07205128669739\n\n\nEpoch 35/300\n\ntrain loss: 0.06539770989101144\ntrain loss_segm: 0.06539770989101144\ntrain loss_shape: 0.04433489190060881\ntrain loss_recon: 0.12726527112948743\ntrain unet DSC: [0.9957552552223206, 0.8739711046218872, 0.882843017578125]\n\ntest loss: 0.05704132352883999\ntest loss_segm: 0.05704132352883999\ntest loss_shape: 0.03751675631755438\ntest loss_recon: 0.1150436859864455\ntest unet DSC: [0.99615478515625, 0.8864747285842896, 0.8979153633117676]\nBest val loss: 0.05672091828324856\nTime: 86.30671906471252\n\n\nEpoch 36/300\n\ntrain loss: 0.06527136900474\ntrain loss_segm: 0.06527136900474\ntrain loss_shape: 0.04419473968917811\ntrain loss_recon: 0.12709618444684184\ntrain unet DSC: [0.9957450032234192, 0.8740596771240234, 0.8832172155380249]\n\ntest loss: 0.05986314840041674\ntest loss_segm: 0.05986314840041674\ntest loss_shape: 0.03817885364286411\ntest loss_recon: 0.1179188887278239\ntest unet DSC: [0.9962176084518433, 0.8855596780776978, 0.8904687762260437]\nBest val loss: 0.05672091828324856\nTime: 85.94784569740295\n\n\nEpoch 37/300\n\ntrain loss: 0.06525039248451402\ntrain loss_segm: 0.06525039248451402\ntrain loss_shape: 0.04412165239359005\ntrain loss_recon: 0.1270392816277999\ntrain unet DSC: [0.9957606792449951, 0.8741153478622437, 0.8831746578216553]\n\ntest loss: 0.0706417352343217\ntest loss_segm: 0.0706417352343217\ntest loss_shape: 0.04287157737864898\ntest loss_recon: 0.13016510773927736\ntest unet DSC: [0.9957332015037537, 0.8706510663032532, 0.8699540495872498]\nBest val loss: 0.05672091828324856\nTime: 85.79180455207825\n\n\nEpoch 38/300\n\ntrain loss: 0.06535127360515203\ntrain loss_segm: 0.06535127360515203\ntrain loss_shape: 0.04424716961346095\ntrain loss_recon: 0.12734229164787486\ntrain unet DSC: [0.9957454800605774, 0.8739941716194153, 0.8830718994140625]\n\ntest loss: 0.059114597928829685\ntest loss_segm: 0.059114597928829685\ntest loss_shape: 0.03799782889202619\ntest loss_recon: 0.11670822669298221\ntest unet DSC: [0.9961389303207397, 0.8851521611213684, 0.8929377794265747]\nBest val loss: 0.05672091828324856\nTime: 85.89148759841919\n\n\nEpoch 39/300\n\ntrain loss: 0.0651106634236212\ntrain loss_segm: 0.0651106634236212\ntrain loss_shape: 0.04401295926846281\ntrain loss_recon: 0.12667359015609644\ntrain unet DSC: [0.9957663416862488, 0.8744155168533325, 0.883437991142273]\n\ntest loss: 0.06049600644753529\ntest loss_segm: 0.06049600644753529\ntest loss_shape: 0.038228124093550905\ntest loss_recon: 0.11910687807278755\ntest unet DSC: [0.9962451457977295, 0.8861252069473267, 0.8874388337135315]\nBest val loss: 0.05672091828324856\nTime: 85.97663068771362\n\n\nEpoch 40/300\n\ntrain loss: 0.06475144179089914\ntrain loss_segm: 0.06475144179089914\ntrain loss_shape: 0.04372856431180918\ntrain loss_recon: 0.12605158814901038\ntrain unet DSC: [0.9957752227783203, 0.8749459981918335, 0.8841043710708618]\n\ntest loss: 0.0636831484735012\ntest loss_segm: 0.0636831484735012\ntest loss_shape: 0.039734671608759806\ntest loss_recon: 0.12116524195059752\ntest unet DSC: [0.9960749745368958, 0.8806706070899963, 0.8821491599082947]\nBest val loss: 0.05672091828324856\nTime: 85.90706276893616\n\n\nEpoch 41/300\n\ntrain loss: 0.06487622173337997\ntrain loss_segm: 0.06487622173337997\ntrain loss_shape: 0.043705729979880245\ntrain loss_recon: 0.12611676092389262\ntrain unet DSC: [0.9957851767539978, 0.8748912811279297, 0.8837694525718689]\n\ntest loss: 0.05701054403415093\ntest loss_segm: 0.05701054403415093\ntest loss_shape: 0.03705346894760927\ntest loss_recon: 0.11478967238695194\ntest unet DSC: [0.996218740940094, 0.8871399164199829, 0.8978062272071838]\nBest val loss: 0.05672091828324856\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.75502490997314\n\n\nEpoch 42/300\n\ntrain loss: 0.06357374756679504\ntrain loss_segm: 0.06357374756679504\ntrain loss_shape: 0.04299380073818979\ntrain loss_recon: 0.12431336165983466\ntrain unet DSC: [0.9958418011665344, 0.8770228624343872, 0.8861323595046997]\n\ntest loss: 0.05653032985253212\ntest loss_segm: 0.05653032985253212\ntest loss_shape: 0.03642685080950077\ntest loss_recon: 0.11278015986467019\ntest unet DSC: [0.9962472915649414, 0.887475311756134, 0.8986863493919373]\nBest val loss: 0.05653032985253212\nTime: 85.7839241027832\n\n\nEpoch 43/300\n\ntrain loss: 0.06338955539785608\ntrain loss_segm: 0.06338955539785608\ntrain loss_shape: 0.04284292650467987\ntrain loss_recon: 0.12379094467887396\ntrain unet DSC: [0.9958492517471313, 0.8773464560508728, 0.8864143490791321]\n\ntest loss: 0.056684695375271335\ntest loss_segm: 0.056684695375271335\ntest loss_shape: 0.03655378472728607\ntest loss_recon: 0.11342000961303711\ntest unet DSC: [0.9962617754936218, 0.8882620334625244, 0.8976090550422668]\nBest val loss: 0.05653032985253212\nTime: 85.8061728477478\n\n\nEpoch 44/300\n\ntrain loss: 0.06334727222123478\ntrain loss_segm: 0.06334727222123478\ntrain loss_shape: 0.04272165521979332\ntrain loss_recon: 0.12363585829734802\ntrain unet DSC: [0.9958547949790955, 0.8774077892303467, 0.886420726776123]\n\ntest loss: 0.056344372435257986\ntest loss_segm: 0.056344372435257986\ntest loss_shape: 0.03690159115462731\ntest loss_recon: 0.11341909567515056\ntest unet DSC: [0.9962197542190552, 0.8877027630805969, 0.8990713953971863]\nBest val loss: 0.056344372435257986\nTime: 85.70457601547241\n\n\nEpoch 45/300\n\ntrain loss: 0.06326898519845703\ntrain loss_segm: 0.06326898519845703\ntrain loss_shape: 0.0426922555116913\ntrain loss_recon: 0.12351739557483528\ntrain unet DSC: [0.9958645701408386, 0.8775467872619629, 0.8865267634391785]\n\ntest loss: 0.05635538878731239\ntest loss_segm: 0.05635538878731239\ntest loss_shape: 0.036841901687857434\ntest loss_recon: 0.1139200910543784\ntest unet DSC: [0.9962561130523682, 0.888757586479187, 0.8981940150260925]\nBest val loss: 0.056344372435257986\nTime: 85.96789169311523\n\n\nEpoch 46/300\n\ntrain loss: 0.0631357515112886\ntrain loss_segm: 0.0631357515112886\ntrain loss_shape: 0.042612158870206605\ntrain loss_recon: 0.12329488539997535\ntrain unet DSC: [0.9958675503730774, 0.8777713179588318, 0.8868035674095154]\n\ntest loss: 0.061080924880046114\ntest loss_segm: 0.061080924880046114\ntest loss_shape: 0.038141957842386685\ntest loss_recon: 0.11823204236152844\ntest unet DSC: [0.9961431622505188, 0.8834640383720398, 0.8879858255386353]\nBest val loss: 0.056344372435257986\nTime: 86.241952419281\n\n\nEpoch 47/300\n\ntrain loss: 0.0632953805306667\ntrain loss_segm: 0.0632953805306667\ntrain loss_shape: 0.042635670191125026\ntrain loss_recon: 0.12352970317949223\ntrain unet DSC: [0.9958591461181641, 0.8775055408477783, 0.886523962020874]\n\ntest loss: 0.05845263141852159\ntest loss_segm: 0.05845263141852159\ntest loss_shape: 0.036740065194093265\ntest loss_recon: 0.11420602370531131\ntest unet DSC: [0.9962022304534912, 0.885664701461792, 0.8941866755485535]\nBest val loss: 0.056344372435257986\nTime: 86.16463947296143\n\n\nEpoch 48/300\n\ntrain loss: 0.06321203350266323\ntrain loss_segm: 0.06321203350266323\ntrain loss_shape: 0.04262699490955359\ntrain loss_recon: 0.12341166862958594\ntrain unet DSC: [0.99586421251297, 0.8776652812957764, 0.8866446018218994]\n\ntest loss: 0.06023121758913382\ntest loss_segm: 0.06023121758913382\ntest loss_shape: 0.03777500533331663\ntest loss_recon: 0.1174634053156926\ntest unet DSC: [0.9961821436882019, 0.8850957155227661, 0.8893401026725769]\nBest val loss: 0.056344372435257986\nTime: 85.94959950447083\n\n\nEpoch 49/300\n\ntrain loss: 0.06312191106756276\ntrain loss_segm: 0.06312191106756276\ntrain loss_shape: 0.042571994723587095\ntrain loss_recon: 0.12323451494868798\ntrain unet DSC: [0.9958723187446594, 0.8778380155563354, 0.8867522478103638]\n\ntest loss: 0.058954457537486\ntest loss_segm: 0.058954457537486\ntest loss_shape: 0.037326913995620534\ntest loss_recon: 0.11568369468053182\ntest unet DSC: [0.9962334036827087, 0.8863473534584045, 0.8920256495475769]\nBest val loss: 0.056344372435257986\nTime: 85.76434206962585\n\n\nEpoch 50/300\n\ntrain loss: 0.06313301776123198\ntrain loss_segm: 0.06313301776123198\ntrain loss_shape: 0.04252804952520359\ntrain loss_recon: 0.1231613264808172\ntrain unet DSC: [0.9958715438842773, 0.8777138590812683, 0.8867402672767639]\n\ntest loss: 0.05850640044380457\ntest loss_segm: 0.05850640044380457\ntest loss_shape: 0.036971802894885726\ntest loss_recon: 0.11499373118082683\ntest unet DSC: [0.9962028861045837, 0.8862797617912292, 0.8937221169471741]\nBest val loss: 0.056344372435257986\nTime: 86.10944938659668\n\n\nEpoch 51/300\n\ntrain loss: 0.06304796976096268\ntrain loss_segm: 0.06304796976096268\ntrain loss_shape: 0.04249875584640835\ntrain loss_recon: 0.12309064103078239\ntrain unet DSC: [0.99587482213974, 0.8779382705688477, 0.8868808746337891]\n\ntest loss: 0.057551512446923137\ntest loss_segm: 0.057551512446923137\ntest loss_shape: 0.03659337491561205\ntest loss_recon: 0.11356591261350192\ntest unet DSC: [0.996240496635437, 0.8873477578163147, 0.8956188559532166]\nBest val loss: 0.056344372435257986\nTime: 86.27023124694824\n\n\nEpoch 52/300\n\ntrain loss: 0.06300772658160216\ntrain loss_segm: 0.06300772658160216\ntrain loss_shape: 0.042416603905679306\ntrain loss_recon: 0.12298597414282304\ntrain unet DSC: [0.9958756566047668, 0.8779974579811096, 0.8869924545288086]\n\ntest loss: 0.05644798040007933\ntest loss_segm: 0.05644798040007933\ntest loss_shape: 0.03637215194220726\ntest loss_recon: 0.1125336060157189\ntest unet DSC: [0.9962571263313293, 0.8878733515739441, 0.8985784649848938]\nBest val loss: 0.056344372435257986\nTime: 86.21011257171631\n\n\nEpoch 53/300\n\ntrain loss: 0.06305690751045565\ntrain loss_segm: 0.06305690751045565\ntrain loss_shape: 0.04251346225508406\ntrain loss_recon: 0.12311276416235332\ntrain unet DSC: [0.9958670139312744, 0.8778658509254456, 0.8869312405586243]\n\ntest loss: 0.058964719566015095\ntest loss_segm: 0.058964719566015095\ntest loss_shape: 0.03883226898809274\ntest loss_recon: 0.11889783694193913\ntest unet DSC: [0.9959444999694824, 0.8800877332687378, 0.8970162868499756]\nBest val loss: 0.056344372435257986\nTime: 86.70194792747498\n\n\nEpoch 54/300\n\ntrain loss: 0.06293648809100254\ntrain loss_segm: 0.06293648809100254\ntrain loss_shape: 0.04235704521401019\ntrain loss_recon: 0.1227735386619085\ntrain unet DSC: [0.9958851337432861, 0.8781870603561401, 0.8870674967765808]\n\ntest loss: 0.05717597911373163\ntest loss_segm: 0.05717597911373163\ntest loss_shape: 0.037320826919032976\ntest loss_recon: 0.1151001499249385\ntest unet DSC: [0.9961252212524414, 0.8847110271453857, 0.898781418800354]\nBest val loss: 0.056344372435257986\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.8292818069458\n\n\nEpoch 55/300\n\ntrain loss: 0.06272434761535518\ntrain loss_segm: 0.06272434761535518\ntrain loss_shape: 0.0422421050128303\ntrain loss_recon: 0.12262192443956303\ntrain unet DSC: [0.9958878755569458, 0.8784540891647339, 0.8874855637550354]\n\ntest loss: 0.05667971867399338\ntest loss_segm: 0.05667971867399338\ntest loss_shape: 0.0367200429049822\ntest loss_recon: 0.11348581314086914\ntest unet DSC: [0.9962573051452637, 0.8883557319641113, 0.8973370790481567]\nBest val loss: 0.056344372435257986\nTime: 86.3228108882904\n\n\nEpoch 56/300\n\ntrain loss: 0.06262608542095256\ntrain loss_segm: 0.06262608542095256\ntrain loss_shape: 0.04224618776595291\ntrain loss_recon: 0.12239230546770216\ntrain unet DSC: [0.9958932399749756, 0.8786418437957764, 0.8876947164535522]\n\ntest loss: 0.05758065443772536\ntest loss_segm: 0.05758065443772536\ntest loss_shape: 0.03683542269162643\ntest loss_recon: 0.11432649539067195\ntest unet DSC: [0.9962625503540039, 0.888026773929596, 0.8948869109153748]\nBest val loss: 0.056344372435257986\nTime: 86.6214907169342\n\n\nEpoch 57/300\n\ntrain loss: 0.06270821465627302\ntrain loss_segm: 0.06270821465627302\ntrain loss_shape: 0.04213710115114345\ntrain loss_recon: 0.12230965759180769\ntrain unet DSC: [0.9958906769752502, 0.8784489631652832, 0.8874969482421875]\n\ntest loss: 0.05708705891783421\ntest loss_segm: 0.05708705891783421\ntest loss_shape: 0.03668277801420444\ntest loss_recon: 0.11389970015256833\ntest unet DSC: [0.9962683320045471, 0.888445258140564, 0.8961929082870483]\nBest val loss: 0.056344372435257986\nTime: 86.57955002784729\n\n\nEpoch 58/300\n\ntrain loss: 0.06264879561583453\ntrain loss_segm: 0.06264879561583453\ntrain loss_shape: 0.04224234868925583\ntrain loss_recon: 0.12251971492284461\ntrain unet DSC: [0.9958897829055786, 0.8785911202430725, 0.8875813484191895]\n\ntest loss: 0.06009419520313923\ntest loss_segm: 0.06009419520313923\ntest loss_shape: 0.03756360915035774\ntest loss_recon: 0.11712736808336698\ntest unet DSC: [0.9961475729942322, 0.884515643119812, 0.890417754650116]\nBest val loss: 0.056344372435257986\nTime: 86.40950751304626\n\n\nEpoch 59/300\n\ntrain loss: 0.06263364830254754\ntrain loss_segm: 0.06263364830254754\ntrain loss_shape: 0.04219698863504808\ntrain loss_recon: 0.12234472587138792\ntrain unet DSC: [0.9958906769752502, 0.8785668611526489, 0.8876780271530151]\n\ntest loss: 0.056230632636027456\ntest loss_segm: 0.056230632636027456\ntest loss_shape: 0.036274731206970334\ntest loss_recon: 0.11270504425733517\ntest unet DSC: [0.9962826371192932, 0.888918936252594, 0.8982431292533875]\nBest val loss: 0.056230632636027456\nTime: 85.9988796710968\n\n\nEpoch 60/300\n\ntrain loss: 0.06260801276451425\ntrain loss_segm: 0.06260801276451425\ntrain loss_shape: 0.04216323337883134\ntrain loss_recon: 0.12230601076838336\ntrain unet DSC: [0.9958963394165039, 0.8786728978157043, 0.8875632882118225]\n\ntest loss: 0.05662266738139666\ntest loss_segm: 0.05662266738139666\ntest loss_shape: 0.03665406352434403\ntest loss_recon: 0.11334669589996338\ntest unet DSC: [0.9962696433067322, 0.8887885808944702, 0.8971776962280273]\nBest val loss: 0.056230632636027456\nTime: 86.14174818992615\n\n\nEpoch 61/300\n\ntrain loss: 0.06255851978365379\ntrain loss_segm: 0.06255851978365379\ntrain loss_shape: 0.04208710813258268\ntrain loss_recon: 0.1221394818040389\ntrain unet DSC: [0.9959028959274292, 0.878739058971405, 0.8877365589141846]\n\ntest loss: 0.056365399119945675\ntest loss_segm: 0.056365399119945675\ntest loss_shape: 0.036481206759046286\ntest loss_recon: 0.11318483260961679\ntest unet DSC: [0.9962760806083679, 0.888856828212738, 0.8980090618133545]\nBest val loss: 0.056230632636027456\nTime: 86.19164633750916\n\n\nEpoch 62/300\n\ntrain loss: 0.06258728275004821\ntrain loss_segm: 0.06258728275004821\ntrain loss_shape: 0.0421429169423218\ntrain loss_recon: 0.12228312258478961\ntrain unet DSC: [0.9958988428115845, 0.8787227869033813, 0.8876537680625916]\n\ntest loss: 0.05764471978331224\ntest loss_segm: 0.05764471978331224\ntest loss_shape: 0.03654968743331921\ntest loss_recon: 0.11403477191925049\ntest unet DSC: [0.9962704181671143, 0.8879658579826355, 0.8947531580924988]\nBest val loss: 0.056230632636027456\nTime: 86.1323676109314\n\n\nEpoch 63/300\n\ntrain loss: 0.06263627394845214\ntrain loss_segm: 0.06263627394845214\ntrain loss_shape: 0.042169657099661945\ntrain loss_recon: 0.1223747552950171\ntrain unet DSC: [0.9958963394165039, 0.8786124587059021, 0.8875168561935425]\n\ntest loss: 0.05829683299630116\ntest loss_segm: 0.05829683299630116\ntest loss_shape: 0.03670948853668494\ntest loss_recon: 0.11468055003728622\ntest unet DSC: [0.9962445497512817, 0.8871963620185852, 0.8934745788574219]\nBest val loss: 0.056230632636027456\nTime: 88.1134660243988\n\n\nEpoch 64/300\n\ntrain loss: 0.06260521287876594\ntrain loss_segm: 0.06260521287876594\ntrain loss_shape: 0.04216644472053534\ntrain loss_recon: 0.12222845154472545\ntrain unet DSC: [0.9959005117416382, 0.878731906414032, 0.8876979351043701]\n\ntest loss: 0.056263561527698465\ntest loss_segm: 0.056263561527698465\ntest loss_shape: 0.036429453641176224\ntest loss_recon: 0.11302215777910672\ntest unet DSC: [0.9962189793586731, 0.8870616555213928, 0.8995855450630188]\nBest val loss: 0.056230632636027456\nTime: 87.03094387054443\n\n\nEpoch 65/300\n\ntrain loss: 0.06251814843544477\ntrain loss_segm: 0.06251814843544477\ntrain loss_shape: 0.04216132204555258\ntrain loss_recon: 0.1223273801652691\ntrain unet DSC: [0.9958919882774353, 0.8787478804588318, 0.8879370093345642]\n\ntest loss: 0.05640972138215334\ntest loss_segm: 0.05640972138215334\ntest loss_shape: 0.036131531095657594\ntest loss_recon: 0.11260532721495017\ntest unet DSC: [0.9963027834892273, 0.8893055319786072, 0.8973515033721924]\nBest val loss: 0.056230632636027456\nTime: 86.24084901809692\n\n\nEpoch 66/300\n\ntrain loss: 0.06255734445456462\ntrain loss_segm: 0.06255734445456462\ntrain loss_shape: 0.04211378967554509\ntrain loss_recon: 0.12216667019868199\ntrain unet DSC: [0.9958964586257935, 0.8787868618965149, 0.8877207040786743]\n\ntest loss: 0.056223596326815777\ntest loss_segm: 0.056223596326815777\ntest loss_shape: 0.03610959276556969\ntest loss_recon: 0.11215609312057495\ntest unet DSC: [0.9962851405143738, 0.888671875, 0.8985596895217896]\nBest val loss: 0.056223596326815777\nTime: 85.72363495826721\n\n\nEpoch 67/300\n\ntrain loss: 0.06253193971948533\ntrain loss_segm: 0.06253193971948533\ntrain loss_shape: 0.04210326955005338\ntrain loss_recon: 0.1220843995673747\ntrain unet DSC: [0.995905339717865, 0.8787829279899597, 0.8877628445625305]\n\ntest loss: 0.05748189995304132\ntest loss_segm: 0.05748189995304132\ntest loss_shape: 0.0367350584994524\ntest loss_recon: 0.11417126961243458\ntest unet DSC: [0.9962357878684998, 0.8875834941864014, 0.8957257270812988]\nBest val loss: 0.056223596326815777\nTime: 85.64099478721619\n\n\nEpoch 68/300\n\ntrain loss: 0.06257754097445102\ntrain loss_segm: 0.06257754097445102\ntrain loss_shape: 0.042118417853607405\ntrain loss_recon: 0.12218481905852692\ntrain unet DSC: [0.9959002137184143, 0.8787561058998108, 0.8876526951789856]\n\ntest loss: 0.06055107473945006\ntest loss_segm: 0.06055107473945006\ntest loss_shape: 0.037902021303008765\ntest loss_recon: 0.11783456038206051\ntest unet DSC: [0.9961532354354858, 0.8843374848365784, 0.889012336730957]\nBest val loss: 0.056223596326815777\nTime: 86.15795660018921\n\n\nEpoch 69/300\n\ntrain loss: 0.062510441281373\ntrain loss_segm: 0.062510441281373\ntrain loss_shape: 0.04207693155902096\ntrain loss_recon: 0.12216071878807454\ntrain unet DSC: [0.9958947896957397, 0.8787172436714172, 0.8878558278083801]\n\ntest loss: 0.05598950099486571\ntest loss_segm: 0.05598950099486571\ntest loss_shape: 0.036476691563924156\ntest loss_recon: 0.11294342615665534\ntest unet DSC: [0.99623703956604, 0.8882423043251038, 0.8995957970619202]\nBest val loss: 0.05598950099486571\nTime: 86.38264560699463\n\n\nEpoch 70/300\n\ntrain loss: 0.0624780740966148\ntrain loss_segm: 0.0624780740966148\ntrain loss_shape: 0.042028481376510635\ntrain loss_recon: 0.1219686413113075\ntrain unet DSC: [0.9959131479263306, 0.8790551424026489, 0.8878254890441895]\n\ntest loss: 0.05675608311326076\ntest loss_segm: 0.05675608311326076\ntest loss_shape: 0.03652375210554172\ntest loss_recon: 0.11325958447578625\ntest unet DSC: [0.9962573051452637, 0.8883099555969238, 0.8973880410194397]\nBest val loss: 0.05598950099486571\nTime: 86.32487869262695\n\n\nEpoch 71/300\n\ntrain loss: 0.06252620024960252\ntrain loss_segm: 0.06252620024960252\ntrain loss_shape: 0.04211859720983083\ntrain loss_recon: 0.12208564711522453\ntrain unet DSC: [0.9958975315093994, 0.8787960410118103, 0.8878125548362732]\n\ntest loss: 0.05627990781496733\ntest loss_segm: 0.05627990781496733\ntest loss_shape: 0.036673314296282254\ntest loss_recon: 0.113414466381073\ntest unet DSC: [0.996218740940094, 0.8872373700141907, 0.8994045853614807]\nBest val loss: 0.05598950099486571\nTime: 86.00367426872253\n\n\nEpoch 72/300\n\ntrain loss: 0.06251684559768514\ntrain loss_segm: 0.06251684559768514\ntrain loss_shape: 0.0420901398587076\ntrain loss_recon: 0.12220655626888517\ntrain unet DSC: [0.9959011673927307, 0.8788329362869263, 0.8878256678581238]\n\ntest loss: 0.057302142565067\ntest loss_segm: 0.057302142565067\ntest loss_shape: 0.03678748895151493\ntest loss_recon: 0.1141244555130983\ntest unet DSC: [0.9962573051452637, 0.8882108926773071, 0.89566969871521]\nBest val loss: 0.05598950099486571\nTime: 85.59316492080688\n\n\nEpoch 73/300\n\ntrain loss: 0.0625232674558706\ntrain loss_segm: 0.0625232674558706\ntrain loss_shape: 0.04204040223473235\ntrain loss_recon: 0.12203289021419574\ntrain unet DSC: [0.9959036111831665, 0.8788028359413147, 0.8877789378166199]\n\ntest loss: 0.05815191452319805\ntest loss_segm: 0.05815191452319805\ntest loss_shape: 0.03700383972281065\ntest loss_recon: 0.11485502353081337\ntest unet DSC: [0.9962491989135742, 0.8875502347946167, 0.8934789896011353]\nBest val loss: 0.05598950099486571\nTime: 86.27042150497437\n\n\nEpoch 74/300\n\ntrain loss: 0.062424886618045315\ntrain loss_segm: 0.062424886618045315\ntrain loss_shape: 0.042030071085201036\ntrain loss_recon: 0.12197941390773918\ntrain unet DSC: [0.9959099888801575, 0.8790063261985779, 0.8879438042640686]\n\ntest loss: 0.0559708439768889\ntest loss_segm: 0.0559708439768889\ntest loss_shape: 0.036408352450682566\ntest loss_recon: 0.1126293326035524\ntest unet DSC: [0.9962614178657532, 0.8887390494346619, 0.8992277979850769]\nBest val loss: 0.0559708439768889\nTime: 87.02375388145447\n\n\nEpoch 75/300\n\ntrain loss: 0.062439388088599036\ntrain loss_segm: 0.062439388088599036\ntrain loss_shape: 0.04204573774639564\ntrain loss_recon: 0.12197576972502697\ntrain unet DSC: [0.9959031939506531, 0.878900945186615, 0.8879454135894775]\n\ntest loss: 0.05772587150717393\ntest loss_segm: 0.05772587150717393\ntest loss_shape: 0.0364495585553157\ntest loss_recon: 0.114058011617416\ntest unet DSC: [0.996265709400177, 0.8878089785575867, 0.8946703672409058]\nBest val loss: 0.0559708439768889\nTime: 86.51968264579773\n\n\nEpoch 76/300\n\ntrain loss: 0.06247869629082801\ntrain loss_segm: 0.06247869629082801\ntrain loss_shape: 0.04208776109580752\ntrain loss_recon: 0.12209351907802533\ntrain unet DSC: [0.9959036707878113, 0.8789389729499817, 0.8878846168518066]\n\ntest loss: 0.05714905233337329\ntest loss_segm: 0.05714905233337329\ntest loss_shape: 0.036653175663489565\ntest loss_recon: 0.11380249720353347\ntest unet DSC: [0.9962704181671143, 0.8884335160255432, 0.8959153890609741]\nBest val loss: 0.0559708439768889\nTime: 86.18420100212097\n\n\nEpoch 77/300\n\ntrain loss: 0.06242765627707107\ntrain loss_segm: 0.06242765627707107\ntrain loss_shape: 0.041989928724456435\ntrain loss_recon: 0.12191743828073333\ntrain unet DSC: [0.9959079623222351, 0.8789419531822205, 0.8879366517066956]\n\ntest loss: 0.05696304467244026\ntest loss_segm: 0.05696304467244026\ntest loss_shape: 0.03699343021099384\ntest loss_recon: 0.11430018834578685\ntest unet DSC: [0.9961514472961426, 0.8849656581878662, 0.8991740345954895]\nBest val loss: 0.0559708439768889\nTime: 86.05201172828674\n\n\nEpoch 78/300\n\ntrain loss: 0.06242395072138008\ntrain loss_segm: 0.06242395072138008\ntrain loss_shape: 0.04205555143424227\ntrain loss_recon: 0.12201016990444329\ntrain unet DSC: [0.995907187461853, 0.8790391087532043, 0.8879300951957703]\n\ntest loss: 0.05585702480031894\ntest loss_segm: 0.05585702480031894\ntest loss_shape: 0.03598045963698473\ntest loss_recon: 0.11197432799217029\ntest unet DSC: [0.9962864518165588, 0.8886845707893372, 0.8995956778526306]\nBest val loss: 0.05585702480031894\nTime: 86.6932909488678\n\n\nEpoch 79/300\n\ntrain loss: 0.06239097831841511\ntrain loss_segm: 0.06239097831841511\ntrain loss_shape: 0.041908815550275993\ntrain loss_recon: 0.12176331387290472\ntrain unet DSC: [0.9959145188331604, 0.8790706396102905, 0.8880118727684021]\n\ntest loss: 0.0568082468249859\ntest loss_segm: 0.0568082468249859\ntest loss_shape: 0.03620976663361757\ntest loss_recon: 0.11283580309305435\ntest unet DSC: [0.9962831139564514, 0.8885255455970764, 0.8968533277511597]\nBest val loss: 0.05585702480031894\nTime: 86.07538747787476\n\n\nEpoch 80/300\n\ntrain loss: 0.06239043131376369\ntrain loss_segm: 0.06239043131376369\ntrain loss_shape: 0.0419794137860778\ntrain loss_recon: 0.12182655251478847\ntrain unet DSC: [0.995907723903656, 0.8790162205696106, 0.8880240321159363]\n\ntest loss: 0.05594129936817365\ntest loss_segm: 0.05594129936817365\ntest loss_shape: 0.036118555384186596\ntest loss_recon: 0.11220366679705106\ntest unet DSC: [0.9962970018386841, 0.8893592953681946, 0.8988229036331177]\nBest val loss: 0.05585702480031894\nTime: 86.55939888954163\n\n\nEpoch 81/300\n\ntrain loss: 0.062445608616063866\ntrain loss_segm: 0.062445608616063866\ntrain loss_shape: 0.04201979838500294\ntrain loss_recon: 0.12199714553507068\ntrain unet DSC: [0.9959019422531128, 0.8789539337158203, 0.8879078030586243]\n\ntest loss: 0.0590439141751864\ntest loss_segm: 0.0590439141751864\ntest loss_shape: 0.03720686794855656\ntest loss_recon: 0.11584015381641877\ntest unet DSC: [0.996174156665802, 0.8856080174446106, 0.8926689028739929]\nBest val loss: 0.05585702480031894\nTime: 86.17688059806824\n\n\nEpoch 82/300\n\ntrain loss: 0.062387599315069896\ntrain loss_segm: 0.062387599315069896\ntrain loss_shape: 0.042000355552646175\ntrain loss_recon: 0.12191884200784224\ntrain unet DSC: [0.9959002137184143, 0.8789154291152954, 0.888067901134491]\n\ntest loss: 0.058620526240422174\ntest loss_segm: 0.058620526240422174\ntest loss_shape: 0.037002111140352026\ntest loss_recon: 0.11531165165778919\ntest unet DSC: [0.9962005615234375, 0.8863365054130554, 0.8932597637176514]\nBest val loss: 0.05585702480031894\nTime: 86.64912939071655\n\n\nEpoch 83/300\n\ntrain loss: 0.06242952488740034\ntrain loss_segm: 0.06242952488740034\ntrain loss_shape: 0.04193392695411097\ntrain loss_recon: 0.12174035853977445\ntrain unet DSC: [0.9959123730659485, 0.8790079355239868, 0.887924075126648]\n\ntest loss: 0.05622531301700152\ntest loss_segm: 0.05622531301700152\ntest loss_shape: 0.036316702810999676\ntest loss_recon: 0.11281472291701879\ntest unet DSC: [0.9962858557701111, 0.8892202377319336, 0.8980430960655212]\nBest val loss: 0.05585702480031894\nTime: 86.55269241333008\n\n\nEpoch 84/300\n\ntrain loss: 0.0623796493333729\ntrain loss_segm: 0.0623796493333729\ntrain loss_shape: 0.04197467630139635\ntrain loss_recon: 0.12192335196688206\ntrain unet DSC: [0.9959092736244202, 0.8790996074676514, 0.8880621194839478]\n\ntest loss: 0.06183461462840056\ntest loss_segm: 0.06183461462840056\ntest loss_shape: 0.0386014247360902\ntest loss_recon: 0.11979164527012752\ntest unet DSC: [0.9960566759109497, 0.8822664022445679, 0.8868579268455505]\nBest val loss: 0.05585702480031894\nTime: 86.90472960472107\n\n\nEpoch 85/300\n\ntrain loss: 0.06238807928807373\ntrain loss_segm: 0.06238807928807373\ntrain loss_shape: 0.04197525912070576\ntrain loss_recon: 0.12188496129422248\ntrain unet DSC: [0.9959017634391785, 0.8789957761764526, 0.8880549669265747]\n\ntest loss: 0.05589544992798414\ntest loss_segm: 0.05589544992798414\ntest loss_shape: 0.03614235067596802\ntest loss_recon: 0.11224883489119701\ntest unet DSC: [0.9962829351425171, 0.8890690207481384, 0.8991368412971497]\nBest val loss: 0.05585702480031894\nTime: 85.93305802345276\n\n\nEpoch 86/300\n\ntrain loss: 0.062431853901170474\ntrain loss_segm: 0.062431853901170474\ntrain loss_shape: 0.04198921455329732\ntrain loss_recon: 0.12191202859335308\ntrain unet DSC: [0.9959036111831665, 0.8789694309234619, 0.8879369497299194]\n\ntest loss: 0.05707705584474099\ntest loss_segm: 0.05707705584474099\ntest loss_shape: 0.036564462268963836\ntest loss_recon: 0.11355281945986626\ntest unet DSC: [0.9962756633758545, 0.8885579109191895, 0.8960140943527222]\nBest val loss: 0.05585702480031894\nTime: 86.19778490066528\n\n\nEpoch 87/300\n\ntrain loss: 0.062293946695855903\ntrain loss_segm: 0.062293946695855903\ntrain loss_shape: 0.04187769148074373\ntrain loss_recon: 0.12174304225776769\ntrain unet DSC: [0.9959145188331604, 0.8792392611503601, 0.8881753087043762]\n\ntest loss: 0.056591289738814034\ntest loss_segm: 0.056591289738814034\ntest loss_shape: 0.035983481229497835\ntest loss_recon: 0.11224004397025475\ntest unet DSC: [0.9962857365608215, 0.8885310292243958, 0.8975447416305542]\nBest val loss: 0.05585702480031894\nTime: 86.57480597496033\n\n\nEpoch 88/300\n\ntrain loss: 0.062409081201575976\ntrain loss_segm: 0.062409081201575976\ntrain loss_shape: 0.042028190662400634\ntrain loss_recon: 0.12193124618711351\ntrain unet DSC: [0.9959061741828918, 0.8789709806442261, 0.8880183696746826]\n\ntest loss: 0.059324051898259386\ntest loss_segm: 0.059324051898259386\ntest loss_shape: 0.03737495481394804\ntest loss_recon: 0.11620119901803824\ntest unet DSC: [0.9961874485015869, 0.8857258558273315, 0.891432523727417]\nBest val loss: 0.05585702480031894\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.61750149726868\n\n\nEpoch 89/300\n\ntrain loss: 0.06233110974389541\ntrain loss_segm: 0.06233110974389541\ntrain loss_shape: 0.041864165753314767\ntrain loss_recon: 0.12169686714305153\ntrain unet DSC: [0.9959128499031067, 0.8791349530220032, 0.8880634307861328]\n\ntest loss: 0.056259870242614016\ntest loss_segm: 0.056259870242614016\ntest loss_shape: 0.036216613908226676\ntest loss_recon: 0.11259296307196984\ntest unet DSC: [0.9962907433509827, 0.889104962348938, 0.8980738520622253]\nBest val loss: 0.05585702480031894\nTime: 85.80316638946533\n\n\nEpoch 90/300\n\ntrain loss: 0.06227073351605029\ntrain loss_segm: 0.06227073351605029\ntrain loss_shape: 0.04185261798998977\ntrain loss_recon: 0.12161833840080455\ntrain unet DSC: [0.9959183931350708, 0.8792925477027893, 0.8881486058235168]\n\ntest loss: 0.05605093533029923\ntest loss_segm: 0.05605093533029923\ntest loss_shape: 0.036238482795082606\ntest loss_recon: 0.1123238511574574\ntest unet DSC: [0.9962772727012634, 0.8888280987739563, 0.8989472985267639]\nBest val loss: 0.05585702480031894\nTime: 86.38991069793701\n\n\nEpoch 91/300\n\ntrain loss: 0.06235015184818944\ntrain loss_segm: 0.06235015184818944\ntrain loss_shape: 0.041940747984225236\ntrain loss_recon: 0.12177014124544361\ntrain unet DSC: [0.9959108233451843, 0.8790764212608337, 0.8880863189697266]\n\ntest loss: 0.057039445313887716\ntest loss_segm: 0.057039445313887716\ntest loss_shape: 0.036503532041723914\ntest loss_recon: 0.11342875162760417\ntest unet DSC: [0.9962678551673889, 0.8883532881736755, 0.8963137865066528]\nBest val loss: 0.05585702480031894\nTime: 85.14234566688538\n\n\nEpoch 92/300\n\ntrain loss: 0.06229097862975507\ntrain loss_segm: 0.06229097862975507\ntrain loss_shape: 0.04190428555011749\ntrain loss_recon: 0.12175528346737728\ntrain unet DSC: [0.9959108233451843, 0.8792529702186584, 0.8881832957267761]\n\ntest loss: 0.05597358416670408\ntest loss_segm: 0.05597358416670408\ntest loss_shape: 0.03635466872499539\ntest loss_recon: 0.11271336751106457\ntest unet DSC: [0.9962641596794128, 0.8887497186660767, 0.8992237448692322]\nBest val loss: 0.05585702480031894\nTime: 85.81182217597961\n\n\nEpoch 93/300\n\ntrain loss: 0.06230575605472432\ntrain loss_segm: 0.06230575605472432\ntrain loss_shape: 0.04188013951518113\ntrain loss_recon: 0.12161093271231349\ntrain unet DSC: [0.9959156513214111, 0.8791717290878296, 0.8881273865699768]\n\ntest loss: 0.06179022015287326\ntest loss_segm: 0.06179022015287326\ntest loss_shape: 0.03840001524449923\ntest loss_recon: 0.11944998533297808\ntest unet DSC: [0.9960497617721558, 0.8820310831069946, 0.8872164487838745]\nBest val loss: 0.05585702480031894\nTime: 85.83580327033997\n\n\nEpoch 94/300\n\ntrain loss: 0.062275711447000504\ntrain loss_segm: 0.062275711447000504\ntrain loss_shape: 0.04189232366654692\ntrain loss_recon: 0.12168898084495641\ntrain unet DSC: [0.9959172010421753, 0.8792343139648438, 0.8882105350494385]\n\ntest loss: 0.06293729453896865\ntest loss_segm: 0.06293729453896865\ntest loss_shape: 0.038786168998250596\ntest loss_recon: 0.12071796258290608\ntest unet DSC: [0.9960209727287292, 0.8807423710823059, 0.8847419619560242]\nBest val loss: 0.05585702480031894\nTime: 87.40278816223145\n\n\nEpoch 95/300\n\ntrain loss: 0.06222411256896544\ntrain loss_segm: 0.06222411256896544\ntrain loss_shape: 0.041838404262744926\ntrain loss_recon: 0.12149054566516151\ntrain unet DSC: [0.9959167242050171, 0.8793267607688904, 0.888326108455658]\n\ntest loss: 0.05605377065829742\ntest loss_segm: 0.05605377065829742\ntest loss_shape: 0.03616549514043026\ntest loss_recon: 0.11216178918496156\ntest unet DSC: [0.9962838888168335, 0.8890243172645569, 0.8987610936164856]\nBest val loss: 0.05585702480031894\nTime: 86.3711166381836\n\n\nEpoch 96/300\n\ntrain loss: 0.06226469524487664\ntrain loss_segm: 0.06226469524487664\ntrain loss_shape: 0.04184234342714654\ntrain loss_recon: 0.12159884692747382\ntrain unet DSC: [0.9959210753440857, 0.8793408274650574, 0.8882532119750977]\n\ntest loss: 0.0613323864646447\ntest loss_segm: 0.0613323864646447\ntest loss_shape: 0.03840661636338784\ntest loss_recon: 0.11920157762674186\ntest unet DSC: [0.9960871934890747, 0.8830224275588989, 0.8877549767494202]\nBest val loss: 0.05585702480031894\nTime: 86.10184717178345\n\n\nEpoch 97/300\n\ntrain loss: 0.06225455450955071\ntrain loss_segm: 0.06225455450955071\ntrain loss_shape: 0.04189615246332899\ntrain loss_recon: 0.12171879259845879\ntrain unet DSC: [0.9959138035774231, 0.8792693018913269, 0.8882527351379395]\n\ntest loss: 0.05937865309608288\ntest loss_segm: 0.05937865309608288\ntest loss_shape: 0.03740056341466231\ntest loss_recon: 0.11633391563708965\ntest unet DSC: [0.9961868524551392, 0.8856996297836304, 0.8913931250572205]\nBest val loss: 0.05585702480031894\nTime: 85.46837115287781\n\n\nEpoch 98/300\n\ntrain loss: 0.06230881984663915\ntrain loss_segm: 0.06230881984663915\ntrain loss_shape: 0.0418752268475445\ntrain loss_recon: 0.12170397216760659\ntrain unet DSC: [0.99591064453125, 0.8791466951370239, 0.8882278203964233]\n\ntest loss: 0.05625015946152883\ntest loss_segm: 0.05625015946152883\ntest loss_shape: 0.036161719176631704\ntest loss_recon: 0.1124988656777602\ntest unet DSC: [0.9962956309318542, 0.8891992568969727, 0.8979679942131042]\nBest val loss: 0.05585702480031894\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.58746004104614\n\n\nEpoch 99/300\n\ntrain loss: 0.06224603012581415\ntrain loss_segm: 0.06224603012581415\ntrain loss_shape: 0.04184940018797222\ntrain loss_recon: 0.12155758089657072\ntrain unet DSC: [0.9959205985069275, 0.8793385028839111, 0.8881673812866211]\n\ntest loss: 0.05602007024945357\ntest loss_segm: 0.05602007024945357\ntest loss_shape: 0.03611971662403681\ntest loss_recon: 0.1121528148651123\ntest unet DSC: [0.9962928891181946, 0.8892065286636353, 0.8986111879348755]\nBest val loss: 0.05585702480031894\nTime: 85.65977787971497\n\n\nEpoch 100/300\n\ntrain loss: 0.06226386976298652\ntrain loss_segm: 0.06226386976298652\ntrain loss_shape: 0.04188253742324401\ntrain loss_recon: 0.12164390652994567\ntrain unet DSC: [0.9959201216697693, 0.8793339729309082, 0.8882109522819519]\n\ntest loss: 0.05671278310891909\ntest loss_segm: 0.05671278310891909\ntest loss_shape: 0.03660297484543079\ntest loss_recon: 0.11335276181881244\ntest unet DSC: [0.9962762594223022, 0.8888840079307556, 0.8968269228935242]\nBest val loss: 0.05585702480031894\nTime: 85.75019073486328\n\nValidation loss stopped to decrease for 30 epochs. Training terminated.\nBest epoch: 78\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678445036225
        }
      },
      "id": "cd5aafc5-7ec9-4618-8330-0e9976565909"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.001, lambda_recon=0.01,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/300\n\ntrain loss: 0.3248138095759138\ntrain loss_segm: 0.3248138095759138\ntrain loss_shape: 0.22586101717964002\ntrain loss_recon: 0.4618474124353143\ntrain unet DSC: [0.9364240765571594, 0.535178005695343, 0.5045997500419617]\n\ntest loss: 0.16810433834027022\ntest loss_segm: 0.16810433834027022\ntest loss_shape: 0.11919112159655644\ntest loss_recon: 0.28791040640610915\ntest unet DSC: [0.9890183806419373, 0.7258719205856323, 0.7246515154838562]\nBest val loss: 0.16810433834027022\nTime: 85.78967618942261\n\n\nEpoch 2/300\n\ntrain loss: 0.14535284622371952\ntrain loss_segm: 0.14535284622371952\ntrain loss_shape: 0.14725861495620088\ntrain loss_recon: 0.3009297259246247\ntrain unet DSC: [0.9898287653923035, 0.7447307705879211, 0.7751407623291016]\n\ntest loss: 0.10818571998522832\ntest loss_segm: 0.10818571998522832\ntest loss_shape: 0.08088792822299859\ntest loss_recon: 0.21016045869925085\ntest unet DSC: [0.9930209517478943, 0.8082422018051147, 0.8318229913711548]\nBest val loss: 0.10818571998522832\nTime: 85.56679153442383\n\n\nEpoch 3/300\n\ntrain loss: 0.11339442455504514\ntrain loss_segm: 0.11339442455504514\ntrain loss_shape: 0.10868258953471727\ntrain loss_recon: 0.24213814622239221\ntrain unet DSC: [0.9926949739456177, 0.7984561324119568, 0.8148534297943115]\n\ntest loss: 0.08362546792397133\ntest loss_segm: 0.08362546792397133\ntest loss_shape: 0.06178346008826525\ntest loss_recon: 0.16982287474167654\ntest unet DSC: [0.9943739771842957, 0.844120442867279, 0.8648650050163269]\nBest val loss: 0.08362546792397133\nTime: 86.43108558654785\n\n\nEpoch 4/300\n\ntrain loss: 0.0994488885508308\ntrain loss_segm: 0.0994488885508308\ntrain loss_shape: 0.0898105342742763\ntrain loss_recon: 0.21070171119291572\ntrain unet DSC: [0.9936588406562805, 0.8205509185791016, 0.8337979912757874]\n\ntest loss: 0.07894404862935726\ntest loss_segm: 0.07894404862935726\ntest loss_shape: 0.057430291978212505\ntest loss_recon: 0.16076946258544922\ntest unet DSC: [0.9948918223381042, 0.8563284277915955, 0.8650639057159424]\nBest val loss: 0.07894404862935726\nTime: 85.4486711025238\n\n\nEpoch 5/300\n\ntrain loss: 0.091812480712616\ntrain loss_segm: 0.091812480712616\ntrain loss_shape: 0.07849295776856097\ntrain loss_recon: 0.19112859307965147\ntrain unet DSC: [0.9941670894622803, 0.8324654698371887, 0.8439645767211914]\n\ntest loss: 0.07380163020048386\ntest loss_segm: 0.07380163020048386\ntest loss_shape: 0.05389857874848904\ntest loss_recon: 0.15372041861216226\ntest unet DSC: [0.9949735999107361, 0.8587595820426941, 0.8797082901000977]\nBest val loss: 0.07380163020048386\nTime: 85.88261246681213\n\n\nEpoch 6/300\n\ntrain loss: 0.08637349067043655\ntrain loss_segm: 0.08637349067043655\ntrain loss_shape: 0.07087824625682228\ntrain loss_recon: 0.17672386916377877\ntrain unet DSC: [0.9945303797721863, 0.8414053320884705, 0.8513614535331726]\n\ntest loss: 0.0706340033465471\ntest loss_segm: 0.0706340033465471\ntest loss_shape: 0.04748926435907682\ntest loss_recon: 0.13787383910937187\ntest unet DSC: [0.9955207705497742, 0.8658947944641113, 0.8803613185882568]\nBest val loss: 0.0706340033465471\nTime: 86.0919861793518\n\n\nEpoch 7/300\n\ntrain loss: 0.08314030872115606\ntrain loss_segm: 0.08314030872115606\ntrain loss_shape: 0.06587963711611833\ntrain loss_recon: 0.16750565246690677\ntrain unet DSC: [0.9947434663772583, 0.8465717434883118, 0.8557443618774414]\n\ntest loss: 0.06873732327650754\ntest loss_segm: 0.06873732327650754\ntest loss_shape: 0.04500172468714225\ntest loss_recon: 0.13300737967857948\ntest unet DSC: [0.9956784248352051, 0.8706305027008057, 0.8805238604545593]\nBest val loss: 0.06873732327650754\nTime: 85.49457573890686\n\n\nEpoch 8/300\n\ntrain loss: 0.08083069216035589\ntrain loss_segm: 0.08083069216035589\ntrain loss_shape: 0.06209353463653522\ntrain loss_recon: 0.16048104664947413\ntrain unet DSC: [0.9948973655700684, 0.8501636981964111, 0.859063446521759]\n\ntest loss: 0.06674672300234819\ntest loss_segm: 0.06674672300234819\ntest loss_shape: 0.045530055004816786\ntest loss_recon: 0.13270972019586807\ntest unet DSC: [0.9955908060073853, 0.8706260919570923, 0.8853594660758972]\nBest val loss: 0.06674672300234819\nTime: 85.5110650062561\n\n\nEpoch 9/300\n\ntrain loss: 0.07879305395144451\ntrain loss_segm: 0.07879305395144451\ntrain loss_shape: 0.059483640037382705\ntrain loss_recon: 0.155292456285863\ntrain unet DSC: [0.9950258135795593, 0.8535557985305786, 0.86211097240448]\n\ntest loss: 0.06645688395469616\ntest loss_segm: 0.06645688395469616\ntest loss_shape: 0.042940761225345805\ntest loss_recon: 0.1284587704218351\ntest unet DSC: [0.9958820939064026, 0.8755793571472168, 0.8806353807449341]\nBest val loss: 0.06645688395469616\nTime: 85.90553164482117\n\n\nEpoch 10/300\n\ntrain loss: 0.07729655447639996\ntrain loss_segm: 0.07729655447639996\ntrain loss_shape: 0.057111534396115736\ntrain loss_recon: 0.15119478974161268\ntrain unet DSC: [0.9951131343841553, 0.8558208346366882, 0.864433765411377]\n\ntest loss: 0.06362654918279403\ntest loss_segm: 0.06362654918279403\ntest loss_shape: 0.04158937654052025\ntest loss_recon: 0.12413466129547511\ntest unet DSC: [0.9959220886230469, 0.8774099349975586, 0.8884526491165161]\nBest val loss: 0.06362654918279403\nTime: 86.24173498153687\n\n\nEpoch 11/300\n\ntrain loss: 0.07590481351259389\ntrain loss_segm: 0.07590481351259389\ntrain loss_shape: 0.05533408880516698\ntrain loss_recon: 0.14765432250650623\ntrain unet DSC: [0.9951886534690857, 0.8579604625701904, 0.8666585683822632]\n\ntest loss: 0.06463729504209298\ntest loss_segm: 0.06463729504209298\ntest loss_shape: 0.04496528275119953\ntest loss_recon: 0.13253550498913497\ntest unet DSC: [0.9957005381584167, 0.875882089138031, 0.8872156143188477]\nBest val loss: 0.06362654918279403\nTime: 85.85022568702698\n\n\nEpoch 12/300\n\ntrain loss: 0.07492538739608813\ntrain loss_segm: 0.07492538739608813\ntrain loss_shape: 0.054013560254928435\ntrain loss_recon: 0.1454176819777187\ntrain unet DSC: [0.9952610731124878, 0.8596615195274353, 0.8682461380958557]\n\ntest loss: 0.06945760710499226\ntest loss_segm: 0.06945760710499226\ntest loss_shape: 0.04340237789811232\ntest loss_recon: 0.12881299471243834\ntest unet DSC: [0.9959121942520142, 0.8744577169418335, 0.8719973564147949]\nBest val loss: 0.06362654918279403\nTime: 85.41739916801453\n\n\nEpoch 13/300\n\ntrain loss: 0.07440800747916668\ntrain loss_segm: 0.07440800747916668\ntrain loss_shape: 0.05297141783882545\ntrain loss_recon: 0.14368721880490268\ntrain unet DSC: [0.9952859878540039, 0.8604244589805603, 0.8689380884170532]\n\ntest loss: 0.06125047955757532\ntest loss_segm: 0.06125047955757532\ntest loss_shape: 0.04065325923072986\ntest loss_recon: 0.12216168489211644\ntest unet DSC: [0.9960082769393921, 0.8825273513793945, 0.8902848958969116]\nBest val loss: 0.06125047955757532\nTime: 85.57941222190857\n\n\nEpoch 14/300\n\ntrain loss: 0.0737560624374619\ntrain loss_segm: 0.0737560624374619\ntrain loss_shape: 0.052116404558661615\ntrain loss_recon: 0.14236710267730907\ntrain unet DSC: [0.9953327178955078, 0.8615157604217529, 0.8699665069580078]\n\ntest loss: 0.06342119379685475\ntest loss_segm: 0.06342119379685475\ntest loss_shape: 0.04006445904572805\ntest loss_recon: 0.12041743596394856\ntest unet DSC: [0.9960063099861145, 0.8790482878684998, 0.8871000409126282]\nBest val loss: 0.06125047955757532\nTime: 86.4130687713623\n\n\nEpoch 15/300\n\ntrain loss: 0.07257021534480626\ntrain loss_segm: 0.07257021534480626\ntrain loss_shape: 0.05094236012878297\ntrain loss_recon: 0.13997880001611349\ntrain unet DSC: [0.9953955411911011, 0.8634449243545532, 0.8720430731773376]\n\ntest loss: 0.06239276971572485\ntest loss_segm: 0.06239276971572485\ntest loss_shape: 0.04209116674386538\ntest loss_recon: 0.12554815029486632\ntest unet DSC: [0.9959701299667358, 0.8828677535057068, 0.88714599609375]\nBest val loss: 0.06125047955757532\nTime: 87.51587080955505\n\n\nEpoch 16/300\n\ntrain loss: 0.07183533642865435\ntrain loss_segm: 0.07183533642865435\ntrain loss_shape: 0.05016619341943083\ntrain loss_recon: 0.1384030870998962\ntrain unet DSC: [0.9954304695129395, 0.8645896315574646, 0.8732754588127136]\n\ntest loss: 0.063020936667155\ntest loss_segm: 0.063020936667155\ntest loss_shape: 0.040544844208619535\ntest loss_recon: 0.1205831399330726\ntest unet DSC: [0.9959679841995239, 0.875464916229248, 0.8903447389602661]\nBest val loss: 0.06125047955757532\nTime: 86.02242136001587\n\n\nEpoch 17/300\n\ntrain loss: 0.0715018150078345\ntrain loss_segm: 0.0715018150078345\ntrain loss_shape: 0.0496240628623887\ntrain loss_recon: 0.13733101721051372\ntrain unet DSC: [0.99547278881073, 0.8653268814086914, 0.8736770153045654]\n\ntest loss: 0.06086525129966247\ntest loss_segm: 0.06086525129966247\ntest loss_shape: 0.039743460523776517\ntest loss_recon: 0.12019197298930241\ntest unet DSC: [0.9960974454879761, 0.8832622170448303, 0.8909730315208435]\nBest val loss: 0.06086525129966247\nTime: 85.93664455413818\n\n\nEpoch 18/300\n\ntrain loss: 0.07108221181868753\ntrain loss_segm: 0.07108221181868753\ntrain loss_shape: 0.04912051442963413\ntrain loss_recon: 0.1366689371157296\ntrain unet DSC: [0.9954771399497986, 0.8658417463302612, 0.8744752407073975]\n\ntest loss: 0.06456783337470813\ntest loss_segm: 0.06456783337470813\ntest loss_shape: 0.04051528154657437\ntest loss_recon: 0.12285837607505994\ntest unet DSC: [0.99605792760849, 0.879507303237915, 0.8825803995132446]\nBest val loss: 0.06086525129966247\nTime: 85.91939353942871\n\n\nEpoch 19/300\n\ntrain loss: 0.07085853541576409\ntrain loss_segm: 0.07085853541576409\ntrain loss_shape: 0.04868285525354404\ntrain loss_recon: 0.1360030789164048\ntrain unet DSC: [0.9954991340637207, 0.8662611842155457, 0.8748472929000854]\n\ntest loss: 0.060601825993030496\ntest loss_segm: 0.060601825993030496\ntest loss_shape: 0.038622754697616286\ntest loss_recon: 0.11701143093598194\ntest unet DSC: [0.9961193799972534, 0.8832006454467773, 0.8923841714859009]\nBest val loss: 0.060601825993030496\nTime: 86.4611542224884\n\n\nEpoch 20/300\n\ntrain loss: 0.06998830654104299\ntrain loss_segm: 0.06998830654104299\ntrain loss_shape: 0.04792211329729497\ntrain loss_recon: 0.13450578006008004\ntrain unet DSC: [0.9955427050590515, 0.8677182793617249, 0.8762475252151489]\n\ntest loss: 0.06566989049315453\ntest loss_segm: 0.06566989049315453\ntest loss_shape: 0.04032520773128057\ntest loss_recon: 0.12255399196575849\ntest unet DSC: [0.9959150552749634, 0.8768818378448486, 0.8814367651939392]\nBest val loss: 0.060601825993030496\nTime: 86.17814111709595\n\n\nEpoch 21/300\n\ntrain loss: 0.06966517042793051\ntrain loss_segm: 0.06966517042793051\ntrain loss_shape: 0.04742734777870812\ntrain loss_recon: 0.13346785156032706\ntrain unet DSC: [0.9955702424049377, 0.8682632446289062, 0.8767320513725281]\n\ntest loss: 0.06186314930136387\ntest loss_segm: 0.06186314930136387\ntest loss_shape: 0.039887952976501904\ntest loss_recon: 0.12065379283367059\ntest unet DSC: [0.99617600440979, 0.8844789266586304, 0.8868604898452759]\nBest val loss: 0.060601825993030496\nTime: 85.833078622818\n\n\nEpoch 22/300\n\ntrain loss: 0.06961649708166907\ntrain loss_segm: 0.06961649708166907\ntrain loss_shape: 0.04737779863555975\ntrain loss_recon: 0.13350837894632847\ntrain unet DSC: [0.9955826997756958, 0.868424117565155, 0.8767957091331482]\n\ntest loss: 0.05967704130288882\ntest loss_segm: 0.05967704130288882\ntest loss_shape: 0.039565165551045\ntest loss_recon: 0.11824295612481925\ntest unet DSC: [0.9960663914680481, 0.8840768337249756, 0.8934862017631531]\nBest val loss: 0.05967704130288882\nTime: 86.54213905334473\n\n\nEpoch 23/300\n\ntrain loss: 0.06932200071743772\ntrain loss_segm: 0.06932200071743772\ntrain loss_shape: 0.04698743657032146\ntrain loss_recon: 0.13282252867010574\ntrain unet DSC: [0.9955950379371643, 0.8688973784446716, 0.8773089051246643]\n\ntest loss: 0.062345047027636796\ntest loss_segm: 0.062345047027636796\ntest loss_shape: 0.041355862258336484\ntest loss_recon: 0.12352296939262977\ntest unet DSC: [0.9958436489105225, 0.8757655024528503, 0.8924762010574341]\nBest val loss: 0.05967704130288882\nTime: 86.33152055740356\n\n\nEpoch 24/300\n\ntrain loss: 0.0686771195570502\ntrain loss_segm: 0.0686771195570502\ntrain loss_shape: 0.046471106194997135\ntrain loss_recon: 0.13165374873559685\ntrain unet DSC: [0.9956334829330444, 0.870000422000885, 0.8783262968063354]\n\ntest loss: 0.059130736268483676\ntest loss_segm: 0.059130736268483676\ntest loss_shape: 0.03849541362470541\ntest loss_recon: 0.1178015546920972\ntest unet DSC: [0.9960911870002747, 0.8843567967414856, 0.8957636952400208]\nBest val loss: 0.059130736268483676\nTime: 86.2247986793518\n\n\nEpoch 25/300\n\ntrain loss: 0.06835336865314955\ntrain loss_segm: 0.06835336865314955\ntrain loss_shape: 0.046111241289520565\ntrain loss_recon: 0.13102564479731307\ntrain unet DSC: [0.9956527948379517, 0.8705440759658813, 0.878866970539093]\n\ntest loss: 0.062227540577833466\ntest loss_segm: 0.062227540577833466\ntest loss_shape: 0.039596298757271886\ntest loss_recon: 0.1201279423175714\ntest unet DSC: [0.9960851669311523, 0.881524384021759, 0.8891382217407227]\nBest val loss: 0.059130736268483676\nTime: 86.40011024475098\n\n\nEpoch 26/300\n\ntrain loss: 0.06842057284297823\ntrain loss_segm: 0.06842057284297823\ntrain loss_shape: 0.04606054272927061\ntrain loss_recon: 0.13092258459405054\ntrain unet DSC: [0.9956550002098083, 0.8704447746276855, 0.87870192527771]\n\ntest loss: 0.05973028143246969\ntest loss_segm: 0.05973028143246969\ntest loss_shape: 0.038747499481989786\ntest loss_recon: 0.11816386840282342\ntest unet DSC: [0.9960789084434509, 0.8818649649620056, 0.895550549030304]\nBest val loss: 0.059130736268483676\nTime: 86.27179622650146\n\n\nEpoch 27/300\n\ntrain loss: 0.06823427083937428\ntrain loss_segm: 0.06823427083937428\ntrain loss_shape: 0.04586803271800657\ntrain loss_recon: 0.13076511251775524\ntrain unet DSC: [0.9956579208374023, 0.870718777179718, 0.879095733165741]\n\ntest loss: 0.05903139796394568\ntest loss_segm: 0.05903139796394568\ntest loss_shape: 0.03840461502281519\ntest loss_recon: 0.11693916106835389\ntest unet DSC: [0.9961302280426025, 0.8849896192550659, 0.8953688144683838]\nBest val loss: 0.05903139796394568\nTime: 85.84872174263\n\n\nEpoch 28/300\n\ntrain loss: 0.06793497434428221\ntrain loss_segm: 0.06793497434428221\ntrain loss_shape: 0.04558139579677129\ntrain loss_recon: 0.13016402570507193\ntrain unet DSC: [0.9956862330436707, 0.8713493943214417, 0.8795603513717651]\n\ntest loss: 0.06070865442355474\ntest loss_segm: 0.06070865442355474\ntest loss_shape: 0.038440483884933666\ntest loss_recon: 0.11739008548932198\ntest unet DSC: [0.9960904121398926, 0.8823876976966858, 0.8925184607505798]\nBest val loss: 0.05903139796394568\nTime: 86.2838864326477\n\n\nEpoch 29/300\n\ntrain loss: 0.06781430580193483\ntrain loss_segm: 0.06781430580193483\ntrain loss_shape: 0.0454283757253161\ntrain loss_recon: 0.12986688900597487\ntrain unet DSC: [0.9956899881362915, 0.8715121150016785, 0.8798826336860657]\n\ntest loss: 0.06083694291420472\ntest loss_segm: 0.06083694291420472\ntest loss_shape: 0.03806877422791261\ntest loss_recon: 0.1172164831406031\ntest unet DSC: [0.9962031841278076, 0.8847811818122864, 0.8897032141685486]\nBest val loss: 0.05903139796394568\nTime: 92.38240957260132\n\n\nEpoch 30/300\n\ntrain loss: 0.06749920407899573\ntrain loss_segm: 0.06749920407899573\ntrain loss_shape: 0.04516483164286312\ntrain loss_recon: 0.12927021331424954\ntrain unet DSC: [0.9957020282745361, 0.8720189332962036, 0.880420446395874]\n\ntest loss: 0.05889132590248035\ntest loss_segm: 0.05889132590248035\ntest loss_shape: 0.03781561180949211\ntest loss_recon: 0.1153849348043784\ntest unet DSC: [0.9961742758750916, 0.8854714632034302, 0.8951913714408875]\nBest val loss: 0.05889132590248035\nTime: 87.71650576591492\n\n\nEpoch 31/300\n\ntrain loss: 0.06730539759597447\ntrain loss_segm: 0.06730539759597447\ntrain loss_shape: 0.044940129252551475\ntrain loss_recon: 0.12880385299272176\ntrain unet DSC: [0.9957096576690674, 0.872257649898529, 0.8807116150856018]\n\ntest loss: 0.05967258623777292\ntest loss_segm: 0.05967258623777292\ntest loss_shape: 0.0388409364490937\ntest loss_recon: 0.11747243159856552\ntest unet DSC: [0.9961072206497192, 0.8839358687400818, 0.8942135572433472]\nBest val loss: 0.05889132590248035\nTime: 85.62767195701599\n\n\nEpoch 32/300\n\ntrain loss: 0.06690625811019275\ntrain loss_segm: 0.06690625811019275\ntrain loss_shape: 0.044652552662205094\ntrain loss_recon: 0.12822017752671544\ntrain unet DSC: [0.9957390427589417, 0.8730025291442871, 0.8812321424484253]\n\ntest loss: 0.06466426824529965\ntest loss_segm: 0.06466426824529965\ntest loss_shape: 0.040446882446606956\ntest loss_recon: 0.12243355848850349\ntest unet DSC: [0.9960206151008606, 0.8797398209571838, 0.8820621967315674]\nBest val loss: 0.05889132590248035\nTime: 85.87825393676758\n\n\nEpoch 33/300\n\ntrain loss: 0.06679169105106517\ntrain loss_segm: 0.06679169105106517\ntrain loss_shape: 0.044449207196130026\ntrain loss_recon: 0.12778928498678568\ntrain unet DSC: [0.9957418441772461, 0.8731019496917725, 0.8815248012542725]\n\ntest loss: 0.058652726790079705\ntest loss_segm: 0.058652726790079705\ntest loss_shape: 0.03828980525334676\ntest loss_recon: 0.11673956039624336\ntest unet DSC: [0.9961246848106384, 0.8859910368919373, 0.895530104637146]\nBest val loss: 0.058652726790079705\nTime: 87.01923847198486\n\n\nEpoch 34/300\n\ntrain loss: 0.06642261550679238\ntrain loss_segm: 0.06642261550679238\ntrain loss_shape: 0.04419853861290443\ntrain loss_recon: 0.12737083359609677\ntrain unet DSC: [0.9957582354545593, 0.8737707138061523, 0.8822227120399475]\n\ntest loss: 0.05851745748749146\ntest loss_segm: 0.05851745748749146\ntest loss_shape: 0.03738481651705045\ntest loss_recon: 0.1146815449763567\ntest unet DSC: [0.9962146282196045, 0.8860923647880554, 0.8955711722373962]\nBest val loss: 0.05851745748749146\nTime: 87.38478398323059\n\n\nEpoch 35/300\n\ntrain loss: 0.06630163795397251\ntrain loss_segm: 0.06630163795397251\ntrain loss_shape: 0.04411399432847017\ntrain loss_recon: 0.12693004517615597\ntrain unet DSC: [0.995771586894989, 0.8739945888519287, 0.8822465538978577]\n\ntest loss: 0.07031018487535991\ntest loss_segm: 0.07031018487535991\ntest loss_shape: 0.04188811960510719\ntest loss_recon: 0.1286618709564209\ntest unet DSC: [0.9955945014953613, 0.8695196509361267, 0.8742153644561768]\nBest val loss: 0.05851745748749146\nTime: 86.37763667106628\n\n\nEpoch 36/300\n\ntrain loss: 0.06615971604102774\ntrain loss_segm: 0.06615971604102774\ntrain loss_shape: 0.04392817632967158\ntrain loss_recon: 0.12668187520172022\ntrain unet DSC: [0.9957754611968994, 0.8741509318351746, 0.8825231790542603]\n\ntest loss: 0.060851890402726636\ntest loss_segm: 0.060851890402726636\ntest loss_shape: 0.03897027795513471\ntest loss_recon: 0.11894089289200611\ntest unet DSC: [0.996146559715271, 0.8841803073883057, 0.8904359340667725]\nBest val loss: 0.05851745748749146\nTime: 85.41293621063232\n\n\nEpoch 37/300\n\ntrain loss: 0.06619039644734769\ntrain loss_segm: 0.06619039644734769\ntrain loss_shape: 0.04395630477067036\ntrain loss_recon: 0.12678022814702383\ntrain unet DSC: [0.9957746863365173, 0.8740864992141724, 0.8825746178627014]\n\ntest loss: 0.060721830488779605\ntest loss_segm: 0.060721830488779605\ntest loss_shape: 0.03814608427003408\ntest loss_recon: 0.1169752646715213\ntest unet DSC: [0.9961756467819214, 0.8843194842338562, 0.8899967670440674]\nBest val loss: 0.05851745748749146\nTime: 85.97725081443787\n\n\nEpoch 38/300\n\ntrain loss: 0.06603697729827482\ntrain loss_segm: 0.06603697729827482\ntrain loss_shape: 0.04383460347410999\ntrain loss_recon: 0.12660877463183826\ntrain unet DSC: [0.9957775473594666, 0.8743404150009155, 0.882811427116394]\n\ntest loss: 0.06205685169268877\ntest loss_segm: 0.06205685169268877\ntest loss_shape: 0.038013031849494346\ntest loss_recon: 0.11799892095419076\ntest unet DSC: [0.995988130569458, 0.8806656002998352, 0.8903831839561462]\nBest val loss: 0.05851745748749146\nTime: 85.28534984588623\n\n\nEpoch 39/300\n\ntrain loss: 0.06587450234573099\ntrain loss_segm: 0.06587450234573099\ntrain loss_shape: 0.04364310645887369\ntrain loss_recon: 0.12608611357362964\ntrain unet DSC: [0.9957891702651978, 0.8745729327201843, 0.8831175565719604]\n\ntest loss: 0.058331787204131104\ntest loss_segm: 0.058331787204131104\ntest loss_shape: 0.03879283683804365\ntest loss_recon: 0.11748878466777313\ntest unet DSC: [0.9960331916809082, 0.8845868110656738, 0.897513747215271]\nBest val loss: 0.058331787204131104\nTime: 85.60554265975952\n\n\nEpoch 40/300\n\ntrain loss: 0.06572198228839832\ntrain loss_segm: 0.06572198228839832\ntrain loss_shape: 0.043537553022556665\ntrain loss_recon: 0.12583216949354245\ntrain unet DSC: [0.9957963824272156, 0.874849259853363, 0.8833776712417603]\n\ntest loss: 0.058811343824252106\ntest loss_segm: 0.058811343824252106\ntest loss_shape: 0.038270700054290965\ntest loss_recon: 0.11599451914811745\ntest unet DSC: [0.9961360096931458, 0.8854338526725769, 0.8952771425247192]\nBest val loss: 0.058331787204131104\nTime: 85.40840983390808\n\n\nEpoch 41/300\n\ntrain loss: 0.06573820144786865\ntrain loss_segm: 0.06573820144786865\ntrain loss_shape: 0.04354199017339115\ntrain loss_recon: 0.12591078198408778\ntrain unet DSC: [0.9957922697067261, 0.8748475313186646, 0.8834074139595032]\n\ntest loss: 0.058687192984880544\ntest loss_segm: 0.058687192984880544\ntest loss_shape: 0.03781236555331793\ntest loss_recon: 0.1159048997438871\ntest unet DSC: [0.9961774945259094, 0.885710597038269, 0.8956555128097534]\nBest val loss: 0.058331787204131104\nTime: 85.8852789402008\n\n\nEpoch 42/300\n\ntrain loss: 0.06554810252465025\ntrain loss_segm: 0.06554810252465025\ntrain loss_shape: 0.04336652631246591\ntrain loss_recon: 0.12560683488845825\ntrain unet DSC: [0.9958032369613647, 0.8750649094581604, 0.8836387991905212]\n\ntest loss: 0.06065664746058293\ntest loss_segm: 0.06065664746058293\ntest loss_shape: 0.038178826085267924\ntest loss_recon: 0.11679378839639518\ntest unet DSC: [0.9961884617805481, 0.8848288059234619, 0.8900576829910278]\nBest val loss: 0.058331787204131104\nTime: 85.92123889923096\n\n\nEpoch 43/300\n\ntrain loss: 0.06547068490917925\ntrain loss_segm: 0.06547068490917925\ntrain loss_shape: 0.04332385834636567\ntrain loss_recon: 0.12536947523491293\ntrain unet DSC: [0.9958060383796692, 0.8752262592315674, 0.8837262988090515]\n\ntest loss: 0.05798793049194874\ntest loss_segm: 0.05798793049194874\ntest loss_shape: 0.036716713737218805\ntest loss_recon: 0.11372190408217601\ntest unet DSC: [0.9962275624275208, 0.886341392993927, 0.8970831632614136]\nBest val loss: 0.05798793049194874\nTime: 85.77776050567627\n\n\nEpoch 44/300\n\ntrain loss: 0.06512204321879375\ntrain loss_segm: 0.06512204321879375\ntrain loss_shape: 0.04314418488382539\ntrain loss_recon: 0.12489053041120118\ntrain unet DSC: [0.995814859867096, 0.8757521510124207, 0.8844574689865112]\n\ntest loss: 0.06605686486149445\ntest loss_segm: 0.06605686486149445\ntest loss_shape: 0.039964874394429035\ntest loss_recon: 0.12270396947860718\ntest unet DSC: [0.9960740208625793, 0.8794084191322327, 0.8778025507926941]\nBest val loss: 0.05798793049194874\nTime: 85.88471293449402\n\n\nEpoch 45/300\n\ntrain loss: 0.06530589282606976\ntrain loss_segm: 0.06530589282606976\ntrain loss_shape: 0.04321012318228619\ntrain loss_recon: 0.1252197608163085\ntrain unet DSC: [0.9958016872406006, 0.8754034638404846, 0.8841818571090698]\n\ntest loss: 0.06240285369448173\ntest loss_segm: 0.06240285369448173\ntest loss_shape: 0.038181572483900264\ntest loss_recon: 0.11865036304180439\ntest unet DSC: [0.9961366653442383, 0.8822969198226929, 0.887174665927887]\nBest val loss: 0.05798793049194874\nTime: 85.60718846321106\n\n\nEpoch 46/300\n\ntrain loss: 0.06488358865998968\ntrain loss_segm: 0.06488358865998968\ntrain loss_shape: 0.04294108693735509\ntrain loss_recon: 0.12442311493656304\ntrain unet DSC: [0.9958301782608032, 0.8761467337608337, 0.8849252462387085]\n\ntest loss: 0.06282106748758218\ntest loss_segm: 0.06282106748758218\ntest loss_shape: 0.038390583907946564\ntest loss_recon: 0.11861250339410244\ntest unet DSC: [0.9960159063339233, 0.8805093169212341, 0.8873274326324463]\nBest val loss: 0.05798793049194874\nTime: 85.4562623500824\n\n\nEpoch 47/300\n\ntrain loss: 0.06536092573800419\ntrain loss_segm: 0.06536092573800419\ntrain loss_shape: 0.043202362813149826\ntrain loss_recon: 0.12521922965592977\ntrain unet DSC: [0.9957999587059021, 0.8752624988555908, 0.8839699625968933]\n\ntest loss: 0.061247253647217385\ntest loss_segm: 0.061247253647217385\ntest loss_shape: 0.038082354391614594\ntest loss_recon: 0.11740232278139164\ntest unet DSC: [0.996164083480835, 0.8845295906066895, 0.8885563611984253]\nBest val loss: 0.05798793049194874\nTime: 85.36987113952637\n\n\nEpoch 48/300\n\ntrain loss: 0.06494225132503087\ntrain loss_segm: 0.06494225132503087\ntrain loss_shape: 0.0429178267434428\ntrain loss_recon: 0.12450473067126697\ntrain unet DSC: [0.9958245158195496, 0.8760178685188293, 0.8848411440849304]\n\ntest loss: 0.0586670281795355\ntest loss_segm: 0.0586670281795355\ntest loss_shape: 0.03709286365371484\ntest loss_recon: 0.11406718156276605\ntest unet DSC: [0.9961421489715576, 0.8839613199234009, 0.8968091011047363]\nBest val loss: 0.05798793049194874\nTime: 86.13451051712036\n\n\nEpoch 49/300\n\ntrain loss: 0.06445578103767166\ntrain loss_segm: 0.06445578103767166\ntrain loss_shape: 0.04263275710842277\ntrain loss_recon: 0.12371545086933088\ntrain unet DSC: [0.9958475828170776, 0.876844584941864, 0.8856459856033325]\n\ntest loss: 0.05997917724725527\ntest loss_segm: 0.05997917724725527\ntest loss_shape: 0.037334432204564415\ntest loss_recon: 0.11625983776190342\ntest unet DSC: [0.9962275624275208, 0.8862753510475159, 0.8908430933952332]\nBest val loss: 0.05798793049194874\nTime: 86.07077360153198\n\n\nEpoch 50/300\n\ntrain loss: 0.0645582361877719\ntrain loss_segm: 0.0645582361877719\ntrain loss_shape: 0.04265747667302059\ntrain loss_recon: 0.12392365592944471\ntrain unet DSC: [0.9958325028419495, 0.8765552639961243, 0.8855533003807068]\n\ntest loss: 0.05827231160723246\ntest loss_segm: 0.05827231160723246\ntest loss_shape: 0.037856163982397474\ntest loss_recon: 0.11607098732239161\ntest unet DSC: [0.9961785674095154, 0.8873990178108215, 0.8950890898704529]\nBest val loss: 0.05798793049194874\nTime: 85.90868735313416\n\n\nEpoch 51/300\n\ntrain loss: 0.06427047401666641\ntrain loss_segm: 0.06427047401666641\ntrain loss_shape: 0.04250868317918687\ntrain loss_recon: 0.1235695451120787\ntrain unet DSC: [0.9958464503288269, 0.877099335193634, 0.886079728603363]\n\ntest loss: 0.0629490818350743\ntest loss_segm: 0.0629490818350743\ntest loss_shape: 0.03824921659170053\ntest loss_recon: 0.11903717120488484\ntest unet DSC: [0.9960957169532776, 0.8813714385032654, 0.8856704831123352]\nBest val loss: 0.05798793049194874\nTime: 85.67131924629211\n\n\nEpoch 52/300\n\ntrain loss: 0.0643007368839617\ntrain loss_segm: 0.0643007368839617\ntrain loss_shape: 0.0424826783658583\ntrain loss_recon: 0.12357939685447307\ntrain unet DSC: [0.9958416819572449, 0.876922607421875, 0.8860406279563904]\n\ntest loss: 0.06133881965890909\ntest loss_segm: 0.06133881965890909\ntest loss_shape: 0.03776423160273295\ntest loss_recon: 0.11712002907043849\ntest unet DSC: [0.9961575269699097, 0.8837770223617554, 0.8884472846984863]\nBest val loss: 0.05798793049194874\nTime: 85.54858946800232\n\n\nEpoch 53/300\n\ntrain loss: 0.06420063505632968\ntrain loss_segm: 0.06420063505632968\ntrain loss_shape: 0.0424037534460614\ntrain loss_recon: 0.12334501667867732\ntrain unet DSC: [0.9958542585372925, 0.8771905303001404, 0.8862030506134033]\n\ntest loss: 0.0584713825239585\ntest loss_segm: 0.0584713825239585\ntest loss_shape: 0.037033761923129745\ntest loss_recon: 0.11435540364338802\ntest unet DSC: [0.9961967468261719, 0.8862959742546082, 0.8956541419029236]\nBest val loss: 0.05798793049194874\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.63970708847046\n\n\nEpoch 54/300\n\ntrain loss: 0.06343390766672696\ntrain loss_segm: 0.06343390766672696\ntrain loss_shape: 0.04199839548408231\ntrain loss_recon: 0.1221822202205658\ntrain unet DSC: [0.9958878755569458, 0.8785274624824524, 0.8874438405036926]\n\ntest loss: 0.05748322128485411\ntest loss_segm: 0.05748322128485411\ntest loss_shape: 0.036267751159194186\ntest loss_recon: 0.11248325078915326\ntest unet DSC: [0.9962541460990906, 0.8872552514076233, 0.8975452780723572]\nBest val loss: 0.05748322128485411\nTime: 86.1077356338501\n\n\nEpoch 55/300\n\ntrain loss: 0.06322081778434259\ntrain loss_segm: 0.06322081778434259\ntrain loss_shape: 0.04174768721944169\ntrain loss_recon: 0.12160983266709727\ntrain unet DSC: [0.9959097504615784, 0.8788654804229736, 0.8877700567245483]\n\ntest loss: 0.05766964044708472\ntest loss_segm: 0.05766964044708472\ntest loss_shape: 0.03666649376734709\ntest loss_recon: 0.11337192089129718\ntest unet DSC: [0.9962535500526428, 0.8878737092018127, 0.8965891599655151]\nBest val loss: 0.05748322128485411\nTime: 87.93619656562805\n\n\nEpoch 56/300\n\ntrain loss: 0.06312810516432871\ntrain loss_segm: 0.06312810516432871\ntrain loss_shape: 0.041746352127269855\ntrain loss_recon: 0.1215699846985974\ntrain unet DSC: [0.9959021210670471, 0.8789210319519043, 0.8879072070121765]\n\ntest loss: 0.05865463347007067\ntest loss_segm: 0.05865463347007067\ntest loss_shape: 0.03675091080367565\ntest loss_recon: 0.11409905629280286\ntest unet DSC: [0.9962377548217773, 0.8869956731796265, 0.894304633140564]\nBest val loss: 0.05748322128485411\nTime: 86.01345801353455\n\n\nEpoch 57/300\n\ntrain loss: 0.06310903420082375\ntrain loss_segm: 0.06310903420082375\ntrain loss_shape: 0.0417632563864883\ntrain loss_recon: 0.1215582437153104\ntrain unet DSC: [0.99590003490448, 0.8789535760879517, 0.8880035877227783]\n\ntest loss: 0.05920551134607731\ntest loss_segm: 0.05920551134607731\ntest loss_shape: 0.036702677416495785\ntest loss_recon: 0.11407935619354248\ntest unet DSC: [0.996210515499115, 0.8855748176574707, 0.893682062625885]\nBest val loss: 0.05748322128485411\nTime: 86.67721605300903\n\n\nEpoch 58/300\n\ntrain loss: 0.06301940475366538\ntrain loss_segm: 0.06301940475366538\ntrain loss_shape: 0.04162275222943553\ntrain loss_recon: 0.12129550530940672\ntrain unet DSC: [0.9959145188331604, 0.8791513442993164, 0.8881084322929382]\n\ntest loss: 0.05893094809009479\ntest loss_segm: 0.05893094809009479\ntest loss_shape: 0.03676331559052834\ntest loss_recon: 0.11406508928690201\ntest unet DSC: [0.9962171912193298, 0.8862090706825256, 0.8940469026565552]\nBest val loss: 0.05748322128485411\nTime: 86.45150637626648\n\n\nEpoch 59/300\n\ntrain loss: 0.06302993058498148\ntrain loss_segm: 0.06302993058498148\ntrain loss_shape: 0.04169481419781341\ntrain loss_recon: 0.12145831049243107\ntrain unet DSC: [0.9959080219268799, 0.8791181445121765, 0.8880710601806641]\n\ntest loss: 0.05772588363824747\ntest loss_segm: 0.05772588363824747\ntest loss_shape: 0.036423895603571184\ntest loss_recon: 0.11313768380727524\ntest unet DSC: [0.9962812662124634, 0.8885554075241089, 0.895704984664917]\nBest val loss: 0.05748322128485411\nTime: 85.95495057106018\n\n\nEpoch 60/300\n\ntrain loss: 0.06300277376099478\ntrain loss_segm: 0.06300277376099478\ntrain loss_shape: 0.041592626207614246\ntrain loss_recon: 0.12124545891073686\ntrain unet DSC: [0.9959191679954529, 0.8792160749435425, 0.8881499767303467]\n\ntest loss: 0.05698663120468458\ntest loss_segm: 0.05698663120468458\ntest loss_shape: 0.03624414079464399\ntest loss_recon: 0.11212208484992003\ntest unet DSC: [0.9962568879127502, 0.8880673050880432, 0.898443341255188]\nBest val loss: 0.05698663120468458\nTime: 86.04655933380127\n\n\nEpoch 61/300\n\ntrain loss: 0.0628824106688741\ntrain loss_segm: 0.0628824106688741\ntrain loss_shape: 0.04156611098235921\ntrain loss_recon: 0.12115036799937864\ntrain unet DSC: [0.9959151148796082, 0.8793473839759827, 0.8884136080741882]\n\ntest loss: 0.057233499410824895\ntest loss_segm: 0.057233499410824895\ntest loss_shape: 0.03628455775861557\ntest loss_recon: 0.11248007798806214\ntest unet DSC: [0.996253490447998, 0.8875001072883606, 0.8981655836105347]\nBest val loss: 0.05698663120468458\nTime: 85.98952436447144\n\n\nEpoch 62/300\n\ntrain loss: 0.0629199766141327\ntrain loss_segm: 0.0629199766141327\ntrain loss_shape: 0.04153873931758011\ntrain loss_recon: 0.12106428568876243\ntrain unet DSC: [0.99592125415802, 0.879327118396759, 0.8882169127464294]\n\ntest loss: 0.057091107830787316\ntest loss_segm: 0.057091107830787316\ntest loss_shape: 0.036573169800715566\ntest loss_recon: 0.11280260024926601\ntest unet DSC: [0.9962314963340759, 0.8876370191574097, 0.8986356854438782]\nBest val loss: 0.05698663120468458\nTime: 85.71544289588928\n\n\nEpoch 63/300\n\ntrain loss: 0.06288525231088264\ntrain loss_segm: 0.06288525231088264\ntrain loss_shape: 0.0415265321778723\ntrain loss_recon: 0.12113530575474606\ntrain unet DSC: [0.9959235787391663, 0.8794424533843994, 0.8883435130119324]\n\ntest loss: 0.0580133325778521\ntest loss_segm: 0.0580133325778521\ntest loss_shape: 0.036592077798186205\ntest loss_recon: 0.11317850534732525\ntest unet DSC: [0.9962528944015503, 0.8875291347503662, 0.8955928087234497]\nBest val loss: 0.05698663120468458\nTime: 85.81344628334045\n\n\nEpoch 64/300\n\ntrain loss: 0.06288969471956356\ntrain loss_segm: 0.06288969471956356\ntrain loss_shape: 0.04152159939838361\ntrain loss_recon: 0.12105399785162527\ntrain unet DSC: [0.9959211945533752, 0.8794664740562439, 0.8883302211761475]\n\ntest loss: 0.058580917616685234\ntest loss_segm: 0.058580917616685234\ntest loss_shape: 0.03639728566392874\ntest loss_recon: 0.11347398085471912\ntest unet DSC: [0.9962228536605835, 0.8868327140808105, 0.8946141004562378]\nBest val loss: 0.05698663120468458\nTime: 85.61693072319031\n\n\nEpoch 65/300\n\ntrain loss: 0.06283855417012414\ntrain loss_segm: 0.06283855417012414\ntrain loss_shape: 0.041568666248570516\ntrain loss_recon: 0.12118590594846991\ntrain unet DSC: [0.9959189295768738, 0.8795121312141418, 0.8884474635124207]\n\ntest loss: 0.05752554478553625\ntest loss_segm: 0.05752554478553625\ntest loss_shape: 0.03602545254696638\ntest loss_recon: 0.11220192297911033\ntest unet DSC: [0.9962594509124756, 0.8869935870170593, 0.8977718949317932]\nBest val loss: 0.05698663120468458\nTime: 86.0104591846466\n\n\nEpoch 66/300\n\ntrain loss: 0.06281669079502926\ntrain loss_segm: 0.06281669079502926\ntrain loss_shape: 0.04143709748323205\ntrain loss_recon: 0.1208810221545304\ntrain unet DSC: [0.9959192276000977, 0.8794600367546082, 0.8885676860809326]\n\ntest loss: 0.05964632179492559\ntest loss_segm: 0.05964632179492559\ntest loss_shape: 0.036985717761592984\ntest loss_recon: 0.11518762356195694\ntest unet DSC: [0.9962066411972046, 0.8858140110969543, 0.8922606110572815]\nBest val loss: 0.05698663120468458\nTime: 85.4803512096405\n\n\nEpoch 67/300\n\ntrain loss: 0.06274977456070954\ntrain loss_segm: 0.06274977456070954\ntrain loss_shape: 0.04143932391005226\ntrain loss_recon: 0.1208394939386392\ntrain unet DSC: [0.9959278106689453, 0.8796077966690063, 0.8886091113090515]\n\ntest loss: 0.05700479629330146\ntest loss_segm: 0.05700479629330146\ntest loss_shape: 0.036408441475568674\ntest loss_recon: 0.11270520625970303\ntest unet DSC: [0.996224045753479, 0.8876842856407166, 0.8988128900527954]\nBest val loss: 0.05698663120468458\nTime: 85.98150730133057\n\n\nEpoch 68/300\n\ntrain loss: 0.0627449629474667\ntrain loss_segm: 0.0627449629474667\ntrain loss_shape: 0.041498606741617\ntrain loss_recon: 0.1209163262119776\ntrain unet DSC: [0.9959295988082886, 0.879691481590271, 0.88860023021698]\n\ntest loss: 0.05948653607032238\ntest loss_segm: 0.05948653607032238\ntest loss_shape: 0.037177977605890006\ntest loss_recon: 0.11505889434080857\ntest unet DSC: [0.9962393641471863, 0.8864638209342957, 0.8918946385383606]\nBest val loss: 0.05698663120468458\nTime: 85.97835445404053\n\n\nEpoch 69/300\n\ntrain loss: 0.06267902453111697\ntrain loss_segm: 0.06267902453111697\ntrain loss_shape: 0.041413495576457134\ntrain loss_recon: 0.12084531821782075\ntrain unet DSC: [0.9959288239479065, 0.8797076344490051, 0.8887953162193298]\n\ntest loss: 0.06121890046275579\ntest loss_segm: 0.06121890046275579\ntest loss_shape: 0.037554252080810376\ntest loss_recon: 0.11722530615635407\ntest unet DSC: [0.9961528182029724, 0.8840819001197815, 0.8886325359344482]\nBest val loss: 0.05698663120468458\nTime: 85.94538068771362\n\n\nEpoch 70/300\n\ntrain loss: 0.06271671119464349\ntrain loss_segm: 0.06271671119464349\ntrain loss_shape: 0.04138407262065743\ntrain loss_recon: 0.12073269260080555\ntrain unet DSC: [0.9959290027618408, 0.8796951770782471, 0.8886576890945435]\n\ntest loss: 0.058726259340078406\ntest loss_segm: 0.058726259340078406\ntest loss_shape: 0.036671762187511496\ntest loss_recon: 0.11414041274633163\ntest unet DSC: [0.9962427020072937, 0.887130081653595, 0.8938243389129639]\nBest val loss: 0.05698663120468458\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.80023312568665\n\n\nEpoch 71/300\n\ntrain loss: 0.06251831576699697\ntrain loss_segm: 0.06251831576699697\ntrain loss_shape: 0.04129504608108273\ntrain loss_recon: 0.12047833241993867\ntrain unet DSC: [0.9959361553192139, 0.8799905776977539, 0.8889786005020142]\n\ntest loss: 0.057720949730047814\ntest loss_segm: 0.057720949730047814\ntest loss_shape: 0.03653194998892454\ntest loss_recon: 0.11298974660726693\ntest unet DSC: [0.9961931109428406, 0.88528972864151, 0.8985527157783508]\nBest val loss: 0.05698663120468458\nTime: 86.11216711997986\n\n\nEpoch 72/300\n\ntrain loss: 0.06244242035701305\ntrain loss_segm: 0.06244242035701305\ntrain loss_shape: 0.041279591384190545\ntrain loss_recon: 0.12044339768494232\ntrain unet DSC: [0.9959357380867004, 0.8800811767578125, 0.8890983462333679]\n\ntest loss: 0.05838458220928143\ntest loss_segm: 0.05838458220928143\ntest loss_shape: 0.0366292115396414\ntest loss_recon: 0.11373693820757744\ntest unet DSC: [0.9962579011917114, 0.8876810073852539, 0.8944076895713806]\nBest val loss: 0.05698663120468458\nTime: 86.88417959213257\n\n\nEpoch 73/300\n\ntrain loss: 0.06242805705228938\ntrain loss_segm: 0.06242805705228938\ntrain loss_shape: 0.04119026972145974\ntrain loss_recon: 0.120305662668204\ntrain unet DSC: [0.9959431886672974, 0.8801466822624207, 0.8891286849975586]\n\ntest loss: 0.05733381374142109\ntest loss_segm: 0.05733381374142109\ntest loss_shape: 0.03637405250890133\ntest loss_recon: 0.11294679611157148\ntest unet DSC: [0.996199369430542, 0.8862255811691284, 0.8989750742912292]\nBest val loss: 0.05698663120468458\nTime: 85.60036754608154\n\n\nEpoch 74/300\n\ntrain loss: 0.062457177198574514\ntrain loss_segm: 0.062457177198574514\ntrain loss_shape: 0.04121731605993796\ntrain loss_recon: 0.12031530202189579\ntrain unet DSC: [0.9959421157836914, 0.8800894618034363, 0.8890799880027771]\n\ntest loss: 0.057754117708939776\ntest loss_segm: 0.057754117708939776\ntest loss_shape: 0.03630540820841606\ntest loss_recon: 0.11288548432863675\ntest unet DSC: [0.9962794184684753, 0.8883851170539856, 0.895803689956665]\nBest val loss: 0.05698663120468458\nTime: 85.6019868850708\n\n\nEpoch 75/300\n\ntrain loss: 0.06244008813666392\ntrain loss_segm: 0.06244008813666392\ntrain loss_shape: 0.04133164055079599\ntrain loss_recon: 0.12057227952570855\ntrain unet DSC: [0.9959353804588318, 0.8801568150520325, 0.8891106843948364]\n\ntest loss: 0.06211384328512045\ntest loss_segm: 0.06211384328512045\ntest loss_shape: 0.037984431554109625\ntest loss_recon: 0.11783028260255471\ntest unet DSC: [0.9961457848548889, 0.8833028078079224, 0.8862881660461426]\nBest val loss: 0.05698663120468458\nTime: 85.75705456733704\n\n\nEpoch 76/300\n\ntrain loss: 0.06247097369330593\ntrain loss_segm: 0.06247097369330593\ntrain loss_shape: 0.04123279730541796\ntrain loss_recon: 0.12042237535307679\ntrain unet DSC: [0.9959433674812317, 0.8801431059837341, 0.8890193700790405]\n\ntest loss: 0.05692629544780804\ntest loss_segm: 0.05692629544780804\ntest loss_shape: 0.03589047548862604\ntest loss_recon: 0.11165088720810719\ntest unet DSC: [0.9962853789329529, 0.88853520154953, 0.8982651829719543]\nBest val loss: 0.05692629544780804\nTime: 86.18080353736877\n\n\nEpoch 77/300\n\ntrain loss: 0.06240318953708003\ntrain loss_segm: 0.06240318953708003\ntrain loss_shape: 0.04117130662632894\ntrain loss_recon: 0.1202608641189865\ntrain unet DSC: [0.9959424138069153, 0.8801454901695251, 0.8891919255256653]\n\ntest loss: 0.05848010696279697\ntest loss_segm: 0.05848010696279697\ntest loss_shape: 0.036671476295361154\ntest loss_recon: 0.1137574498470013\ntest unet DSC: [0.996260404586792, 0.8876307606697083, 0.8941627740859985]\nBest val loss: 0.05692629544780804\nTime: 86.3720211982727\n\n\nEpoch 78/300\n\ntrain loss: 0.062397925324643715\ntrain loss_segm: 0.062397925324643715\ntrain loss_shape: 0.041242614223421376\ntrain loss_recon: 0.12031135717524757\ntrain unet DSC: [0.9959444403648376, 0.8802341222763062, 0.8891626596450806]\n\ntest loss: 0.05727125838016852\ntest loss_segm: 0.05727125838016852\ntest loss_shape: 0.036384073109963\ntest loss_recon: 0.11259982524774013\ntest unet DSC: [0.9962313771247864, 0.8868521451950073, 0.8985580801963806]\nBest val loss: 0.05692629544780804\nTime: 86.01972723007202\n\n\nEpoch 79/300\n\ntrain loss: 0.06244303887309153\ntrain loss_segm: 0.06244303887309153\ntrain loss_shape: 0.041215554021204574\ntrain loss_recon: 0.12027594069891338\ntrain unet DSC: [0.9959456920623779, 0.8801380395889282, 0.8890986442565918]\n\ntest loss: 0.05886695782343546\ntest loss_segm: 0.05886695782343546\ntest loss_shape: 0.037650107621000364\ntest loss_recon: 0.11579612432382046\ntest unet DSC: [0.9960767030715942, 0.8824318051338196, 0.8975304961204529]\nBest val loss: 0.05692629544780804\nTime: 85.86309432983398\n\n\nEpoch 80/300\n\ntrain loss: 0.06235863553666616\ntrain loss_segm: 0.06235863553666616\ntrain loss_shape: 0.04128643054562279\ntrain loss_recon: 0.12032748174063768\ntrain unet DSC: [0.9959459900856018, 0.880272388458252, 0.8891913294792175]\n\ntest loss: 0.061748710580361195\ntest loss_segm: 0.061748710580361195\ntest loss_shape: 0.03783515726144497\ntest loss_recon: 0.1175265847108303\ntest unet DSC: [0.9961328506469727, 0.8833944797515869, 0.887550950050354]\nBest val loss: 0.05692629544780804\nTime: 85.99834489822388\n\n\nEpoch 81/300\n\ntrain loss: 0.0624375725990232\ntrain loss_segm: 0.0624375725990232\ntrain loss_shape: 0.041300358934493006\ntrain loss_recon: 0.1203963794285738\ntrain unet DSC: [0.9959389567375183, 0.8801797032356262, 0.8892596364021301]\n\ntest loss: 0.0577226232450742\ntest loss_segm: 0.0577226232450742\ntest loss_shape: 0.036183792524612866\ntest loss_recon: 0.11277091655975734\ntest unet DSC: [0.9962862730026245, 0.8883763551712036, 0.8958756923675537]\nBest val loss: 0.05692629544780804\nTime: 86.17799663543701\n\n\nEpoch 82/300\n\ntrain loss: 0.06240346457196187\ntrain loss_segm: 0.06240346457196187\ntrain loss_shape: 0.04115843522963645\ntrain loss_recon: 0.120155465376528\ntrain unet DSC: [0.9959494471549988, 0.8801910877227783, 0.8891401886940002]\n\ntest loss: 0.06499829362982358\ntest loss_segm: 0.06499829362982358\ntest loss_shape: 0.03915759696601293\ntest loss_recon: 0.12142959924844596\ntest unet DSC: [0.9959943890571594, 0.8790518641471863, 0.8810673952102661]\nBest val loss: 0.05692629544780804\nTime: 85.79954743385315\n\n\nEpoch 83/300\n\ntrain loss: 0.06242971131695977\ntrain loss_segm: 0.06242971131695977\ntrain loss_shape: 0.041224529966712\ntrain loss_recon: 0.12035054609745363\ntrain unet DSC: [0.995941162109375, 0.8801771998405457, 0.8891659379005432]\n\ntest loss: 0.06836751046089026\ntest loss_segm: 0.06836751046089026\ntest loss_shape: 0.040838585259058535\ntest loss_recon: 0.126557483122899\ntest unet DSC: [0.9957113265991211, 0.8728979825973511, 0.8764365911483765]\nBest val loss: 0.05692629544780804\nTime: 86.57163596153259\n\n\nEpoch 84/300\n\ntrain loss: 0.0623691892840817\ntrain loss_segm: 0.0623691892840817\ntrain loss_shape: 0.04119923420816283\ntrain loss_recon: 0.12023624966416178\ntrain unet DSC: [0.9959470629692078, 0.8803133368492126, 0.8892786502838135]\n\ntest loss: 0.05722413422205509\ntest loss_segm: 0.05722413422205509\ntest loss_shape: 0.03641261150821661\ntest loss_recon: 0.11261765773479755\ntest unet DSC: [0.9962651133537292, 0.888374924659729, 0.8975127935409546]\nBest val loss: 0.05692629544780804\nTime: 85.48744106292725\n\n\nEpoch 85/300\n\ntrain loss: 0.062404114918052395\ntrain loss_segm: 0.062404114918052395\ntrain loss_shape: 0.041267718742542626\ntrain loss_recon: 0.12036510283433938\ntrain unet DSC: [0.9959462881088257, 0.8802005648612976, 0.8890486359596252]\n\ntest loss: 0.05705814235485517\ntest loss_segm: 0.05705814235485517\ntest loss_shape: 0.03609694397220245\ntest loss_recon: 0.11200460715171619\ntest unet DSC: [0.9962623715400696, 0.8878545761108398, 0.8985327482223511]\nBest val loss: 0.05692629544780804\nTime: 85.52079677581787\n\n\nEpoch 86/300\n\ntrain loss: 0.06239223475509052\ntrain loss_segm: 0.06239223475509052\ntrain loss_shape: 0.041159384188395515\ntrain loss_recon: 0.12023085355758667\ntrain unet DSC: [0.9959456324577332, 0.8802044987678528, 0.8891485929489136]\n\ntest loss: 0.06084817504653564\ntest loss_segm: 0.06084817504653564\ntest loss_shape: 0.037460083237443216\ntest loss_recon: 0.1165509392053653\ntest unet DSC: [0.9961377382278442, 0.884242057800293, 0.8897590637207031]\nBest val loss: 0.05692629544780804\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 86.3504900932312\n\n\nEpoch 87/300\n\ntrain loss: 0.062349384131876726\ntrain loss_segm: 0.062349384131876726\ntrain loss_shape: 0.041140616741738745\ntrain loss_recon: 0.12006631003150457\ntrain unet DSC: [0.9959518909454346, 0.8803436756134033, 0.8892360925674438]\n\ntest loss: 0.056968309940435946\ntest loss_segm: 0.056968309940435946\ntest loss_shape: 0.036216529516073376\ntest loss_recon: 0.11226091323754726\ntest unet DSC: [0.9962721467018127, 0.8886567950248718, 0.8980261087417603]\nBest val loss: 0.05692629544780804\nTime: 85.44749689102173\n\n\nEpoch 88/300\n\ntrain loss: 0.06226367413809028\ntrain loss_segm: 0.06226367413809028\ntrain loss_shape: 0.04111578975674472\ntrain loss_recon: 0.12008968825581708\ntrain unet DSC: [0.9959502220153809, 0.8804557919502258, 0.8894246816635132]\n\ntest loss: 0.05694749722113976\ntest loss_segm: 0.05694749722113976\ntest loss_shape: 0.03633612575821388\ntest loss_recon: 0.112530984939673\ntest unet DSC: [0.9962428212165833, 0.8877737522125244, 0.8989457488059998]\nBest val loss: 0.05692629544780804\nTime: 86.07331871986389\n\n\nEpoch 89/300\n\ntrain loss: 0.062277524083664146\ntrain loss_segm: 0.062277524083664146\ntrain loss_shape: 0.04115201426740689\ntrain loss_recon: 0.12011167293862451\ntrain unet DSC: [0.9959450960159302, 0.8804037570953369, 0.8894342184066772]\n\ntest loss: 0.05706683221535805\ntest loss_segm: 0.05706683221535805\ntest loss_shape: 0.03609608056453558\ntest loss_recon: 0.11204849145351312\ntest unet DSC: [0.9962702989578247, 0.8882097005844116, 0.8981239199638367]\nBest val loss: 0.05692629544780804\nTime: 85.66458535194397\n\n\nEpoch 90/300\n\ntrain loss: 0.06228233562617362\ntrain loss_segm: 0.06228233562617362\ntrain loss_shape: 0.041078511861306205\ntrain loss_recon: 0.12002126212361493\ntrain unet DSC: [0.9959537386894226, 0.8804507851600647, 0.8894452452659607]\n\ntest loss: 0.05970970445718521\ntest loss_segm: 0.05970970445718521\ntest loss_shape: 0.03690813147486784\ntest loss_recon: 0.1149058250280527\ntest unet DSC: [0.9962023496627808, 0.8858668208122253, 0.8917720317840576]\nBest val loss: 0.05692629544780804\nTime: 85.74614477157593\n\n\nEpoch 91/300\n\ntrain loss: 0.062279321724855445\ntrain loss_segm: 0.062279321724855445\ntrain loss_shape: 0.041144348044372815\ntrain loss_recon: 0.12013517263569409\ntrain unet DSC: [0.9959484934806824, 0.8803620934486389, 0.8894031643867493]\n\ntest loss: 0.058202230490935154\ntest loss_segm: 0.058202230490935154\ntest loss_shape: 0.0366258192807436\ntest loss_recon: 0.11365120991682395\ntest unet DSC: [0.9962617754936218, 0.887875497341156, 0.8947398066520691]\nBest val loss: 0.05692629544780804\nTime: 86.1258602142334\n\n\nEpoch 92/300\n\ntrain loss: 0.06231629174155525\ntrain loss_segm: 0.06231629174155525\ntrain loss_shape: 0.04122425662943079\ntrain loss_recon: 0.1201962428756907\ntrain unet DSC: [0.9959438443183899, 0.8803577423095703, 0.8893907070159912]\n\ntest loss: 0.05813633058315668\ntest loss_segm: 0.05813633058315668\ntest loss_shape: 0.03639345835798826\ntest loss_recon: 0.11318263335105701\ntest unet DSC: [0.9962658286094666, 0.8878350853919983, 0.8950071334838867]\nBest val loss: 0.05692629544780804\nTime: 85.99808359146118\n\n\nEpoch 93/300\n\ntrain loss: 0.06235350531679165\ntrain loss_segm: 0.06235350531679165\ntrain loss_shape: 0.04117550269429442\ntrain loss_recon: 0.12026069209545473\ntrain unet DSC: [0.9959461092948914, 0.8802646398544312, 0.8892907500267029]\n\ntest loss: 0.05760593186968412\ntest loss_segm: 0.05760593186968412\ntest loss_shape: 0.036228080972647056\ntest loss_recon: 0.11268035876445281\ntest unet DSC: [0.9962809681892395, 0.88840252161026, 0.8962158560752869]\nBest val loss: 0.05692629544780804\nTime: 85.68360304832458\n\n\nEpoch 94/300\n\ntrain loss: 0.06234299259472497\ntrain loss_segm: 0.06234299259472497\ntrain loss_shape: 0.04118838359283496\ntrain loss_recon: 0.12024544311475151\ntrain unet DSC: [0.9959438443183899, 0.8802511692047119, 0.8892748951911926]\n\ntest loss: 0.057604171049136385\ntest loss_segm: 0.057604171049136385\ntest loss_shape: 0.036135281603305765\ntest loss_recon: 0.11236914915916248\ntest unet DSC: [0.9962783455848694, 0.8882465958595276, 0.8963699340820312]\nBest val loss: 0.05692629544780804\nTime: 86.41441917419434\n\n\nEpoch 95/300\n\ntrain loss: 0.062279351314977756\ntrain loss_segm: 0.062279351314977756\ntrain loss_shape: 0.04112306661620925\ntrain loss_recon: 0.12003617271592346\ntrain unet DSC: [0.9959545135498047, 0.8804332613945007, 0.8893720507621765]\n\ntest loss: 0.05704449174495844\ntest loss_segm: 0.05704449174495844\ntest loss_shape: 0.03607693902001931\ntest loss_recon: 0.11226533162288177\ntest unet DSC: [0.9962766766548157, 0.8884803652763367, 0.8978918790817261]\nBest val loss: 0.05692629544780804\nTime: 85.67456650733948\n\n\nEpoch 96/300\n\ntrain loss: 0.06228648732074454\ntrain loss_segm: 0.06228648732074454\ntrain loss_shape: 0.041130743237047254\ntrain loss_recon: 0.12005818531483034\ntrain unet DSC: [0.9959535002708435, 0.8804019093513489, 0.8893636465072632]\n\ntest loss: 0.05768565184030777\ntest loss_segm: 0.05768565184030777\ntest loss_shape: 0.03621867131919433\ntest loss_recon: 0.11265983489843515\ntest unet DSC: [0.9962717890739441, 0.8881701827049255, 0.8962414264678955]\nBest val loss: 0.05692629544780804\nValidation loss stopped to decrease for 10 epochs (LR /= 5).\nTime: 85.7052686214447\n\n\nEpoch 97/300\n\ntrain loss: 0.062325604778679114\ntrain loss_segm: 0.062325604778679114\ntrain loss_shape: 0.04122177308684663\ntrain loss_recon: 0.1202716506734679\ntrain unet DSC: [0.9959458708763123, 0.8803042769432068, 0.8892644643783569]\n\ntest loss: 0.05705458288773512\ntest loss_segm: 0.05705458288773512\ntest loss_shape: 0.036374192302807785\ntest loss_recon: 0.11257985616341615\ntest unet DSC: [0.996238112449646, 0.8875221610069275, 0.8988215327262878]\nBest val loss: 0.05692629544780804\nTime: 85.6927318572998\n\n\nEpoch 98/300\n\ntrain loss: 0.06224312513029274\ntrain loss_segm: 0.06224312513029274\ntrain loss_shape: 0.04115211220953283\ntrain loss_recon: 0.12005091996132573\ntrain unet DSC: [0.9959556460380554, 0.8805310130119324, 0.8894438743591309]\n\ntest loss: 0.06029245992883658\ntest loss_segm: 0.06029245992883658\ntest loss_shape: 0.03833001489058519\ntest loss_recon: 0.11776949656315339\ntest unet DSC: [0.995978832244873, 0.8791508078575134, 0.8961109519004822]\nBest val loss: 0.05692629544780804\nTime: 85.86461544036865\n\nValidation loss stopped to decrease for 30 epochs. Training terminated.\nBest epoch: 76\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678453470882
        }
      },
      "id": "cae40ca5-0fc2-4780-a943-c47e74c2049b"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.1, lambda_recon=0.1,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEpoch 1/300\n\ntrain loss: 0.405095580918125\ntrain loss_segm: 0.405095580918125\ntrain loss_shape: 0.19913736711951752\ntrain loss_recon: 0.42969625735584693\ntrain unet DSC: [0.9322459697723389, 0.5393196940422058, 0.45848700404167175]\n\ntest loss: 0.19002980910814726\ntest loss_segm: 0.19002980910814726\ntest loss_shape: 0.09654266635576884\ntest loss_recon: 0.2566548540041997\ntest unet DSC: [0.9892118573188782, 0.7091829776763916, 0.7857166528701782]\nBest val loss: 0.19002980910814726\nTime: 85.30607652664185\n\n\nEpoch 2/300\n\ntrain loss: 0.18207280573588383\ntrain loss_segm: 0.18207280573588383\ntrain loss_shape: 0.12333071071513091\ntrain loss_recon: 0.26518886451479756\ntrain unet DSC: [0.9898360967636108, 0.74237060546875, 0.7732837200164795]\n\ntest loss: 0.14960497655929664\ntest loss_segm: 0.14960497655929664\ntest loss_shape: 0.06922480177420837\ntest loss_recon: 0.19778631895016402\ntest unet DSC: [0.9912402629852295, 0.7452601194381714, 0.829781174659729]\nBest val loss: 0.14960497655929664\nTime: 86.16869330406189\n\n\nEpoch 3/300\n\ntrain loss: 0.14508942326035681\ntrain loss_segm: 0.14508942326035681\ntrain loss_shape: 0.09149960140827336\ntrain loss_recon: 0.2110644103605536\ntrain unet DSC: [0.9921848773956299, 0.7880577445030212, 0.8090702295303345]\n\ntest loss: 0.10968703910326347\ntest loss_segm: 0.10968703910326347\ntest loss_shape: 0.056647285723533385\ntest loss_recon: 0.1517353653907776\ntest unet DSC: [0.9941388368606567, 0.8265618681907654, 0.8578456044197083]\nBest val loss: 0.10968703910326347\nTime: 85.40591049194336\n\n\nEpoch 4/300\n\ntrain loss: 0.12778736136948007\ntrain loss_segm: 0.12778736136948007\ntrain loss_shape: 0.07649938285916666\ntrain loss_recon: 0.18632870723929587\ntrain unet DSC: [0.9933139681816101, 0.8114596009254456, 0.8262047171592712]\n\ntest loss: 0.09787271592097405\ntest loss_segm: 0.09787271592097405\ntest loss_shape: 0.05109612156565373\ntest loss_recon: 0.14255878100028405\ntest unet DSC: [0.994786262512207, 0.8501076102256775, 0.8674456477165222]\nBest val loss: 0.09787271592097405\nTime: 85.72657871246338\n\n\nEpoch 5/300\n\ntrain loss: 0.11599835744009743\ntrain loss_segm: 0.11599835744009743\ntrain loss_shape: 0.06752504877557483\ntrain loss_recon: 0.17009676709959778\ntrain unet DSC: [0.9940119981765747, 0.827494740486145, 0.8393900394439697]\n\ntest loss: 0.10584267095113412\ntest loss_segm: 0.10584267095113412\ntest loss_shape: 0.052272590975730844\ntest loss_recon: 0.1502106418976417\ntest unet DSC: [0.9946255683898926, 0.843938946723938, 0.84761643409729]\nBest val loss: 0.09787271592097405\nTime: 84.94259762763977\n\n\nEpoch 6/300\n\ntrain loss: 0.10963460722867446\ntrain loss_segm: 0.10963460722867446\ntrain loss_shape: 0.06238574327170095\ntrain loss_recon: 0.16129484131366392\ntrain unet DSC: [0.9943838119506836, 0.8360995650291443, 0.8462225198745728]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(): clean(model)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_recon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, dataloaders, learning_rate, lambda_shape, lambda_recon)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m dataloaders[mode]:\n\u001b[1;32m     66\u001b[0m     image \u001b[38;5;241m=\u001b[39m subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][tio\u001b[38;5;241m.\u001b[39mDATA]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m     label \u001b[38;5;241m=\u001b[39m subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][tio\u001b[38;5;241m.\u001b[39mDATA]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678748482909
        }
      },
      "id": "5a457b55-a313-41f6-9409-7f7fa98651e8"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.1, lambda_recon=0.01,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678453971422
        }
      },
      "id": "3237deb6-63ab-453b-819a-465277b25bd8"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in globals(): clean(model)\n",
        "model = Model().to(device)\n",
        "train(\n",
        "    model=model, n_epochs=300, dataloaders=dataloaders, learning_rate=1e-3, \n",
        "    lambda_shape=0.01, lambda_recon=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678453971449
        }
      },
      "id": "40229ad7-9cb4-487b-bca1-62d478b5bb9e"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}