{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"sourceType":"competition"},{"sourceId":4264054,"sourceType":"datasetVersion","datasetId":2406209},{"sourceId":4455714,"sourceType":"datasetVersion","datasetId":2608678}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 0. Imports and Constants","metadata":{}},{"cell_type":"code","source":"# install packages\n!pip3 install -U torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116\n!pip3 install python-gdcm pylibjpeg pylibjpeg-libjpeg pydicom\n!pip3 install ensemble-boxes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport torch\nimport torchvision as tv\nfrom torchvision.models.feature_extraction import create_feature_extractor\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data.sampler import SequentialSampler\nfrom ensemble_boxes import *\nfrom tqdm import tqdm\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify paths and parameters\nPATH_MAIN = '../input/rsna-2022-cervical-spine-fracture-detection'\nPATH_IMAGES = f'{PATH_MAIN}/train_images'\nPATH_FRACTURE = f'{PATH_MAIN}/train.csv'\nPATH_BOXES = f'{PATH_MAIN}/train_bounding_boxes.csv'\nPATH_SEGMENTATION = '../input/rsna-2022-spine-fracture-detection-metadata/train_segmented.csv'  # to be changed\n\nIMAGE_SHAPE = (512, 512)\nNUM_VERTEBRAE = 7\nNUM_FOLDS = 5\nSEED = 0\nBATCH_SIZE = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use PyTorch CUDA for computation\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1. Load Data","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n    '''\n    Load image data from given path. Return the image pixel array and the file metadata.\n    \n    Parameters:\n        image_path: str\n            Path to the dicom file of the target image.\n    \n    Return:\n        image: numpy.ndarray\n            The image pixel array with shape (H, W, C).\n            Pixel values range from 0 to 1.\n        metadata: pydicom.dataset.FileDataset\n            The metadata of the dicom file.\n    '''\n    # load metadata and alter photometric interpretation of image pixels\n    metadata = pydicom.dcmread(image_path)\n    # extract image pixel array and rescale to [0, 255]\n    image = apply_voi_lut(metadata.pixel_array, metadata)\n    if image.shape != IMAGE_SHAPE:\n        image = cv2.resize(image, dsize=IMAGE_SHAPE, interpolation=cv2.INTER_CUBIC)\n    image = (image - np.min(image)) / (np.max(image) - np.min(image))\n    image = np.stack([image] * 3, axis=-1)\n    return image, metadata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load an image\nimage, metadata = load_image(f'{PATH_IMAGES}/1.2.826.0.1.3680043.17625/150.dcm')\nprint(f'image shape: {image.shape}')\nprint(f'pixel range: [{np.min(image)}, {np.max(image)}]')\nplt.imshow(image, cmap='gray')\n\n# x, y, w, h = 214.0,113.0,150.0,136.0\n# rect = patches.Rectangle(\n#     (x, y), w, h, \n#     linewidth=1, edgecolor='r', facecolor='none'\n# )\n# plt.gca().add_patch(rect)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_boxes = pd.read_csv(PATH_BOXES)\ndf_boxes.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset into folds by Group K-Fold mechanism\n# ensure that the slices from the same patient do not appear in training and test set simultaneously\nnp.random.seed(SEED)\ngroup_kfold = GroupKFold(NUM_FOLDS)\nfolds = group_kfold.split(df_boxes, groups=df_boxes.StudyInstanceUID)\nfor fold, (_, test_indices) in enumerate(folds):\n    df_boxes.loc[test_indices, 'Fold'] = fold\ndf_boxes.iloc[:,-1] = df_boxes.iloc[:,-1].astype(np.uint8)\nprint(f'fold indices: {sorted(df_boxes.Fold.unique())}')\ndf_boxes.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(torch.utils.data.Dataset):\n    '''\n    An image dataset to extract slice images and their bounding boxes.\n    '''\n    def __init__(self, df, image_dir):\n        '''\n        Initialize the image dataset.\n        \n        Parameters:\n            df: pandas.core.frame.DataFrame\n                The dataframe containing segmentation information of each slice.\n            image_dir: str\n                The path to the image directory.\n        '''\n        super().__init__()\n        self.df = df\n        self.image_dir = image_dir\n    \n    def __len__(self):\n        '''\n        Length of the dataset\n        \n        Return:\n            length: int\n                Total number of slices in the dataset.\n        '''\n        length = len(self.df)\n        return length\n    \n    def __getitem__(self, idx):\n        '''\n        Retrieve the idx-th slice of the dataset and its corresponding labels.\n        \n        Parameters:\n            idx: int\n                The index of the slice in the dataset to retrieve.\n        \n        Return:\n            ...\n        '''\n        # get path to the slice image\n        patient_uid = self.df.iloc[idx].StudyInstanceUID\n        slice_number = self.df.iloc[idx].slice_number\n        image_path = os.path.join(self.image_dir, patient_uid, f'{slice_number}.dcm')\n        # load the slice image\n        image, _ = load_image(image_path)  # shape (H, W, C)\n        image = np.transpose(image, (2, 0, 1))  # shape (C, H, W)\n        image = torch.as_tensor(image).type(torch.FloatTensor)\n        \n        # get bounding box position\n        x, y = self.df.iloc[idx].x, self.df.iloc[idx].y \n        w, h = self.df.iloc[idx].width, self.df.iloc[idx].height\n        if patient_uid == '1.2.826.0.1.3680043.22678':\n            # image 22678 has shape 768x768 (need to resize the bounding box)\n            ratio = 512 / 768\n            x *= ratio\n            y *= ratio\n            w *= ratio\n            h *= ratio\n        boxes = [[x, y, x+w, y+h]]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # object labels\n        labels = torch.ones((1,), dtype=torch.int64)\n        \n        # image unique id\n        image_id = torch.tensor([idx])\n        \n        # bounding box area\n        area = torch.tensor([w * h], dtype=torch.float32)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((1,), dtype=torch.int64)\n        \n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n\n        return image, target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Model Construction","metadata":{}},{"cell_type":"code","source":"ds_train = ImageDataset(df_boxes[df_boxes.Fold != 0], PATH_IMAGES)\nds_test = ImageDataset(df_boxes[df_boxes.Fold == 0], PATH_IMAGES)\ncollate_fn = lambda batch: tuple(zip(*batch))\n\ndl_train = torch.utils.data.DataLoader(\n    ds_train,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=os.cpu_count(),\n    collate_fn=collate_fn\n)\n\ndl_test = torch.utils.data.DataLoader(\n    ds_test,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=os.cpu_count(),\n    collate_fn=collate_fn\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_iou(true, pred):\n    '''\n    Compute the Intersection over Union (IoU).\n\n    Parameters:\n        true: numpy.ndarray\n            Coordinates of the ground-truth box in [xmin, ymin, xmax, ymax] format.\n        pred: numpy.ndarray\n            Coordinates of the predicted box in [xmin, ymin, xmax, ymax] format.\n            \n    Return:\n        iou: float\n            Intersection over Union.\n    '''\n    # Calculate overlap area\n    dx = min(true[2], pred[2]) - max(true[0], pred[0]) + 1\n    if dx < 0:\n        return 0.0\n    dy = min(true[3], pred[3]) - max(true[1], pred[1]) + 1\n    if dy < 0:\n        return 0.0\n\n    intersection = dx * dy\n    # Calculate union area\n    union = (\n        (true[2] - true[0] + 1) * (true[3] - true[1] + 1) +\n        (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1) -\n        intersection\n    )\n    iou = intersection / union\n    return iou\n\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None):\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = compute_iou(gts[gt_idx], pred)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None):\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    # for pred_idx, pred in enumerate(preds_sorted):\n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fn = (gts.sum(axis=1) > 0).sum()\n\n    return tp / (tp + fp + fn)\n\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco'):\n    \"\"\"Calculates image precision.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n\n    for threshold in thresholds:\n        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n                                                     form=form, ious=ious)\n        image_precision += precision_at_threshold / n_threshold\n\n    return image_precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_wbf(predictions, image_index, image_size=512, iou_thr=0.55, skip_box_thr=0.5, weights=None):\n    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tv.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel = model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_hist = Averager()\nbest_val = None\npatience = 2 # early stop patience\n\nfor epoch in range(num_epochs):\n    start_time = time.time() # start timer\n    loss_hist.reset() # init averager\n    model.train() # train mode\n    for itr, (images, targets) in enumerate(tqdm(dl_train)):\n        images = [image.to(device) for image in images]\n        targets = [\n            {\n                k: v.to(device) if k == 'labels' else v.double().to(device) \n                for k, v in t.items()\n            } for t in targets\n        ]\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n#         if (itr+1) % 50 == 0:\n#             print(f\"Iteration #{itr+1} loss: {loss_value}\")\n            \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    # At every epoch we will also calculate the validation IOU\n    validation_image_precisions = []\n    iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n    model.eval()\n    \n    for images, targets in tqdm(dl_test):\n        images = [image.to(device) for image in images]\n        targets = [\n            {\n                k: v.to(device) if k == 'labels' else v.double().to(device) \n                for k, v in t.items()\n            } for t in targets\n        ]\n        \n        #outputs = model(images)\n        predictions = [model(images)]\n        \n        for i, image in enumerate(images):\n            boxes, scores, labels = run_wbf(predictions, image_index=i)\n            boxes = boxes.astype(np.int32).clip(min=0, max=511)\n            \n            preds = boxes\n            #outputs[i]['boxes'].data.cpu().numpy()\n            #scores = outputs[i]['scores'].data.cpu().numpy()\n            preds_sorted_idx = np.argsort(scores)[::-1]\n            preds_sorted = preds[preds_sorted_idx]\n            gt_boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n            image_precision = calculate_image_precision(\n                preds_sorted,\n                gt_boxes,\n                thresholds=iou_thresholds,\n                form='coco'\n            )\n            validation_image_precisions.append(image_precision)\n    \n    val_iou = np.mean(validation_image_precisions)\n    print(\n        f\"Epoch #{epoch+1} loss: {loss_hist.value}\",\n        \"Validation IOU: {0:.4f}\".format(val_iou)\n    )\n    model_path = 'best_model.pth'\n    if not best_val:\n        best_val = val_iou  # So any validation roc_auc we have is the best one for now\n        print(\"Saving model\")\n        torch.save(model, model_path)  # Saving the model\n        #continue\n    if val_iou >= best_val:\n        print(\"Saving model as IOU is increased from\", best_val, \"to\", val_iou)\n        best_val = val_iou\n        patience = 2  # Resetting patience since we have new best validation accuracy\n        torch.save(model, model_path)  # Saving current best model torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')\n    else:\n        patience -= 1\n        if patience == 0:\n            print('Early stopping. Best Validation IOU: {:.3f}'.format(best_val))\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}